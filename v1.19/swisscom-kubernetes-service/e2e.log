I0622 11:57:09.671043      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-240813907
I0622 11:57:09.671065      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0622 11:57:09.671266      20 e2e.go:129] Starting e2e run "f046d819-3099-466f-abb3-9a26bdefcca5" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1624363028 - Will randomize all specs
Will run 305 of 5484 specs

Jun 22 11:57:09.683: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 11:57:09.686: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 11:57:09.708: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 11:57:09.745: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 11:57:09.745: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jun 22 11:57:09.745: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 11:57:09.752: INFO: e2e test version: v1.19.9
Jun 22 11:57:09.754: INFO: kube-apiserver version: v1.19.9+vmware.1
Jun 22 11:57:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 11:57:09.759: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:57:09.760: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename deployment
Jun 22 11:57:09.790: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jun 22 11:57:09.797: INFO: PSP annotation exists on dry run pod: "fluent-bit"; assuming PodSecurityPolicy is enabled
Jun 22 11:57:09.831: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 11:57:09.965: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 11:57:14.970: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 11:57:18.977: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 22 11:57:18.998: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2029 /apis/apps/v1/namespaces/deployment-2029/deployments/test-cleanup-deployment ebafb917-c9cc-4ce4-92c5-ae9aebb2fb6f 145739 1 2021-06-22 11:57:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-06-22 11:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00317c678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 11:57:19.012: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-2029 /apis/apps/v1/namespaces/deployment-2029/replicasets/test-cleanup-deployment-5d446bdd47 77535256-14fd-4855-826c-fc49ba43076d 145741 1 2021-06-22 11:57:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ebafb917-c9cc-4ce4-92c5-ae9aebb2fb6f 0xc00317cca7 0xc00317cca8}] []  [{kube-controller-manager Update apps/v1 2021-06-22 11:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebafb917-c9cc-4ce4-92c5-ae9aebb2fb6f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00317cd38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 11:57:19.012: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 22 11:57:19.012: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2029 /apis/apps/v1/namespaces/deployment-2029/replicasets/test-cleanup-controller e40997d0-12d9-41cf-86a5-57bf4083cde9 145740 1 2021-06-22 11:57:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ebafb917-c9cc-4ce4-92c5-ae9aebb2fb6f 0xc00317cb97 0xc00317cb98}] []  [{e2e.test Update apps/v1 2021-06-22 11:57:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 11:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"ebafb917-c9cc-4ce4-92c5-ae9aebb2fb6f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00317cc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 11:57:19.030: INFO: Pod "test-cleanup-controller-gqw78" is available:
&Pod{ObjectMeta:{test-cleanup-controller-gqw78 test-cleanup-controller- deployment-2029 /api/v1/namespaces/deployment-2029/pods/test-cleanup-controller-gqw78 b6ada4b4-a08f-4bbd-a900-294b7199d6c7 145736 0 2021-06-22 11:57:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller e40997d0-12d9-41cf-86a5-57bf4083cde9 0xc00317d267 0xc00317d268}] []  [{kube-controller-manager Update v1 2021-06-22 11:57:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40997d0-12d9-41cf-86a5-57bf4083cde9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 11:57:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.7.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrfs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrfs9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrfs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 11:57:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 11:57:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 11:57:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 11:57:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:11.32.7.2,StartTime:2021-06-22 11:57:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 11:57:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://90db01ac12e75e723e62793546c6042c3795f63dd3449b6180f3455ac3089abc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.7.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 11:57:19.030: INFO: Pod "test-cleanup-deployment-5d446bdd47-rwj6r" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-rwj6r test-cleanup-deployment-5d446bdd47- deployment-2029 /api/v1/namespaces/deployment-2029/pods/test-cleanup-deployment-5d446bdd47-rwj6r ab553707-f952-49b9-b035-ea765d0d393d 145745 0 2021-06-22 11:57:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 77535256-14fd-4855-826c-fc49ba43076d 0xc00317d427 0xc00317d428}] []  [{kube-controller-manager Update v1 2021-06-22 11:57:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77535256-14fd-4855-826c-fc49ba43076d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xrfs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xrfs9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xrfs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 11:57:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:57:19.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2029" for this suite.

• [SLOW TEST:9.309 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":1,"skipped":23,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:57:19.073: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 11:57:19.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa" in namespace "projected-5380" to be "Succeeded or Failed"
Jun 22 11:57:19.229: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.1218ms
Jun 22 11:57:21.233: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019277139s
Jun 22 11:57:23.238: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024184733s
Jun 22 11:57:25.243: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029173711s
Jun 22 11:57:27.248: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034226073s
STEP: Saw pod success
Jun 22 11:57:27.248: INFO: Pod "downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa" satisfied condition "Succeeded or Failed"
Jun 22 11:57:27.251: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa container client-container: <nil>
STEP: delete the pod
Jun 22 11:57:27.290: INFO: Waiting for pod downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa to disappear
Jun 22 11:57:27.293: INFO: Pod downwardapi-volume-7144ab84-be14-4040-931d-ae8022c157aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:57:27.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5380" for this suite.

• [SLOW TEST:8.230 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":31,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:57:27.304: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7059
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7059
I0622 11:57:27.479666      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7059, replica count: 2
I0622 11:57:30.529941      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 11:57:33.530251      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 11:57:36.530500      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:57:36.530: INFO: Creating new exec pod
Jun 22 11:57:41.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 11:57:42.497: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 11:57:42.497: INFO: stdout: ""
Jun 22 11:57:42.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 10.100.205.144 80'
Jun 22 11:57:42.674: INFO: stderr: "+ nc -zv -t -w 2 10.100.205.144 80\nConnection to 10.100.205.144 80 port [tcp/http] succeeded!\n"
Jun 22 11:57:42.674: INFO: stdout: ""
Jun 22 11:57:42.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.5 32213'
Jun 22 11:57:42.854: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.5 32213\nConnection to 11.0.1.5 32213 port [tcp/32213] succeeded!\n"
Jun 22 11:57:42.854: INFO: stdout: ""
Jun 22 11:57:42.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 32213'
Jun 22 11:57:43.041: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 32213\nConnection to 11.0.1.3 32213 port [tcp/32213] succeeded!\n"
Jun 22 11:57:43.041: INFO: stdout: ""
Jun 22 11:57:43.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.5 32213'
Jun 22 11:57:43.212: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.5 32213\nConnection to 11.0.1.5 32213 port [tcp/32213] succeeded!\n"
Jun 22 11:57:43.212: INFO: stdout: ""
Jun 22 11:57:43.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7059 exec execpodg8qs9 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 32213'
Jun 22 11:57:43.388: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 32213\nConnection to 11.0.1.3 32213 port [tcp/32213] succeeded!\n"
Jun 22 11:57:43.388: INFO: stdout: ""
Jun 22 11:57:43.388: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:57:43.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7059" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.119 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":3,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:57:43.424: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 11:57:44.218: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 11:57:46.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959864, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959864, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959864, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959864, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 11:57:49.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:58:01.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2317" for this suite.
STEP: Destroying namespace "webhook-2317-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":4,"skipped":75,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:58:01.419: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Jun 22 11:58:01.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 22 11:58:01.655: INFO: stderr: ""
Jun 22 11:58:01.655: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jun 22 11:58:01.655: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 22 11:58:01.655: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5363" to be "running and ready, or succeeded"
Jun 22 11:58:01.660: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.501598ms
Jun 22 11:58:03.664: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009641654s
Jun 22 11:58:05.668: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012915378s
Jun 22 11:58:07.675: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02009344s
Jun 22 11:58:09.688: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.033584335s
Jun 22 11:58:09.688: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 22 11:58:09.688: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 22 11:58:09.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator'
Jun 22 11:58:09.783: INFO: stderr: ""
Jun 22 11:58:09.783: INFO: stdout: "I0622 11:58:08.301741       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z29 369\nI0622 11:58:08.501810       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/x6xl 599\nI0622 11:58:08.701824       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/gdz 578\nI0622 11:58:08.901833       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/nkhb 444\nI0622 11:58:09.101823       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/5z45 516\nI0622 11:58:09.301840       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/8pdw 393\nI0622 11:58:09.501820       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/7bqc 491\nI0622 11:58:09.701826       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vpf 326\n"
STEP: limiting log lines
Jun 22 11:58:09.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator --tail=1'
Jun 22 11:58:09.900: INFO: stderr: ""
Jun 22 11:58:09.901: INFO: stdout: "I0622 11:58:09.701826       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vpf 326\n"
Jun 22 11:58:09.901: INFO: got output "I0622 11:58:09.701826       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vpf 326\n"
STEP: limiting log bytes
Jun 22 11:58:09.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator --limit-bytes=1'
Jun 22 11:58:09.986: INFO: stderr: ""
Jun 22 11:58:09.986: INFO: stdout: "I"
Jun 22 11:58:09.986: INFO: got output "I"
STEP: exposing timestamps
Jun 22 11:58:09.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 22 11:58:10.071: INFO: stderr: ""
Jun 22 11:58:10.071: INFO: stdout: "2021-06-22T11:58:09.901953964Z I0622 11:58:09.901821       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cj92 316\n"
Jun 22 11:58:10.071: INFO: got output "2021-06-22T11:58:09.901953964Z I0622 11:58:09.901821       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cj92 316\n"
STEP: restricting to a time range
Jun 22 11:58:12.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator --since=1s'
Jun 22 11:58:12.668: INFO: stderr: ""
Jun 22 11:58:12.668: INFO: stdout: "I0622 11:58:11.701825       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/d8gg 234\nI0622 11:58:11.901817       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/dpqt 247\nI0622 11:58:12.101817       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/2z8k 345\nI0622 11:58:12.301810       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/d8x2 526\nI0622 11:58:12.501817       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/q28 283\n"
Jun 22 11:58:12.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 logs logs-generator logs-generator --since=24h'
Jun 22 11:58:12.756: INFO: stderr: ""
Jun 22 11:58:12.756: INFO: stdout: "I0622 11:58:08.301741       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z29 369\nI0622 11:58:08.501810       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/x6xl 599\nI0622 11:58:08.701824       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/gdz 578\nI0622 11:58:08.901833       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/nkhb 444\nI0622 11:58:09.101823       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/5z45 516\nI0622 11:58:09.301840       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/8pdw 393\nI0622 11:58:09.501820       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/7bqc 491\nI0622 11:58:09.701826       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vpf 326\nI0622 11:58:09.901821       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cj92 316\nI0622 11:58:10.101834       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/57j 583\nI0622 11:58:10.301812       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/cl7 559\nI0622 11:58:10.501818       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/d4w4 333\nI0622 11:58:10.701822       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/snb 369\nI0622 11:58:10.901823       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/xr4 225\nI0622 11:58:11.101818       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/7kz 461\nI0622 11:58:11.301820       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/gwp 554\nI0622 11:58:11.501827       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/8md5 200\nI0622 11:58:11.701825       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/d8gg 234\nI0622 11:58:11.901817       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/dpqt 247\nI0622 11:58:12.101817       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/2z8k 345\nI0622 11:58:12.301810       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/d8x2 526\nI0622 11:58:12.501817       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/q28 283\nI0622 11:58:12.701815       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/z5xw 396\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Jun 22 11:58:12.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-5363 delete pod logs-generator'
Jun 22 11:58:21.875: INFO: stderr: ""
Jun 22 11:58:21.875: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:58:21.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5363" for this suite.

• [SLOW TEST:20.472 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":5,"skipped":81,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:58:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 11:58:22.401: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 11:58:24.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959902, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959902, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959902, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759959902, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 11:58:27.423: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 11:58:27.427: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:58:28.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1479" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.795 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":6,"skipped":83,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:58:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-1165
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1165 to expose endpoints map[]
Jun 22 11:58:28.844: INFO: successfully validated that service endpoint-test2 in namespace services-1165 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1165
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1165 to expose endpoints map[pod1:[80]]
Jun 22 11:58:32.878: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Jun 22 11:58:37.884: INFO: successfully validated that service endpoint-test2 in namespace services-1165 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1165
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1165 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 22 11:58:40.916: INFO: successfully validated that service endpoint-test2 in namespace services-1165 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1165
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1165 to expose endpoints map[pod2:[80]]
Jun 22 11:58:40.950: INFO: successfully validated that service endpoint-test2 in namespace services-1165 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1165
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1165 to expose endpoints map[]
Jun 22 11:58:40.980: INFO: successfully validated that service endpoint-test2 in namespace services-1165 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:58:40.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1165" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:12.323 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":7,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:58:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:58:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1627" for this suite.

• [SLOW TEST:16.239 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":8,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:58:57.251: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 22 11:59:01.912: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4636 pod-service-account-d8dac270-db13-4587-bc9f-b12ab7e670a2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 22 11:59:02.081: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4636 pod-service-account-d8dac270-db13-4587-bc9f-b12ab7e670a2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 22 11:59:02.269: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4636 pod-service-account-d8dac270-db13-4587-bc9f-b12ab7e670a2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:59:02.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4636" for this suite.

• [SLOW TEST:5.199 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":9,"skipped":144,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:59:02.451: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 11:59:02.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36" in namespace "downward-api-9436" to be "Succeeded or Failed"
Jun 22 11:59:02.610: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.562805ms
Jun 22 11:59:04.614: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946144s
Jun 22 11:59:06.619: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014536089s
Jun 22 11:59:08.623: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018900727s
Jun 22 11:59:10.627: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023353855s
STEP: Saw pod success
Jun 22 11:59:10.627: INFO: Pod "downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36" satisfied condition "Succeeded or Failed"
Jun 22 11:59:10.631: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36 container client-container: <nil>
STEP: delete the pod
Jun 22 11:59:10.650: INFO: Waiting for pod downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36 to disappear
Jun 22 11:59:10.652: INFO: Pod downwardapi-volume-ba8c520e-9381-452f-8cd4-2f940720de36 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 11:59:10.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9436" for this suite.

• [SLOW TEST:8.212 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":158,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 11:59:10.665: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 22 11:59:10.812: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 12:00:10.851: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:00:10.854: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-2562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 22 12:00:21.028: INFO: found a healthy node: 06998c1b-9fed-44d7-827f-f702404ff383
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:00:35.133: INFO: pods created so far: [1 1 1]
Jun 22 12:00:35.133: INFO: length of pods created so far: 3
Jun 22 12:00:43.143: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:00:50.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2562" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:00:50.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1145" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:99.565 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":11,"skipped":159,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:00:50.231: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9245
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 12:00:50.373: INFO: Waiting up to 5m0s for pod "pod-f139fd4b-8045-447c-b246-d061f24c1959" in namespace "emptydir-9245" to be "Succeeded or Failed"
Jun 22 12:00:50.384: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959": Phase="Pending", Reason="", readiness=false. Elapsed: 11.178593ms
Jun 22 12:00:52.389: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015475957s
Jun 22 12:00:54.393: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020142836s
Jun 22 12:00:56.397: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023738235s
Jun 22 12:00:58.400: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027272226s
STEP: Saw pod success
Jun 22 12:00:58.400: INFO: Pod "pod-f139fd4b-8045-447c-b246-d061f24c1959" satisfied condition "Succeeded or Failed"
Jun 22 12:00:58.403: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod pod-f139fd4b-8045-447c-b246-d061f24c1959 container test-container: <nil>
STEP: delete the pod
Jun 22 12:00:58.432: INFO: Waiting for pod pod-f139fd4b-8045-447c-b246-d061f24c1959 to disappear
Jun 22 12:00:58.437: INFO: Pod pod-f139fd4b-8045-447c-b246-d061f24c1959 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:00:58.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9245" for this suite.

• [SLOW TEST:8.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":180,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:00:58.450: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4886
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-4c82a2b6-c380-49f7-8e9e-59997bffb258
STEP: Creating secret with name s-test-opt-upd-9433059e-0683-463e-a885-3128bcf62d2f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4c82a2b6-c380-49f7-8e9e-59997bffb258
STEP: Updating secret s-test-opt-upd-9433059e-0683-463e-a885-3128bcf62d2f
STEP: Creating secret with name s-test-opt-create-47883c8c-dba3-45d7-9c1b-e1dd73e6eff1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:02:17.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4886" for this suite.

• [SLOW TEST:78.596 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":187,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:02:17.047: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1411
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 22 12:02:17.182: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:02:35.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1411" for this suite.

• [SLOW TEST:18.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":14,"skipped":199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:02:35.089: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5426
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 22 12:02:45.257: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5426 PodName:pod-sharedvolume-8a07aff8-1cab-415e-8ebc-c58310eaf0de ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:02:45.257: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:02:45.349: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:02:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5426" for this suite.

• [SLOW TEST:10.271 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":15,"skipped":234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:02:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-890912b9-8430-4b9f-be37-7c380b5bcd66
STEP: Creating a pod to test consume secrets
Jun 22 12:02:45.509: INFO: Waiting up to 5m0s for pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a" in namespace "secrets-2916" to be "Succeeded or Failed"
Jun 22 12:02:45.513: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346086ms
Jun 22 12:02:47.519: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522732s
Jun 22 12:02:49.523: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013501045s
Jun 22 12:02:51.527: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0180303s
Jun 22 12:02:53.530: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021304112s
Jun 22 12:02:55.535: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025394237s
STEP: Saw pod success
Jun 22 12:02:55.535: INFO: Pod "pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a" satisfied condition "Succeeded or Failed"
Jun 22 12:02:55.538: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:02:55.555: INFO: Waiting for pod pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a to disappear
Jun 22 12:02:55.560: INFO: Pod pod-secrets-d047ef70-7fa0-4a6d-8e69-28e20c36838a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:02:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2916" for this suite.

• [SLOW TEST:10.209 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":302,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:02:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:02:55.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78" in namespace "projected-781" to be "Succeeded or Failed"
Jun 22 12:02:55.726: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.513179ms
Jun 22 12:02:57.730: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012971832s
Jun 22 12:02:59.735: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017367861s
Jun 22 12:03:01.739: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022202211s
Jun 22 12:03:03.743: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025727673s
Jun 22 12:03:05.747: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030217281s
Jun 22 12:03:07.751: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03390152s
Jun 22 12:03:09.755: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037624525s
Jun 22 12:03:11.759: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041916229s
Jun 22 12:03:13.763: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.046169359s
Jun 22 12:03:15.768: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 20.051078696s
Jun 22 12:03:17.773: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 22.055781271s
Jun 22 12:03:19.777: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 24.059588665s
Jun 22 12:03:21.782: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 26.064885871s
Jun 22 12:03:23.786: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Pending", Reason="", readiness=false. Elapsed: 28.069081896s
Jun 22 12:03:25.791: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.07379347s
STEP: Saw pod success
Jun 22 12:03:25.791: INFO: Pod "downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78" satisfied condition "Succeeded or Failed"
Jun 22 12:03:25.794: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78 container client-container: <nil>
STEP: delete the pod
Jun 22 12:03:25.815: INFO: Waiting for pod downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78 to disappear
Jun 22 12:03:25.819: INFO: Pod downwardapi-volume-510f0f32-37b1-42e4-bf90-d3f5e554ae78 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:03:25.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-781" for this suite.

• [SLOW TEST:30.258 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":304,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:03:25.835: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-ea2e1e70-9455-481b-bf62-452ce5625f1d
STEP: Creating a pod to test consume secrets
Jun 22 12:03:25.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64" in namespace "projected-3955" to be "Succeeded or Failed"
Jun 22 12:03:25.990: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257955ms
Jun 22 12:03:27.995: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010110869s
Jun 22 12:03:29.999: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01426216s
Jun 22 12:03:32.003: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018155142s
Jun 22 12:03:34.007: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022320708s
Jun 22 12:03:36.011: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026340559s
STEP: Saw pod success
Jun 22 12:03:36.011: INFO: Pod "pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64" satisfied condition "Succeeded or Failed"
Jun 22 12:03:36.014: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:03:36.033: INFO: Waiting for pod pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64 to disappear
Jun 22 12:03:36.036: INFO: Pod pod-projected-secrets-0bd2807b-3b3a-458e-b36f-dd750d215d64 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:03:36.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3955" for this suite.

• [SLOW TEST:10.211 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":321,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:03:36.046: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:03:36.180: INFO: Creating deployment "webserver-deployment"
Jun 22 12:03:36.184: INFO: Waiting for observed generation 1
Jun 22 12:03:38.192: INFO: Waiting for all required pods to come up
Jun 22 12:03:38.199: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 12:04:06.214: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 22 12:04:06.220: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 22 12:04:06.229: INFO: Updating deployment webserver-deployment
Jun 22 12:04:06.229: INFO: Waiting for observed generation 2
Jun 22 12:04:08.237: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 12:04:08.240: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 12:04:08.243: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 12:04:08.252: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 12:04:08.252: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 12:04:08.255: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 12:04:08.260: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 22 12:04:08.261: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 22 12:04:08.269: INFO: Updating deployment webserver-deployment
Jun 22 12:04:08.269: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 22 12:04:08.290: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 12:04:10.320: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 22 12:04:10.326: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4544 /apis/apps/v1/namespaces/deployment-4544/deployments/webserver-deployment 728215f0-eb09-40b9-a5f8-deb15d17f025 148105 3 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002afff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-22 12:04:08 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-22 12:04:08 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 12:04:10.329: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-4544 /apis/apps/v1/namespaces/deployment-4544/replicasets/webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 148103 3 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 728215f0-eb09-40b9-a5f8-deb15d17f025 0xc002b863e7 0xc002b863e8}] []  [{kube-controller-manager Update apps/v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"728215f0-eb09-40b9-a5f8-deb15d17f025\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b86468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 12:04:10.329: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 22 12:04:10.329: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-4544 /apis/apps/v1/namespaces/deployment-4544/replicasets/webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 148094 3 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 728215f0-eb09-40b9-a5f8-deb15d17f025 0xc002b864c7 0xc002b864c8}] []  [{kube-controller-manager Update apps/v1 2021-06-22 12:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"728215f0-eb09-40b9-a5f8-deb15d17f025\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b86538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 22 12:04:10.342: INFO: Pod "webserver-deployment-795d758f88-2jz4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2jz4v webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-2jz4v 74b33c44-76f9-412a-8a2d-77e2204ce3bc 148067 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b86bf7 0xc002b86bf8}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.342: INFO: Pod "webserver-deployment-795d758f88-48hdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-48hdd webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-48hdd 039e21d8-c36d-4462-9ad0-7359cf9ae2ab 148114 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b86e10 0xc002b86e11}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.343: INFO: Pod "webserver-deployment-795d758f88-6jwv4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6jwv4 webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-6jwv4 7b85db9d-750f-4383-8199-20c7fb332959 148091 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87000 0xc002b87001}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.344: INFO: Pod "webserver-deployment-795d758f88-84jt5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-84jt5 webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-84jt5 7d2283bb-9ec7-4ba0-8e27-23477002e9cd 148002 0 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b871a0 0xc002b871a1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.344: INFO: Pod "webserver-deployment-795d758f88-88f5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-88f5f webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-88f5f e0a4dcdf-122c-47c5-a605-8dc58a61059e 147979 0 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b873b0 0xc002b873b1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.345: INFO: Pod "webserver-deployment-795d758f88-cf9db" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cf9db webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-cf9db 419f25fb-7117-474c-869f-687ca4533813 148001 0 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87560 0xc002b87561}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:,StartTime:2021-06-22 12:04:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.345: INFO: Pod "webserver-deployment-795d758f88-d5gpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d5gpw webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-d5gpw cf8ff6f1-5fe3-4302-8812-95f0200f8cf6 148096 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87740 0xc002b87741}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.346: INFO: Pod "webserver-deployment-795d758f88-l57hj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l57hj webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-l57hj 5d0f187a-5f7f-4d70-a106-8762bc718b0e 148101 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87970 0xc002b87971}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.346: INFO: Pod "webserver-deployment-795d758f88-lcdx7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lcdx7 webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-lcdx7 27228e00-40b4-4966-a63d-138f97c67398 148090 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87b20 0xc002b87b21}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.346: INFO: Pod "webserver-deployment-795d758f88-pjt6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pjt6b webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-pjt6b 1d9dfaad-68d9-4f6a-b08d-b9b8891565cc 147984 0 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87d60 0xc002b87d61}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:,StartTime:2021-06-22 12:04:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.347: INFO: Pod "webserver-deployment-795d758f88-qxqql" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qxqql webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-qxqql e0feee1d-e9c3-4603-8ca3-33e506516090 147971 0 2021-06-22 12:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002b87f70 0xc002b87f71}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.347: INFO: Pod "webserver-deployment-795d758f88-wwsrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wwsrf webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-wwsrf 6a0dfb58-340a-4196-8463-ba1311981a12 148108 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002bd2100 0xc002bd2101}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.347: INFO: Pod "webserver-deployment-795d758f88-zlpvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zlpvh webserver-deployment-795d758f88- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-795d758f88-zlpvh b5ef7274-7b7a-42d9-bc07-43b014abf076 148055 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 48416d1a-9257-4a71-b49b-db43bedf7220 0xc002bd22d0 0xc002bd22d1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48416d1a-9257-4a71-b49b-db43bedf7220\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.348: INFO: Pod "webserver-deployment-dd94f59b7-2rjxz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2rjxz webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-2rjxz cd740cf6-70a7-4502-991b-d4806dcec5a5 147857 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd2480 0xc002bd2481}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:11.32.8.3,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://6c089d2306ebe8f7d064d47349eee67bd851b4361f90e14c92ec86c1be3e0c65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.348: INFO: Pod "webserver-deployment-dd94f59b7-2vbht" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2vbht webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-2vbht 52cd65b8-5ea0-4940-ba5a-76cdc50419a3 148099 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd2640 0xc002bd2641}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.348: INFO: Pod "webserver-deployment-dd94f59b7-4wz8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4wz8r webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-4wz8r 9c7e9d3e-82bf-49ad-81ed-0c869ea27cbb 148026 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd27d0 0xc002bd27d1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.349: INFO: Pod "webserver-deployment-dd94f59b7-57v2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-57v2p webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-57v2p 9d5979fc-c75a-4167-a51f-24d6377d7b99 148095 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd29a0 0xc002bd29a1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.349: INFO: Pod "webserver-deployment-dd94f59b7-7d84v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7d84v webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-7d84v d6524b3f-2d1e-43fb-a029-c42fb550576c 148104 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd2b10 0xc002bd2b11}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.349: INFO: Pod "webserver-deployment-dd94f59b7-gghrl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gghrl webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-gghrl 713ed3c3-9362-4539-ae57-c2313f43c200 147863 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd2d00 0xc002bd2d01}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:11.32.8.4,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://2b51296194f4bec7e3390dcb5bb9ad66c08139f471cf90cee13de5e1a1cd7273,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.349: INFO: Pod "webserver-deployment-dd94f59b7-h8vsb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h8vsb webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-h8vsb ed771db5-8458-4950-b5a2-aa0e75ce9bb5 147906 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd2ee0 0xc002bd2ee1}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:11.32.8.9,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://29c935bb84924e512919d3926f0ca073983e1f655d22fe79d1bc8e2f998e85b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.350: INFO: Pod "webserver-deployment-dd94f59b7-hpl8g" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hpl8g webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-hpl8g b6700f9d-f675-4f47-bbaa-5802310fd577 148069 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd30d0 0xc002bd30d1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:54b7d611-8263-481b-bda6-56b40bff2a2f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.5,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.350: INFO: Pod "webserver-deployment-dd94f59b7-k4txx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-k4txx webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-k4txx c737690f-9017-4376-93ea-741edf2051f7 147845 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3290 0xc002bd3291}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:11.32.8.5,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://defa2cec46bb085d1236ab037294ae8872fbb34702a143c6848521a64d37f6c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.350: INFO: Pod "webserver-deployment-dd94f59b7-kvl46" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kvl46 webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-kvl46 1037a421-39ea-4465-aa1a-07934bfa9b61 148116 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3470 0xc002bd3471}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.351: INFO: Pod "webserver-deployment-dd94f59b7-n8mpd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n8mpd webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-n8mpd 2788b4d3-5ba0-4d92-a6ac-71b12b6788f1 147909 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3640 0xc002bd3641}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:11.32.8.7,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://695ac43b94a48d464d5d0ce742271b9db925ad61d5a69d90aa984c37cea426ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.351: INFO: Pod "webserver-deployment-dd94f59b7-nxx9g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nxx9g webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-nxx9g 92840560-7813-4876-aeb9-b454995c5192 147860 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd37e0 0xc002bd37e1}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:11.32.8.2,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://6edaf6335d35a3b9975797e1d79a980608f8996a13e5f8d64733548316a05a33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.351: INFO: Pod "webserver-deployment-dd94f59b7-rhllp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rhllp webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-rhllp 07254772-dcbd-4944-9a90-389547dd2ebf 148107 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3970 0xc002bd3971}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.351: INFO: Pod "webserver-deployment-dd94f59b7-sf2k9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sf2k9 webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-sf2k9 8ebb17af-7ff3-4fdc-b31c-6ca9f7f0fb12 148128 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3b30 0xc002bd3b31}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.352: INFO: Pod "webserver-deployment-dd94f59b7-t2vfl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t2vfl webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-t2vfl cbd848fb-daf1-46f8-bd40-105cf510c3d7 147854 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3cc0 0xc002bd3cc1}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:11.32.8.6,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://843f0e8427170f4f061bf2bcaa31a59660a6fa6f7ead6c1c16bc3211fcdb59ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.352: INFO: Pod "webserver-deployment-dd94f59b7-tx5fq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tx5fq webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-tx5fq 51f9c632-2dbc-4866-bfe0-df6373dea351 148100 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002bd3ed0 0xc002bd3ed1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.352: INFO: Pod "webserver-deployment-dd94f59b7-v2ldw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v2ldw webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-v2ldw 296401e7-8841-4aec-b746-b06008c7361f 148092 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002c14060 0xc002c14061}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.353: INFO: Pod "webserver-deployment-dd94f59b7-vgww2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vgww2 webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-vgww2 60ff8541-5d5c-427a-b417-30d151bbd7ea 148062 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002c14280 0xc002c14281}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.353: INFO: Pod "webserver-deployment-dd94f59b7-wbgn2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wbgn2 webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-wbgn2 c2703605-c5a1-4f25-be04-f1320f310e69 147899 0 2021-06-22 12:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002c14470 0xc002c14471}] []  [{kube-controller-manager Update v1 2021-06-22 12:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:03:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.8.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:1d96d19c-1b78-44d8-b822-ba104bc5daa5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.3,PodIP:11.32.8.8,StartTime:2021-06-22 12:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:03:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://571f0b05ac6242a83818042b7461bdb669dcd904516baa3d1e04e7e82d7f3351,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.8.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 12:04:10.353: INFO: Pod "webserver-deployment-dd94f59b7-zg9jh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zg9jh webserver-deployment-dd94f59b7- deployment-4544 /api/v1/namespaces/deployment-4544/pods/webserver-deployment-dd94f59b7-zg9jh 826bdc71-5a13-4709-9b22-3ba74b455ffb 148082 0 2021-06-22 12:04:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 7bd4fe52-a4dd-4979-ae30-d2f587a2b283 0xc002c146a0 0xc002c146a1}] []  [{kube-controller-manager Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd4fe52-a4dd-4979-ae30-d2f587a2b283\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d7b5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d7b5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d7b5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:2352dbd9-b599-409b-9a0b-5bade7a216ea,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:04:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.7,PodIP:,StartTime:2021-06-22 12:04:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:04:10.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4544" for this suite.

• [SLOW TEST:34.318 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":19,"skipped":329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:04:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 12:04:10.512: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 12:04:10.524: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 12:04:10.532: INFO: 
Logging pods the apiserver thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383 before test
Jun 22 12:04:10.549: INFO: webserver-deployment-795d758f88-48hdd from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-795d758f88-84jt5 from deployment-4544 started at 2021-06-22 12:04:06 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-795d758f88-d5gpw from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-795d758f88-qxqql from deployment-4544 started at 2021-06-22 12:04:06 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-4wz8r from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-nxx9g from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-rhllp from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-sf2k9 from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-v2ldw from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: webserver-deployment-dd94f59b7-vgww2 from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.549: INFO: fluent-bit-5z5qp from pks-system started at 2021-06-21 23:48:42 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:04:10.549: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:04:10.549: INFO: node-exporter-nrj7p from pks-system started at 2021-06-21 23:32:52 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Jun 22 12:04:10.549: INFO: telegraf-kp96k from pks-system started at 2021-06-21 23:48:42 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:04:10.549: INFO: sonobuoy from sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 12:04:10.549: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:04:10.549: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:04:10.549: INFO: 
Logging pods the apiserver thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 before test
Jun 22 12:04:10.569: INFO: webserver-deployment-795d758f88-2jz4v from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-795d758f88-lcdx7 from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-795d758f88-pjt6b from deployment-4544 started at 2021-06-22 12:04:06 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-dd94f59b7-7d84v from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-dd94f59b7-k4txx from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-dd94f59b7-tx5fq from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.569: INFO: webserver-deployment-dd94f59b7-wbgn2 from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.569: INFO: coredns-645ccbcd68-7dlmn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:04:10.569: INFO: metrics-server-7d476fdfbd-md64k from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 12:04:10.569: INFO: fluent-bit-8rhpl from pks-system started at 2021-06-21 23:47:44 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:04:10.569: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:04:10.569: INFO: metric-controller-7f5cb8ff6d-7npkg from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 12:04:10.569: INFO: node-exporter-w6bbv from pks-system started at 2021-06-21 23:42:34 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:04:10.569: INFO: observability-manager-6cf797f97-8nqdm from pks-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 12:04:10.569: INFO: sink-controller-f6bc7f774-zprjs from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 12:04:10.569: INFO: telegraf-j56d5 from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.569: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:04:10.570: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:04:10.570: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:04:10.570: INFO: 
Logging pods the apiserver thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea before test
Jun 22 12:04:10.587: INFO: webserver-deployment-795d758f88-88f5f from deployment-4544 started at 2021-06-22 12:04:06 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.587: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.587: INFO: webserver-deployment-795d758f88-l57hj from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.587: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.587: INFO: webserver-deployment-795d758f88-wwsrf from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.587: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.587: INFO: webserver-deployment-795d758f88-zlpvh from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.587: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-2rjxz from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-57v2p from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-kvl46 from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-n8mpd from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-t2vfl from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.588: INFO: webserver-deployment-dd94f59b7-zg9jh from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.588: INFO: fluent-bit-vtnl7 from pks-system started at 2021-06-21 23:52:24 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:04:10.588: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:04:10.588: INFO: node-exporter-82sxc from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:04:10.588: INFO: telegraf-dwvlw from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:04:10.588: INFO: telemetry-agent-8444c9c47b-7xllf from pks-system started at 2021-06-21 23:55:50 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.588: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 12:04:10.588: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.589: INFO: 	Container e2e ready: true, restart count 0
Jun 22 12:04:10.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:04:10.589: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:04:10.589: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:04:10.589: INFO: 
Logging pods the apiserver thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f before test
Jun 22 12:04:10.604: INFO: webserver-deployment-795d758f88-6jwv4 from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.605: INFO: webserver-deployment-795d758f88-cf9db from deployment-4544 started at 2021-06-22 12:04:06 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.605: INFO: webserver-deployment-dd94f59b7-2vbht from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.605: INFO: webserver-deployment-dd94f59b7-gghrl from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.605: INFO: webserver-deployment-dd94f59b7-h8vsb from deployment-4544 started at 2021-06-22 12:03:36 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: true, restart count 0
Jun 22 12:04:10.605: INFO: webserver-deployment-dd94f59b7-hpl8g from deployment-4544 started at 2021-06-22 12:04:08 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container httpd ready: false, restart count 0
Jun 22 12:04:10.605: INFO: coredns-645ccbcd68-pbmnn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:04:10.605: INFO: coredns-645ccbcd68-s8fjw from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:04:10.605: INFO: event-controller-85d6bb4d4c-fz5nk from pks-system started at 2021-06-21 23:47:39 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 12:04:10.605: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:04:10.605: INFO: fluent-bit-f425q from pks-system started at 2021-06-21 23:48:02 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:04:10.605: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:04:10.605: INFO: node-exporter-8qrc5 from pks-system started at 2021-06-21 23:46:22 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:04:10.605: INFO: telegraf-9z5xm from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:04:10.605: INFO: validator-69f557d7c6-bl8gf from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 12:04:10.605: INFO: 	Container validator ready: true, restart count 0
Jun 22 12:04:10.605: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 12:04:10.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:04:10.606: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5a558ca0-b981-4605-9b2e-4655239f5523 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-5a558ca0-b981-4605-9b2e-4655239f5523 off the node 06998c1b-9fed-44d7-827f-f702404ff383
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5a558ca0-b981-4605-9b2e-4655239f5523
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:24.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5487" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:314.352 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":20,"skipped":358,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:24.719: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 12:09:24.866: INFO: Waiting up to 5m0s for pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c" in namespace "emptydir-6167" to be "Succeeded or Failed"
Jun 22 12:09:24.874: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461673ms
Jun 22 12:09:26.879: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013143181s
Jun 22 12:09:28.883: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017634135s
Jun 22 12:09:30.887: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021506852s
Jun 22 12:09:32.892: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025974563s
STEP: Saw pod success
Jun 22 12:09:32.892: INFO: Pod "pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c" satisfied condition "Succeeded or Failed"
Jun 22 12:09:32.895: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c container test-container: <nil>
STEP: delete the pod
Jun 22 12:09:32.925: INFO: Waiting for pod pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c to disappear
Jun 22 12:09:32.929: INFO: Pod pod-f2cbcf4c-5a88-4497-8bb2-b78dffd4496c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:32.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6167" for this suite.

• [SLOW TEST:8.222 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":372,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:32.943: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 22 12:09:41.617: INFO: Successfully updated pod "annotationupdate8c55339e-e5b5-4b40-a8bf-e74fbaaf6d45"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:45.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1251" for this suite.

• [SLOW TEST:12.711 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":383,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-cf5d5658-9fac-42aa-8040-960cc756b89c
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:45.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2934" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":23,"skipped":389,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:45.805: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6909
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 22 12:09:45.951: INFO: Waiting up to 5m0s for pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2" in namespace "emptydir-6909" to be "Succeeded or Failed"
Jun 22 12:09:45.961: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.644841ms
Jun 22 12:09:47.965: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014172139s
Jun 22 12:09:49.970: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019058253s
Jun 22 12:09:51.975: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023675043s
Jun 22 12:09:53.979: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027612006s
STEP: Saw pod success
Jun 22 12:09:53.979: INFO: Pod "pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2" satisfied condition "Succeeded or Failed"
Jun 22 12:09:53.981: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2 container test-container: <nil>
STEP: delete the pod
Jun 22 12:09:54.003: INFO: Waiting for pod pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2 to disappear
Jun 22 12:09:54.007: INFO: Pod pod-1c089500-f5dc-4e0d-b2ea-0ee2d266f0c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:54.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6909" for this suite.

• [SLOW TEST:8.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":389,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:54.022: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 22 12:09:54.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4606 create -f -'
Jun 22 12:09:55.185: INFO: stderr: ""
Jun 22 12:09:55.185: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 22 12:09:56.189: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:09:56.189: INFO: Found 0 / 1
Jun 22 12:09:57.190: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:09:57.190: INFO: Found 0 / 1
Jun 22 12:09:58.190: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:09:58.190: INFO: Found 1 / 1
Jun 22 12:09:58.190: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 22 12:09:58.193: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:09:58.193: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 12:09:58.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4606 patch pod agnhost-primary-g9p2l -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 12:09:58.278: INFO: stderr: ""
Jun 22 12:09:58.278: INFO: stdout: "pod/agnhost-primary-g9p2l patched\n"
STEP: checking annotations
Jun 22 12:09:58.281: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:09:58.281: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:09:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4606" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":25,"skipped":398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:09:58.292: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7631
STEP: creating service affinity-clusterip-transition in namespace services-7631
STEP: creating replication controller affinity-clusterip-transition in namespace services-7631
I0622 12:09:58.443571      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7631, replica count: 3
I0622 12:10:01.493878      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:10:04.494085      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:10:07.494290      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:10:10.494609      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 12:10:10.501: INFO: Creating new exec pod
Jun 22 12:10:15.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun 22 12:10:15.709: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 22 12:10:15.709: INFO: stdout: ""
Jun 22 12:10:15.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c nc -zv -t -w 2 10.100.194.164 80'
Jun 22 12:10:15.892: INFO: stderr: "+ nc -zv -t -w 2 10.100.194.164 80\nConnection to 10.100.194.164 80 port [tcp/http] succeeded!\n"
Jun 22 12:10:15.892: INFO: stdout: ""
Jun 22 12:10:15.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:10:16.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:10:16.209: INFO: stdout: "\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6"
Jun 22 12:10:16.209: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.209: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.209: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.209: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.209: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.210: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:16.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:10:16.486: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:10:16.486: INFO: stdout: "\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6"
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:16.486: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:46.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:10:46.757: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:10:46.757: INFO: stdout: "\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd"
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:10:46.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:11:16.757: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:11:16.757: INFO: stdout: "\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd"
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:16.757: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:46.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:11:46.748: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:11:46.748: INFO: stdout: "\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6"
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:11:46.748: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:12:16.778: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:12:16.778: INFO: stdout: "\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-n2nwb"
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:16.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7631 exec execpod-affinitysgzl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.164:80/ ; done'
Jun 22 12:12:17.050: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.164:80/\n"
Jun 22 12:12:17.050: INFO: stdout: "\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-hl5dd\naffinity-clusterip-transition-n2nwb\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6\naffinity-clusterip-transition-ztmb6"
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-hl5dd
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-n2nwb
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: Received response from host: affinity-clusterip-transition-ztmb6
Jun 22 12:12:17.050: INFO: [affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-n2nwb affinity-clusterip-transition-hl5dd affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-hl5dd affinity-clusterip-transition-n2nwb affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6 affinity-clusterip-transition-ztmb6]
Jun 22 12:12:17.050: FAIL: Affinity should hold but didn't.

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.checkAffinityFailed(0xc0031e3800, 0x60, 0x80, 0x4c74f7b, 0x20)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241 +0xdf
k8s.io/kubernetes/test/e2e/network.checkAffinity(0x540f680, 0xc001755600, 0xc001952400, 0xc0032850d0, 0xe, 0x50, 0x1, 0xc001952401)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:200 +0x225
k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithOptionalTransition(0xc000a42b00, 0x540f680, 0xc001755600, 0xc000511200, 0x1)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3526 +0x88c
k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithTransition(...)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3466
k8s.io/kubernetes/test/e2e/network.glob..func24.27()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2493 +0xa5
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc002309e00, 0x4debd60)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
Jun 22 12:12:17.051: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7631, will wait for the garbage collector to delete the pods
Jun 22 12:12:17.123: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.433306ms
Jun 22 12:12:17.723: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.12142ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "services-7631".
STEP: Found 23 events.
Jun 22 12:12:22.945: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-transition-hl5dd: { } Scheduled: Successfully assigned services-7631/affinity-clusterip-transition-hl5dd to 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:12:22.945: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-transition-n2nwb: { } Scheduled: Successfully assigned services-7631/affinity-clusterip-transition-n2nwb to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:22.945: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-transition-ztmb6: { } Scheduled: Successfully assigned services-7631/affinity-clusterip-transition-ztmb6 to 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:12:22.945: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for execpod-affinitysgzl2: { } Scheduled: Successfully assigned services-7631/execpod-affinitysgzl2 to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:09:58 +0000 UTC - event for affinity-clusterip-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-transition-ztmb6
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:09:58 +0000 UTC - event for affinity-clusterip-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-transition-hl5dd
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:09:58 +0000 UTC - event for affinity-clusterip-transition: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-transition-n2nwb
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-n2nwb: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-n2nwb: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-n2nwb: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-ztmb6: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Created: Created container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-ztmb6: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Started: Started container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:06 +0000 UTC - event for affinity-clusterip-transition-ztmb6: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:07 +0000 UTC - event for affinity-clusterip-transition-hl5dd: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Started: Started container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:07 +0000 UTC - event for affinity-clusterip-transition-hl5dd: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Created: Created container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:07 +0000 UTC - event for affinity-clusterip-transition-hl5dd: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:11 +0000 UTC - event for execpod-affinitysgzl2: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:11 +0000 UTC - event for execpod-affinitysgzl2: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container agnhost-container
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:10:11 +0000 UTC - event for execpod-affinitysgzl2: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container agnhost-container
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:12:17 +0000 UTC - event for affinity-clusterip-transition-hl5dd: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Killing: Stopping container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:12:17 +0000 UTC - event for affinity-clusterip-transition-n2nwb: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:12:17 +0000 UTC - event for affinity-clusterip-transition-ztmb6: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Killing: Stopping container affinity-clusterip-transition
Jun 22 12:12:22.945: INFO: At 2021-06-22 12:12:17 +0000 UTC - event for execpod-affinitysgzl2: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container agnhost-container
Jun 22 12:12:22.947: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Jun 22 12:12:22.947: INFO: 
Jun 22 12:12:22.955: INFO: 
Logging node info for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:22.958: INFO: Node Info: &Node{ObjectMeta:{06998c1b-9fed-44d7-827f-f702404ff383   /api/v1/nodes/06998c1b-9fed-44d7-827f-f702404ff383 d6e824ee-0f24-4194-ba8c-f2897d500b67 149682 0 2021-06-21 23:48:31 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:8b60bac6-7336-4a89-974a-c41f2875f963 bosh.zone:WL-OLT-Dev-02 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-02 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.4 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.4] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:00:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {e2e.test Update v1 2021-06-22 12:04:20 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e034f-72fb-1ea5-cc8c-80ef6bea7bb1,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:24 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:24 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:24 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:10:24 +0000 UTC,LastTransitionTime:2021-06-21 23:48:41 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.4,},NodeAddress{Type:InternalIP,Address:11.0.1.4,},NodeAddress{Type:Hostname,Address:11.0.1.4,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:c5dcfa0ab1ae479d98fa8e37e017524d,SystemUUID:420E034F-72FB-1EA5-CC8C-80EF6BEA7BB1,BootID:0d938454-6d4a-4970-b6fd-51fd6a99bee7,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:12:22.959: INFO: 
Logging kubelet events for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:22.961: INFO: 
Logging pods the kubelet thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:22.975: INFO: telegraf-kp96k started at 2021-06-21 23:48:42 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:22.975: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:12:22.975: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:12:22.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:12:22.975: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:12:22.975: INFO: node-exporter-nrj7p started at 2021-06-21 23:32:52 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:22.975: INFO: 	Container prometheus-node-exporter ready: true, restart count 1
Jun 22 12:12:22.975: INFO: fluent-bit-5z5qp started at 2021-06-21 23:48:42 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:12:22.975: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:12:22.975: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:12:22.975: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:12:22.975: INFO: sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:22.975: INFO: 	Container kube-sonobuoy ready: true, restart count 0
W0622 12:12:22.979741      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:12:22.979860      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:12:22.979913      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:12:23.005: INFO: 
Latency metrics for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:12:23.005: INFO: 
Logging node info for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:12:23.009: INFO: Node Info: &Node{ObjectMeta:{1d96d19c-1b78-44d8-b822-ba104bc5daa5   /api/v1/nodes/1d96d19c-1b78-44d8-b822-ba104bc5daa5 3382db90-e9d9-4d99-92c9-eed5527dcd67 149665 0 2021-06-21 23:42:33 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:38c4eb74-265f-4dd6-a9f0-1bac74785fba bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.3 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.3] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-21 23:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420eaaab-5adc-eb3a-9a4f-3b73f38acf24,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364527616 0} {<nil>} 8168484Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259670016 0} {<nil>} 8066084Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:19 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:19 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:10:19 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:10:19 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.3,},NodeAddress{Type:InternalIP,Address:11.0.1.3,},NodeAddress{Type:Hostname,Address:11.0.1.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:fd3f65ecda8a4cb3bd31bfb34c08ea2c,SystemUUID:420EAAAB-5ADC-EB3A-9A4F-3B73F38ACF24,BootID:8812dfc4-73c0-4087-8280-61b5eb4cbf39,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:12:23.009: INFO: 
Logging kubelet events for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:12:23.012: INFO: 
Logging pods the kubelet thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:12:23.033: INFO: metrics-server-7d476fdfbd-md64k started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.033: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 12:12:23.033: INFO: coredns-645ccbcd68-7dlmn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:12:23.034: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:12:23.034: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:12:23.034: INFO: fluent-bit-8rhpl started at 2021-06-21 23:47:44 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:12:23.034: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:12:23.034: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:12:23.034: INFO: telegraf-j56d5 started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:12:23.034: INFO: node-exporter-w6bbv started at 2021-06-21 23:42:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:12:23.034: INFO: observability-manager-6cf797f97-8nqdm started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 12:12:23.034: INFO: metric-controller-7f5cb8ff6d-7npkg started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 12:12:23.034: INFO: sink-controller-f6bc7f774-zprjs started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.034: INFO: 	Container sink-controller ready: true, restart count 0
W0622 12:12:23.038465      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:12:23.038482      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:12:23.038487      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:12:23.063: INFO: 
Latency metrics for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:12:23.063: INFO: 
Logging node info for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:12:23.067: INFO: Node Info: &Node{ObjectMeta:{2352dbd9-b599-409b-9a0b-5bade7a216ea   /api/v1/nodes/2352dbd9-b599-409b-9a0b-5bade7a216ea 4ffa18f4-da34-4eb7-8052-e83631b76176 149955 0 2021-06-21 23:52:14 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:975f3130-3bff-4253-88c9-75060b9b5a34 bosh.zone:WL-ZHH-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-ZHH-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.7 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.7] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:52:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-21 23:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e7dbb-3f7c-5670-16cd-e87a1cc6ae33,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:12:00 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:12:00 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:12:00 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:12:00 +0000 UTC,LastTransitionTime:2021-06-21 23:52:24 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.7,},NodeAddress{Type:InternalIP,Address:11.0.1.7,},NodeAddress{Type:Hostname,Address:11.0.1.7,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:90e66be501cd48dc83f0fad01e6c038c,SystemUUID:420E7DBB-3F7C-5670-16CD-E87A1CC6AE33,BootID:eaba17bb-8b7e-4b67-ac5d-85bb9e5c744d,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:12:23.067: INFO: 
Logging kubelet events for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:12:23.070: INFO: 
Logging pods the kubelet thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:12:23.098: INFO: node-exporter-82sxc started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.098: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:12:23.098: INFO: telemetry-agent-8444c9c47b-7xllf started at 2021-06-21 23:55:50 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.098: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 12:12:23.098: INFO: telegraf-dwvlw started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.098: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:12:23.098: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:12:23.099: INFO: 	Container e2e ready: true, restart count 0
Jun 22 12:12:23.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:12:23.099: INFO: fluent-bit-vtnl7 started at 2021-06-21 23:52:24 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:12:23.099: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:12:23.099: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:12:23.099: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:12:23.099: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:12:23.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:12:23.099: INFO: 	Container systemd-logs ready: true, restart count 0
W0622 12:12:23.109576      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:12:23.109766      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:12:23.109942      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:12:23.151: INFO: 
Latency metrics for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:12:23.152: INFO: 
Logging node info for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:12:23.155: INFO: Node Info: &Node{ObjectMeta:{54b7d611-8263-481b-bda6-56b40bff2a2f   /api/v1/nodes/54b7d611-8263-481b-bda6-56b40bff2a2f d071e496-861f-42ba-a9f8-529501f196ca 148951 0 2021-06-21 23:46:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:bd75bf6d-fb7a-49e2-a49b-8b225c259383 bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.5 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.5] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:46:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-21 23:47:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e2237-ebbd-7ba1-c525-758ac4c56ba2,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:07:26 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:07:26 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:07:26 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:07:26 +0000 UTC,LastTransitionTime:2021-06-21 23:46:22 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.5,},NodeAddress{Type:InternalIP,Address:11.0.1.5,},NodeAddress{Type:Hostname,Address:11.0.1.5,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:20304c03bce241438791dd1ad3ceb11a,SystemUUID:420E2237-EBBD-7BA1-C525-758AC4C56BA2,BootID:880882cc-1831-4c1a-bc3f-4b0766aa256e,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:12:23.155: INFO: 
Logging kubelet events for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:12:23.159: INFO: 
Logging pods the kubelet thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:12:23.183: INFO: coredns-645ccbcd68-pbmnn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.183: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:12:23.183: INFO: coredns-645ccbcd68-s8fjw started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:12:23.184: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:12:23.184: INFO: telegraf-9z5xm started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:12:23.184: INFO: validator-69f557d7c6-bl8gf started at 2021-06-21 23:47:39 +0000 UTC (1+1 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Init container patch-ca ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container validator ready: true, restart count 0
Jun 22 12:12:23.184: INFO: node-exporter-8qrc5 started at 2021-06-21 23:46:22 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:12:23.184: INFO: event-controller-85d6bb4d4c-fz5nk started at 2021-06-21 23:47:39 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:12:23.184: INFO: fluent-bit-f425q started at 2021-06-21 23:48:02 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:12:23.184: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:12:23.184: INFO: 	Container ghostunnel ready: true, restart count 0
W0622 12:12:23.188923      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:12:23.189094      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:12:23.189201      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:12:23.225: INFO: 
Latency metrics for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:12:23.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7631" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• Failure [144.945 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  Jun 22 12:12:17.050: Affinity should hold but didn't.

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241
------------------------------
{"msg":"FAILED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":25,"skipped":433,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:12:23.238: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:12:23.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7100" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":448,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:12:23.419: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:12:23.588: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 12:12:23.615: INFO: Number of nodes with available pods: 0
Jun 22 12:12:23.615: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:24.627: INFO: Number of nodes with available pods: 0
Jun 22 12:12:24.627: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:25.626: INFO: Number of nodes with available pods: 0
Jun 22 12:12:25.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:26.625: INFO: Number of nodes with available pods: 0
Jun 22 12:12:26.625: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:27.625: INFO: Number of nodes with available pods: 0
Jun 22 12:12:27.625: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:28.624: INFO: Number of nodes with available pods: 0
Jun 22 12:12:28.624: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:29.627: INFO: Number of nodes with available pods: 0
Jun 22 12:12:29.627: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:30.625: INFO: Number of nodes with available pods: 0
Jun 22 12:12:30.625: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:31.626: INFO: Number of nodes with available pods: 0
Jun 22 12:12:31.627: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:32.629: INFO: Number of nodes with available pods: 2
Jun 22 12:12:32.629: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:33.626: INFO: Number of nodes with available pods: 2
Jun 22 12:12:33.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:34.634: INFO: Number of nodes with available pods: 2
Jun 22 12:12:34.634: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:35.626: INFO: Number of nodes with available pods: 2
Jun 22 12:12:35.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:36.627: INFO: Number of nodes with available pods: 2
Jun 22 12:12:36.627: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:37.625: INFO: Number of nodes with available pods: 2
Jun 22 12:12:37.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:38.626: INFO: Number of nodes with available pods: 2
Jun 22 12:12:38.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:39.626: INFO: Number of nodes with available pods: 2
Jun 22 12:12:39.626: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:40.633: INFO: Number of nodes with available pods: 2
Jun 22 12:12:40.633: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:41.625: INFO: Number of nodes with available pods: 2
Jun 22 12:12:41.625: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:12:42.626: INFO: Number of nodes with available pods: 4
Jun 22 12:12:42.626: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 22 12:12:42.667: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:42.667: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:42.667: INFO: Wrong image for pod: daemon-set-p8qtw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:42.667: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:43.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:43.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:43.679: INFO: Wrong image for pod: daemon-set-p8qtw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:43.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:44.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:44.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:44.680: INFO: Wrong image for pod: daemon-set-p8qtw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:44.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:45.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:45.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:45.679: INFO: Wrong image for pod: daemon-set-p8qtw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:45.680: INFO: Pod daemon-set-p8qtw is not available
Jun 22 12:12:45.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:46.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:46.679: INFO: Pod daemon-set-9j22v is not available
Jun 22 12:12:46.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:46.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:47.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:47.679: INFO: Pod daemon-set-9j22v is not available
Jun 22 12:12:47.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:47.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:48.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:48.679: INFO: Pod daemon-set-9j22v is not available
Jun 22 12:12:48.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:48.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:49.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:49.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:49.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:50.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:50.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:50.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:51.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:51.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:51.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:51.680: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:52.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:52.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:52.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:52.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:53.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:53.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:53.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:53.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:54.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:54.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:54.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:54.680: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:55.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:55.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:55.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:55.680: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:56.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:56.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:56.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:56.680: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:57.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:57.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:57.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:57.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:58.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:58.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:58.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:58.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:12:59.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:59.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:59.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:12:59.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:13:00.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:00.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:00.679: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:00.679: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:13:01.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:01.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:01.680: INFO: Wrong image for pod: daemon-set-phtt2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:01.680: INFO: Pod daemon-set-phtt2 is not available
Jun 22 12:13:02.677: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:02.677: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:03.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:03.680: INFO: Pod daemon-set-lw4c2 is not available
Jun 22 12:13:03.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:04.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:04.680: INFO: Pod daemon-set-lw4c2 is not available
Jun 22 12:13:04.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:05.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:05.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:06.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:06.679: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:06.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:07.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:07.679: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:07.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:08.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:08.680: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:08.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:09.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:09.680: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:09.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:10.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:10.680: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:10.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:11.680: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:11.680: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:11.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:12.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:12.679: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:12.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:13.679: INFO: Wrong image for pod: daemon-set-4t5tf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:13.679: INFO: Pod daemon-set-4t5tf is not available
Jun 22 12:13:13.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:14.681: INFO: Pod daemon-set-c8j4b is not available
Jun 22 12:13:14.681: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:15.679: INFO: Pod daemon-set-c8j4b is not available
Jun 22 12:13:15.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:16.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:17.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:17.679: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:18.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:18.679: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:19.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:19.680: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:20.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:20.680: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:21.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:21.680: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:22.679: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:22.679: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:23.680: INFO: Wrong image for pod: daemon-set-m8xq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 12:13:23.680: INFO: Pod daemon-set-m8xq2 is not available
Jun 22 12:13:24.679: INFO: Pod daemon-set-npqgx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 22 12:13:24.692: INFO: Number of nodes with available pods: 3
Jun 22 12:13:24.692: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:13:25.704: INFO: Number of nodes with available pods: 3
Jun 22 12:13:25.704: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:13:26.713: INFO: Number of nodes with available pods: 4
Jun 22 12:13:26.713: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9018, will wait for the garbage collector to delete the pods
Jun 22 12:13:26.787: INFO: Deleting DaemonSet.extensions daemon-set took: 5.266478ms
Jun 22 12:13:27.387: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.179031ms
Jun 22 12:13:32.690: INFO: Number of nodes with available pods: 0
Jun 22 12:13:32.690: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 12:13:32.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9018/daemonsets","resourceVersion":"150445"},"items":null}

Jun 22 12:13:32.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9018/pods","resourceVersion":"150445"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:13:32.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9018" for this suite.

• [SLOW TEST:69.307 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":27,"skipped":449,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:13:32.727: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 22 12:13:32.872: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 12:14:32.909: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 22 12:14:32.941: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 22 12:14:32.969: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 22 12:14:32.986: INFO: Created pod: pod2-sched-preemption-medium-priority
Jun 22 12:14:33.011: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:15:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3644" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:88.503 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":28,"skipped":460,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:15:01.230: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-5566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 22 12:15:01.372: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 22 12:15:01.378: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 22 12:15:01.378: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 22 12:15:01.389: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 22 12:15:01.389: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 22 12:15:01.407: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 22 12:15:01.407: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 22 12:15:08.461: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:15:08.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5566" for this suite.

• [SLOW TEST:7.264 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":29,"skipped":463,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:15:08.495: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-5f7e129f-acb4-4e8e-ba93-f5b18c148bbc
STEP: Creating a pod to test consume secrets
Jun 22 12:15:08.644: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e" in namespace "projected-9269" to be "Succeeded or Failed"
Jun 22 12:15:08.653: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.599717ms
Jun 22 12:15:10.657: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012738369s
Jun 22 12:15:12.662: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017205838s
Jun 22 12:15:14.665: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02051851s
Jun 22 12:15:16.669: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024649581s
Jun 22 12:15:18.674: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029127418s
STEP: Saw pod success
Jun 22 12:15:18.674: INFO: Pod "pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e" satisfied condition "Succeeded or Failed"
Jun 22 12:15:18.677: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:15:18.708: INFO: Waiting for pod pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e to disappear
Jun 22 12:15:18.712: INFO: Pod pod-projected-secrets-01cb9a0c-b04f-4be6-89d4-9901c3aefd6e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:15:18.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9269" for this suite.

• [SLOW TEST:10.230 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":463,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:15:18.726: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5021 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5021;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5021 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5021;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5021.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5021.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5021.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5021.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5021.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5021.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.207.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.207.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.207.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.207.21_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5021 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5021;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5021 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5021;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5021.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5021.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5021.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5021.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5021.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5021.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5021.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5021.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5021.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.207.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.207.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.207.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.207.21_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 12:15:28.933: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.937: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.940: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.943: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.950: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.953: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.956: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.959: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.963: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.966: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.969: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.973: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.976: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.979: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.983: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.986: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.990: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.993: INFO: Unable to read jessie_udp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:28.998: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.002: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.005: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.009: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.013: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.016: INFO: Unable to read jessie_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.019: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.023: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.026: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:29.026: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021 wheezy_udp@dns-test-service.dns-5021.svc wheezy_tcp@dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.test-service-2.dns-5021.svc wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021 jessie_udp@dns-test-service.dns-5021.svc jessie_tcp@dns-test-service.dns-5021.svc jessie_udp@_http._tcp.dns-test-service.dns-5021.svc jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc jessie_udp@_http._tcp.test-service-2.dns-5021.svc jessie_tcp@_http._tcp.test-service-2.dns-5021.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR]

Jun 22 12:15:34.031: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.034: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.041: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.047: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.050: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.054: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.057: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.060: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.063: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.067: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.070: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.073: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.076: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.083: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.096: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.100: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.103: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.106: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.111: INFO: Unable to read jessie_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.114: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.117: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.121: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:34.121: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021 wheezy_udp@dns-test-service.dns-5021.svc wheezy_tcp@dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.test-service-2.dns-5021.svc wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021.svc jessie_udp@_http._tcp.dns-test-service.dns-5021.svc jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc jessie_udp@_http._tcp.test-service-2.dns-5021.svc jessie_tcp@_http._tcp.test-service-2.dns-5021.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR]

Jun 22 12:15:39.031: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.035: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.041: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.054: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.057: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.061: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.064: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.067: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.071: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.074: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.077: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.083: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.097: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.100: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.103: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.107: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.111: INFO: Unable to read jessie_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.114: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.117: INFO: Unable to read 10.100.207.21_udp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.121: INFO: Unable to read 10.100.207.21_tcp@PTR from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:39.121: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021 wheezy_udp@dns-test-service.dns-5021.svc wheezy_tcp@dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.test-service-2.dns-5021.svc wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021.svc jessie_udp@_http._tcp.dns-test-service.dns-5021.svc jessie_tcp@_http._tcp.dns-test-service.dns-5021.svc jessie_udp@_http._tcp.test-service-2.dns-5021.svc jessie_tcp@_http._tcp.test-service-2.dns-5021.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.100.207.21_udp@PTR 10.100.207.21_tcp@PTR]

Jun 22 12:15:44.031: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.034: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.041: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.055: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.059: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.063: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.066: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.070: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.080: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.087: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:44.126: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.test-service-2.dns-5021.svc wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021.svc]

Jun 22 12:15:49.031: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.034: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.041: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.055: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.058: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.067: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.071: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.077: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.090: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.097: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.106: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:49.134: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021 wheezy_tcp@dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.dns-test-service.dns-5021.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5021.svc wheezy_udp@_http._tcp.test-service-2.dns-5021.svc wheezy_tcp@_http._tcp.test-service-2.dns-5021.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021.svc]

Jun 22 12:15:54.046: INFO: Unable to read wheezy_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:54.088: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:54.095: INFO: Unable to read jessie_udp@dns-test-service.dns-5021 from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:54.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-5021.svc from pod dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456: the server could not find the requested resource (get pods dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456)
Jun 22 12:15:54.132: INFO: Lookups using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 failed for: [wheezy_udp@dns-test-service.dns-5021 jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-5021 jessie_tcp@dns-test-service.dns-5021.svc]

Jun 22 12:15:59.125: INFO: DNS probes using dns-5021/dns-test-2a0656a9-5171-4a60-b91a-b5bb34dd5456 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:15:59.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5021" for this suite.

• [SLOW TEST:40.560 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":31,"skipped":470,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:15:59.288: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1256
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-cdbeb77d-7c2f-462e-bc44-9560501d9362
STEP: Creating configMap with name cm-test-opt-upd-5a05281e-8dbc-48b9-b17e-ee7143e678f6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cdbeb77d-7c2f-462e-bc44-9560501d9362
STEP: Updating configmap cm-test-opt-upd-5a05281e-8dbc-48b9-b17e-ee7143e678f6
STEP: Creating configMap with name cm-test-opt-create-59fe86d7-38c1-4c1f-838e-8c1da6421ada
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:18:45.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1256" for this suite.

• [SLOW TEST:166.637 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":515,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:18:45.925: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-2815/secret-test-032e034c-820e-476c-bb14-d6fa1097a7e2
STEP: Creating a pod to test consume secrets
Jun 22 12:18:46.077: INFO: Waiting up to 5m0s for pod "pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec" in namespace "secrets-2815" to be "Succeeded or Failed"
Jun 22 12:18:46.080: INFO: Pod "pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346489ms
Jun 22 12:18:48.085: INFO: Pod "pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942998s
Jun 22 12:18:50.090: INFO: Pod "pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012487043s
STEP: Saw pod success
Jun 22 12:18:50.090: INFO: Pod "pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec" satisfied condition "Succeeded or Failed"
Jun 22 12:18:50.093: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec container env-test: <nil>
STEP: delete the pod
Jun 22 12:18:50.124: INFO: Waiting for pod pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec to disappear
Jun 22 12:18:50.128: INFO: Pod pod-configmaps-22e4aae1-b2a3-4885-a5b5-762c48ce36ec no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:18:50.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2815" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":516,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:18:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3789
STEP: Creating secret with name secret-test-4c4db789-dcb6-4100-9366-20231197a764
STEP: Creating a pod to test consume secrets
Jun 22 12:18:50.469: INFO: Waiting up to 5m0s for pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad" in namespace "secrets-3630" to be "Succeeded or Failed"
Jun 22 12:18:50.478: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad": Phase="Pending", Reason="", readiness=false. Elapsed: 9.849462ms
Jun 22 12:18:52.482: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013740166s
Jun 22 12:18:54.486: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017309368s
Jun 22 12:18:56.489: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020911183s
Jun 22 12:18:58.493: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024721322s
STEP: Saw pod success
Jun 22 12:18:58.493: INFO: Pod "pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad" satisfied condition "Succeeded or Failed"
Jun 22 12:18:58.496: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:18:58.515: INFO: Waiting for pod pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad to disappear
Jun 22 12:18:58.519: INFO: Pod pod-secrets-a4c196da-d23f-45a4-9cef-de7578255cad no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:18:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3630" for this suite.
STEP: Destroying namespace "secret-namespace-3789" for this suite.

• [SLOW TEST:8.407 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":530,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:18:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 12:18:58.696: INFO: Waiting up to 5m0s for pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3" in namespace "emptydir-5795" to be "Succeeded or Failed"
Jun 22 12:18:58.704: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.804657ms
Jun 22 12:19:00.708: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012094933s
Jun 22 12:19:02.712: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015833615s
Jun 22 12:19:04.717: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020437401s
Jun 22 12:19:06.722: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02556308s
STEP: Saw pod success
Jun 22 12:19:06.722: INFO: Pod "pod-d09dc698-e3dd-4def-8d07-c4af3423dae3" satisfied condition "Succeeded or Failed"
Jun 22 12:19:06.725: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-d09dc698-e3dd-4def-8d07-c4af3423dae3 container test-container: <nil>
STEP: delete the pod
Jun 22 12:19:06.742: INFO: Waiting for pod pod-d09dc698-e3dd-4def-8d07-c4af3423dae3 to disappear
Jun 22 12:19:06.746: INFO: Pod pod-d09dc698-e3dd-4def-8d07-c4af3423dae3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:19:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5795" for this suite.

• [SLOW TEST:8.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":530,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:19:06.759: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:19:06.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6977" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":36,"skipped":571,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:19:06.939: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 12:19:07.081: INFO: Waiting up to 5m0s for pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b" in namespace "emptydir-2089" to be "Succeeded or Failed"
Jun 22 12:19:07.090: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.098064ms
Jun 22 12:19:09.095: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01416466s
Jun 22 12:19:11.099: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018813022s
Jun 22 12:19:13.104: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023396531s
Jun 22 12:19:15.109: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028684484s
Jun 22 12:19:17.114: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.033155291s
STEP: Saw pod success
Jun 22 12:19:17.114: INFO: Pod "pod-b46e5d34-5641-424e-8b55-f4393e034b0b" satisfied condition "Succeeded or Failed"
Jun 22 12:19:17.117: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-b46e5d34-5641-424e-8b55-f4393e034b0b container test-container: <nil>
STEP: delete the pod
Jun 22 12:19:17.135: INFO: Waiting for pod pod-b46e5d34-5641-424e-8b55-f4393e034b0b to disappear
Jun 22 12:19:17.139: INFO: Pod pod-b46e5d34-5641-424e-8b55-f4393e034b0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:19:17.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2089" for this suite.

• [SLOW TEST:10.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":602,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:19:17.154: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6848.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6848.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 12:19:27.383: INFO: DNS probes using dns-6848/dns-test-e9966875-577b-46f5-9025-a5561e6fc3ef succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:19:27.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6848" for this suite.

• [SLOW TEST:10.253 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":38,"skipped":644,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:19:27.408: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jun 22 12:19:27.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7407 cluster-info'
Jun 22 12:19:27.623: INFO: stderr: ""
Jun 22 12:19:27.623: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.192.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:19:27.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7407" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":39,"skipped":658,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:19:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 12:19:27.804: INFO: Number of nodes with available pods: 0
Jun 22 12:19:27.804: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:28.814: INFO: Number of nodes with available pods: 0
Jun 22 12:19:28.814: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:29.815: INFO: Number of nodes with available pods: 0
Jun 22 12:19:29.815: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:30.815: INFO: Number of nodes with available pods: 0
Jun 22 12:19:30.815: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:31.815: INFO: Number of nodes with available pods: 0
Jun 22 12:19:31.815: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:32.814: INFO: Number of nodes with available pods: 0
Jun 22 12:19:32.815: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:33.815: INFO: Number of nodes with available pods: 0
Jun 22 12:19:33.815: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:34.814: INFO: Number of nodes with available pods: 0
Jun 22 12:19:34.814: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:19:35.815: INFO: Number of nodes with available pods: 1
Jun 22 12:19:35.815: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 12:19:36.814: INFO: Number of nodes with available pods: 2
Jun 22 12:19:36.814: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:37.814: INFO: Number of nodes with available pods: 2
Jun 22 12:19:37.814: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:38.815: INFO: Number of nodes with available pods: 2
Jun 22 12:19:38.815: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:39.815: INFO: Number of nodes with available pods: 2
Jun 22 12:19:39.815: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:40.815: INFO: Number of nodes with available pods: 2
Jun 22 12:19:40.815: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:41.814: INFO: Number of nodes with available pods: 2
Jun 22 12:19:41.814: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:42.815: INFO: Number of nodes with available pods: 2
Jun 22 12:19:42.815: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:43.815: INFO: Number of nodes with available pods: 2
Jun 22 12:19:43.815: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:44.816: INFO: Number of nodes with available pods: 2
Jun 22 12:19:44.816: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 12:19:45.814: INFO: Number of nodes with available pods: 3
Jun 22 12:19:45.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:46.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:46.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:47.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:47.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:48.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:48.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:49.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:49.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:50.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:50.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:51.814: INFO: Number of nodes with available pods: 3
Jun 22 12:19:51.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:52.814: INFO: Number of nodes with available pods: 3
Jun 22 12:19:52.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:53.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:53.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:54.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:54.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:55.817: INFO: Number of nodes with available pods: 3
Jun 22 12:19:55.817: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:56.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:56.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:57.814: INFO: Number of nodes with available pods: 3
Jun 22 12:19:57.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:58.815: INFO: Number of nodes with available pods: 3
Jun 22 12:19:58.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:19:59.814: INFO: Number of nodes with available pods: 3
Jun 22 12:19:59.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:00.814: INFO: Number of nodes with available pods: 3
Jun 22 12:20:00.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:01.814: INFO: Number of nodes with available pods: 3
Jun 22 12:20:01.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:02.815: INFO: Number of nodes with available pods: 3
Jun 22 12:20:02.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:03.814: INFO: Number of nodes with available pods: 3
Jun 22 12:20:03.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:04.815: INFO: Number of nodes with available pods: 3
Jun 22 12:20:04.815: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:05.814: INFO: Number of nodes with available pods: 3
Jun 22 12:20:05.814: INFO: Node 54b7d611-8263-481b-bda6-56b40bff2a2f is running more than one daemon pod
Jun 22 12:20:06.814: INFO: Number of nodes with available pods: 4
Jun 22 12:20:06.814: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 12:20:06.841: INFO: Number of nodes with available pods: 3
Jun 22 12:20:06.841: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:20:07.851: INFO: Number of nodes with available pods: 3
Jun 22 12:20:07.851: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:20:08.852: INFO: Number of nodes with available pods: 3
Jun 22 12:20:08.852: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:20:09.851: INFO: Number of nodes with available pods: 3
Jun 22 12:20:09.851: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:20:10.851: INFO: Number of nodes with available pods: 3
Jun 22 12:20:10.851: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:20:11.851: INFO: Number of nodes with available pods: 4
Jun 22 12:20:11.851: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5959, will wait for the garbage collector to delete the pods
Jun 22 12:20:11.912: INFO: Deleting DaemonSet.extensions daemon-set took: 5.142962ms
Jun 22 12:20:12.512: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.137993ms
Jun 22 12:20:21.916: INFO: Number of nodes with available pods: 0
Jun 22 12:20:21.916: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 12:20:21.919: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5959/daemonsets","resourceVersion":"152316"},"items":null}

Jun 22 12:20:21.922: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5959/pods","resourceVersion":"152316"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:20:21.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5959" for this suite.

• [SLOW TEST:54.326 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":40,"skipped":685,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:20:21.965: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:20:38.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5742" for this suite.

• [SLOW TEST:16.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":41,"skipped":740,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:20:38.149: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jun 22 12:20:38.292: INFO: Waiting up to 5m0s for pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b" in namespace "var-expansion-3092" to be "Succeeded or Failed"
Jun 22 12:20:38.301: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122056ms
Jun 22 12:20:40.305: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012622413s
Jun 22 12:20:42.309: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016727022s
Jun 22 12:20:44.313: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02079313s
Jun 22 12:20:46.318: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02505984s
Jun 22 12:20:48.322: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029115199s
STEP: Saw pod success
Jun 22 12:20:48.322: INFO: Pod "var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b" satisfied condition "Succeeded or Failed"
Jun 22 12:20:48.325: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b container dapi-container: <nil>
STEP: delete the pod
Jun 22 12:20:48.357: INFO: Waiting for pod var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b to disappear
Jun 22 12:20:48.359: INFO: Pod var-expansion-f0967be0-fab6-402c-9d3d-7f1aecacdf4b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:20:48.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3092" for this suite.

• [SLOW TEST:10.225 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":42,"skipped":776,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:20:48.374: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-7c3fb530-f699-4c0d-a8ce-6ffcedf5ad04 in namespace container-probe-7437
Jun 22 12:20:56.579: INFO: Started pod liveness-7c3fb530-f699-4c0d-a8ce-6ffcedf5ad04 in namespace container-probe-7437
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 12:20:56.582: INFO: Initial restart count of pod liveness-7c3fb530-f699-4c0d-a8ce-6ffcedf5ad04 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:24:57.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7437" for this suite.

• [SLOW TEST:248.737 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":784,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:24:57.112: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 22 12:26:57.780: INFO: Successfully updated pod "var-expansion-bb02dd6c-2052-449e-a449-ba178f7ed5ba"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 22 12:27:31.788: INFO: Deleting pod "var-expansion-bb02dd6c-2052-449e-a449-ba178f7ed5ba" in namespace "var-expansion-7298"
Jun 22 12:27:31.797: INFO: Wait up to 5m0s for pod "var-expansion-bb02dd6c-2052-449e-a449-ba178f7ed5ba" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:28:13.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7298" for this suite.

• [SLOW TEST:196.705 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":44,"skipped":786,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:28:13.818: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-2a050cc7-cc2d-4406-a328-eda697b66308
STEP: Creating a pod to test consume secrets
Jun 22 12:28:13.967: INFO: Waiting up to 5m0s for pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da" in namespace "secrets-2447" to be "Succeeded or Failed"
Jun 22 12:28:13.982: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da": Phase="Pending", Reason="", readiness=false. Elapsed: 14.593953ms
Jun 22 12:28:15.986: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018979486s
Jun 22 12:28:17.990: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023065658s
Jun 22 12:28:19.995: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027353557s
Jun 22 12:28:22.000: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032400863s
STEP: Saw pod success
Jun 22 12:28:22.000: INFO: Pod "pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da" satisfied condition "Succeeded or Failed"
Jun 22 12:28:22.003: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:28:22.029: INFO: Waiting for pod pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da to disappear
Jun 22 12:28:22.033: INFO: Pod pod-secrets-f6deeb0f-0f85-41d9-890c-903d404f25da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:28:22.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2447" for this suite.

• [SLOW TEST:8.228 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":795,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:28:22.046: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:28:22.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 create -f -'
Jun 22 12:28:23.097: INFO: stderr: ""
Jun 22 12:28:23.097: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 22 12:28:23.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 create -f -'
Jun 22 12:28:23.276: INFO: stderr: ""
Jun 22 12:28:23.276: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 22 12:28:24.280: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:28:24.280: INFO: Found 0 / 1
Jun 22 12:28:25.279: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:28:25.279: INFO: Found 1 / 1
Jun 22 12:28:25.279: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 12:28:25.282: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:28:25.282: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 12:28:25.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 describe pod agnhost-primary-8bfdm'
Jun 22 12:28:25.368: INFO: stderr: ""
Jun 22 12:28:25.369: INFO: stdout: "Name:         agnhost-primary-8bfdm\nNamespace:    kubectl-4609\nPriority:     0\nNode:         06998c1b-9fed-44d7-827f-f702404ff383/11.0.1.4\nStart Time:   Tue, 22 Jun 2021 12:28:23 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           11.32.8.2\nIPs:\n  IP:           11.32.8.2\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://65c772b7bfa79bf66c1d85608045f113f3e15fc03f6dfe96d799790d022d8b45\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 22 Jun 2021 12:28:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pkqjh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-pkqjh:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-pkqjh\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4609/agnhost-primary-8bfdm to 06998c1b-9fed-44d7-827f-f702404ff383\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun 22 12:28:25.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 describe rc agnhost-primary'
Jun 22 12:28:25.463: INFO: stderr: ""
Jun 22 12:28:25.463: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4609\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-8bfdm\n"
Jun 22 12:28:25.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 describe service agnhost-primary'
Jun 22 12:28:25.552: INFO: stderr: ""
Jun 22 12:28:25.552: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4609\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.100.199.3\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         11.32.8.2:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 12:28:25.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 describe node 06998c1b-9fed-44d7-827f-f702404ff383'
Jun 22 12:28:25.668: INFO: stderr: ""
Jun 22 12:28:25.668: INFO: stdout: "Name:               06998c1b-9fed-44d7-827f-f702404ff383\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    bosh.id=8b60bac6-7336-4a89-974a-c41f2875f963\n                    bosh.zone=WL-OLT-Dev-02\n                    failure-domain.beta.kubernetes.io/zone=WL-OLT-Dev-02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=11.0.1.4\n                    kubernetes.io/os=linux\n                    pks-system/cluster.name=e2e5247b-a9ad-41da-9919-fa9bccbdd539\n                    pks-system/cluster.uuid=service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94\n                    spec.ip=11.0.1.4\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 21 Jun 2021 23:48:31 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  06998c1b-9fed-44d7-827f-f702404ff383\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 22 Jun 2021 12:28:22 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 22 Jun 2021 12:24:36 +0000   Mon, 21 Jun 2021 23:48:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 22 Jun 2021 12:24:36 +0000   Mon, 21 Jun 2021 23:48:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 22 Jun 2021 12:24:36 +0000   Mon, 21 Jun 2021 23:48:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 22 Jun 2021 12:24:36 +0000   Mon, 21 Jun 2021 23:48:41 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  11.0.1.4\n  InternalIP:  11.0.1.4\n  Hostname:    11.0.1.4\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      32894832Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8168492Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      30315877122\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8066092Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 c5dcfa0ab1ae479d98fa8e37e017524d\n  System UUID:                420E034F-72FB-1EA5-CC8C-80EF6BEA7BB1\n  Boot ID:                    0d938454-6d4a-4970-b6fd-51fd6a99bee7\n  Kernel Version:             4.15.0-142-generic\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.19.9+vmware.1\n  Kube-Proxy Version:         v1.19.9+vmware.1\nProviderID:                   vsphere://420e034f-72fb-1ea5-cc8c-80ef6bea7bb1\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kubectl-4609                agnhost-primary-8bfdm                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  pks-system                  fluent-bit-5z5qp                                           0 (0%)        0 (0%)      100Mi (1%)       100Mi (1%)     12h\n  pks-system                  node-exporter-nrj7p                                        10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)      12h\n  pks-system                  telegraf-kp96k                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    10m (0%)    10m (0%)\n  memory                 150Mi (1%)  150Mi (1%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  example.com/fakecpu    0           0\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
Jun 22 12:28:25.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4609 describe namespace kubectl-4609'
Jun 22 12:28:25.753: INFO: stderr: ""
Jun 22 12:28:25.753: INFO: stdout: "Name:         kubectl-4609\nLabels:       e2e-framework=kubectl\n              e2e-run=f046d819-3099-466f-abb3-9a26bdefcca5\nAnnotations:  ncp/extpoolid: 9e4dec76-13a9-482e-a705-d94192f1349e\n              ncp/snat_ip: 100.104.66.108\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:28:25.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4609" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":46,"skipped":822,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:28:25.765: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2075
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 12:28:25.899: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 22 12:28:25.978: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:27.981: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:29.982: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:31.982: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:33.982: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:35.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:37.982: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:39.982: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:41.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:43.982: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:28:45.981: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 22 12:28:45.987: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:28:47.991: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:28:49.991: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 22 12:28:49.997: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 22 12:28:50.002: INFO: The status of Pod netserver-3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:52.006: INFO: The status of Pod netserver-3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:54.007: INFO: The status of Pod netserver-3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:28:56.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:28:58.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:00.023: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:02.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:04.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:06.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:08.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:10.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:12.006: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:29:14.006: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun 22 12:29:18.037: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.9.6:8080/dial?request=hostname&protocol=udp&host=11.32.9.3&port=8081&tries=1'] Namespace:pod-network-test-2075 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:29:18.037: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:29:18.142: INFO: Waiting for responses: map[]
Jun 22 12:29:18.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.9.6:8080/dial?request=hostname&protocol=udp&host=11.32.9.2&port=8081&tries=1'] Namespace:pod-network-test-2075 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:29:18.146: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:29:18.252: INFO: Waiting for responses: map[]
Jun 22 12:29:18.261: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.9.6:8080/dial?request=hostname&protocol=udp&host=11.32.9.4&port=8081&tries=1'] Namespace:pod-network-test-2075 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:29:18.261: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:29:18.371: INFO: Waiting for responses: map[]
Jun 22 12:29:18.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.9.6:8080/dial?request=hostname&protocol=udp&host=11.32.9.5&port=8081&tries=1'] Namespace:pod-network-test-2075 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:29:18.374: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:29:18.466: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:29:18.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2075" for this suite.

• [SLOW TEST:52.715 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":827,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:29:18.486: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1762.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 180.201.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.201.180_udp@PTR;check="$$(dig +tcp +noall +answer +search 180.201.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.201.180_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1762.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 180.201.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.201.180_udp@PTR;check="$$(dig +tcp +noall +answer +search 180.201.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.201.180_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 12:29:28.685: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.692: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.695: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.717: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.726: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:28.749: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:33.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.761: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.764: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.787: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.790: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.793: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:33.816: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:38.755: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.766: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.772: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.797: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.804: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.807: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:38.826: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:43.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.761: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.765: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.789: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.799: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:43.820: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:48.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.761: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.764: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.787: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.790: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.794: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.797: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:48.817: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:53.755: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.765: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.790: INFO: Unable to read jessie_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.795: INFO: Unable to read jessie_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.799: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.802: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59: the server could not find the requested resource (get pods dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59)
Jun 22 12:29:53.823: INFO: Lookups using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@dns-test-service.dns-1762.svc.cluster.local jessie_tcp@dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Jun 22 12:29:58.820: INFO: DNS probes using dns-1762/dns-test-940d2e1b-33c1-4781-ba31-5cfa6e190d59 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:29:58.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1762" for this suite.

• [SLOW TEST:40.425 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":48,"skipped":887,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:29:58.915: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jun 22 12:29:59.064: INFO: Waiting up to 5m0s for pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b" in namespace "containers-5604" to be "Succeeded or Failed"
Jun 22 12:29:59.069: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839825ms
Jun 22 12:30:01.074: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009729406s
Jun 22 12:30:03.078: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014255182s
Jun 22 12:30:05.083: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018700175s
Jun 22 12:30:07.086: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022488711s
STEP: Saw pod success
Jun 22 12:30:07.086: INFO: Pod "client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b" satisfied condition "Succeeded or Failed"
Jun 22 12:30:07.089: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b container test-container: <nil>
STEP: delete the pod
Jun 22 12:30:07.114: INFO: Waiting for pod client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b to disappear
Jun 22 12:30:07.119: INFO: Pod client-containers-6f4fa87f-36d6-4d9b-990e-aaffa915e99b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:30:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5604" for this suite.

• [SLOW TEST:8.214 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":899,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:30:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 12:30:07.275: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154422 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:07.276: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154422 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 12:30:17.285: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154461 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:17.285: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154461 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 12:30:27.293: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154492 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:27.293: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154492 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 12:30:37.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154521 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:37.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-a ca41a99f-b3f4-4ece-81d2-393b5552a0fd 154521 0 2021-06-22 12:30:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 12:30:47.316: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-b 2be89011-fe65-47b8-8b84-e4a634717f1e 154550 0 2021-06-22 12:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:47.316: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-b 2be89011-fe65-47b8-8b84-e4a634717f1e 154550 0 2021-06-22 12:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 12:30:57.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-b 2be89011-fe65-47b8-8b84-e4a634717f1e 154580 0 2021-06-22 12:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 12:30:57.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-configmap-b 2be89011-fe65-47b8-8b84-e4a634717f1e 154580 0 2021-06-22 12:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-22 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:31:07.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8740" for this suite.

• [SLOW TEST:60.202 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":50,"skipped":908,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:31:07.334: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 12:31:07.756: INFO: Pod name wrapped-volume-race-b2d5cb17-0348-44f5-b41f-fa38661e052d: Found 2 pods out of 5
Jun 22 12:31:12.771: INFO: Pod name wrapped-volume-race-b2d5cb17-0348-44f5-b41f-fa38661e052d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b2d5cb17-0348-44f5-b41f-fa38661e052d in namespace emptydir-wrapper-4967, will wait for the garbage collector to delete the pods
Jun 22 12:32:20.867: INFO: Deleting ReplicationController wrapped-volume-race-b2d5cb17-0348-44f5-b41f-fa38661e052d took: 7.873525ms
Jun 22 12:32:21.467: INFO: Terminating ReplicationController wrapped-volume-race-b2d5cb17-0348-44f5-b41f-fa38661e052d pods took: 600.119995ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 12:32:34.790: INFO: Pod name wrapped-volume-race-2a6e626a-c9ff-434f-83e1-da7863a0c2f2: Found 0 pods out of 5
Jun 22 12:32:39.801: INFO: Pod name wrapped-volume-race-2a6e626a-c9ff-434f-83e1-da7863a0c2f2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2a6e626a-c9ff-434f-83e1-da7863a0c2f2 in namespace emptydir-wrapper-4967, will wait for the garbage collector to delete the pods
Jun 22 12:32:57.894: INFO: Deleting ReplicationController wrapped-volume-race-2a6e626a-c9ff-434f-83e1-da7863a0c2f2 took: 6.840414ms
Jun 22 12:32:58.494: INFO: Terminating ReplicationController wrapped-volume-race-2a6e626a-c9ff-434f-83e1-da7863a0c2f2 pods took: 600.126934ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 12:33:14.013: INFO: Pod name wrapped-volume-race-c60dce90-47cf-4c56-9cf9-5593005679a5: Found 0 pods out of 5
Jun 22 12:33:19.025: INFO: Pod name wrapped-volume-race-c60dce90-47cf-4c56-9cf9-5593005679a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c60dce90-47cf-4c56-9cf9-5593005679a5 in namespace emptydir-wrapper-4967, will wait for the garbage collector to delete the pods
Jun 22 12:33:31.112: INFO: Deleting ReplicationController wrapped-volume-race-c60dce90-47cf-4c56-9cf9-5593005679a5 took: 8.803622ms
Jun 22 12:33:31.812: INFO: Terminating ReplicationController wrapped-volume-race-c60dce90-47cf-4c56-9cf9-5593005679a5 pods took: 700.213734ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:33:36.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4967" for this suite.

• [SLOW TEST:149.051 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":51,"skipped":917,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:33:36.386: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 12:33:36.526: INFO: Waiting up to 5m0s for pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284" in namespace "emptydir-8463" to be "Succeeded or Failed"
Jun 22 12:33:36.533: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284": Phase="Pending", Reason="", readiness=false. Elapsed: 6.993563ms
Jun 22 12:33:38.538: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011775183s
Jun 22 12:33:40.542: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016126041s
Jun 22 12:33:42.547: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020410965s
Jun 22 12:33:44.551: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024381057s
STEP: Saw pod success
Jun 22 12:33:44.551: INFO: Pod "pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284" satisfied condition "Succeeded or Failed"
Jun 22 12:33:44.553: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284 container test-container: <nil>
STEP: delete the pod
Jun 22 12:33:44.587: INFO: Waiting for pod pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284 to disappear
Jun 22 12:33:44.590: INFO: Pod pod-8c46a7b3-c950-42b2-b3cd-dba911b9e284 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:33:44.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8463" for this suite.

• [SLOW TEST:8.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":933,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:33:44.601: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 22 12:33:44.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 create -f -'
Jun 22 12:33:44.973: INFO: stderr: ""
Jun 22 12:33:44.973: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 12:33:44.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:33:45.072: INFO: stderr: ""
Jun 22 12:33:45.072: INFO: stdout: "update-demo-nautilus-k9ll2 update-demo-nautilus-qbcvn "
Jun 22 12:33:45.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:33:45.150: INFO: stderr: ""
Jun 22 12:33:45.150: INFO: stdout: ""
Jun 22 12:33:45.150: INFO: update-demo-nautilus-k9ll2 is created but not running
Jun 22 12:33:50.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:33:50.237: INFO: stderr: ""
Jun 22 12:33:50.237: INFO: stdout: "update-demo-nautilus-k9ll2 update-demo-nautilus-qbcvn "
Jun 22 12:33:50.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:33:50.321: INFO: stderr: ""
Jun 22 12:33:50.321: INFO: stdout: "true"
Jun 22 12:33:50.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:33:50.397: INFO: stderr: ""
Jun 22 12:33:50.397: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:33:50.397: INFO: validating pod update-demo-nautilus-k9ll2
Jun 22 12:33:50.404: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:33:50.404: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:33:50.404: INFO: update-demo-nautilus-k9ll2 is verified up and running
Jun 22 12:33:50.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-qbcvn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:33:50.475: INFO: stderr: ""
Jun 22 12:33:50.475: INFO: stdout: "true"
Jun 22 12:33:50.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-qbcvn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:33:50.548: INFO: stderr: ""
Jun 22 12:33:50.548: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:33:50.548: INFO: validating pod update-demo-nautilus-qbcvn
Jun 22 12:33:50.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:33:50.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:33:50.560: INFO: update-demo-nautilus-qbcvn is verified up and running
STEP: scaling down the replication controller
Jun 22 12:33:50.562: INFO: scanned /root for discovery docs: <nil>
Jun 22 12:33:50.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 22 12:33:51.694: INFO: stderr: ""
Jun 22 12:33:51.694: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 12:33:51.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:33:51.771: INFO: stderr: ""
Jun 22 12:33:51.771: INFO: stdout: "update-demo-nautilus-k9ll2 update-demo-nautilus-qbcvn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 12:33:56.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:33:56.847: INFO: stderr: ""
Jun 22 12:33:56.847: INFO: stdout: "update-demo-nautilus-k9ll2 "
Jun 22 12:33:56.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:33:56.925: INFO: stderr: ""
Jun 22 12:33:56.925: INFO: stdout: "true"
Jun 22 12:33:56.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:33:57.007: INFO: stderr: ""
Jun 22 12:33:57.007: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:33:57.007: INFO: validating pod update-demo-nautilus-k9ll2
Jun 22 12:33:57.011: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:33:57.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:33:57.011: INFO: update-demo-nautilus-k9ll2 is verified up and running
STEP: scaling up the replication controller
Jun 22 12:33:57.013: INFO: scanned /root for discovery docs: <nil>
Jun 22 12:33:57.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 22 12:33:58.106: INFO: stderr: ""
Jun 22 12:33:58.106: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 12:33:58.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:33:58.235: INFO: stderr: ""
Jun 22 12:33:58.235: INFO: stdout: "update-demo-nautilus-25d5k update-demo-nautilus-k9ll2 "
Jun 22 12:33:58.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-25d5k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:33:58.307: INFO: stderr: ""
Jun 22 12:33:58.307: INFO: stdout: ""
Jun 22 12:33:58.307: INFO: update-demo-nautilus-25d5k is created but not running
Jun 22 12:34:03.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:34:03.386: INFO: stderr: ""
Jun 22 12:34:03.386: INFO: stdout: "update-demo-nautilus-25d5k update-demo-nautilus-k9ll2 "
Jun 22 12:34:03.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-25d5k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:34:03.473: INFO: stderr: ""
Jun 22 12:34:03.474: INFO: stdout: "true"
Jun 22 12:34:03.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-25d5k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:34:03.543: INFO: stderr: ""
Jun 22 12:34:03.543: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:34:03.543: INFO: validating pod update-demo-nautilus-25d5k
Jun 22 12:34:03.551: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:34:03.551: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:34:03.551: INFO: update-demo-nautilus-25d5k is verified up and running
Jun 22 12:34:03.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:34:03.620: INFO: stderr: ""
Jun 22 12:34:03.620: INFO: stdout: "true"
Jun 22 12:34:03.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods update-demo-nautilus-k9ll2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:34:03.694: INFO: stderr: ""
Jun 22 12:34:03.694: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:34:03.694: INFO: validating pod update-demo-nautilus-k9ll2
Jun 22 12:34:03.698: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:34:03.698: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:34:03.698: INFO: update-demo-nautilus-k9ll2 is verified up and running
STEP: using delete to clean up resources
Jun 22 12:34:03.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 delete --grace-period=0 --force -f -'
Jun 22 12:34:03.779: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 12:34:03.779: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 12:34:03.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get rc,svc -l name=update-demo --no-headers'
Jun 22 12:34:03.856: INFO: stderr: "No resources found in kubectl-3403 namespace.\n"
Jun 22 12:34:03.856: INFO: stdout: ""
Jun 22 12:34:03.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 12:34:03.930: INFO: stderr: ""
Jun 22 12:34:03.930: INFO: stdout: "update-demo-nautilus-25d5k\nupdate-demo-nautilus-k9ll2\n"
Jun 22 12:34:04.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get rc,svc -l name=update-demo --no-headers'
Jun 22 12:34:04.535: INFO: stderr: "No resources found in kubectl-3403 namespace.\n"
Jun 22 12:34:04.535: INFO: stdout: ""
Jun 22 12:34:04.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-3403 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 12:34:04.621: INFO: stderr: ""
Jun 22 12:34:04.621: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:34:04.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3403" for this suite.

• [SLOW TEST:20.032 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":53,"skipped":946,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:34:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:34:04.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2" in namespace "projected-2862" to be "Succeeded or Failed"
Jun 22 12:34:04.786: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.292811ms
Jun 22 12:34:06.790: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010525554s
Jun 22 12:34:08.795: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014900614s
Jun 22 12:34:10.799: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019286301s
Jun 22 12:34:12.803: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023564216s
Jun 22 12:34:14.807: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027343103s
STEP: Saw pod success
Jun 22 12:34:14.807: INFO: Pod "downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2" satisfied condition "Succeeded or Failed"
Jun 22 12:34:14.810: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2 container client-container: <nil>
STEP: delete the pod
Jun 22 12:34:14.825: INFO: Waiting for pod downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2 to disappear
Jun 22 12:34:14.831: INFO: Pod downwardapi-volume-38769c75-a4de-4b38-8ae3-41321ec463c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:34:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2862" for this suite.

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":950,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:34:14.860: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:34:15.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6048" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":55,"skipped":952,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:34:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:34:32.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-401" for this suite.

• [SLOW TEST:17.195 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":56,"skipped":956,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:34:32.255: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:34:32.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:34:34.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962072, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:34:37.872: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:34:37.879: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:34:39.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7075" for this suite.
STEP: Destroying namespace "webhook-7075-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":57,"skipped":965,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:34:39.127: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:34:39.283: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:34:41.288: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:34:43.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:34:45.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:34:47.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:49.288: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:51.288: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:53.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:55.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:57.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:34:59.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:35:01.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:35:03.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:35:05.287: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = false)
Jun 22 12:35:07.288: INFO: The status of Pod test-webserver-a8201316-406f-4983-97c6-2a02e7cafa25 is Running (Ready = true)
Jun 22 12:35:07.291: INFO: Container started at 2021-06-22 12:34:46 +0000 UTC, pod became ready at 2021-06-22 12:35:06 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:35:07.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6318" for this suite.

• [SLOW TEST:28.179 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":1008,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:35:07.306: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 12:35:15.465: INFO: &Pod{ObjectMeta:{send-events-fa718170-ab6d-4476-8cb8-a9e9dc447aa0  events-4655 /api/v1/namespaces/events-4655/pods/send-events-fa718170-ab6d-4476-8cb8-a9e9dc447aa0 390b43b3-22df-411c-b216-5b3038068c38 156472 0 2021-06-22 12:35:07 +0000 UTC <nil> <nil> map[name:foo time:441396883] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-06-22 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 12:35:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.7.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjtlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjtlp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjtlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:35:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:35:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:11.32.7.2,StartTime:2021-06-22 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 12:35:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://8d5e8bb2f254f8e96a2278eb7bf6ef5f912e656beccb00e495f5232c3aa8eace,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.7.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 22 12:35:17.469: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 12:35:19.474: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:35:19.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4655" for this suite.

• [SLOW TEST:12.190 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":59,"skipped":1009,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:35:19.497: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:35:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5387" for this suite.

• [SLOW TEST:9.179 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":60,"skipped":1034,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:35:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 12:35:28.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7775 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jun 22 12:35:28.895: INFO: stderr: ""
Jun 22 12:35:28.895: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Jun 22 12:35:28.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7775 delete pods e2e-test-httpd-pod'
Jun 22 12:35:32.230: INFO: stderr: ""
Jun 22 12:35:32.230: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:35:32.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7775" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":61,"skipped":1043,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:35:32.246: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:37:32.401: INFO: Deleting pod "var-expansion-8a7e401a-9c9d-4410-821b-baf8f656efa0" in namespace "var-expansion-8619"
Jun 22 12:37:32.407: INFO: Wait up to 5m0s for pod "var-expansion-8a7e401a-9c9d-4410-821b-baf8f656efa0" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:37:36.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8619" for this suite.

• [SLOW TEST:124.181 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":62,"skipped":1095,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:37:36.427: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-105
STEP: creating replication controller nodeport-test in namespace services-105
I0622 12:37:36.585779      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-105, replica count: 2
I0622 12:37:39.636105      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:37:42.636290      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:37:45.636453      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 12:37:45.636: INFO: Creating new exec pod
Jun 22 12:37:50.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 22 12:37:50.850: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 22 12:37:50.850: INFO: stdout: ""
Jun 22 12:37:50.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 10.100.206.173 80'
Jun 22 12:37:51.019: INFO: stderr: "+ nc -zv -t -w 2 10.100.206.173 80\nConnection to 10.100.206.173 80 port [tcp/http] succeeded!\n"
Jun 22 12:37:51.019: INFO: stdout: ""
Jun 22 12:37:51.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 31893'
Jun 22 12:37:51.179: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 31893\nConnection to 11.0.1.3 31893 port [tcp/31893] succeeded!\n"
Jun 22 12:37:51.179: INFO: stdout: ""
Jun 22 12:37:51.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 31893'
Jun 22 12:37:51.346: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 31893\nConnection to 11.0.1.4 31893 port [tcp/31893] succeeded!\n"
Jun 22 12:37:51.346: INFO: stdout: ""
Jun 22 12:37:51.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 31893'
Jun 22 12:37:51.515: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 31893\nConnection to 11.0.1.3 31893 port [tcp/31893] succeeded!\n"
Jun 22 12:37:51.515: INFO: stdout: ""
Jun 22 12:37:51.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-105 exec execpodch4fk -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 31893'
Jun 22 12:37:51.677: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 31893\nConnection to 11.0.1.4 31893 port [tcp/31893] succeeded!\n"
Jun 22 12:37:51.677: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:37:51.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-105" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.260 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":63,"skipped":1116,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:37:51.688: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 12:38:03.878: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 12:38:03.881: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 12:38:05.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 12:38:05.885: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 12:38:07.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 12:38:07.886: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:38:07.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4599" for this suite.

• [SLOW TEST:16.225 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1133,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:38:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2723
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:38:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:38:14.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2723" for this suite.

• [SLOW TEST:6.994 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":65,"skipped":1180,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:38:14.910: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:38:15.046: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 22 12:38:16.082: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:38:16.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2450" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":66,"skipped":1217,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:38:16.116: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-e2d4c9e3-64cb-44a1-b659-cee02534bd75
STEP: Creating a pod to test consume configMaps
Jun 22 12:38:16.275: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185" in namespace "projected-2800" to be "Succeeded or Failed"
Jun 22 12:38:16.281: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30585ms
Jun 22 12:38:18.285: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009787165s
Jun 22 12:38:20.291: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016220748s
Jun 22 12:38:22.295: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019963106s
Jun 22 12:38:24.299: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023598057s
Jun 22 12:38:26.303: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027603838s
STEP: Saw pod success
Jun 22 12:38:26.303: INFO: Pod "pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185" satisfied condition "Succeeded or Failed"
Jun 22 12:38:26.305: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:38:26.323: INFO: Waiting for pod pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185 to disappear
Jun 22 12:38:26.326: INFO: Pod pod-projected-configmaps-2b0e2d81-5732-46ff-97ef-d76889461185 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:38:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2800" for this suite.

• [SLOW TEST:10.221 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":1219,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:38:26.336: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-3271
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:38:26.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3271" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":68,"skipped":1220,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:38:26.529: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:38:26.700: INFO: Create a RollingUpdate DaemonSet
Jun 22 12:38:26.705: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 12:38:26.713: INFO: Number of nodes with available pods: 0
Jun 22 12:38:26.713: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:27.724: INFO: Number of nodes with available pods: 0
Jun 22 12:38:27.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:28.724: INFO: Number of nodes with available pods: 0
Jun 22 12:38:28.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:29.725: INFO: Number of nodes with available pods: 0
Jun 22 12:38:29.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:30.723: INFO: Number of nodes with available pods: 0
Jun 22 12:38:30.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:31.722: INFO: Number of nodes with available pods: 0
Jun 22 12:38:31.722: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:32.724: INFO: Number of nodes with available pods: 0
Jun 22 12:38:32.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:33.725: INFO: Number of nodes with available pods: 0
Jun 22 12:38:33.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:34.723: INFO: Number of nodes with available pods: 1
Jun 22 12:38:34.723: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:35.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:35.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:36.725: INFO: Number of nodes with available pods: 2
Jun 22 12:38:36.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:37.723: INFO: Number of nodes with available pods: 2
Jun 22 12:38:37.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:38.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:38.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:39.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:39.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:40.725: INFO: Number of nodes with available pods: 2
Jun 22 12:38:40.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:41.725: INFO: Number of nodes with available pods: 2
Jun 22 12:38:41.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:42.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:42.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:43.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:43.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:44.725: INFO: Number of nodes with available pods: 2
Jun 22 12:38:44.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:45.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:45.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:46.723: INFO: Number of nodes with available pods: 2
Jun 22 12:38:46.723: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:47.723: INFO: Number of nodes with available pods: 2
Jun 22 12:38:47.723: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:48.723: INFO: Number of nodes with available pods: 2
Jun 22 12:38:48.723: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:49.723: INFO: Number of nodes with available pods: 2
Jun 22 12:38:49.723: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:50.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:50.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:51.725: INFO: Number of nodes with available pods: 2
Jun 22 12:38:51.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:52.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:52.725: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:53.724: INFO: Number of nodes with available pods: 2
Jun 22 12:38:53.724: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 12:38:54.723: INFO: Number of nodes with available pods: 4
Jun 22 12:38:54.723: INFO: Number of running nodes: 4, number of available pods: 4
Jun 22 12:38:54.723: INFO: Update the DaemonSet to trigger a rollout
Jun 22 12:38:54.732: INFO: Updating DaemonSet daemon-set
Jun 22 12:38:57.755: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 12:38:57.763: INFO: Updating DaemonSet daemon-set
Jun 22 12:38:57.763: INFO: Make sure DaemonSet rollback is complete
Jun 22 12:38:57.773: INFO: Wrong image for pod: daemon-set-5k4nk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 12:38:57.773: INFO: Pod daemon-set-5k4nk is not available
Jun 22 12:38:58.793: INFO: Wrong image for pod: daemon-set-5k4nk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 12:38:58.793: INFO: Pod daemon-set-5k4nk is not available
Jun 22 12:38:59.793: INFO: Wrong image for pod: daemon-set-5k4nk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 12:38:59.793: INFO: Pod daemon-set-5k4nk is not available
Jun 22 12:39:00.792: INFO: Wrong image for pod: daemon-set-5k4nk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 12:39:00.793: INFO: Pod daemon-set-5k4nk is not available
Jun 22 12:39:01.793: INFO: Wrong image for pod: daemon-set-5k4nk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 12:39:01.793: INFO: Pod daemon-set-5k4nk is not available
Jun 22 12:39:02.792: INFO: Pod daemon-set-vmxz5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5809, will wait for the garbage collector to delete the pods
Jun 22 12:39:02.863: INFO: Deleting DaemonSet.extensions daemon-set took: 5.728207ms
Jun 22 12:39:03.463: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.144308ms
Jun 22 12:39:14.668: INFO: Number of nodes with available pods: 0
Jun 22 12:39:14.668: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 12:39:14.671: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5809/daemonsets","resourceVersion":"157689"},"items":null}

Jun 22 12:39:14.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5809/pods","resourceVersion":"157689"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:39:14.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5809" for this suite.

• [SLOW TEST:48.173 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":69,"skipped":1269,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:39:14.701: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9132
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-a3fe4903-bd63-428f-90de-925bf6bef144
STEP: Creating configMap with name cm-test-opt-upd-f1e05c07-5e15-41a7-aa08-44db5db7103a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a3fe4903-bd63-428f-90de-925bf6bef144
STEP: Updating configmap cm-test-opt-upd-f1e05c07-5e15-41a7-aa08-44db5db7103a
STEP: Creating configMap with name cm-test-opt-create-cab8cfdc-4c8b-4717-b900-f2a2bb0c4f96
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:40:27.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9132" for this suite.

• [SLOW TEST:72.520 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1270,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:40:27.223: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 12:40:27.365: INFO: Waiting up to 5m0s for pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339" in namespace "emptydir-552" to be "Succeeded or Failed"
Jun 22 12:40:27.369: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164048ms
Jun 22 12:40:29.373: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00768785s
Jun 22 12:40:31.377: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011717742s
Jun 22 12:40:33.384: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018694901s
Jun 22 12:40:35.388: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022818951s
STEP: Saw pod success
Jun 22 12:40:35.388: INFO: Pod "pod-cf01174f-1037-4da4-ac8d-e430a4a9b339" satisfied condition "Succeeded or Failed"
Jun 22 12:40:35.391: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod pod-cf01174f-1037-4da4-ac8d-e430a4a9b339 container test-container: <nil>
STEP: delete the pod
Jun 22 12:40:35.435: INFO: Waiting for pod pod-cf01174f-1037-4da4-ac8d-e430a4a9b339 to disappear
Jun 22 12:40:35.438: INFO: Pod pod-cf01174f-1037-4da4-ac8d-e430a4a9b339 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:40:35.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-552" for this suite.

• [SLOW TEST:8.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1278,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:40:35.450: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:40:43.622: INFO: Waiting up to 5m0s for pod "client-envvars-112d9876-5537-4461-bbd6-eb66bd262258" in namespace "pods-9382" to be "Succeeded or Failed"
Jun 22 12:40:43.628: INFO: Pod "client-envvars-112d9876-5537-4461-bbd6-eb66bd262258": Phase="Pending", Reason="", readiness=false. Elapsed: 6.415838ms
Jun 22 12:40:45.632: INFO: Pod "client-envvars-112d9876-5537-4461-bbd6-eb66bd262258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010248382s
Jun 22 12:40:47.637: INFO: Pod "client-envvars-112d9876-5537-4461-bbd6-eb66bd262258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014713667s
STEP: Saw pod success
Jun 22 12:40:47.637: INFO: Pod "client-envvars-112d9876-5537-4461-bbd6-eb66bd262258" satisfied condition "Succeeded or Failed"
Jun 22 12:40:47.640: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod client-envvars-112d9876-5537-4461-bbd6-eb66bd262258 container env3cont: <nil>
STEP: delete the pod
Jun 22 12:40:47.657: INFO: Waiting for pod client-envvars-112d9876-5537-4461-bbd6-eb66bd262258 to disappear
Jun 22 12:40:47.661: INFO: Pod client-envvars-112d9876-5537-4461-bbd6-eb66bd262258 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:40:47.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9382" for this suite.

• [SLOW TEST:12.220 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":1278,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:40:47.671: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-7415
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:40:47.808: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Creating first CR 
Jun 22 12:40:48.365: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:40:48Z]] name:name1 resourceVersion:158112 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:47ecf8e6-09fd-4c2d-998e-384bc2c2469b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 22 12:40:58.370: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:40:58Z]] name:name2 resourceVersion:158169 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1046e6e1-16be-4b9b-a11d-ff611e7b8f64] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 22 12:41:08.377: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:41:08Z]] name:name1 resourceVersion:158198 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:47ecf8e6-09fd-4c2d-998e-384bc2c2469b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 22 12:41:18.382: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:41:18Z]] name:name2 resourceVersion:158228 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1046e6e1-16be-4b9b-a11d-ff611e7b8f64] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 22 12:41:28.389: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:41:08Z]] name:name1 resourceVersion:158258 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:47ecf8e6-09fd-4c2d-998e-384bc2c2469b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 22 12:41:38.395: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-22T12:40:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-22T12:41:18Z]] name:name2 resourceVersion:158287 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1046e6e1-16be-4b9b-a11d-ff611e7b8f64] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:41:48.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7415" for this suite.

• [SLOW TEST:61.249 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":73,"skipped":1321,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:41:48.920: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:41:49.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f" in namespace "downward-api-7188" to be "Succeeded or Failed"
Jun 22 12:41:49.072: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.185616ms
Jun 22 12:41:51.076: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013026476s
Jun 22 12:41:53.081: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018026217s
Jun 22 12:41:55.086: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02224444s
Jun 22 12:41:57.090: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026861379s
STEP: Saw pod success
Jun 22 12:41:57.090: INFO: Pod "downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f" satisfied condition "Succeeded or Failed"
Jun 22 12:41:57.093: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f container client-container: <nil>
STEP: delete the pod
Jun 22 12:41:57.110: INFO: Waiting for pod downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f to disappear
Jun 22 12:41:57.114: INFO: Pod downwardapi-volume-32bc6453-bfb4-4a7b-9f42-396f234ac98f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:41:57.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7188" for this suite.

• [SLOW TEST:8.204 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":1360,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:41:57.124: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:41:57.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-892" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":75,"skipped":1371,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:41:57.301: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-a5f2ccaa-fa03-4477-b371-1f7dd6c60057
STEP: Creating a pod to test consume configMaps
Jun 22 12:41:57.456: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7" in namespace "configmap-8498" to be "Succeeded or Failed"
Jun 22 12:41:57.458: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.914502ms
Jun 22 12:41:59.463: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007511636s
Jun 22 12:42:01.467: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011688768s
Jun 22 12:42:03.471: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015488319s
Jun 22 12:42:05.475: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019355007s
STEP: Saw pod success
Jun 22 12:42:05.475: INFO: Pod "pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7" satisfied condition "Succeeded or Failed"
Jun 22 12:42:05.478: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:42:05.494: INFO: Waiting for pod pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7 to disappear
Jun 22 12:42:05.499: INFO: Pod pod-configmaps-2fb3a92e-7646-44ff-b714-4a2c1a35f4f7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:05.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8498" for this suite.

• [SLOW TEST:8.209 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1373,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:05.510: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 12:42:05.659: INFO: Waiting up to 5m0s for pod "pod-de0cc63a-1863-47be-b055-d407e329c724" in namespace "emptydir-8748" to be "Succeeded or Failed"
Jun 22 12:42:05.664: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716846ms
Jun 22 12:42:07.668: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008676999s
Jun 22 12:42:09.672: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012713078s
Jun 22 12:42:11.676: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017367104s
Jun 22 12:42:13.681: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022071126s
STEP: Saw pod success
Jun 22 12:42:13.681: INFO: Pod "pod-de0cc63a-1863-47be-b055-d407e329c724" satisfied condition "Succeeded or Failed"
Jun 22 12:42:13.684: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-de0cc63a-1863-47be-b055-d407e329c724 container test-container: <nil>
STEP: delete the pod
Jun 22 12:42:13.701: INFO: Waiting for pod pod-de0cc63a-1863-47be-b055-d407e329c724 to disappear
Jun 22 12:42:13.706: INFO: Pod pod-de0cc63a-1863-47be-b055-d407e329c724 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:13.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8748" for this suite.

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1379,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-ca2b60eb-15a3-4ac3-9e96-1e3d67d747fb
STEP: Creating a pod to test consume secrets
Jun 22 12:42:13.865: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48" in namespace "projected-431" to be "Succeeded or Failed"
Jun 22 12:42:13.867: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751457ms
Jun 22 12:42:15.873: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00825452s
Jun 22 12:42:17.877: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01212768s
Jun 22 12:42:19.881: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016749586s
Jun 22 12:42:21.886: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021778643s
STEP: Saw pod success
Jun 22 12:42:21.886: INFO: Pod "pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48" satisfied condition "Succeeded or Failed"
Jun 22 12:42:21.889: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:42:21.906: INFO: Waiting for pod pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48 to disappear
Jun 22 12:42:21.910: INFO: Pod pod-projected-secrets-229695a3-84e1-48ae-9495-6d15685d6d48 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:21.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-431" for this suite.

• [SLOW TEST:8.199 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1387,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:21.923: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9ab82968-8bb5-49af-b849-c9a6f83d19cd
STEP: Creating a pod to test consume secrets
Jun 22 12:42:22.073: INFO: Waiting up to 5m0s for pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b" in namespace "secrets-5181" to be "Succeeded or Failed"
Jun 22 12:42:22.082: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.245434ms
Jun 22 12:42:24.085: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012434194s
Jun 22 12:42:26.089: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015987922s
Jun 22 12:42:28.092: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019007519s
Jun 22 12:42:30.100: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027150454s
STEP: Saw pod success
Jun 22 12:42:30.100: INFO: Pod "pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b" satisfied condition "Succeeded or Failed"
Jun 22 12:42:30.105: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:42:30.144: INFO: Waiting for pod pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b to disappear
Jun 22 12:42:30.149: INFO: Pod pod-secrets-ba9d34c8-6999-4366-8187-2d46e56b120b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:30.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5181" for this suite.

• [SLOW TEST:8.242 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1388,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:30.165: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:42:30.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5" in namespace "projected-5463" to be "Succeeded or Failed"
Jun 22 12:42:30.313: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334732ms
Jun 22 12:42:32.317: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007345224s
Jun 22 12:42:34.321: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011301549s
Jun 22 12:42:36.324: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014899281s
Jun 22 12:42:38.328: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018940067s
Jun 22 12:42:40.332: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.022649858s
STEP: Saw pod success
Jun 22 12:42:40.332: INFO: Pod "downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5" satisfied condition "Succeeded or Failed"
Jun 22 12:42:40.335: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5 container client-container: <nil>
STEP: delete the pod
Jun 22 12:42:40.351: INFO: Waiting for pod downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5 to disappear
Jun 22 12:42:40.357: INFO: Pod downwardapi-volume-97065727-d4ec-41e4-ba39-3c739eddf8b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:40.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5463" for this suite.

• [SLOW TEST:10.201 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1400,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:40.370: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5713
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede
Jun 22 12:42:40.513: INFO: Pod name my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede: Found 0 pods out of 1
Jun 22 12:42:45.518: INFO: Pod name my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede: Found 1 pods out of 1
Jun 22 12:42:45.518: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede" are running
Jun 22 12:42:49.524: INFO: Pod "my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede-864w4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 12:42:40 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 12:42:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 12:42:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 12:42:40 +0000 UTC Reason: Message:}])
Jun 22 12:42:49.525: INFO: Trying to dial the pod
Jun 22 12:42:54.542: INFO: Controller my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede: Got expected result from replica 1 [my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede-864w4]: "my-hostname-basic-ee17879f-8f5c-446d-b998-a350e0845ede-864w4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:42:54.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5713" for this suite.

• [SLOW TEST:14.182 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":81,"skipped":1417,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:42:54.555: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:02.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-892" for this suite.

• [SLOW TEST:8.224 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1463,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:02.779: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:43:03.445: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 12:43:05.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962583, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962583, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962583, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962583, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:43:08.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:08.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1002" for this suite.
STEP: Destroying namespace "webhook-1002-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.944 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":83,"skipped":1482,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:08.724: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8401
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:43:08.861: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8401" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":84,"skipped":1486,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:09.926: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:43:10.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:43:12.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962590, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:43:15.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:15.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7319" for this suite.
STEP: Destroying namespace "webhook-7319-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":85,"skipped":1491,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:15.755: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 22 12:43:15.896: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4891" for this suite.

• [SLOW TEST:9.302 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":86,"skipped":1518,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:25.057: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:43:25.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6" in namespace "projected-3430" to be "Succeeded or Failed"
Jun 22 12:43:25.199: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765383ms
Jun 22 12:43:27.203: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007041361s
Jun 22 12:43:29.207: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011159757s
Jun 22 12:43:31.213: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016419896s
Jun 22 12:43:33.216: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020180391s
Jun 22 12:43:35.220: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024209369s
STEP: Saw pod success
Jun 22 12:43:35.220: INFO: Pod "downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6" satisfied condition "Succeeded or Failed"
Jun 22 12:43:35.223: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6 container client-container: <nil>
STEP: delete the pod
Jun 22 12:43:35.239: INFO: Waiting for pod downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6 to disappear
Jun 22 12:43:35.244: INFO: Pod downwardapi-volume-b7af6a75-805d-410d-b5b8-0f51c5c2b4c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:35.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3430" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1520,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:35.257: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:46.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8651" for this suite.

• [SLOW TEST:11.206 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":88,"skipped":1524,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:46.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6966" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":89,"skipped":1526,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:46.610: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 12:43:46.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2339 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 22 12:43:47.565: INFO: stderr: ""
Jun 22 12:43:47.565: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 22 12:43:52.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2339 get pod e2e-test-httpd-pod -o json'
Jun 22 12:43:52.691: INFO: stderr: ""
Jun 22 12:43:52.691: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-06-22T12:43:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-22T12:43:47Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"11.32.9.2\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-22T12:43:50Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2339\",\n        \"resourceVersion\": \"159231\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2339/pods/e2e-test-httpd-pod\",\n        \"uid\": \"2308a587-e354-4b3a-a69a-ef347d6686ba\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-glhsv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"06998c1b-9fed-44d7-827f-f702404ff383\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-glhsv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-glhsv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-22T12:43:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-22T12:43:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-22T12:43:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-22T12:43:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://46275ae53d194acfe04de1c851c65c06f8c2ff55982a78e166c9230ee862a40f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-22T12:43:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"11.0.1.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"11.32.9.2\",\n        \"podIPs\": [\n            {\n                \"ip\": \"11.32.9.2\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-22T12:43:47Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 22 12:43:52.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2339 replace -f -'
Jun 22 12:43:52.951: INFO: stderr: ""
Jun 22 12:43:52.951: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jun 22 12:43:52.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2339 delete pods e2e-test-httpd-pod'
Jun 22 12:43:54.669: INFO: stderr: ""
Jun 22 12:43:54.669: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:43:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2339" for this suite.

• [SLOW TEST:8.073 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":90,"skipped":1531,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:43:54.684: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:43:54.825: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c" in namespace "security-context-test-1324" to be "Succeeded or Failed"
Jun 22 12:43:54.831: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.803837ms
Jun 22 12:43:56.834: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009269119s
Jun 22 12:43:58.838: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012711985s
Jun 22 12:44:00.841: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016371589s
Jun 22 12:44:02.845: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019679218s
Jun 22 12:44:04.848: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.022946217s
Jun 22 12:44:04.848: INFO: Pod "busybox-user-65534-0df0a86c-23f2-4110-a093-14b3ae89e16c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:44:04.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1324" for this suite.

• [SLOW TEST:10.177 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1558,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:44:04.864: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:44:05.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0" in namespace "downward-api-6739" to be "Succeeded or Failed"
Jun 22 12:44:05.010: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482812ms
Jun 22 12:44:07.016: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009968329s
Jun 22 12:44:09.020: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014042685s
Jun 22 12:44:11.024: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018083054s
Jun 22 12:44:13.028: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021807196s
Jun 22 12:44:15.031: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025508703s
STEP: Saw pod success
Jun 22 12:44:15.031: INFO: Pod "downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0" satisfied condition "Succeeded or Failed"
Jun 22 12:44:15.034: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0 container client-container: <nil>
STEP: delete the pod
Jun 22 12:44:15.052: INFO: Waiting for pod downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0 to disappear
Jun 22 12:44:15.055: INFO: Pod downwardapi-volume-8c15e878-d473-49fb-bd92-68a3a09fe4d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:44:15.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6739" for this suite.

• [SLOW TEST:10.200 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":92,"skipped":1571,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:44:15.067: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 12:44:15.209: INFO: Waiting up to 5m0s for pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef" in namespace "emptydir-3547" to be "Succeeded or Failed"
Jun 22 12:44:15.220: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.116088ms
Jun 22 12:44:17.224: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01417318s
Jun 22 12:44:19.228: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018210868s
Jun 22 12:44:21.232: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022744785s
Jun 22 12:44:23.236: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026201399s
Jun 22 12:44:25.240: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.030108519s
STEP: Saw pod success
Jun 22 12:44:25.240: INFO: Pod "pod-18336a46-ac63-4f2f-a84b-db61b5e608ef" satisfied condition "Succeeded or Failed"
Jun 22 12:44:25.242: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-18336a46-ac63-4f2f-a84b-db61b5e608ef container test-container: <nil>
STEP: delete the pod
Jun 22 12:44:25.261: INFO: Waiting for pod pod-18336a46-ac63-4f2f-a84b-db61b5e608ef to disappear
Jun 22 12:44:25.264: INFO: Pod pod-18336a46-ac63-4f2f-a84b-db61b5e608ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:44:25.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3547" for this suite.

• [SLOW TEST:10.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1589,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:44:25.280: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5103.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5103.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5103.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5103.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5103.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5103.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 12:44:35.522: INFO: DNS probes using dns-5103/dns-test-df9f3127-8947-402e-8876-29a8f392e889 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:44:35.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5103" for this suite.

• [SLOW TEST:10.275 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":94,"skipped":1601,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:44:35.555: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-0f63b177-7143-4dd9-b60d-f0e9282a599b
STEP: Creating a pod to test consume secrets
Jun 22 12:44:35.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff" in namespace "projected-3374" to be "Succeeded or Failed"
Jun 22 12:44:35.712: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.24957ms
Jun 22 12:44:37.717: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014529624s
Jun 22 12:44:39.721: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019028014s
Jun 22 12:44:41.731: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029076919s
Jun 22 12:44:43.735: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032788386s
STEP: Saw pod success
Jun 22 12:44:43.735: INFO: Pod "pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff" satisfied condition "Succeeded or Failed"
Jun 22 12:44:43.738: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:44:43.753: INFO: Waiting for pod pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff to disappear
Jun 22 12:44:43.758: INFO: Pod pod-projected-secrets-639cccb9-cdbf-4f47-80d1-68ea671231ff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:44:43.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3374" for this suite.

• [SLOW TEST:8.214 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1612,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:44:43.772: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-8998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun 22 12:44:43.906: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 12:45:43.938: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:45:43.942: INFO: Starting informer...
STEP: Starting pods...
Jun 22 12:45:44.160: INFO: Pod1 is running on 06998c1b-9fed-44d7-827f-f702404ff383. Tainting Node
Jun 22 12:46:38.386: INFO: Pod2 is running on 06998c1b-9fed-44d7-827f-f702404ff383. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 22 12:46:45.956: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 22 12:47:11.874: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:47:11.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8998" for this suite.

• [SLOW TEST:148.189 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":96,"skipped":1627,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:47:11.962: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6713
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jun 22 12:47:12.103: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-6713 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:47:12.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6713" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":97,"skipped":1639,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:47:12.179: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 22 12:47:12.324: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 12:48:12.360: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 22 12:48:12.383: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 22 12:48:12.418: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 22 12:48:12.443: INFO: Created pod: pod2-sched-preemption-medium-priority
Jun 22 12:48:12.470: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:48:40.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9546" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:88.424 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":98,"skipped":1639,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:48:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jun 22 12:48:40.747: INFO: created test-event-1
Jun 22 12:48:40.750: INFO: created test-event-2
Jun 22 12:48:40.754: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 22 12:48:40.759: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 22 12:48:40.777: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:48:40.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5970" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":99,"skipped":1669,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:48:40.800: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8220
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 12:48:40.984: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 22 12:48:41.045: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:48:43.050: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:48:45.050: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:48:47.049: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 12:48:49.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:48:51.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:48:53.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:48:55.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:48:57.072: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 12:48:59.049: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 22 12:48:59.055: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:49:01.059: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:49:03.059: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:49:05.059: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 22 12:49:07.059: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 22 12:49:07.065: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 22 12:49:09.068: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 22 12:49:09.074: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:49:11.077: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:49:13.077: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:49:15.077: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 12:49:17.077: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun 22 12:49:21.125: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.32.8.3:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:49:21.125: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:49:21.225: INFO: Found all expected endpoints: [netserver-0]
Jun 22 12:49:21.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.32.8.2:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:49:21.228: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:49:21.326: INFO: Found all expected endpoints: [netserver-1]
Jun 22 12:49:21.330: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.32.8.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:49:21.330: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:49:21.428: INFO: Found all expected endpoints: [netserver-2]
Jun 22 12:49:21.431: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.32.8.4:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:49:21.431: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:49:21.521: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:21.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8220" for this suite.

• [SLOW TEST:40.735 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1690,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:21.540: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:49:21.988: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 12:49:23.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962962, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962962, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962962, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962961, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:49:27.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:49:27.014: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-451-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:28.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4505" for this suite.
STEP: Destroying namespace "webhook-4505-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":101,"skipped":1738,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:28.224: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-1941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:49:28.615: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 22 12:49:28.617: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 22 12:49:28.617: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.617: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 22 12:49:28.617: INFO: Checking APIGroup: extensions
Jun 22 12:49:28.619: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun 22 12:49:28.619: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun 22 12:49:28.619: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun 22 12:49:28.619: INFO: Checking APIGroup: apps
Jun 22 12:49:28.621: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 22 12:49:28.621: INFO: Versions found [{apps/v1 v1}]
Jun 22 12:49:28.621: INFO: apps/v1 matches apps/v1
Jun 22 12:49:28.621: INFO: Checking APIGroup: events.k8s.io
Jun 22 12:49:28.623: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 22 12:49:28.623: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.623: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 22 12:49:28.623: INFO: Checking APIGroup: authentication.k8s.io
Jun 22 12:49:28.625: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 22 12:49:28.625: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.625: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 22 12:49:28.625: INFO: Checking APIGroup: authorization.k8s.io
Jun 22 12:49:28.627: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 22 12:49:28.627: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.627: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 22 12:49:28.627: INFO: Checking APIGroup: autoscaling
Jun 22 12:49:28.630: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 22 12:49:28.630: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 22 12:49:28.630: INFO: autoscaling/v1 matches autoscaling/v1
Jun 22 12:49:28.630: INFO: Checking APIGroup: batch
Jun 22 12:49:28.632: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 22 12:49:28.632: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 22 12:49:28.632: INFO: batch/v1 matches batch/v1
Jun 22 12:49:28.632: INFO: Checking APIGroup: certificates.k8s.io
Jun 22 12:49:28.634: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 22 12:49:28.634: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.634: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 22 12:49:28.634: INFO: Checking APIGroup: networking.k8s.io
Jun 22 12:49:28.636: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 22 12:49:28.636: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.636: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 22 12:49:28.636: INFO: Checking APIGroup: policy
Jun 22 12:49:28.638: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun 22 12:49:28.638: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun 22 12:49:28.638: INFO: policy/v1beta1 matches policy/v1beta1
Jun 22 12:49:28.638: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 22 12:49:28.640: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 22 12:49:28.640: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.640: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 22 12:49:28.640: INFO: Checking APIGroup: storage.k8s.io
Jun 22 12:49:28.642: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 22 12:49:28.642: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.642: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 22 12:49:28.642: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 22 12:49:28.644: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 22 12:49:28.644: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.644: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 22 12:49:28.644: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 22 12:49:28.645: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 22 12:49:28.645: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.645: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 22 12:49:28.645: INFO: Checking APIGroup: scheduling.k8s.io
Jun 22 12:49:28.647: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 22 12:49:28.647: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.647: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 22 12:49:28.647: INFO: Checking APIGroup: coordination.k8s.io
Jun 22 12:49:28.649: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 22 12:49:28.649: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.649: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 22 12:49:28.649: INFO: Checking APIGroup: node.k8s.io
Jun 22 12:49:28.651: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jun 22 12:49:28.651: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.651: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jun 22 12:49:28.651: INFO: Checking APIGroup: discovery.k8s.io
Jun 22 12:49:28.653: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun 22 12:49:28.653: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.653: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun 22 12:49:28.653: INFO: Checking APIGroup: nsx.vmware.com
Jun 22 12:49:28.655: INFO: PreferredVersion.GroupVersion: nsx.vmware.com/v1
Jun 22 12:49:28.655: INFO: Versions found [{nsx.vmware.com/v1 v1}]
Jun 22 12:49:28.655: INFO: nsx.vmware.com/v1 matches nsx.vmware.com/v1
Jun 22 12:49:28.655: INFO: Checking APIGroup: vmware.com
Jun 22 12:49:28.657: INFO: PreferredVersion.GroupVersion: vmware.com/v1alpha1
Jun 22 12:49:28.657: INFO: Versions found [{vmware.com/v1alpha1 v1alpha1}]
Jun 22 12:49:28.657: INFO: vmware.com/v1alpha1 matches vmware.com/v1alpha1
Jun 22 12:49:28.657: INFO: Checking APIGroup: pksapi.io
Jun 22 12:49:28.659: INFO: PreferredVersion.GroupVersion: pksapi.io/v1beta1
Jun 22 12:49:28.659: INFO: Versions found [{pksapi.io/v1beta1 v1beta1}]
Jun 22 12:49:28.659: INFO: pksapi.io/v1beta1 matches pksapi.io/v1beta1
Jun 22 12:49:28.659: INFO: Checking APIGroup: metrics.k8s.io
Jun 22 12:49:28.661: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 22 12:49:28.661: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 22 12:49:28.661: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:28.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1941" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":102,"skipped":1746,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:28.676: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0ecfdc29-dd5c-4d6f-a28d-d6d4bfb5850b
STEP: Creating a pod to test consume configMaps
Jun 22 12:49:28.819: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9" in namespace "configmap-9996" to be "Succeeded or Failed"
Jun 22 12:49:28.824: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433741ms
Jun 22 12:49:30.828: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008766524s
Jun 22 12:49:32.832: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012846042s
Jun 22 12:49:34.836: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016615824s
Jun 22 12:49:36.839: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019988622s
Jun 22 12:49:38.844: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024086075s
STEP: Saw pod success
Jun 22 12:49:38.844: INFO: Pod "pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9" satisfied condition "Succeeded or Failed"
Jun 22 12:49:38.847: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:49:38.878: INFO: Waiting for pod pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9 to disappear
Jun 22 12:49:38.881: INFO: Pod pod-configmaps-ee4aa1a4-110c-41d5-b724-95ba5fff33c9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:38.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9996" for this suite.

• [SLOW TEST:10.217 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":103,"skipped":1764,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:49:39.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 12:49:41.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962979, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962979, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962979, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759962979, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:49:44.467: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:44.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3460" for this suite.
STEP: Destroying namespace "webhook-3460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":104,"skipped":1767,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:44.599: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 22 12:49:44.804: INFO: Waiting up to 5m0s for pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a" in namespace "downward-api-8690" to be "Succeeded or Failed"
Jun 22 12:49:44.808: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.511172ms
Jun 22 12:49:46.813: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009011075s
Jun 22 12:49:48.818: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014452601s
Jun 22 12:49:50.823: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019277289s
Jun 22 12:49:52.828: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02438259s
Jun 22 12:49:54.833: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.02880421s
STEP: Saw pod success
Jun 22 12:49:54.833: INFO: Pod "downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a" satisfied condition "Succeeded or Failed"
Jun 22 12:49:54.836: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a container dapi-container: <nil>
STEP: delete the pod
Jun 22 12:49:54.853: INFO: Waiting for pod downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a to disappear
Jun 22 12:49:54.858: INFO: Pod downward-api-65e6adac-f79b-421e-bc59-e1017ba6375a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:54.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8690" for this suite.

• [SLOW TEST:10.269 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1784,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:54.870: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jun 22 12:49:55.012: INFO: created test-pod-1
Jun 22 12:49:55.018: INFO: created test-pod-2
Jun 22 12:49:55.026: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:49:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4051" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":106,"skipped":1792,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:49:55.101: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 12:50:01.269: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 12:50:01.269943      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:50:01.269966      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:50:01.269972      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:50:01.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8852" for this suite.

• [SLOW TEST:6.180 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":107,"skipped":1842,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:50:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:01.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4103" for this suite.

• [SLOW TEST:60.157 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1893,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:51:01.583: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985" in namespace "downward-api-6339" to be "Succeeded or Failed"
Jun 22 12:51:01.587: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Pending", Reason="", readiness=false. Elapsed: 4.770844ms
Jun 22 12:51:03.591: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446753s
Jun 22 12:51:05.595: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012457569s
Jun 22 12:51:07.599: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01623712s
Jun 22 12:51:09.603: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020355997s
Jun 22 12:51:11.607: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024003614s
STEP: Saw pod success
Jun 22 12:51:11.607: INFO: Pod "downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985" satisfied condition "Succeeded or Failed"
Jun 22 12:51:11.613: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985 container client-container: <nil>
STEP: delete the pod
Jun 22 12:51:11.633: INFO: Waiting for pod downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985 to disappear
Jun 22 12:51:11.636: INFO: Pod downwardapi-volume-34ad1187-91f1-4f06-af30-8d71c149b985 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:11.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6339" for this suite.

• [SLOW TEST:10.209 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1908,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:11.648: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:23.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-988" for this suite.

• [SLOW TEST:12.179 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1911,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:23.828: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-4777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4777 to expose endpoints map[]
Jun 22 12:51:23.982: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun 22 12:51:24.993: INFO: successfully validated that service multi-endpoint-test in namespace services-4777 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4777 to expose endpoints map[pod1:[100]]
Jun 22 12:51:29.028: INFO: successfully validated that service multi-endpoint-test in namespace services-4777 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4777 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 22 12:51:33.062: INFO: successfully validated that service multi-endpoint-test in namespace services-4777 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-4777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4777 to expose endpoints map[pod2:[101]]
Jun 22 12:51:33.099: INFO: successfully validated that service multi-endpoint-test in namespace services-4777 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4777 to expose endpoints map[]
Jun 22 12:51:33.129: INFO: successfully validated that service multi-endpoint-test in namespace services-4777 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:33.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4777" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.327 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":111,"skipped":1919,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:33.155: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:33.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4329" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":112,"skipped":1932,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:33.326: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:51:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6239
I0622 12:51:33.521074      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6239, replica count: 1
I0622 12:51:34.571465      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:35.571757      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:36.571910      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:37.572121      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:38.572254      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:39.572438      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:51:40.572707      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 12:51:40.685: INFO: Created: latency-svc-qfqxj
Jun 22 12:51:40.690: INFO: Got endpoints: latency-svc-qfqxj [17.835149ms]
Jun 22 12:51:40.707: INFO: Created: latency-svc-hm4lj
Jun 22 12:51:40.707: INFO: Got endpoints: latency-svc-hm4lj [16.935286ms]
Jun 22 12:51:40.718: INFO: Created: latency-svc-4pkjt
Jun 22 12:51:40.725: INFO: Got endpoints: latency-svc-4pkjt [34.587947ms]
Jun 22 12:51:40.732: INFO: Created: latency-svc-8kjzp
Jun 22 12:51:40.742: INFO: Created: latency-svc-2w4gx
Jun 22 12:51:40.743: INFO: Got endpoints: latency-svc-8kjzp [52.219696ms]
Jun 22 12:51:40.752: INFO: Created: latency-svc-kgfds
Jun 22 12:51:40.755: INFO: Got endpoints: latency-svc-2w4gx [63.706597ms]
Jun 22 12:51:40.760: INFO: Got endpoints: latency-svc-kgfds [69.585429ms]
Jun 22 12:51:40.767: INFO: Created: latency-svc-smbkd
Jun 22 12:51:40.777: INFO: Got endpoints: latency-svc-smbkd [85.838431ms]
Jun 22 12:51:40.789: INFO: Created: latency-svc-rjrjg
Jun 22 12:51:40.789: INFO: Got endpoints: latency-svc-rjrjg [98.35672ms]
Jun 22 12:51:40.803: INFO: Created: latency-svc-cbvh9
Jun 22 12:51:40.806: INFO: Got endpoints: latency-svc-cbvh9 [115.473246ms]
Jun 22 12:51:40.807: INFO: Created: latency-svc-pr6np
Jun 22 12:51:40.815: INFO: Got endpoints: latency-svc-pr6np [124.186858ms]
Jun 22 12:51:40.817: INFO: Created: latency-svc-zf98w
Jun 22 12:51:40.825: INFO: Got endpoints: latency-svc-zf98w [134.146399ms]
Jun 22 12:51:40.837: INFO: Created: latency-svc-2dckt
Jun 22 12:51:40.842: INFO: Got endpoints: latency-svc-2dckt [150.665519ms]
Jun 22 12:51:40.843: INFO: Created: latency-svc-xzbcp
Jun 22 12:51:40.850: INFO: Got endpoints: latency-svc-xzbcp [159.073118ms]
Jun 22 12:51:40.877: INFO: Created: latency-svc-fxbtl
Jun 22 12:51:40.878: INFO: Got endpoints: latency-svc-fxbtl [187.045802ms]
Jun 22 12:51:40.881: INFO: Created: latency-svc-dd94k
Jun 22 12:51:40.886: INFO: Created: latency-svc-vn2ln
Jun 22 12:51:40.886: INFO: Got endpoints: latency-svc-dd94k [194.9486ms]
Jun 22 12:51:40.898: INFO: Got endpoints: latency-svc-vn2ln [206.883434ms]
Jun 22 12:51:40.907: INFO: Created: latency-svc-hj8hr
Jun 22 12:51:40.907: INFO: Got endpoints: latency-svc-hj8hr [199.408761ms]
Jun 22 12:51:40.913: INFO: Created: latency-svc-4m99r
Jun 22 12:51:40.926: INFO: Got endpoints: latency-svc-4m99r [200.315095ms]
Jun 22 12:51:40.927: INFO: Created: latency-svc-mfc57
Jun 22 12:51:40.935: INFO: Got endpoints: latency-svc-mfc57 [191.823407ms]
Jun 22 12:51:40.935: INFO: Created: latency-svc-zjgqn
Jun 22 12:51:40.943: INFO: Got endpoints: latency-svc-zjgqn [188.238265ms]
Jun 22 12:51:40.943: INFO: Created: latency-svc-jnkgm
Jun 22 12:51:40.951: INFO: Got endpoints: latency-svc-jnkgm [190.375625ms]
Jun 22 12:51:40.953: INFO: Created: latency-svc-hqw5k
Jun 22 12:51:40.963: INFO: Got endpoints: latency-svc-hqw5k [186.127199ms]
Jun 22 12:51:40.970: INFO: Created: latency-svc-wrxsc
Jun 22 12:51:40.978: INFO: Got endpoints: latency-svc-wrxsc [188.384196ms]
Jun 22 12:51:40.986: INFO: Created: latency-svc-g5j89
Jun 22 12:51:40.995: INFO: Got endpoints: latency-svc-g5j89 [188.595288ms]
Jun 22 12:51:40.998: INFO: Created: latency-svc-nppnr
Jun 22 12:51:41.004: INFO: Got endpoints: latency-svc-nppnr [188.956163ms]
Jun 22 12:51:41.015: INFO: Created: latency-svc-fqv4z
Jun 22 12:51:41.017: INFO: Created: latency-svc-ldzzf
Jun 22 12:51:41.025: INFO: Got endpoints: latency-svc-fqv4z [200.259512ms]
Jun 22 12:51:41.036: INFO: Got endpoints: latency-svc-ldzzf [194.335615ms]
Jun 22 12:51:41.040: INFO: Created: latency-svc-p4sg7
Jun 22 12:51:41.043: INFO: Got endpoints: latency-svc-p4sg7 [192.697226ms]
Jun 22 12:51:41.055: INFO: Created: latency-svc-rxchl
Jun 22 12:51:41.055: INFO: Created: latency-svc-7r5lz
Jun 22 12:51:41.065: INFO: Created: latency-svc-8np7l
Jun 22 12:51:41.076: INFO: Got endpoints: latency-svc-rxchl [197.549759ms]
Jun 22 12:51:41.076: INFO: Created: latency-svc-tbmnz
Jun 22 12:51:41.076: INFO: Got endpoints: latency-svc-8np7l [190.341327ms]
Jun 22 12:51:41.076: INFO: Got endpoints: latency-svc-7r5lz [40.12723ms]
Jun 22 12:51:41.087: INFO: Got endpoints: latency-svc-tbmnz [189.342954ms]
Jun 22 12:51:41.092: INFO: Created: latency-svc-vbzwz
Jun 22 12:51:41.100: INFO: Got endpoints: latency-svc-vbzwz [193.090437ms]
Jun 22 12:51:41.104: INFO: Created: latency-svc-svmc7
Jun 22 12:51:41.110: INFO: Got endpoints: latency-svc-svmc7 [184.825559ms]
Jun 22 12:51:41.118: INFO: Created: latency-svc-z9rbr
Jun 22 12:51:41.129: INFO: Got endpoints: latency-svc-z9rbr [194.478281ms]
Jun 22 12:51:41.132: INFO: Created: latency-svc-vhksn
Jun 22 12:51:41.140: INFO: Got endpoints: latency-svc-vhksn [196.878548ms]
Jun 22 12:51:41.140: INFO: Created: latency-svc-t788t
Jun 22 12:51:41.149: INFO: Got endpoints: latency-svc-t788t [197.670751ms]
Jun 22 12:51:41.153: INFO: Created: latency-svc-2z2t6
Jun 22 12:51:41.159: INFO: Got endpoints: latency-svc-2z2t6 [196.322102ms]
Jun 22 12:51:41.169: INFO: Created: latency-svc-x7p7d
Jun 22 12:51:41.172: INFO: Created: latency-svc-xxlcs
Jun 22 12:51:41.180: INFO: Got endpoints: latency-svc-x7p7d [202.440628ms]
Jun 22 12:51:41.191: INFO: Got endpoints: latency-svc-xxlcs [196.410052ms]
Jun 22 12:51:41.196: INFO: Created: latency-svc-tcmq6
Jun 22 12:51:41.198: INFO: Created: latency-svc-pkl59
Jun 22 12:51:41.211: INFO: Created: latency-svc-cn64j
Jun 22 12:51:41.211: INFO: Created: latency-svc-cx29r
Jun 22 12:51:41.212: INFO: Created: latency-svc-bh7hx
Jun 22 12:51:41.221: INFO: Created: latency-svc-8tqwg
Jun 22 12:51:41.225: INFO: Created: latency-svc-pgbzd
Jun 22 12:51:41.237: INFO: Got endpoints: latency-svc-pkl59 [232.692506ms]
Jun 22 12:51:41.238: INFO: Created: latency-svc-5bb2g
Jun 22 12:51:41.250: INFO: Created: latency-svc-x6z74
Jun 22 12:51:41.254: INFO: Created: latency-svc-2gwnv
Jun 22 12:51:41.265: INFO: Created: latency-svc-fj2w2
Jun 22 12:51:41.280: INFO: Created: latency-svc-g2bcc
Jun 22 12:51:41.284: INFO: Created: latency-svc-bct89
Jun 22 12:51:41.289: INFO: Got endpoints: latency-svc-tcmq6 [263.657864ms]
Jun 22 12:51:41.294: INFO: Created: latency-svc-k88v5
Jun 22 12:51:41.309: INFO: Created: latency-svc-j47kl
Jun 22 12:51:41.311: INFO: Created: latency-svc-cv27m
Jun 22 12:51:41.318: INFO: Created: latency-svc-lpzr2
Jun 22 12:51:41.335: INFO: Got endpoints: latency-svc-cn64j [292.58503ms]
Jun 22 12:51:41.347: INFO: Created: latency-svc-n2mjc
Jun 22 12:51:41.386: INFO: Got endpoints: latency-svc-cx29r [309.351096ms]
Jun 22 12:51:41.395: INFO: Created: latency-svc-nfp9t
Jun 22 12:51:41.437: INFO: Got endpoints: latency-svc-bh7hx [360.955594ms]
Jun 22 12:51:41.447: INFO: Created: latency-svc-xmtpd
Jun 22 12:51:41.488: INFO: Got endpoints: latency-svc-8tqwg [411.800566ms]
Jun 22 12:51:41.498: INFO: Created: latency-svc-qpndj
Jun 22 12:51:41.537: INFO: Got endpoints: latency-svc-pgbzd [449.395433ms]
Jun 22 12:51:41.550: INFO: Created: latency-svc-qb7mk
Jun 22 12:51:41.585: INFO: Got endpoints: latency-svc-5bb2g [485.043074ms]
Jun 22 12:51:41.595: INFO: Created: latency-svc-hvfrf
Jun 22 12:51:41.636: INFO: Got endpoints: latency-svc-x6z74 [525.249879ms]
Jun 22 12:51:41.647: INFO: Created: latency-svc-fhbfl
Jun 22 12:51:41.705: INFO: Got endpoints: latency-svc-2gwnv [575.825233ms]
Jun 22 12:51:41.712: INFO: Created: latency-svc-g9kxb
Jun 22 12:51:41.736: INFO: Got endpoints: latency-svc-fj2w2 [595.491703ms]
Jun 22 12:51:41.746: INFO: Created: latency-svc-66jmw
Jun 22 12:51:41.789: INFO: Got endpoints: latency-svc-g2bcc [639.978683ms]
Jun 22 12:51:41.797: INFO: Created: latency-svc-9whbx
Jun 22 12:51:41.839: INFO: Got endpoints: latency-svc-bct89 [679.229991ms]
Jun 22 12:51:41.848: INFO: Created: latency-svc-hm2hb
Jun 22 12:51:41.886: INFO: Got endpoints: latency-svc-k88v5 [705.287401ms]
Jun 22 12:51:41.895: INFO: Created: latency-svc-27mq2
Jun 22 12:51:41.936: INFO: Got endpoints: latency-svc-j47kl [744.681907ms]
Jun 22 12:51:41.946: INFO: Created: latency-svc-gfhd2
Jun 22 12:51:41.987: INFO: Got endpoints: latency-svc-cv27m [749.9406ms]
Jun 22 12:51:41.994: INFO: Created: latency-svc-l2ffh
Jun 22 12:51:42.038: INFO: Got endpoints: latency-svc-lpzr2 [749.111211ms]
Jun 22 12:51:42.047: INFO: Created: latency-svc-58kr7
Jun 22 12:51:42.085: INFO: Got endpoints: latency-svc-n2mjc [749.694018ms]
Jun 22 12:51:42.095: INFO: Created: latency-svc-xs7nn
Jun 22 12:51:42.138: INFO: Got endpoints: latency-svc-nfp9t [751.617049ms]
Jun 22 12:51:42.148: INFO: Created: latency-svc-dwhgf
Jun 22 12:51:42.185: INFO: Got endpoints: latency-svc-xmtpd [748.031491ms]
Jun 22 12:51:42.198: INFO: Created: latency-svc-5vfll
Jun 22 12:51:42.235: INFO: Got endpoints: latency-svc-qpndj [746.524929ms]
Jun 22 12:51:42.251: INFO: Created: latency-svc-dn68c
Jun 22 12:51:42.285: INFO: Got endpoints: latency-svc-qb7mk [748.477965ms]
Jun 22 12:51:42.296: INFO: Created: latency-svc-xg9pd
Jun 22 12:51:42.337: INFO: Got endpoints: latency-svc-hvfrf [751.523125ms]
Jun 22 12:51:42.349: INFO: Created: latency-svc-s47sg
Jun 22 12:51:42.386: INFO: Got endpoints: latency-svc-fhbfl [749.901327ms]
Jun 22 12:51:42.394: INFO: Created: latency-svc-4h689
Jun 22 12:51:42.439: INFO: Got endpoints: latency-svc-g9kxb [733.765979ms]
Jun 22 12:51:42.447: INFO: Created: latency-svc-f8trz
Jun 22 12:51:42.485: INFO: Got endpoints: latency-svc-66jmw [749.582093ms]
Jun 22 12:51:42.495: INFO: Created: latency-svc-2njcc
Jun 22 12:51:42.535: INFO: Got endpoints: latency-svc-9whbx [746.272601ms]
Jun 22 12:51:42.547: INFO: Created: latency-svc-f7x55
Jun 22 12:51:42.586: INFO: Got endpoints: latency-svc-hm2hb [747.128003ms]
Jun 22 12:51:42.593: INFO: Created: latency-svc-jsgnq
Jun 22 12:51:42.638: INFO: Got endpoints: latency-svc-27mq2 [751.950812ms]
Jun 22 12:51:42.646: INFO: Created: latency-svc-nc86b
Jun 22 12:51:42.685: INFO: Got endpoints: latency-svc-gfhd2 [749.369084ms]
Jun 22 12:51:42.697: INFO: Created: latency-svc-qftjn
Jun 22 12:51:42.738: INFO: Got endpoints: latency-svc-l2ffh [750.624334ms]
Jun 22 12:51:42.748: INFO: Created: latency-svc-qh5k7
Jun 22 12:51:42.785: INFO: Got endpoints: latency-svc-58kr7 [746.491989ms]
Jun 22 12:51:42.792: INFO: Created: latency-svc-mztwr
Jun 22 12:51:42.836: INFO: Got endpoints: latency-svc-xs7nn [750.513918ms]
Jun 22 12:51:42.847: INFO: Created: latency-svc-mrbtz
Jun 22 12:51:42.886: INFO: Got endpoints: latency-svc-dwhgf [747.981929ms]
Jun 22 12:51:42.895: INFO: Created: latency-svc-pfp4j
Jun 22 12:51:42.935: INFO: Got endpoints: latency-svc-5vfll [749.760271ms]
Jun 22 12:51:42.947: INFO: Created: latency-svc-jrfmq
Jun 22 12:51:42.986: INFO: Got endpoints: latency-svc-dn68c [750.945605ms]
Jun 22 12:51:42.991: INFO: Created: latency-svc-6p8w9
Jun 22 12:51:43.035: INFO: Got endpoints: latency-svc-xg9pd [749.649103ms]
Jun 22 12:51:43.047: INFO: Created: latency-svc-xgnr7
Jun 22 12:51:43.085: INFO: Got endpoints: latency-svc-s47sg [748.745686ms]
Jun 22 12:51:43.100: INFO: Created: latency-svc-jn8qf
Jun 22 12:51:43.138: INFO: Got endpoints: latency-svc-4h689 [752.196728ms]
Jun 22 12:51:43.148: INFO: Created: latency-svc-b7xkb
Jun 22 12:51:43.186: INFO: Got endpoints: latency-svc-f8trz [746.877373ms]
Jun 22 12:51:43.197: INFO: Created: latency-svc-lsfzw
Jun 22 12:51:43.235: INFO: Got endpoints: latency-svc-2njcc [749.446984ms]
Jun 22 12:51:43.243: INFO: Created: latency-svc-ck2rp
Jun 22 12:51:43.286: INFO: Got endpoints: latency-svc-f7x55 [750.506746ms]
Jun 22 12:51:43.296: INFO: Created: latency-svc-zq4ss
Jun 22 12:51:43.337: INFO: Got endpoints: latency-svc-jsgnq [750.874418ms]
Jun 22 12:51:43.347: INFO: Created: latency-svc-hvt6x
Jun 22 12:51:43.386: INFO: Got endpoints: latency-svc-nc86b [748.428798ms]
Jun 22 12:51:43.396: INFO: Created: latency-svc-4jcv5
Jun 22 12:51:43.436: INFO: Got endpoints: latency-svc-qftjn [750.36393ms]
Jun 22 12:51:43.442: INFO: Created: latency-svc-2lx57
Jun 22 12:51:43.487: INFO: Got endpoints: latency-svc-qh5k7 [749.148646ms]
Jun 22 12:51:43.492: INFO: Created: latency-svc-bjd9x
Jun 22 12:51:43.538: INFO: Got endpoints: latency-svc-mztwr [753.36035ms]
Jun 22 12:51:43.547: INFO: Created: latency-svc-kw5dh
Jun 22 12:51:43.587: INFO: Got endpoints: latency-svc-mrbtz [751.304089ms]
Jun 22 12:51:43.599: INFO: Created: latency-svc-rj5hs
Jun 22 12:51:43.637: INFO: Got endpoints: latency-svc-pfp4j [751.857717ms]
Jun 22 12:51:43.647: INFO: Created: latency-svc-wj2jn
Jun 22 12:51:43.687: INFO: Got endpoints: latency-svc-jrfmq [752.280233ms]
Jun 22 12:51:43.693: INFO: Created: latency-svc-vfktc
Jun 22 12:51:43.735: INFO: Got endpoints: latency-svc-6p8w9 [749.474577ms]
Jun 22 12:51:43.753: INFO: Created: latency-svc-hxg69
Jun 22 12:51:43.788: INFO: Got endpoints: latency-svc-xgnr7 [753.427148ms]
Jun 22 12:51:43.799: INFO: Created: latency-svc-89wlq
Jun 22 12:51:43.841: INFO: Got endpoints: latency-svc-jn8qf [755.808249ms]
Jun 22 12:51:43.852: INFO: Created: latency-svc-nmfn6
Jun 22 12:51:43.885: INFO: Got endpoints: latency-svc-b7xkb [747.22534ms]
Jun 22 12:51:43.893: INFO: Created: latency-svc-7bsml
Jun 22 12:51:43.935: INFO: Got endpoints: latency-svc-lsfzw [748.634126ms]
Jun 22 12:51:43.953: INFO: Created: latency-svc-wh6kj
Jun 22 12:51:43.985: INFO: Got endpoints: latency-svc-ck2rp [750.434061ms]
Jun 22 12:51:43.997: INFO: Created: latency-svc-c7bj7
Jun 22 12:51:44.039: INFO: Got endpoints: latency-svc-zq4ss [753.448379ms]
Jun 22 12:51:44.049: INFO: Created: latency-svc-w765p
Jun 22 12:51:44.086: INFO: Got endpoints: latency-svc-hvt6x [749.531243ms]
Jun 22 12:51:44.094: INFO: Created: latency-svc-dwrcw
Jun 22 12:51:44.136: INFO: Got endpoints: latency-svc-4jcv5 [749.909641ms]
Jun 22 12:51:44.149: INFO: Created: latency-svc-q8b92
Jun 22 12:51:44.189: INFO: Got endpoints: latency-svc-2lx57 [752.592963ms]
Jun 22 12:51:44.201: INFO: Created: latency-svc-vt4d8
Jun 22 12:51:44.236: INFO: Got endpoints: latency-svc-bjd9x [749.487534ms]
Jun 22 12:51:44.248: INFO: Created: latency-svc-nwvwp
Jun 22 12:51:44.286: INFO: Got endpoints: latency-svc-kw5dh [747.471422ms]
Jun 22 12:51:44.293: INFO: Created: latency-svc-bn9lh
Jun 22 12:51:44.337: INFO: Got endpoints: latency-svc-rj5hs [749.737859ms]
Jun 22 12:51:44.352: INFO: Created: latency-svc-mlnq9
Jun 22 12:51:44.388: INFO: Got endpoints: latency-svc-wj2jn [750.302825ms]
Jun 22 12:51:44.400: INFO: Created: latency-svc-8hjx4
Jun 22 12:51:44.437: INFO: Got endpoints: latency-svc-vfktc [749.600366ms]
Jun 22 12:51:44.451: INFO: Created: latency-svc-6h2w7
Jun 22 12:51:44.487: INFO: Got endpoints: latency-svc-hxg69 [751.796874ms]
Jun 22 12:51:44.495: INFO: Created: latency-svc-2tx27
Jun 22 12:51:44.536: INFO: Got endpoints: latency-svc-89wlq [747.53033ms]
Jun 22 12:51:44.547: INFO: Created: latency-svc-k247f
Jun 22 12:51:44.585: INFO: Got endpoints: latency-svc-nmfn6 [743.636528ms]
Jun 22 12:51:44.596: INFO: Created: latency-svc-pv45c
Jun 22 12:51:44.635: INFO: Got endpoints: latency-svc-7bsml [749.546716ms]
Jun 22 12:51:44.646: INFO: Created: latency-svc-kmlk7
Jun 22 12:51:44.687: INFO: Got endpoints: latency-svc-wh6kj [751.328618ms]
Jun 22 12:51:44.702: INFO: Created: latency-svc-dtgnx
Jun 22 12:51:44.738: INFO: Got endpoints: latency-svc-c7bj7 [752.219582ms]
Jun 22 12:51:44.746: INFO: Created: latency-svc-dxcrj
Jun 22 12:51:44.786: INFO: Got endpoints: latency-svc-w765p [747.008466ms]
Jun 22 12:51:44.798: INFO: Created: latency-svc-whfq6
Jun 22 12:51:44.838: INFO: Got endpoints: latency-svc-dwrcw [751.792571ms]
Jun 22 12:51:44.850: INFO: Created: latency-svc-lcqt7
Jun 22 12:51:44.885: INFO: Got endpoints: latency-svc-q8b92 [749.238149ms]
Jun 22 12:51:44.891: INFO: Created: latency-svc-k428v
Jun 22 12:51:44.936: INFO: Got endpoints: latency-svc-vt4d8 [747.460626ms]
Jun 22 12:51:44.946: INFO: Created: latency-svc-r4s82
Jun 22 12:51:44.986: INFO: Got endpoints: latency-svc-nwvwp [749.609998ms]
Jun 22 12:51:44.996: INFO: Created: latency-svc-m4rts
Jun 22 12:51:45.036: INFO: Got endpoints: latency-svc-bn9lh [750.29731ms]
Jun 22 12:51:45.048: INFO: Created: latency-svc-jfhxc
Jun 22 12:51:45.091: INFO: Got endpoints: latency-svc-mlnq9 [754.601872ms]
Jun 22 12:51:45.103: INFO: Created: latency-svc-d7kx7
Jun 22 12:51:45.138: INFO: Got endpoints: latency-svc-8hjx4 [750.197317ms]
Jun 22 12:51:45.147: INFO: Created: latency-svc-j2xws
Jun 22 12:51:45.185: INFO: Got endpoints: latency-svc-6h2w7 [748.490368ms]
Jun 22 12:51:45.196: INFO: Created: latency-svc-l2h8q
Jun 22 12:51:45.234: INFO: Got endpoints: latency-svc-2tx27 [747.206406ms]
Jun 22 12:51:45.249: INFO: Created: latency-svc-jv2xk
Jun 22 12:51:45.286: INFO: Got endpoints: latency-svc-k247f [749.448683ms]
Jun 22 12:51:45.296: INFO: Created: latency-svc-bjh2v
Jun 22 12:51:45.336: INFO: Got endpoints: latency-svc-pv45c [751.379178ms]
Jun 22 12:51:45.347: INFO: Created: latency-svc-pjhfw
Jun 22 12:51:45.386: INFO: Got endpoints: latency-svc-kmlk7 [751.360371ms]
Jun 22 12:51:45.399: INFO: Created: latency-svc-hsjt8
Jun 22 12:51:45.436: INFO: Got endpoints: latency-svc-dtgnx [749.772524ms]
Jun 22 12:51:45.443: INFO: Created: latency-svc-42fnv
Jun 22 12:51:45.488: INFO: Got endpoints: latency-svc-dxcrj [750.765683ms]
Jun 22 12:51:45.497: INFO: Created: latency-svc-x2nqw
Jun 22 12:51:45.536: INFO: Got endpoints: latency-svc-whfq6 [749.309019ms]
Jun 22 12:51:45.546: INFO: Created: latency-svc-rhfmw
Jun 22 12:51:45.586: INFO: Got endpoints: latency-svc-lcqt7 [747.675245ms]
Jun 22 12:51:45.597: INFO: Created: latency-svc-kxmhz
Jun 22 12:51:45.638: INFO: Got endpoints: latency-svc-k428v [752.551567ms]
Jun 22 12:51:45.645: INFO: Created: latency-svc-8kn2f
Jun 22 12:51:45.685: INFO: Got endpoints: latency-svc-r4s82 [748.407938ms]
Jun 22 12:51:45.697: INFO: Created: latency-svc-pw4hp
Jun 22 12:51:45.738: INFO: Got endpoints: latency-svc-m4rts [751.946271ms]
Jun 22 12:51:45.747: INFO: Created: latency-svc-4t2ll
Jun 22 12:51:45.785: INFO: Got endpoints: latency-svc-jfhxc [748.903165ms]
Jun 22 12:51:45.798: INFO: Created: latency-svc-z9n67
Jun 22 12:51:45.836: INFO: Got endpoints: latency-svc-d7kx7 [744.20946ms]
Jun 22 12:51:45.843: INFO: Created: latency-svc-79bnd
Jun 22 12:51:45.886: INFO: Got endpoints: latency-svc-j2xws [747.852913ms]
Jun 22 12:51:45.896: INFO: Created: latency-svc-jfngh
Jun 22 12:51:45.936: INFO: Got endpoints: latency-svc-l2h8q [751.228366ms]
Jun 22 12:51:45.947: INFO: Created: latency-svc-jbww4
Jun 22 12:51:45.986: INFO: Got endpoints: latency-svc-jv2xk [752.026049ms]
Jun 22 12:51:45.997: INFO: Created: latency-svc-5vlrc
Jun 22 12:51:46.037: INFO: Got endpoints: latency-svc-bjh2v [751.27875ms]
Jun 22 12:51:46.051: INFO: Created: latency-svc-mm9dd
Jun 22 12:51:46.086: INFO: Got endpoints: latency-svc-pjhfw [749.223466ms]
Jun 22 12:51:46.095: INFO: Created: latency-svc-cqxgz
Jun 22 12:51:46.137: INFO: Got endpoints: latency-svc-hsjt8 [750.947998ms]
Jun 22 12:51:46.146: INFO: Created: latency-svc-wcsmh
Jun 22 12:51:46.185: INFO: Got endpoints: latency-svc-42fnv [748.568964ms]
Jun 22 12:51:46.196: INFO: Created: latency-svc-rmxxn
Jun 22 12:51:46.236: INFO: Got endpoints: latency-svc-x2nqw [747.368305ms]
Jun 22 12:51:46.243: INFO: Created: latency-svc-kfw7k
Jun 22 12:51:46.285: INFO: Got endpoints: latency-svc-rhfmw [748.753531ms]
Jun 22 12:51:46.296: INFO: Created: latency-svc-zsxl7
Jun 22 12:51:46.337: INFO: Got endpoints: latency-svc-kxmhz [751.376492ms]
Jun 22 12:51:46.345: INFO: Created: latency-svc-wkvs6
Jun 22 12:51:46.387: INFO: Got endpoints: latency-svc-8kn2f [748.827659ms]
Jun 22 12:51:46.397: INFO: Created: latency-svc-wxjp4
Jun 22 12:51:46.435: INFO: Got endpoints: latency-svc-pw4hp [750.153601ms]
Jun 22 12:51:46.441: INFO: Created: latency-svc-42hwn
Jun 22 12:51:46.487: INFO: Got endpoints: latency-svc-4t2ll [749.227179ms]
Jun 22 12:51:46.495: INFO: Created: latency-svc-smw4z
Jun 22 12:51:46.535: INFO: Got endpoints: latency-svc-z9n67 [749.740945ms]
Jun 22 12:51:46.546: INFO: Created: latency-svc-gdhtn
Jun 22 12:51:46.586: INFO: Got endpoints: latency-svc-79bnd [750.448134ms]
Jun 22 12:51:46.598: INFO: Created: latency-svc-2ws6t
Jun 22 12:51:46.636: INFO: Got endpoints: latency-svc-jfngh [749.404564ms]
Jun 22 12:51:46.646: INFO: Created: latency-svc-kgdhp
Jun 22 12:51:46.689: INFO: Got endpoints: latency-svc-jbww4 [752.581398ms]
Jun 22 12:51:46.695: INFO: Created: latency-svc-nb52l
Jun 22 12:51:46.736: INFO: Got endpoints: latency-svc-5vlrc [749.86235ms]
Jun 22 12:51:46.751: INFO: Created: latency-svc-bj66b
Jun 22 12:51:46.789: INFO: Got endpoints: latency-svc-mm9dd [751.69432ms]
Jun 22 12:51:46.802: INFO: Created: latency-svc-gzt8p
Jun 22 12:51:46.841: INFO: Got endpoints: latency-svc-cqxgz [755.070868ms]
Jun 22 12:51:46.849: INFO: Created: latency-svc-bkzzd
Jun 22 12:51:46.891: INFO: Got endpoints: latency-svc-wcsmh [753.674427ms]
Jun 22 12:51:46.897: INFO: Created: latency-svc-x5fw2
Jun 22 12:51:46.938: INFO: Got endpoints: latency-svc-rmxxn [752.940713ms]
Jun 22 12:51:46.945: INFO: Created: latency-svc-95ddb
Jun 22 12:51:46.987: INFO: Got endpoints: latency-svc-kfw7k [751.127828ms]
Jun 22 12:51:47.001: INFO: Created: latency-svc-9qrz9
Jun 22 12:51:47.036: INFO: Got endpoints: latency-svc-zsxl7 [751.024009ms]
Jun 22 12:51:47.048: INFO: Created: latency-svc-hlg82
Jun 22 12:51:47.087: INFO: Got endpoints: latency-svc-wkvs6 [749.309284ms]
Jun 22 12:51:47.102: INFO: Created: latency-svc-njw8g
Jun 22 12:51:47.135: INFO: Got endpoints: latency-svc-wxjp4 [748.088586ms]
Jun 22 12:51:47.145: INFO: Created: latency-svc-97rqb
Jun 22 12:51:47.186: INFO: Got endpoints: latency-svc-42hwn [750.964036ms]
Jun 22 12:51:47.200: INFO: Created: latency-svc-mnztz
Jun 22 12:51:47.236: INFO: Got endpoints: latency-svc-smw4z [748.587674ms]
Jun 22 12:51:47.246: INFO: Created: latency-svc-fq7lv
Jun 22 12:51:47.287: INFO: Got endpoints: latency-svc-gdhtn [752.129513ms]
Jun 22 12:51:47.293: INFO: Created: latency-svc-dq48f
Jun 22 12:51:47.336: INFO: Got endpoints: latency-svc-2ws6t [749.543496ms]
Jun 22 12:51:47.347: INFO: Created: latency-svc-ggmrp
Jun 22 12:51:47.384: INFO: Got endpoints: latency-svc-kgdhp [748.837546ms]
Jun 22 12:51:47.395: INFO: Created: latency-svc-8dg9g
Jun 22 12:51:47.436: INFO: Got endpoints: latency-svc-nb52l [746.442254ms]
Jun 22 12:51:47.447: INFO: Created: latency-svc-knpq4
Jun 22 12:51:47.485: INFO: Got endpoints: latency-svc-bj66b [749.089679ms]
Jun 22 12:51:47.492: INFO: Created: latency-svc-7rmvv
Jun 22 12:51:47.537: INFO: Got endpoints: latency-svc-gzt8p [747.94082ms]
Jun 22 12:51:47.547: INFO: Created: latency-svc-4kpbv
Jun 22 12:51:47.587: INFO: Got endpoints: latency-svc-bkzzd [746.671804ms]
Jun 22 12:51:47.597: INFO: Created: latency-svc-tvb7q
Jun 22 12:51:47.636: INFO: Got endpoints: latency-svc-x5fw2 [744.517327ms]
Jun 22 12:51:47.648: INFO: Created: latency-svc-tn9jw
Jun 22 12:51:47.685: INFO: Got endpoints: latency-svc-95ddb [747.477134ms]
Jun 22 12:51:47.691: INFO: Created: latency-svc-wkgc6
Jun 22 12:51:47.736: INFO: Got endpoints: latency-svc-9qrz9 [749.034511ms]
Jun 22 12:51:47.749: INFO: Created: latency-svc-gmr9f
Jun 22 12:51:47.785: INFO: Got endpoints: latency-svc-hlg82 [749.572771ms]
Jun 22 12:51:47.798: INFO: Created: latency-svc-v562x
Jun 22 12:51:47.838: INFO: Got endpoints: latency-svc-njw8g [751.474464ms]
Jun 22 12:51:47.845: INFO: Created: latency-svc-x2frb
Jun 22 12:51:47.885: INFO: Got endpoints: latency-svc-97rqb [749.810348ms]
Jun 22 12:51:47.896: INFO: Created: latency-svc-9tlfz
Jun 22 12:51:47.941: INFO: Got endpoints: latency-svc-mnztz [755.194108ms]
Jun 22 12:51:47.947: INFO: Created: latency-svc-d5mdw
Jun 22 12:51:47.985: INFO: Got endpoints: latency-svc-fq7lv [748.926535ms]
Jun 22 12:51:47.997: INFO: Created: latency-svc-v67ks
Jun 22 12:51:48.036: INFO: Got endpoints: latency-svc-dq48f [748.84827ms]
Jun 22 12:51:48.043: INFO: Created: latency-svc-hg4lk
Jun 22 12:51:48.086: INFO: Got endpoints: latency-svc-ggmrp [749.89717ms]
Jun 22 12:51:48.096: INFO: Created: latency-svc-9ph9z
Jun 22 12:51:48.134: INFO: Got endpoints: latency-svc-8dg9g [749.906143ms]
Jun 22 12:51:48.147: INFO: Created: latency-svc-4z8f9
Jun 22 12:51:48.187: INFO: Got endpoints: latency-svc-knpq4 [751.237807ms]
Jun 22 12:51:48.197: INFO: Created: latency-svc-gl84d
Jun 22 12:51:48.240: INFO: Got endpoints: latency-svc-7rmvv [755.016861ms]
Jun 22 12:51:48.250: INFO: Created: latency-svc-mh8vp
Jun 22 12:51:48.287: INFO: Got endpoints: latency-svc-4kpbv [750.410052ms]
Jun 22 12:51:48.295: INFO: Created: latency-svc-zmsxd
Jun 22 12:51:48.336: INFO: Got endpoints: latency-svc-tvb7q [749.026671ms]
Jun 22 12:51:48.348: INFO: Created: latency-svc-jfxcm
Jun 22 12:51:48.388: INFO: Got endpoints: latency-svc-tn9jw [751.84498ms]
Jun 22 12:51:48.397: INFO: Created: latency-svc-8mwsr
Jun 22 12:51:48.437: INFO: Got endpoints: latency-svc-wkgc6 [751.051361ms]
Jun 22 12:51:48.447: INFO: Created: latency-svc-86ljf
Jun 22 12:51:48.485: INFO: Got endpoints: latency-svc-gmr9f [748.165917ms]
Jun 22 12:51:48.491: INFO: Created: latency-svc-h5mdk
Jun 22 12:51:48.535: INFO: Got endpoints: latency-svc-v562x [749.795574ms]
Jun 22 12:51:48.585: INFO: Got endpoints: latency-svc-x2frb [746.729178ms]
Jun 22 12:51:48.636: INFO: Got endpoints: latency-svc-9tlfz [750.701465ms]
Jun 22 12:51:48.687: INFO: Got endpoints: latency-svc-d5mdw [746.008346ms]
Jun 22 12:51:48.736: INFO: Got endpoints: latency-svc-v67ks [750.587692ms]
Jun 22 12:51:48.785: INFO: Got endpoints: latency-svc-hg4lk [749.228896ms]
Jun 22 12:51:48.838: INFO: Got endpoints: latency-svc-9ph9z [752.257175ms]
Jun 22 12:51:48.887: INFO: Got endpoints: latency-svc-4z8f9 [752.122517ms]
Jun 22 12:51:48.937: INFO: Got endpoints: latency-svc-gl84d [749.69291ms]
Jun 22 12:51:48.988: INFO: Got endpoints: latency-svc-mh8vp [747.303694ms]
Jun 22 12:51:49.037: INFO: Got endpoints: latency-svc-zmsxd [750.019778ms]
Jun 22 12:51:49.086: INFO: Got endpoints: latency-svc-jfxcm [749.791439ms]
Jun 22 12:51:49.135: INFO: Got endpoints: latency-svc-8mwsr [747.765096ms]
Jun 22 12:51:49.186: INFO: Got endpoints: latency-svc-86ljf [749.383187ms]
Jun 22 12:51:49.238: INFO: Got endpoints: latency-svc-h5mdk [753.872136ms]
Jun 22 12:51:49.239: INFO: Latencies: [16.935286ms 34.587947ms 40.12723ms 52.219696ms 63.706597ms 69.585429ms 85.838431ms 98.35672ms 115.473246ms 124.186858ms 134.146399ms 150.665519ms 159.073118ms 184.825559ms 186.127199ms 187.045802ms 188.238265ms 188.384196ms 188.595288ms 188.956163ms 189.342954ms 190.341327ms 190.375625ms 191.823407ms 192.697226ms 193.090437ms 194.335615ms 194.478281ms 194.9486ms 196.322102ms 196.410052ms 196.878548ms 197.549759ms 197.670751ms 199.408761ms 200.259512ms 200.315095ms 202.440628ms 206.883434ms 232.692506ms 263.657864ms 292.58503ms 309.351096ms 360.955594ms 411.800566ms 449.395433ms 485.043074ms 525.249879ms 575.825233ms 595.491703ms 639.978683ms 679.229991ms 705.287401ms 733.765979ms 743.636528ms 744.20946ms 744.517327ms 744.681907ms 746.008346ms 746.272601ms 746.442254ms 746.491989ms 746.524929ms 746.671804ms 746.729178ms 746.877373ms 747.008466ms 747.128003ms 747.206406ms 747.22534ms 747.303694ms 747.368305ms 747.460626ms 747.471422ms 747.477134ms 747.53033ms 747.675245ms 747.765096ms 747.852913ms 747.94082ms 747.981929ms 748.031491ms 748.088586ms 748.165917ms 748.407938ms 748.428798ms 748.477965ms 748.490368ms 748.568964ms 748.587674ms 748.634126ms 748.745686ms 748.753531ms 748.827659ms 748.837546ms 748.84827ms 748.903165ms 748.926535ms 749.026671ms 749.034511ms 749.089679ms 749.111211ms 749.148646ms 749.223466ms 749.227179ms 749.228896ms 749.238149ms 749.309019ms 749.309284ms 749.369084ms 749.383187ms 749.404564ms 749.446984ms 749.448683ms 749.474577ms 749.487534ms 749.531243ms 749.543496ms 749.546716ms 749.572771ms 749.582093ms 749.600366ms 749.609998ms 749.649103ms 749.69291ms 749.694018ms 749.737859ms 749.740945ms 749.760271ms 749.772524ms 749.791439ms 749.795574ms 749.810348ms 749.86235ms 749.89717ms 749.901327ms 749.906143ms 749.909641ms 749.9406ms 750.019778ms 750.153601ms 750.197317ms 750.29731ms 750.302825ms 750.36393ms 750.410052ms 750.434061ms 750.448134ms 750.506746ms 750.513918ms 750.587692ms 750.624334ms 750.701465ms 750.765683ms 750.874418ms 750.945605ms 750.947998ms 750.964036ms 751.024009ms 751.051361ms 751.127828ms 751.228366ms 751.237807ms 751.27875ms 751.304089ms 751.328618ms 751.360371ms 751.376492ms 751.379178ms 751.474464ms 751.523125ms 751.617049ms 751.69432ms 751.792571ms 751.796874ms 751.84498ms 751.857717ms 751.946271ms 751.950812ms 752.026049ms 752.122517ms 752.129513ms 752.196728ms 752.219582ms 752.257175ms 752.280233ms 752.551567ms 752.581398ms 752.592963ms 752.940713ms 753.36035ms 753.427148ms 753.448379ms 753.674427ms 753.872136ms 754.601872ms 755.016861ms 755.070868ms 755.194108ms 755.808249ms]
Jun 22 12:51:49.239: INFO: 50 %ile: 749.089679ms
Jun 22 12:51:49.239: INFO: 90 %ile: 752.122517ms
Jun 22 12:51:49.239: INFO: 99 %ile: 755.194108ms
Jun 22 12:51:49.239: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6239" for this suite.

• [SLOW TEST:15.928 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":113,"skipped":1935,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:49.254: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:51:49.401: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954" in namespace "projected-4149" to be "Succeeded or Failed"
Jun 22 12:51:49.416: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954": Phase="Pending", Reason="", readiness=false. Elapsed: 14.200463ms
Jun 22 12:51:51.421: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019450185s
Jun 22 12:51:53.425: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023259737s
Jun 22 12:51:55.428: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026714723s
Jun 22 12:51:57.432: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030713276s
STEP: Saw pod success
Jun 22 12:51:57.432: INFO: Pod "downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954" satisfied condition "Succeeded or Failed"
Jun 22 12:51:57.436: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954 container client-container: <nil>
STEP: delete the pod
Jun 22 12:51:57.462: INFO: Waiting for pod downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954 to disappear
Jun 22 12:51:57.467: INFO: Pod downwardapi-volume-d63987a6-290b-4d05-b057-1932ad625954 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:51:57.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4149" for this suite.

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1938,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:51:57.492: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-374a4395-5a28-4a9c-9b50-c00f52621fae
STEP: Creating a pod to test consume configMaps
Jun 22 12:51:57.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0" in namespace "projected-8960" to be "Succeeded or Failed"
Jun 22 12:51:57.671: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.755171ms
Jun 22 12:51:59.675: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011566574s
Jun 22 12:52:01.679: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015796332s
Jun 22 12:52:03.683: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01927721s
Jun 22 12:52:05.686: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022812441s
Jun 22 12:52:07.690: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026137547s
STEP: Saw pod success
Jun 22 12:52:07.690: INFO: Pod "pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0" satisfied condition "Succeeded or Failed"
Jun 22 12:52:07.692: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:52:07.708: INFO: Waiting for pod pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0 to disappear
Jun 22 12:52:07.713: INFO: Pod pod-projected-configmaps-63d4c428-e90c-4335-9ea2-b6015ad131c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:52:07.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8960" for this suite.

• [SLOW TEST:10.231 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":115,"skipped":1943,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:52:07.723: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-k62t
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 12:52:07.924: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-k62t" in namespace "subpath-6974" to be "Succeeded or Failed"
Jun 22 12:52:07.931: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.85892ms
Jun 22 12:52:09.935: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041391s
Jun 22 12:52:11.940: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015910457s
Jun 22 12:52:13.944: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020336992s
Jun 22 12:52:15.949: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025021115s
Jun 22 12:52:17.953: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 10.028943394s
Jun 22 12:52:19.956: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 12.032389311s
Jun 22 12:52:21.962: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 14.037701294s
Jun 22 12:52:23.965: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 16.041512567s
Jun 22 12:52:25.970: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 18.046285073s
Jun 22 12:52:27.974: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 20.050487416s
Jun 22 12:52:29.978: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 22.054053219s
Jun 22 12:52:31.982: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 24.057741692s
Jun 22 12:52:33.986: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 26.06183666s
Jun 22 12:52:35.989: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Running", Reason="", readiness=true. Elapsed: 28.065439219s
Jun 22 12:52:37.994: INFO: Pod "pod-subpath-test-secret-k62t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.06996046s
STEP: Saw pod success
Jun 22 12:52:37.994: INFO: Pod "pod-subpath-test-secret-k62t" satisfied condition "Succeeded or Failed"
Jun 22 12:52:37.997: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-subpath-test-secret-k62t container test-container-subpath-secret-k62t: <nil>
STEP: delete the pod
Jun 22 12:52:38.018: INFO: Waiting for pod pod-subpath-test-secret-k62t to disappear
Jun 22 12:52:38.022: INFO: Pod pod-subpath-test-secret-k62t no longer exists
STEP: Deleting pod pod-subpath-test-secret-k62t
Jun 22 12:52:38.022: INFO: Deleting pod "pod-subpath-test-secret-k62t" in namespace "subpath-6974"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:52:38.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6974" for this suite.

• [SLOW TEST:30.314 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":116,"skipped":1960,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:52:38.038: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2718
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-2718
Jun 22 12:52:38.218: INFO: Found 0 stateful pods, waiting for 1
Jun 22 12:52:48.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 12:52:48.238: INFO: Deleting all statefulset in ns statefulset-2718
Jun 22 12:52:48.241: INFO: Scaling statefulset ss to 0
Jun 22 12:52:58.293: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 12:52:58.296: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:52:58.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2718" for this suite.

• [SLOW TEST:20.283 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":117,"skipped":1963,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:52:58.323: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 22 12:52:58.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 create -f -'
Jun 22 12:52:58.708: INFO: stderr: ""
Jun 22 12:52:58.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 12:52:58.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:52:58.796: INFO: stderr: ""
Jun 22 12:52:58.796: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:52:58.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:52:58.875: INFO: stderr: ""
Jun 22 12:52:58.875: INFO: stdout: ""
Jun 22 12:52:58.875: INFO: update-demo-nautilus-dw5cb is created but not running
Jun 22 12:53:03.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:53:03.952: INFO: stderr: ""
Jun 22 12:53:03.952: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:53:03.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:04.025: INFO: stderr: ""
Jun 22 12:53:04.025: INFO: stdout: "true"
Jun 22 12:53:04.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:04.096: INFO: stderr: ""
Jun 22 12:53:04.096: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:04.096: INFO: validating pod update-demo-nautilus-dw5cb
Jun 22 12:53:04.107: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:04.107: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:04.107: INFO: update-demo-nautilus-dw5cb is verified up and running
Jun 22 12:53:04.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:04.179: INFO: stderr: ""
Jun 22 12:53:04.179: INFO: stdout: ""
Jun 22 12:53:04.179: INFO: update-demo-nautilus-psv8f is created but not running
Jun 22 12:53:09.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:53:09.255: INFO: stderr: ""
Jun 22 12:53:09.255: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:53:09.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:09.329: INFO: stderr: ""
Jun 22 12:53:09.329: INFO: stdout: "true"
Jun 22 12:53:09.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:09.403: INFO: stderr: ""
Jun 22 12:53:09.403: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:09.403: INFO: validating pod update-demo-nautilus-dw5cb
Jun 22 12:53:09.407: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:09.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:09.407: INFO: update-demo-nautilus-dw5cb is verified up and running
Jun 22 12:53:09.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:09.478: INFO: stderr: ""
Jun 22 12:53:09.478: INFO: stdout: ""
Jun 22 12:53:09.478: INFO: update-demo-nautilus-psv8f is created but not running
Jun 22 12:53:14.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:53:14.556: INFO: stderr: ""
Jun 22 12:53:14.556: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:53:14.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:14.649: INFO: stderr: ""
Jun 22 12:53:14.649: INFO: stdout: "true"
Jun 22 12:53:14.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:14.729: INFO: stderr: ""
Jun 22 12:53:14.729: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:14.729: INFO: validating pod update-demo-nautilus-dw5cb
Jun 22 12:53:14.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:14.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:14.733: INFO: update-demo-nautilus-dw5cb is verified up and running
Jun 22 12:53:14.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:14.805: INFO: stderr: ""
Jun 22 12:53:14.805: INFO: stdout: ""
Jun 22 12:53:14.805: INFO: update-demo-nautilus-psv8f is created but not running
Jun 22 12:53:19.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:53:19.888: INFO: stderr: ""
Jun 22 12:53:19.888: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:53:19.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:19.973: INFO: stderr: ""
Jun 22 12:53:19.973: INFO: stdout: "true"
Jun 22 12:53:19.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:20.049: INFO: stderr: ""
Jun 22 12:53:20.049: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:20.049: INFO: validating pod update-demo-nautilus-dw5cb
Jun 22 12:53:20.053: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:20.053: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:20.053: INFO: update-demo-nautilus-dw5cb is verified up and running
Jun 22 12:53:20.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:20.194: INFO: stderr: ""
Jun 22 12:53:20.194: INFO: stdout: ""
Jun 22 12:53:20.194: INFO: update-demo-nautilus-psv8f is created but not running
Jun 22 12:53:25.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 12:53:25.273: INFO: stderr: ""
Jun 22 12:53:25.273: INFO: stdout: "update-demo-nautilus-dw5cb update-demo-nautilus-psv8f "
Jun 22 12:53:25.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:25.345: INFO: stderr: ""
Jun 22 12:53:25.345: INFO: stdout: "true"
Jun 22 12:53:25.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-dw5cb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:25.421: INFO: stderr: ""
Jun 22 12:53:25.421: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:25.421: INFO: validating pod update-demo-nautilus-dw5cb
Jun 22 12:53:25.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:25.425: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:25.425: INFO: update-demo-nautilus-dw5cb is verified up and running
Jun 22 12:53:25.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 12:53:25.496: INFO: stderr: ""
Jun 22 12:53:25.496: INFO: stdout: "true"
Jun 22 12:53:25.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods update-demo-nautilus-psv8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 12:53:25.569: INFO: stderr: ""
Jun 22 12:53:25.569: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 12:53:25.569: INFO: validating pod update-demo-nautilus-psv8f
Jun 22 12:53:25.576: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 12:53:25.576: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 12:53:25.577: INFO: update-demo-nautilus-psv8f is verified up and running
STEP: using delete to clean up resources
Jun 22 12:53:25.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 delete --grace-period=0 --force -f -'
Jun 22 12:53:25.664: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 12:53:25.664: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 12:53:25.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get rc,svc -l name=update-demo --no-headers'
Jun 22 12:53:25.754: INFO: stderr: "No resources found in kubectl-9614 namespace.\n"
Jun 22 12:53:25.754: INFO: stdout: ""
Jun 22 12:53:25.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 12:53:25.838: INFO: stderr: ""
Jun 22 12:53:25.838: INFO: stdout: "update-demo-nautilus-dw5cb\nupdate-demo-nautilus-psv8f\n"
Jun 22 12:53:26.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get rc,svc -l name=update-demo --no-headers'
Jun 22 12:53:26.432: INFO: stderr: "No resources found in kubectl-9614 namespace.\n"
Jun 22 12:53:26.432: INFO: stdout: ""
Jun 22 12:53:26.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9614 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 12:53:26.506: INFO: stderr: ""
Jun 22 12:53:26.506: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:53:26.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9614" for this suite.

• [SLOW TEST:28.194 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":118,"skipped":1988,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:53:26.516: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1673
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:53:47.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9886" for this suite.
STEP: Destroying namespace "nsdeletetest-5592" for this suite.
Jun 22 12:53:47.957: INFO: Namespace nsdeletetest-5592 was already deleted
STEP: Destroying namespace "nsdeletetest-1673" for this suite.

• [SLOW TEST:21.447 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":119,"skipped":1998,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:53:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-b657d5a5-0cf0-402d-b477-74b10c3dfdd5
STEP: Creating a pod to test consume secrets
Jun 22 12:53:48.110: INFO: Waiting up to 5m0s for pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3" in namespace "secrets-2345" to be "Succeeded or Failed"
Jun 22 12:53:48.120: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.673446ms
Jun 22 12:53:50.125: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014758034s
Jun 22 12:53:52.129: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019628439s
Jun 22 12:53:54.134: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023777842s
Jun 22 12:53:56.138: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028413913s
Jun 22 12:53:58.142: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032670252s
Jun 22 12:54:00.160: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050262191s
Jun 22 12:54:02.164: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.054651247s
Jun 22 12:54:04.169: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.059192455s
Jun 22 12:54:06.173: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.063134976s
Jun 22 12:54:08.176: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.066379817s
STEP: Saw pod success
Jun 22 12:54:08.176: INFO: Pod "pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3" satisfied condition "Succeeded or Failed"
Jun 22 12:54:08.179: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 12:54:08.204: INFO: Waiting for pod pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3 to disappear
Jun 22 12:54:08.207: INFO: Pod pod-secrets-1cc4a8dd-5123-4ab3-8dce-b6dc88cf8ef3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:54:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2345" for this suite.

• [SLOW TEST:20.256 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":2000,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:54:08.224: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 12:54:08.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2" in namespace "downward-api-2154" to be "Succeeded or Failed"
Jun 22 12:54:08.378: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.187132ms
Jun 22 12:54:10.382: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01320332s
Jun 22 12:54:12.387: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017636119s
Jun 22 12:54:14.395: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025362684s
Jun 22 12:54:16.399: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Running", Reason="", readiness=true. Elapsed: 8.029899573s
Jun 22 12:54:18.404: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.034580652s
STEP: Saw pod success
Jun 22 12:54:18.404: INFO: Pod "downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2" satisfied condition "Succeeded or Failed"
Jun 22 12:54:18.406: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2 container client-container: <nil>
STEP: delete the pod
Jun 22 12:54:18.424: INFO: Waiting for pod downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2 to disappear
Jun 22 12:54:18.427: INFO: Pod downwardapi-volume-143dc219-4f1c-4c93-807c-afb54cc2f5f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:54:18.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2154" for this suite.

• [SLOW TEST:10.214 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":2054,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:54:18.442: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-993a50d7-6cd8-468b-a44c-cca24fef0d38
STEP: Creating a pod to test consume configMaps
Jun 22 12:54:18.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f" in namespace "configmap-8100" to be "Succeeded or Failed"
Jun 22 12:54:18.618: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04624ms
Jun 22 12:54:20.622: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011855844s
Jun 22 12:54:22.627: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016361517s
Jun 22 12:54:24.631: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020610433s
Jun 22 12:54:26.636: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025060837s
STEP: Saw pod success
Jun 22 12:54:26.636: INFO: Pod "pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f" satisfied condition "Succeeded or Failed"
Jun 22 12:54:26.638: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:54:26.656: INFO: Waiting for pod pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f to disappear
Jun 22 12:54:26.661: INFO: Pod pod-configmaps-8fcb58e4-3dd1-4a78-9cea-81f5af65364f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:54:26.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8100" for this suite.

• [SLOW TEST:8.230 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":2069,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:54:26.674: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 12:54:27.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 12:54:29.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:54:31.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:54:33.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:54:35.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:54:37.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:54:39.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963267, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 12:54:42.446: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:54:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8532" for this suite.
STEP: Destroying namespace "webhook-8532-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:25.974 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":123,"skipped":2140,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:54:52.648: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-685
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-685
STEP: creating replication controller externalsvc in namespace services-685
I0622 12:54:52.821068      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-685, replica count: 2
I0622 12:54:55.871335      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:54:58.871618      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:55:01.871823      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 22 12:55:01.892: INFO: Creating new exec pod
Jun 22 12:55:05.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-685 exec execpod5knsj -- /bin/sh -x -c nslookup nodeport-service.services-685.svc.cluster.local'
Jun 22 12:55:06.857: INFO: stderr: "+ nslookup nodeport-service.services-685.svc.cluster.local\n"
Jun 22 12:55:06.857: INFO: stdout: "Server:\t\t10.100.192.2\nAddress:\t10.100.192.2#53\n\nnodeport-service.services-685.svc.cluster.local\tcanonical name = externalsvc.services-685.svc.cluster.local.\nName:\texternalsvc.services-685.svc.cluster.local\nAddress: 10.100.201.229\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-685, will wait for the garbage collector to delete the pods
Jun 22 12:55:06.928: INFO: Deleting ReplicationController externalsvc took: 9.810336ms
Jun 22 12:55:07.028: INFO: Terminating ReplicationController externalsvc pods took: 100.132875ms
Jun 22 12:55:21.949: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:55:21.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-685" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.341 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":124,"skipped":2153,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:55:21.990: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jun 22 12:55:22.127: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4486 proxy --unix-socket=/tmp/kubectl-proxy-unix240326602/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:55:22.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4486" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":125,"skipped":2200,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:55:22.193: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 22 12:55:30.866: INFO: Successfully updated pod "labelsupdate8325c0cd-822b-4245-aedd-fb7c2eeba348"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:55:32.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4189" for this suite.

• [SLOW TEST:10.702 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2229,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:55:32.896: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-853d86ab-c94e-46a2-9416-038d17944aee
STEP: Creating a pod to test consume configMaps
Jun 22 12:55:33.042: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149" in namespace "projected-3029" to be "Succeeded or Failed"
Jun 22 12:55:33.050: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.556601ms
Jun 22 12:55:35.056: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014007828s
Jun 22 12:55:37.059: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017313716s
Jun 22 12:55:39.063: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020759048s
Jun 22 12:55:41.067: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025286063s
Jun 22 12:55:43.071: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029153892s
STEP: Saw pod success
Jun 22 12:55:43.071: INFO: Pod "pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149" satisfied condition "Succeeded or Failed"
Jun 22 12:55:43.075: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 12:55:43.099: INFO: Waiting for pod pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149 to disappear
Jun 22 12:55:43.104: INFO: Pod pod-projected-configmaps-c352ca1c-5349-4df8-8a80-edeafc091149 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:55:43.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3029" for this suite.

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2252,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:55:43.123: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 22 12:55:55.319: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.319: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.408: INFO: Exec stderr: ""
Jun 22 12:55:55.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.408: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.499: INFO: Exec stderr: ""
Jun 22 12:55:55.499: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.582: INFO: Exec stderr: ""
Jun 22 12:55:55.582: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.582: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.662: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 22 12:55:55.662: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.748: INFO: Exec stderr: ""
Jun 22 12:55:55.748: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.748: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.830: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 22 12:55:55.830: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.830: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:55.949: INFO: Exec stderr: ""
Jun 22 12:55:55.949: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:56.052: INFO: Exec stderr: ""
Jun 22 12:55:56.052: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:56.052: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:56.151: INFO: Exec stderr: ""
Jun 22 12:55:56.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 12:55:56.151: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 12:55:56.239: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:55:56.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1582" for this suite.

• [SLOW TEST:13.127 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2257,"failed":1,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:55:56.252: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9886
STEP: creating service affinity-clusterip in namespace services-9886
STEP: creating replication controller affinity-clusterip in namespace services-9886
I0622 12:55:56.407842      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9886, replica count: 3
I0622 12:55:59.458356      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:02.458563      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:05.458737      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:08.459012      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:11.459236      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:14.459945      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:17.460174      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:20.460332      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:23.460576      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 12:56:26.460790      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 12:56:26.466: INFO: Creating new exec pod
Jun 22 12:56:31.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun 22 12:56:31.713: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 22 12:56:31.713: INFO: stdout: ""
Jun 22 12:56:31.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c nc -zv -t -w 2 10.100.207.49 80'
Jun 22 12:56:31.891: INFO: stderr: "+ nc -zv -t -w 2 10.100.207.49 80\nConnection to 10.100.207.49 80 port [tcp/http] succeeded!\n"
Jun 22 12:56:31.891: INFO: stdout: ""
Jun 22 12:56:31.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:56:32.164: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:56:32.164: INFO: stdout: "\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw"
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:56:32.164: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:57:02.435: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:57:02.435: INFO: stdout: "\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt"
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:02.435: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:57:32.436: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:57:32.436: INFO: stdout: "\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw"
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:57:32.436: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:58:02.437: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:58:02.437: INFO: stdout: "\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t"
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:02.437: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:58:32.443: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:58:32.443: INFO: stdout: "\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-jpcsw\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t"
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-9886 exec execpod-affinitym89w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.207.49:80/ ; done'
Jun 22 12:58:32.701: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.207.49:80/\n"
Jun 22 12:58:32.701: INFO: stdout: "\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-qst4t\naffinity-clusterip-q5lzt\naffinity-clusterip-qst4t\naffinity-clusterip-jpcsw\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt\naffinity-clusterip-q5lzt"
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-qst4t
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-jpcsw
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: Received response from host: affinity-clusterip-q5lzt
Jun 22 12:58:32.701: INFO: [affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-jpcsw affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-qst4t affinity-clusterip-q5lzt affinity-clusterip-qst4t affinity-clusterip-jpcsw affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt affinity-clusterip-q5lzt]
Jun 22 12:58:32.701: FAIL: Affinity should hold but didn't.

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.checkAffinityFailed(0xc005a10000, 0x60, 0x80, 0x4c74f7b, 0x20)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241 +0xdf
k8s.io/kubernetes/test/e2e/network.checkAffinity(0x540f680, 0xc0039749a0, 0xc003492c00, 0xc003ed01f0, 0xd, 0x50, 0x1, 0xc003492c00)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:200 +0x225
k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBServiceWithOptionalTransition(0xc000a42b00, 0x540f680, 0xc0039749a0, 0xc0006b46c0, 0x0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3514 +0x92c
k8s.io/kubernetes/test/e2e/network.execAffinityTestForNonLBService(...)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3470
k8s.io/kubernetes/test/e2e/network.glob..func24.25()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2461 +0xa5
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc002309e00, 0x4debd60)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
Jun 22 12:58:32.702: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9886, will wait for the garbage collector to delete the pods
Jun 22 12:58:32.777: INFO: Deleting ReplicationController affinity-clusterip took: 5.439395ms
Jun 22 12:58:33.377: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.136872ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "services-9886".
STEP: Found 23 events.
Jun 22 12:58:44.697: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-jpcsw: { } Scheduled: Successfully assigned services-9886/affinity-clusterip-jpcsw to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.697: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-q5lzt: { } Scheduled: Successfully assigned services-9886/affinity-clusterip-q5lzt to 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:58:44.697: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-qst4t: { } Scheduled: Successfully assigned services-9886/affinity-clusterip-qst4t to 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:58:44.697: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for execpod-affinitym89w6: { } Scheduled: Successfully assigned services-9886/execpod-affinitym89w6 to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:55:56 +0000 UTC - event for affinity-clusterip: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-jpcsw
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:55:56 +0000 UTC - event for affinity-clusterip: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-qst4t
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:55:56 +0000 UTC - event for affinity-clusterip: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-q5lzt
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-jpcsw: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-jpcsw: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-jpcsw: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-q5lzt: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-q5lzt: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Started: Started container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:03 +0000 UTC - event for affinity-clusterip-q5lzt: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Created: Created container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:22 +0000 UTC - event for affinity-clusterip-qst4t: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Created: Created container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:22 +0000 UTC - event for affinity-clusterip-qst4t: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:23 +0000 UTC - event for affinity-clusterip-qst4t: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Started: Started container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:28 +0000 UTC - event for execpod-affinitym89w6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container agnhost-container
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:28 +0000 UTC - event for execpod-affinitym89w6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:56:28 +0000 UTC - event for execpod-affinitym89w6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container agnhost-container
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:58:32 +0000 UTC - event for execpod-affinitym89w6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container agnhost-container
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:58:33 +0000 UTC - event for affinity-clusterip-jpcsw: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:58:33 +0000 UTC - event for affinity-clusterip-q5lzt: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Killing: Stopping container affinity-clusterip
Jun 22 12:58:44.698: INFO: At 2021-06-22 12:58:33 +0000 UTC - event for affinity-clusterip-qst4t: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Killing: Stopping container affinity-clusterip
Jun 22 12:58:44.701: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Jun 22 12:58:44.701: INFO: 
Jun 22 12:58:44.707: INFO: 
Logging node info for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.711: INFO: Node Info: &Node{ObjectMeta:{06998c1b-9fed-44d7-827f-f702404ff383   /api/v1/nodes/06998c1b-9fed-44d7-827f-f702404ff383 d6e824ee-0f24-4194-ba8c-f2897d500b67 164144 0 2021-06-21 23:48:31 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:8b60bac6-7336-4a89-974a-c41f2875f963 bosh.zone:WL-OLT-Dev-02 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-02 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.4 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.4] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {e2e.test Update v1 2021-06-22 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}}} {kubelet Update v1 2021-06-22 12:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e034f-72fb-1ea5-cc8c-80ef6bea7bb1,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:49 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:49 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:49 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:53:49 +0000 UTC,LastTransitionTime:2021-06-21 23:48:41 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.4,},NodeAddress{Type:InternalIP,Address:11.0.1.4,},NodeAddress{Type:Hostname,Address:11.0.1.4,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:c5dcfa0ab1ae479d98fa8e37e017524d,SystemUUID:420E034F-72FB-1EA5-CC8C-80EF6BEA7BB1,BootID:0d938454-6d4a-4970-b6fd-51fd6a99bee7,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:58:44.711: INFO: 
Logging kubelet events for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.714: INFO: 
Logging pods the kubelet thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.727: INFO: node-exporter-j9gvz started at 2021-06-22 12:47:11 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.728: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:58:44.728: INFO: fluent-bit-r2rws started at 2021-06-22 12:47:11 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:58:44.728: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:58:44.728: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:58:44.728: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:58:44.728: INFO: sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.729: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 12:58:44.729: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:58:44.729: INFO: 	Container sonobuoy-worker ready: false, restart count 4
Jun 22 12:58:44.729: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:58:44.729: INFO: telegraf-7gq7r started at 2021-06-22 12:47:11 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.729: INFO: 	Container telegraf ready: true, restart count 0
W0622 12:58:44.734086      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:58:44.734185      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:58:44.734207      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:58:44.759: INFO: 
Latency metrics for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 12:58:44.759: INFO: 
Logging node info for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:58:44.762: INFO: Node Info: &Node{ObjectMeta:{1d96d19c-1b78-44d8-b822-ba104bc5daa5   /api/v1/nodes/1d96d19c-1b78-44d8-b822-ba104bc5daa5 3382db90-e9d9-4d99-92c9-eed5527dcd67 164105 0 2021-06-21 23:42:33 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:38c4eb74-265f-4dd6-a9f0-1bac74785fba bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.3 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.3] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420eaaab-5adc-eb3a-9a4f-3b73f38acf24,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364527616 0} {<nil>} 8168484Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259670016 0} {<nil>} 8066084Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:44 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:44 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:44 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:53:44 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.3,},NodeAddress{Type:InternalIP,Address:11.0.1.3,},NodeAddress{Type:Hostname,Address:11.0.1.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:fd3f65ecda8a4cb3bd31bfb34c08ea2c,SystemUUID:420EAAAB-5ADC-EB3A-9A4F-3B73F38ACF24,BootID:8812dfc4-73c0-4087-8280-61b5eb4cbf39,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:58:44.763: INFO: 
Logging kubelet events for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:58:44.766: INFO: 
Logging pods the kubelet thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:58:44.782: INFO: telegraf-j56d5 started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.782: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:58:44.782: INFO: node-exporter-w6bbv started at 2021-06-21 23:42:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.782: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:58:44.782: INFO: observability-manager-6cf797f97-8nqdm started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.782: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 12:58:44.782: INFO: metric-controller-7f5cb8ff6d-7npkg started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.782: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 12:58:44.783: INFO: sink-controller-f6bc7f774-zprjs started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.783: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 12:58:44.783: INFO: metrics-server-7d476fdfbd-md64k started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.783: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 12:58:44.783: INFO: coredns-645ccbcd68-7dlmn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.783: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:58:44.783: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:58:44.783: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Jun 22 12:58:44.783: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:58:44.783: INFO: fluent-bit-8rhpl started at 2021-06-21 23:47:44 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:58:44.784: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:58:44.784: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:58:44.784: INFO: 	Container ghostunnel ready: true, restart count 0
W0622 12:58:44.788217      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:58:44.788269      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:58:44.788304      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:58:44.813: INFO: 
Latency metrics for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 12:58:44.813: INFO: 
Logging node info for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:58:44.816: INFO: Node Info: &Node{ObjectMeta:{2352dbd9-b599-409b-9a0b-5bade7a216ea   /api/v1/nodes/2352dbd9-b599-409b-9a0b-5bade7a216ea 4ffa18f4-da34-4eb7-8052-e83631b76176 164112 0 2021-06-21 23:52:14 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:975f3130-3bff-4253-88c9-75060b9b5a34 bosh.zone:WL-ZHH-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-ZHH-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.7 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.7] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:52:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e7dbb-3f7c-5670-16cd-e87a1cc6ae33,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:45 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:45 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:53:45 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:53:45 +0000 UTC,LastTransitionTime:2021-06-21 23:52:24 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.7,},NodeAddress{Type:InternalIP,Address:11.0.1.7,},NodeAddress{Type:Hostname,Address:11.0.1.7,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:90e66be501cd48dc83f0fad01e6c038c,SystemUUID:420E7DBB-3F7C-5670-16CD-E87A1CC6AE33,BootID:eaba17bb-8b7e-4b67-ac5d-85bb9e5c744d,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:58:44.816: INFO: 
Logging kubelet events for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:58:44.819: INFO: 
Logging pods the kubelet thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:58:44.841: INFO: telegraf-dwvlw started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:58:44.841: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Container e2e ready: true, restart count 0
Jun 22 12:58:44.841: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 12:58:44.841: INFO: fluent-bit-vtnl7 started at 2021-06-21 23:52:24 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:58:44.841: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:58:44.841: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:58:44.841: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Jun 22 12:58:44.841: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:58:44.841: INFO: node-exporter-82sxc started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:58:44.841: INFO: telemetry-agent-8444c9c47b-7xllf started at 2021-06-21 23:55:50 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.841: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
W0622 12:58:44.845074      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:58:44.845116      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:58:44.845207      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:58:44.874: INFO: 
Latency metrics for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 12:58:44.874: INFO: 
Logging node info for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:58:44.877: INFO: Node Info: &Node{ObjectMeta:{54b7d611-8263-481b-bda6-56b40bff2a2f   /api/v1/nodes/54b7d611-8263-481b-bda6-56b40bff2a2f d071e496-861f-42ba-a9f8-529501f196ca 165620 0 2021-06-21 23:46:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:bd75bf6d-fb7a-49e2-a49b-8b225c259383 bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.5 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.5] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:46:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e2237-ebbd-7ba1-c525-758ac4c56ba2,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 12:58:42 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 12:58:42 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 12:58:42 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 12:58:42 +0000 UTC,LastTransitionTime:2021-06-21 23:46:22 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.5,},NodeAddress{Type:InternalIP,Address:11.0.1.5,},NodeAddress{Type:Hostname,Address:11.0.1.5,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:20304c03bce241438791dd1ad3ceb11a,SystemUUID:420E2237-EBBD-7BA1-C525-758AC4C56BA2,BootID:880882cc-1831-4c1a-bc3f-4b0766aa256e,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 12:58:44.878: INFO: 
Logging kubelet events for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:58:44.881: INFO: 
Logging pods the kubelet thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:58:44.908: INFO: coredns-645ccbcd68-s8fjw started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Container coredns ready: true, restart count 0
Jun 22 12:58:44.908: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Jun 22 12:58:44.908: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 12:58:44.908: INFO: telegraf-9z5xm started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 12:58:44.908: INFO: validator-69f557d7c6-bl8gf started at 2021-06-21 23:47:39 +0000 UTC (1+1 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Init container patch-ca ready: true, restart count 0
Jun 22 12:58:44.908: INFO: 	Container validator ready: true, restart count 0
Jun 22 12:58:44.908: INFO: node-exporter-8qrc5 started at 2021-06-21 23:46:22 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 12:58:44.908: INFO: event-controller-85d6bb4d4c-fz5nk started at 2021-06-21 23:47:39 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:58:44.908: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 12:58:44.908: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:58:44.908: INFO: fluent-bit-f425q started at 2021-06-21 23:48:02 +0000 UTC (1+2 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 12:58:44.908: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 12:58:44.908: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 12:58:44.908: INFO: coredns-645ccbcd68-pbmnn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 12:58:44.908: INFO: 	Container coredns ready: true, restart count 0
W0622 12:58:44.921494      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 12:58:44.921516      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 12:58:44.921577      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 12:58:44.945: INFO: 
Latency metrics for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 12:58:44.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9886" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• Failure [168.703 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  Jun 22 12:58:32.701: Affinity should hold but didn't.

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241
------------------------------
{"msg":"FAILED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":128,"skipped":2284,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:58:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:58:45.099: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722" in namespace "security-context-test-6654" to be "Succeeded or Failed"
Jun 22 12:58:45.104: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 5.012961ms
Jun 22 12:58:47.109: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009777751s
Jun 22 12:58:49.113: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014750896s
Jun 22 12:58:51.118: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018971406s
Jun 22 12:58:53.122: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023025551s
Jun 22 12:58:55.126: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027430691s
Jun 22 12:58:55.126: INFO: Pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722" satisfied condition "Succeeded or Failed"
Jun 22 12:58:55.133: INFO: Got logs for pod "busybox-privileged-false-b071ac2e-61dc-4f2d-8147-b5fb9fd1b722": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:58:55.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6654" for this suite.

• [SLOW TEST:10.187 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2284,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:58:55.146: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 22 12:58:55.283: INFO: namespace kubectl-9228
Jun 22 12:58:55.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9228 create -f -'
Jun 22 12:58:55.462: INFO: stderr: ""
Jun 22 12:58:55.463: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 22 12:58:56.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:58:56.466: INFO: Found 0 / 1
Jun 22 12:58:57.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:58:57.466: INFO: Found 0 / 1
Jun 22 12:58:58.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:58:58.467: INFO: Found 0 / 1
Jun 22 12:58:59.470: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:58:59.470: INFO: Found 0 / 1
Jun 22 12:59:00.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:00.466: INFO: Found 0 / 1
Jun 22 12:59:01.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:01.468: INFO: Found 0 / 1
Jun 22 12:59:02.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:02.468: INFO: Found 0 / 1
Jun 22 12:59:03.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:03.467: INFO: Found 0 / 1
Jun 22 12:59:04.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:04.467: INFO: Found 0 / 1
Jun 22 12:59:05.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:05.466: INFO: Found 0 / 1
Jun 22 12:59:06.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:06.467: INFO: Found 0 / 1
Jun 22 12:59:07.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:07.467: INFO: Found 0 / 1
Jun 22 12:59:08.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:08.466: INFO: Found 0 / 1
Jun 22 12:59:09.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:09.467: INFO: Found 0 / 1
Jun 22 12:59:10.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:10.466: INFO: Found 0 / 1
Jun 22 12:59:11.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:11.468: INFO: Found 0 / 1
Jun 22 12:59:12.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:12.468: INFO: Found 0 / 1
Jun 22 12:59:13.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:13.467: INFO: Found 0 / 1
Jun 22 12:59:14.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:14.467: INFO: Found 0 / 1
Jun 22 12:59:15.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:15.467: INFO: Found 0 / 1
Jun 22 12:59:16.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:16.468: INFO: Found 0 / 1
Jun 22 12:59:17.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:17.467: INFO: Found 0 / 1
Jun 22 12:59:18.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:18.467: INFO: Found 0 / 1
Jun 22 12:59:19.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:19.467: INFO: Found 0 / 1
Jun 22 12:59:20.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:20.466: INFO: Found 0 / 1
Jun 22 12:59:21.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:21.466: INFO: Found 0 / 1
Jun 22 12:59:22.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:22.467: INFO: Found 0 / 1
Jun 22 12:59:23.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:23.467: INFO: Found 0 / 1
Jun 22 12:59:24.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:24.467: INFO: Found 0 / 1
Jun 22 12:59:25.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:25.467: INFO: Found 0 / 1
Jun 22 12:59:26.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:26.466: INFO: Found 0 / 1
Jun 22 12:59:27.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:27.466: INFO: Found 0 / 1
Jun 22 12:59:28.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:28.467: INFO: Found 0 / 1
Jun 22 12:59:29.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:29.468: INFO: Found 1 / 1
Jun 22 12:59:29.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 12:59:29.471: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 12:59:29.471: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 12:59:29.471: INFO: wait on agnhost-primary startup in kubectl-9228 
Jun 22 12:59:29.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9228 logs agnhost-primary-4wsk7 agnhost-primary'
Jun 22 12:59:29.557: INFO: stderr: ""
Jun 22 12:59:29.557: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 22 12:59:29.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9228 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 22 12:59:29.660: INFO: stderr: ""
Jun 22 12:59:29.660: INFO: stdout: "service/rm2 exposed\n"
Jun 22 12:59:29.663: INFO: Service rm2 in namespace kubectl-9228 found.
STEP: exposing service
Jun 22 12:59:31.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-9228 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 22 12:59:31.755: INFO: stderr: ""
Jun 22 12:59:31.755: INFO: stdout: "service/rm3 exposed\n"
Jun 22 12:59:31.767: INFO: Service rm3 in namespace kubectl-9228 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:59:33.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9228" for this suite.

• [SLOW TEST:38.637 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":130,"skipped":2307,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:59:33.784: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jun 22 12:59:33.924: INFO: created test-podtemplate-1
Jun 22 12:59:33.928: INFO: created test-podtemplate-2
Jun 22 12:59:33.932: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 22 12:59:33.937: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 22 12:59:33.954: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:59:33.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6145" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":131,"skipped":2333,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:59:33.967: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jun 22 12:59:34.118: INFO: Waiting up to 5m0s for pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb" in namespace "containers-412" to be "Succeeded or Failed"
Jun 22 12:59:34.127: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.625129ms
Jun 22 12:59:36.131: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013169729s
Jun 22 12:59:38.135: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017224007s
Jun 22 12:59:40.140: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02136023s
Jun 22 12:59:42.144: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026077951s
Jun 22 12:59:44.147: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029228035s
STEP: Saw pod success
Jun 22 12:59:44.148: INFO: Pod "client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb" satisfied condition "Succeeded or Failed"
Jun 22 12:59:44.151: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb container test-container: <nil>
STEP: delete the pod
Jun 22 12:59:44.169: INFO: Waiting for pod client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb to disappear
Jun 22 12:59:44.172: INFO: Pod client-containers-1da8e8c5-994f-4841-9a72-61722cd44fdb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:59:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-412" for this suite.

• [SLOW TEST:10.215 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2374,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:59:44.183: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-1692/configmap-test-cb2c38f7-7d93-4023-9d11-b1907f7ad4aa
STEP: Creating a pod to test consume configMaps
Jun 22 12:59:44.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb" in namespace "configmap-1692" to be "Succeeded or Failed"
Jun 22 12:59:44.338: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.338385ms
Jun 22 12:59:46.343: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015273584s
Jun 22 12:59:48.348: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019695178s
Jun 22 12:59:50.352: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02401566s
Jun 22 12:59:52.357: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02859549s
Jun 22 12:59:54.361: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.033376955s
STEP: Saw pod success
Jun 22 12:59:54.362: INFO: Pod "pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb" satisfied condition "Succeeded or Failed"
Jun 22 12:59:54.364: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb container env-test: <nil>
STEP: delete the pod
Jun 22 12:59:54.382: INFO: Waiting for pod pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb to disappear
Jun 22 12:59:54.385: INFO: Pod pod-configmaps-ca7089f0-1428-49e2-82b0-8841ebe68ffb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 12:59:54.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1692" for this suite.

• [SLOW TEST:10.211 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2426,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 12:59:54.395: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 12:59:54.531: INFO: Creating deployment "test-recreate-deployment"
Jun 22 12:59:54.536: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 12:59:54.550: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 22 12:59:56.557: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 12:59:56.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 12:59:58.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:00.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:02.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:04.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:06.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:08.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:10.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:12.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:14.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:16.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:18.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:20.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:22.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:24.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:26.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:28.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:30.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:32.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:34.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:36.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:38.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:40.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:42.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:44.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:46.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:48.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:50.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963594, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:00:52.564: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 22 13:00:52.573: INFO: Updating deployment test-recreate-deployment
Jun 22 13:00:52.573: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 22 13:00:52.679: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2025 /apis/apps/v1/namespaces/deployment-2025/deployments/test-recreate-deployment 3b37defb-0afd-49ce-aac8-3c261e33b156 166311 2 2021-06-22 12:59:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035c7f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-22 13:00:52 +0000 UTC,LastTransitionTime:2021-06-22 13:00:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-22 13:00:52 +0000 UTC,LastTransitionTime:2021-06-22 12:59:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 13:00:52.683: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-2025 /apis/apps/v1/namespaces/deployment-2025/replicasets/test-recreate-deployment-f79dd4667 92c2f347-bb84-4652-a0cc-30b0502d591a 166310 1 2021-06-22 13:00:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3b37defb-0afd-49ce-aac8-3c261e33b156 0xc003ed0040 0xc003ed0041}] []  [{kube-controller-manager Update apps/v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b37defb-0afd-49ce-aac8-3c261e33b156\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ed00b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:00:52.683: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 13:00:52.683: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-2025 /apis/apps/v1/namespaces/deployment-2025/replicasets/test-recreate-deployment-c96cf48f f09703e9-5400-4850-8dd2-c36e4f3df2c1 166299 2 2021-06-22 12:59:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3b37defb-0afd-49ce-aac8-3c261e33b156 0xc00372bf4f 0xc00372bf60}] []  [{kube-controller-manager Update apps/v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b37defb-0afd-49ce-aac8-3c261e33b156\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372bfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:00:52.686: INFO: Pod "test-recreate-deployment-f79dd4667-v64sj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-v64sj test-recreate-deployment-f79dd4667- deployment-2025 /api/v1/namespaces/deployment-2025/pods/test-recreate-deployment-f79dd4667-v64sj 3074ba00-7828-4411-96ab-5b62751ccadf 166307 0 2021-06-22 13:00:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 92c2f347-bb84-4652-a0cc-30b0502d591a 0xc003ed0860 0xc003ed0861}] []  [{kube-controller-manager Update v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92c2f347-bb84-4652-a0cc-30b0502d591a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 13:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx789,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx789,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx789,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:,StartTime:2021-06-22 13:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:00:52.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2025" for this suite.

• [SLOW TEST:58.301 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":134,"skipped":2427,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:00:52.697: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 13:01:00.882: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:01.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2585" for this suite.

• [SLOW TEST:9.216 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":135,"skipped":2428,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:01.913: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-q4hg
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 13:01:02.065: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q4hg" in namespace "subpath-8466" to be "Succeeded or Failed"
Jun 22 13:01:02.071: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.136334ms
Jun 22 13:01:04.075: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009309703s
Jun 22 13:01:06.079: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013939036s
Jun 22 13:01:08.083: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017474835s
Jun 22 13:01:10.087: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021774311s
Jun 22 13:01:12.091: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 10.02565647s
Jun 22 13:01:14.095: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 12.029932601s
Jun 22 13:01:16.099: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 14.033904524s
Jun 22 13:01:18.103: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 16.037545936s
Jun 22 13:01:20.110: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 18.044125818s
Jun 22 13:01:22.114: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 20.049108061s
Jun 22 13:01:24.119: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 22.053804346s
Jun 22 13:01:26.124: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 24.058523293s
Jun 22 13:01:28.128: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 26.062311684s
Jun 22 13:01:30.134: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Running", Reason="", readiness=true. Elapsed: 28.068298026s
Jun 22 13:01:32.139: INFO: Pod "pod-subpath-test-projected-q4hg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.073222177s
STEP: Saw pod success
Jun 22 13:01:32.139: INFO: Pod "pod-subpath-test-projected-q4hg" satisfied condition "Succeeded or Failed"
Jun 22 13:01:32.141: INFO: Trying to get logs from node 2352dbd9-b599-409b-9a0b-5bade7a216ea pod pod-subpath-test-projected-q4hg container test-container-subpath-projected-q4hg: <nil>
STEP: delete the pod
Jun 22 13:01:32.171: INFO: Waiting for pod pod-subpath-test-projected-q4hg to disappear
Jun 22 13:01:32.176: INFO: Pod pod-subpath-test-projected-q4hg no longer exists
STEP: Deleting pod pod-subpath-test-projected-q4hg
Jun 22 13:01:32.176: INFO: Deleting pod "pod-subpath-test-projected-q4hg" in namespace "subpath-8466"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8466" for this suite.

• [SLOW TEST:30.275 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":136,"skipped":2434,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:32.189: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:32.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3228" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":137,"skipped":2452,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:32.361: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:32.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9523" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":138,"skipped":2487,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:32.551: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:01:33.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:01:35.364: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963693, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963693, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963693, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963693, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:01:38.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:01:38.379: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7204-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:39.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9941" for this suite.
STEP: Destroying namespace "webhook-9941-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.998 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":139,"skipped":2495,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:39.549: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jun 22 13:01:39.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-8165 api-versions'
Jun 22 13:01:39.790: INFO: stderr: ""
Jun 22 13:01:39.790: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nnsx.vmware.com/v1\npksapi.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvmware.com/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:01:39.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8165" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":140,"skipped":2504,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:01:39.809: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 13:01:40.036: INFO: Number of nodes with available pods: 0
Jun 22 13:01:40.036: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:01:41.048: INFO: Number of nodes with available pods: 0
Jun 22 13:01:41.049: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:01:42.047: INFO: Number of nodes with available pods: 0
Jun 22 13:01:42.047: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:01:43.046: INFO: Number of nodes with available pods: 2
Jun 22 13:01:43.046: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:44.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:44.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:45.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:45.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:46.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:46.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:47.048: INFO: Number of nodes with available pods: 2
Jun 22 13:01:47.048: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:48.048: INFO: Number of nodes with available pods: 2
Jun 22 13:01:48.048: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:49.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:49.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:50.048: INFO: Number of nodes with available pods: 2
Jun 22 13:01:50.048: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:51.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:51.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:52.047: INFO: Number of nodes with available pods: 2
Jun 22 13:01:52.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:53.047: INFO: Number of nodes with available pods: 3
Jun 22 13:01:53.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:54.047: INFO: Number of nodes with available pods: 3
Jun 22 13:01:54.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:55.047: INFO: Number of nodes with available pods: 3
Jun 22 13:01:55.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:56.048: INFO: Number of nodes with available pods: 3
Jun 22 13:01:56.048: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:57.047: INFO: Number of nodes with available pods: 3
Jun 22 13:01:57.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:58.047: INFO: Number of nodes with available pods: 3
Jun 22 13:01:58.047: INFO: Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 is running more than one daemon pod
Jun 22 13:01:59.047: INFO: Number of nodes with available pods: 4
Jun 22 13:01:59.047: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 22 13:01:59.084: INFO: Number of nodes with available pods: 3
Jun 22 13:01:59.085: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 13:02:00.095: INFO: Number of nodes with available pods: 3
Jun 22 13:02:00.095: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 13:02:01.097: INFO: Number of nodes with available pods: 3
Jun 22 13:02:01.097: INFO: Node 2352dbd9-b599-409b-9a0b-5bade7a216ea is running more than one daemon pod
Jun 22 13:02:02.095: INFO: Number of nodes with available pods: 4
Jun 22 13:02:02.095: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1903, will wait for the garbage collector to delete the pods
Jun 22 13:02:02.164: INFO: Deleting DaemonSet.extensions daemon-set took: 9.715645ms
Jun 22 13:02:02.764: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.216742ms
Jun 22 13:02:12.768: INFO: Number of nodes with available pods: 0
Jun 22 13:02:12.768: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 13:02:12.770: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1903/daemonsets","resourceVersion":"166869"},"items":null}

Jun 22 13:02:12.773: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1903/pods","resourceVersion":"166869"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:02:12.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1903" for this suite.

• [SLOW TEST:32.991 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":141,"skipped":2587,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:02:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:02:12.944: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556" in namespace "security-context-test-9587" to be "Succeeded or Failed"
Jun 22 13:02:12.953: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556": Phase="Pending", Reason="", readiness=false. Elapsed: 9.302348ms
Jun 22 13:02:14.957: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012886653s
Jun 22 13:02:16.963: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018781022s
Jun 22 13:02:18.966: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022071814s
Jun 22 13:02:20.970: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025569619s
Jun 22 13:02:20.970: INFO: Pod "alpine-nnp-false-38dc7216-2a0c-4e93-b783-d772f62d4556" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:02:20.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9587" for this suite.

• [SLOW TEST:8.199 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2607,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:02:21.000: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3617
STEP: creating service affinity-nodeport in namespace services-3617
STEP: creating replication controller affinity-nodeport in namespace services-3617
I0622 13:02:21.154194      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-3617, replica count: 3
I0622 13:02:24.204645      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:27.204817      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:30.205113      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:33.205429      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:36.205656      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:39.205918      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:42.206122      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:45.206295      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:48.206489      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:02:51.206686      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:02:51.218: INFO: Creating new exec pod
Jun 22 13:02:56.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun 22 13:02:56.459: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 22 13:02:56.459: INFO: stdout: ""
Jun 22 13:02:56.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 10.100.196.99 80'
Jun 22 13:02:56.630: INFO: stderr: "+ nc -zv -t -w 2 10.100.196.99 80\nConnection to 10.100.196.99 80 port [tcp/http] succeeded!\n"
Jun 22 13:02:56.631: INFO: stdout: ""
Jun 22 13:02:56.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 30619'
Jun 22 13:02:56.803: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 30619\nConnection to 11.0.1.3 30619 port [tcp/30619] succeeded!\n"
Jun 22 13:02:56.803: INFO: stdout: ""
Jun 22 13:02:56.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 30619'
Jun 22 13:02:56.966: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 30619\nConnection to 11.0.1.4 30619 port [tcp/30619] succeeded!\n"
Jun 22 13:02:56.966: INFO: stdout: ""
Jun 22 13:02:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 30619'
Jun 22 13:02:57.130: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 30619\nConnection to 11.0.1.3 30619 port [tcp/30619] succeeded!\n"
Jun 22 13:02:57.130: INFO: stdout: ""
Jun 22 13:02:57.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 30619'
Jun 22 13:02:57.305: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 30619\nConnection to 11.0.1.4 30619 port [tcp/30619] succeeded!\n"
Jun 22 13:02:57.305: INFO: stdout: ""
Jun 22 13:02:57.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-3617 exec execpod-affinitygnvc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.1.4:30619/ ; done'
Jun 22 13:02:57.555: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:30619/\n"
Jun 22 13:02:57.555: INFO: stdout: "\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9\naffinity-nodeport-xqdh9"
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Received response from host: affinity-nodeport-xqdh9
Jun 22 13:02:57.555: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3617, will wait for the garbage collector to delete the pods
Jun 22 13:02:57.626: INFO: Deleting ReplicationController affinity-nodeport took: 5.060701ms
Jun 22 13:02:58.226: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.114528ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:03:13.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3617" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:52.960 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":143,"skipped":2626,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:03:13.961: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:03:30.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5541" for this suite.

• [SLOW TEST:16.238 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":144,"skipped":2644,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:03:30.198: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-b9d8a3e0-2d65-4379-a295-cda13cc3128e
STEP: Creating a pod to test consume configMaps
Jun 22 13:03:30.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f" in namespace "configmap-2332" to be "Succeeded or Failed"
Jun 22 13:03:30.356: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.719203ms
Jun 22 13:03:32.361: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014750823s
Jun 22 13:03:34.366: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019540713s
Jun 22 13:03:36.371: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024692027s
Jun 22 13:03:38.375: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02867353s
Jun 22 13:03:40.378: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.032235523s
STEP: Saw pod success
Jun 22 13:03:40.378: INFO: Pod "pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f" satisfied condition "Succeeded or Failed"
Jun 22 13:03:40.381: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:03:40.399: INFO: Waiting for pod pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f to disappear
Jun 22 13:03:40.402: INFO: Pod pod-configmaps-796edd9c-b56c-446c-bd5c-084a4d257c2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:03:40.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2332" for this suite.

• [SLOW TEST:10.214 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2645,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:03:40.414: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:03:41.407: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:03:43.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:03:45.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:03:47.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:03:49.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:03:51.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:03:53.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759963821, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:03:56.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:03:56.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4146" for this suite.
STEP: Destroying namespace "webhook-4146-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.144 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":146,"skipped":2649,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:03:56.558: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0622 13:03:57.293709      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:03:57.293812      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:03:57.293865      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:03:57.293: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:03:57.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4494" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":147,"skipped":2668,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:03:57.305: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-568
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 13:03:57.438: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 22 13:03:57.519: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:03:59.525: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:04:01.527: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:04:03.524: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:04:05.523: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:04:07.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:09.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:11.524: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:13.524: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:15.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:17.535: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:19.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:21.524: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:04:23.524: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 22 13:04:23.531: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 22 13:04:23.538: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 22 13:04:23.544: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 13:04:25.548: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun 22 13:04:29.593: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.32.10.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-568 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:04:29.593: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:04:30.676: INFO: Found all expected endpoints: [netserver-0]
Jun 22 13:04:30.681: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.32.10.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-568 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:04:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:04:31.774: INFO: Found all expected endpoints: [netserver-1]
Jun 22 13:04:31.778: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.32.10.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-568 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:04:31.778: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:04:32.879: INFO: Found all expected endpoints: [netserver-2]
Jun 22 13:04:32.884: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.32.10.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-568 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:04:32.884: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:04:33.977: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:04:33.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-568" for this suite.

• [SLOW TEST:36.690 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2686,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:04:33.996: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:04:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8151" for this suite.

• [SLOW TEST:18.228 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":149,"skipped":2690,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:04:52.227: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:06.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6553" for this suite.

• [SLOW TEST:14.157 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":150,"skipped":2705,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 22 13:05:06.538: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:06.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8157" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":151,"skipped":2729,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:06.567: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-b362ece3-050e-4251-8d01-f20f3df47c6f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:06.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9633" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":152,"skipped":2736,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:06.720: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3629
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:05:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 13:05:10.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3629 --namespace=crd-publish-openapi-3629 create -f -'
Jun 22 13:05:12.119: INFO: stderr: ""
Jun 22 13:05:12.119: INFO: stdout: "e2e-test-crd-publish-openapi-336-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 13:05:12.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3629 --namespace=crd-publish-openapi-3629 delete e2e-test-crd-publish-openapi-336-crds test-cr'
Jun 22 13:05:12.206: INFO: stderr: ""
Jun 22 13:05:12.206: INFO: stdout: "e2e-test-crd-publish-openapi-336-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 22 13:05:12.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3629 --namespace=crd-publish-openapi-3629 apply -f -'
Jun 22 13:05:12.426: INFO: stderr: ""
Jun 22 13:05:12.426: INFO: stdout: "e2e-test-crd-publish-openapi-336-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 13:05:12.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3629 --namespace=crd-publish-openapi-3629 delete e2e-test-crd-publish-openapi-336-crds test-cr'
Jun 22 13:05:12.502: INFO: stderr: ""
Jun 22 13:05:12.502: INFO: stdout: "e2e-test-crd-publish-openapi-336-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 13:05:12.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3629 explain e2e-test-crd-publish-openapi-336-crds'
Jun 22 13:05:12.689: INFO: stderr: ""
Jun 22 13:05:12.689: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-336-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:16.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3629" for this suite.

• [SLOW TEST:9.511 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":153,"skipped":2757,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:16.231: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 13:05:26.898: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1fd12442-00ca-4116-afe0-72f36ada1c7f"
Jun 22 13:05:26.898: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1fd12442-00ca-4116-afe0-72f36ada1c7f" in namespace "pods-2903" to be "terminated due to deadline exceeded"
Jun 22 13:05:26.902: INFO: Pod "pod-update-activedeadlineseconds-1fd12442-00ca-4116-afe0-72f36ada1c7f": Phase="Running", Reason="", readiness=true. Elapsed: 3.856002ms
Jun 22 13:05:28.905: INFO: Pod "pod-update-activedeadlineseconds-1fd12442-00ca-4116-afe0-72f36ada1c7f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007346379s
Jun 22 13:05:28.905: INFO: Pod "pod-update-activedeadlineseconds-1fd12442-00ca-4116-afe0-72f36ada1c7f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:28.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2903" for this suite.

• [SLOW TEST:12.687 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2766,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Jun 22 13:05:29.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 create -f -'
Jun 22 13:05:29.302: INFO: stderr: ""
Jun 22 13:05:29.302: INFO: stdout: "pod/pause created\n"
Jun 22 13:05:29.302: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 13:05:29.302: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7300" to be "running and ready"
Jun 22 13:05:29.309: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.784459ms
Jun 22 13:05:31.313: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010719133s
Jun 22 13:05:33.318: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01514491s
Jun 22 13:05:35.322: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019730599s
Jun 22 13:05:37.326: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023713255s
Jun 22 13:05:39.330: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027972439s
Jun 22 13:05:41.335: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032786932s
Jun 22 13:05:43.340: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037544991s
Jun 22 13:05:45.344: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042096732s
Jun 22 13:05:47.348: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 18.046069063s
Jun 22 13:05:47.348: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 13:05:47.349: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 22 13:05:47.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 label pods pause testing-label=testing-label-value'
Jun 22 13:05:47.429: INFO: stderr: ""
Jun 22 13:05:47.429: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 22 13:05:47.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 get pod pause -L testing-label'
Jun 22 13:05:47.502: INFO: stderr: ""
Jun 22 13:05:47.502: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          18s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 22 13:05:47.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 label pods pause testing-label-'
Jun 22 13:05:47.590: INFO: stderr: ""
Jun 22 13:05:47.590: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 22 13:05:47.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 get pod pause -L testing-label'
Jun 22 13:05:47.661: INFO: stderr: ""
Jun 22 13:05:47.661: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          18s   \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Jun 22 13:05:47.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 delete --grace-period=0 --force -f -'
Jun 22 13:05:47.746: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:05:47.746: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 13:05:47.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 get rc,svc -l name=pause --no-headers'
Jun 22 13:05:47.822: INFO: stderr: "No resources found in kubectl-7300 namespace.\n"
Jun 22 13:05:47.822: INFO: stdout: ""
Jun 22 13:05:47.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-7300 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 13:05:47.894: INFO: stderr: ""
Jun 22 13:05:47.894: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:47.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7300" for this suite.

• [SLOW TEST:18.986 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1305
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":155,"skipped":2766,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 22 13:05:48.049: INFO: Created pod &Pod{ObjectMeta:{dns-9416  dns-9416 /api/v1/namespaces/dns-9416/pods/dns-9416 f48ee580-2035-4b5a-95c2-ff9e08ffdc87 168249 0 2021-06-22 13:05:48 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-06-22 13:05:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fl5r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fl5r8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fl5r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 13:05:48.058: INFO: The status of Pod dns-9416 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:05:50.061: INFO: The status of Pod dns-9416 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:05:52.062: INFO: The status of Pod dns-9416 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:05:54.061: INFO: The status of Pod dns-9416 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:05:56.061: INFO: The status of Pod dns-9416 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 22 13:05:56.061: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9416 PodName:dns-9416 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:05:56.061: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Verifying customized DNS server is configured on pod...
Jun 22 13:05:56.172: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9416 PodName:dns-9416 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:05:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:05:56.263: INFO: Deleting pod dns-9416...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:56.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9416" for this suite.

• [SLOW TEST:8.381 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":156,"skipped":2769,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:56.289: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2892
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:05:56.425: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:05:57.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2892" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":157,"skipped":2777,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:05:57.590: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 22 13:06:07.745: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9934 PodName:var-expansion-743f01eb-3e56-4e22-8978-9ee1e102d578 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:06:07.745: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: test for file in mounted path
Jun 22 13:06:07.840: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9934 PodName:var-expansion-743f01eb-3e56-4e22-8978-9ee1e102d578 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:06:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: updating the annotation value
Jun 22 13:06:08.452: INFO: Successfully updated pod "var-expansion-743f01eb-3e56-4e22-8978-9ee1e102d578"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 22 13:06:08.455: INFO: Deleting pod "var-expansion-743f01eb-3e56-4e22-8978-9ee1e102d578" in namespace "var-expansion-9934"
Jun 22 13:06:08.472: INFO: Wait up to 5m0s for pod "var-expansion-743f01eb-3e56-4e22-8978-9ee1e102d578" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:06:42.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9934" for this suite.

• [SLOW TEST:44.900 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":158,"skipped":2810,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:06:42.490: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4979/configmap-test-c388b135-3548-4cfc-a944-448e2c46a288
STEP: Creating a pod to test consume configMaps
Jun 22 13:06:42.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5" in namespace "configmap-4979" to be "Succeeded or Failed"
Jun 22 13:06:42.645: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034354ms
Jun 22 13:06:44.649: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014484353s
Jun 22 13:06:46.659: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023910602s
Jun 22 13:06:48.662: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027859418s
Jun 22 13:06:50.666: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031552951s
Jun 22 13:06:52.671: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036168677s
STEP: Saw pod success
Jun 22 13:06:52.671: INFO: Pod "pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5" satisfied condition "Succeeded or Failed"
Jun 22 13:06:52.674: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5 container env-test: <nil>
STEP: delete the pod
Jun 22 13:06:52.700: INFO: Waiting for pod pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5 to disappear
Jun 22 13:06:52.703: INFO: Pod pod-configmaps-6ae5622e-b19f-46b2-857e-d06ba7d493c5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:06:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4979" for this suite.

• [SLOW TEST:10.222 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2815,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:06:52.713: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8678
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8678
STEP: Creating statefulset with conflicting port in namespace statefulset-8678
STEP: Waiting until pod test-pod will start running in namespace statefulset-8678
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8678
Jun 22 13:07:02.904: INFO: Observed stateful pod in namespace: statefulset-8678, name: ss-0, uid: e91ad62e-1c64-4796-85de-eeeec1ca2034, status phase: Pending. Waiting for statefulset controller to delete.
Jun 22 13:07:03.281: INFO: Observed stateful pod in namespace: statefulset-8678, name: ss-0, uid: e91ad62e-1c64-4796-85de-eeeec1ca2034, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 13:07:03.285: INFO: Observed stateful pod in namespace: statefulset-8678, name: ss-0, uid: e91ad62e-1c64-4796-85de-eeeec1ca2034, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 13:07:03.291: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8678
STEP: Removing pod with conflicting port in namespace statefulset-8678
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8678 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 13:07:07.341: INFO: Deleting all statefulset in ns statefulset-8678
Jun 22 13:07:07.344: INFO: Scaling statefulset ss to 0
Jun 22 13:07:27.367: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:07:27.370: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:07:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8678" for this suite.

• [SLOW TEST:34.683 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":160,"skipped":2815,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:07:27.402: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a4d1b611-f1f3-4803-851c-a5bd0b306473
STEP: Creating a pod to test consume configMaps
Jun 22 13:07:27.547: INFO: Waiting up to 5m0s for pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90" in namespace "configmap-9067" to be "Succeeded or Failed"
Jun 22 13:07:27.551: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.563076ms
Jun 22 13:07:29.555: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008264077s
Jun 22 13:07:31.561: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013356051s
Jun 22 13:07:33.564: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016984768s
Jun 22 13:07:35.568: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020816372s
Jun 22 13:07:37.572: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.02447734s
STEP: Saw pod success
Jun 22 13:07:37.572: INFO: Pod "pod-configmaps-da998df1-db81-4198-831e-84bc31536b90" satisfied condition "Succeeded or Failed"
Jun 22 13:07:37.575: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-da998df1-db81-4198-831e-84bc31536b90 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:07:37.593: INFO: Waiting for pod pod-configmaps-da998df1-db81-4198-831e-84bc31536b90 to disappear
Jun 22 13:07:37.596: INFO: Pod pod-configmaps-da998df1-db81-4198-831e-84bc31536b90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:07:37.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9067" for this suite.

• [SLOW TEST:10.205 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":161,"skipped":2886,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:07:37.610: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7083
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:07:37.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7083" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":162,"skipped":2968,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:07:37.755: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 13:07:51.941: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:07:51.950: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 13:07:53.950: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:07:53.954: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 13:07:55.950: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:07:55.954: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 13:07:57.950: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:07:57.954: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 13:07:59.950: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:07:59.954: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 13:08:01.950: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 13:08:01.953: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:01.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4102" for this suite.

• [SLOW TEST:24.214 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2980,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:01.973: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 22 13:08:10.651: INFO: Successfully updated pod "labelsupdatedc62655b-3c26-4605-91fd-02d586a98573"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:12.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4560" for this suite.

• [SLOW TEST:10.706 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2988,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:12.680: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 22 13:08:12.829: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2476 /api/v1/namespaces/watch-2476/configmaps/e2e-watch-test-watch-closed faac0719-078d-43cf-9f9e-4fa5439fa406 169124 0 2021-06-22 13:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:08:12.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2476 /api/v1/namespaces/watch-2476/configmaps/e2e-watch-test-watch-closed faac0719-078d-43cf-9f9e-4fa5439fa406 169125 0 2021-06-22 13:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 22 13:08:12.842: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2476 /api/v1/namespaces/watch-2476/configmaps/e2e-watch-test-watch-closed faac0719-078d-43cf-9f9e-4fa5439fa406 169126 0 2021-06-22 13:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:08:12.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2476 /api/v1/namespaces/watch-2476/configmaps/e2e-watch-test-watch-closed faac0719-078d-43cf-9f9e-4fa5439fa406 169127 0 2021-06-22 13:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:12.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2476" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":165,"skipped":2992,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:12.856: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 22 13:08:13.048: INFO: Waiting up to 5m0s for pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2" in namespace "downward-api-5979" to be "Succeeded or Failed"
Jun 22 13:08:13.053: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.35214ms
Jun 22 13:08:15.056: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007820623s
Jun 22 13:08:17.061: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012598288s
Jun 22 13:08:19.065: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016563354s
Jun 22 13:08:21.068: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020229879s
STEP: Saw pod success
Jun 22 13:08:21.069: INFO: Pod "downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2" satisfied condition "Succeeded or Failed"
Jun 22 13:08:21.072: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2 container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:08:21.087: INFO: Waiting for pod downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2 to disappear
Jun 22 13:08:21.093: INFO: Pod downward-api-8f4f6921-8a9d-4954-8bdf-c38efb05b1c2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:21.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5979" for this suite.

• [SLOW TEST:8.250 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":3038,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:21.106: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-aad20180-eddc-4443-8094-fd6ba617323d
STEP: Creating a pod to test consume secrets
Jun 22 13:08:21.255: INFO: Waiting up to 5m0s for pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732" in namespace "secrets-5394" to be "Succeeded or Failed"
Jun 22 13:08:21.265: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732": Phase="Pending", Reason="", readiness=false. Elapsed: 9.925687ms
Jun 22 13:08:23.269: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014024493s
Jun 22 13:08:25.273: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018529692s
Jun 22 13:08:27.278: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022792408s
Jun 22 13:08:29.282: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027166493s
STEP: Saw pod success
Jun 22 13:08:29.282: INFO: Pod "pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732" satisfied condition "Succeeded or Failed"
Jun 22 13:08:29.285: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 13:08:29.303: INFO: Waiting for pod pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732 to disappear
Jun 22 13:08:29.307: INFO: Pod pod-secrets-52aba4e0-e15e-4b18-b732-2a8f48b71732 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5394" for this suite.

• [SLOW TEST:8.210 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":3042,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:29.317: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 13:08:29.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3586 /api/v1/namespaces/watch-3586/configmaps/e2e-watch-test-resource-version 1e2dd056-1f71-4995-bdda-6a366575b3ff 169258 0 2021-06-22 13:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:08:29.482: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3586 /api/v1/namespaces/watch-3586/configmaps/e2e-watch-test-resource-version 1e2dd056-1f71-4995-bdda-6a366575b3ff 169259 0 2021-06-22 13:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-22 13:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:29.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3586" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":168,"skipped":3045,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:29.493: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 13:08:38.678: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:38.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1985" for this suite.

• [SLOW TEST:9.213 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":3067,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jun 22 13:08:38.851: INFO: Waiting up to 5m0s for pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed" in namespace "var-expansion-174" to be "Succeeded or Failed"
Jun 22 13:08:38.857: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138552ms
Jun 22 13:08:40.861: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009651072s
Jun 22 13:08:42.866: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014614023s
Jun 22 13:08:44.870: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018588104s
Jun 22 13:08:46.874: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023118713s
Jun 22 13:08:48.878: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.02681972s
STEP: Saw pod success
Jun 22 13:08:48.878: INFO: Pod "var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed" satisfied condition "Succeeded or Failed"
Jun 22 13:08:48.881: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:08:48.896: INFO: Waiting for pod var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed to disappear
Jun 22 13:08:48.901: INFO: Pod var-expansion-84e5acf3-3bd3-4b62-9b48-6f74e115f4ed no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:48.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-174" for this suite.

• [SLOW TEST:10.204 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":3077,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:48.914: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 22 13:08:49.083: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 22 13:08:49.092: INFO: starting watch
STEP: patching
STEP: updating
Jun 22 13:08:49.111: INFO: waiting for watch events with expected annotations
Jun 22 13:08:49.111: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:49.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-413" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":171,"skipped":3077,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:49.181: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 13:08:49.328: INFO: Waiting up to 5m0s for pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e" in namespace "emptydir-4018" to be "Succeeded or Failed"
Jun 22 13:08:49.336: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.488664ms
Jun 22 13:08:51.341: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013118339s
Jun 22 13:08:53.345: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017058419s
Jun 22 13:08:55.350: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021979745s
Jun 22 13:08:57.354: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026216842s
STEP: Saw pod success
Jun 22 13:08:57.354: INFO: Pod "pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e" satisfied condition "Succeeded or Failed"
Jun 22 13:08:57.358: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e container test-container: <nil>
STEP: delete the pod
Jun 22 13:08:57.378: INFO: Waiting for pod pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e to disappear
Jun 22 13:08:57.381: INFO: Pod pod-2a8a279b-83b3-4c2f-9d42-2e5e98ae041e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:08:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4018" for this suite.

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":3078,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:08:57.398: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-ba87db0d-6475-447a-8674-e5adb2eb030a in namespace container-probe-4017
Jun 22 13:09:07.563: INFO: Started pod liveness-ba87db0d-6475-447a-8674-e5adb2eb030a in namespace container-probe-4017
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 13:09:07.567: INFO: Initial restart count of pod liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is 0
Jun 22 13:09:21.604: INFO: Restart count of pod container-probe-4017/liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is now 1 (14.036376207s elapsed)
Jun 22 13:09:41.648: INFO: Restart count of pod container-probe-4017/liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is now 2 (34.080368819s elapsed)
Jun 22 13:10:01.689: INFO: Restart count of pod container-probe-4017/liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is now 3 (54.121728447s elapsed)
Jun 22 13:10:21.734: INFO: Restart count of pod container-probe-4017/liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is now 4 (1m14.167206863s elapsed)
Jun 22 13:11:21.871: INFO: Restart count of pod container-probe-4017/liveness-ba87db0d-6475-447a-8674-e5adb2eb030a is now 5 (2m14.303965017s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:11:21.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4017" for this suite.

• [SLOW TEST:144.496 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":3137,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:11:21.897: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-077d7396-d0be-4d59-b05e-82c747a206f1
STEP: Creating a pod to test consume configMaps
Jun 22 13:11:22.044: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb" in namespace "projected-5835" to be "Succeeded or Failed"
Jun 22 13:11:22.048: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016636ms
Jun 22 13:11:24.051: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007607057s
Jun 22 13:11:26.055: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011699723s
Jun 22 13:11:28.060: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016224908s
Jun 22 13:11:30.065: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020998698s
Jun 22 13:11:32.070: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025789404s
STEP: Saw pod success
Jun 22 13:11:32.070: INFO: Pod "pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb" satisfied condition "Succeeded or Failed"
Jun 22 13:11:32.073: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:11:32.097: INFO: Waiting for pod pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb to disappear
Jun 22 13:11:32.101: INFO: Pod pod-projected-configmaps-7605905d-d2e1-4ed6-b2a3-48ea881d15bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:11:32.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5835" for this suite.

• [SLOW TEST:10.215 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":3178,"failed":2,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:11:32.112: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8259
Jun 22 13:11:34.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 22 13:11:34.457: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 22 13:11:34.457: INFO: stdout: "iptables"
Jun 22 13:11:34.457: INFO: proxyMode: iptables
Jun 22 13:11:34.462: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:11:34.465: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:11:36.465: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:11:36.470: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:11:38.465: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:11:38.471: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:11:40.465: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:11:40.470: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:11:42.465: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:11:42.470: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8259
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8259
I0622 13:11:42.488964      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8259, replica count: 3
I0622 13:11:45.539429      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:11:48.539761      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:11:48.545: INFO: Creating new exec pod
Jun 22 13:11:53.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun 22 13:11:53.731: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 22 13:11:53.731: INFO: stdout: ""
Jun 22 13:11:53.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c nc -zv -t -w 2 10.100.194.212 80'
Jun 22 13:11:53.903: INFO: stderr: "+ nc -zv -t -w 2 10.100.194.212 80\nConnection to 10.100.194.212 80 port [tcp/http] succeeded!\n"
Jun 22 13:11:53.903: INFO: stdout: ""
Jun 22 13:11:53.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:11:54.162: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:11:54.163: INFO: stdout: "\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z"
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:11:54.163: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:12:24.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:12:24.453: INFO: stdout: "\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z"
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:24.453: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:54.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:12:54.433: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:12:54.433: INFO: stdout: "\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z"
Jun 22 13:12:54.433: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:54.433: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.433: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:54.433: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.433: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:12:54.434: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:13:24.439: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:13:24.439: INFO: stdout: "\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb"
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:24.439: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:13:54.447: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:13:54.447: INFO: stdout: "\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z"
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.447: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-8259 exec execpod-affinitymflc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.194.212:80/ ; done'
Jun 22 13:13:54.709: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.194.212:80/\n"
Jun 22 13:13:54.709: INFO: stdout: "\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-mbqsr\naffinity-clusterip-timeout-9287z\naffinity-clusterip-timeout-wqhnb\naffinity-clusterip-timeout-mbqsr"
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-9287z
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-wqhnb
Jun 22 13:13:54.709: INFO: Received response from host: affinity-clusterip-timeout-mbqsr
Jun 22 13:13:54.709: INFO: [affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-9287z affinity-clusterip-timeout-9287z affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-mbqsr affinity-clusterip-timeout-9287z affinity-clusterip-timeout-wqhnb affinity-clusterip-timeout-mbqsr]
Jun 22 13:13:54.709: FAIL: Affinity should hold but didn't.

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.checkAffinityFailed(0xc005a88800, 0x60, 0x80, 0x4c74f7b, 0x20)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241 +0xdf
k8s.io/kubernetes/test/e2e/network.checkAffinity(0x540f680, 0xc00324ba20, 0xc003277800, 0xc003ed0ff0, 0xe, 0x50, 0x1, 0xc003277800)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:200 +0x225
k8s.io/kubernetes/test/e2e/network.execAffinityTestForSessionAffinityTimeout(0xc000a42b00, 0x540f680, 0xc00324ba20, 0xc00087f8c0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3447 +0x7a8
k8s.io/kubernetes/test/e2e/network.glob..func24.26()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2477 +0x9c
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc002309e00)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc002309e00, 0x4debd60)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
Jun 22 13:13:54.710: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8259, will wait for the garbage collector to delete the pods
Jun 22 13:13:54.787: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.921906ms
Jun 22 13:13:55.387: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.055251ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "services-8259".
STEP: Found 29 events.
Jun 22 13:14:04.016: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-9287z: { } Scheduled: Successfully assigned services-8259/affinity-clusterip-timeout-9287z to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.016: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-mbqsr: { } Scheduled: Successfully assigned services-8259/affinity-clusterip-timeout-mbqsr to 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:14:04.016: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-wqhnb: { } Scheduled: Successfully assigned services-8259/affinity-clusterip-timeout-wqhnb to 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:14:04.016: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for execpod-affinitymflc6: { } Scheduled: Successfully assigned services-8259/execpod-affinitymflc6 to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.016: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for kube-proxy-mode-detector: { } Scheduled: Successfully assigned services-8259/kube-proxy-mode-detector to 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.016: INFO: At 2021-06-22 13:11:32 +0000 UTC - event for kube-proxy-mode-detector: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container detector
Jun 22 13:14:04.016: INFO: At 2021-06-22 13:11:32 +0000 UTC - event for kube-proxy-mode-detector: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 13:14:04.016: INFO: At 2021-06-22 13:11:32 +0000 UTC - event for kube-proxy-mode-detector: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container detector
Jun 22 13:14:04.016: INFO: At 2021-06-22 13:11:35 +0000 UTC - event for kube-proxy-mode-detector: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container detector
Jun 22 13:14:04.016: INFO: At 2021-06-22 13:11:42 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-9287z
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:42 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-wqhnb
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:42 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-mbqsr
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-9287z: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-9287z: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-9287z: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-mbqsr: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Created: Created container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-mbqsr: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-mbqsr: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Started: Started container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-wqhnb: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Started: Started container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-wqhnb: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Created: Created container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:44 +0000 UTC - event for affinity-clusterip-timeout-wqhnb: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:50 +0000 UTC - event for execpod-affinitymflc6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.20" already present on machine
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:50 +0000 UTC - event for execpod-affinitymflc6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Created: Created container agnhost-container
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:11:50 +0000 UTC - event for execpod-affinitymflc6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Started: Started container agnhost-container
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:13:54 +0000 UTC - event for execpod-affinitymflc6: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container agnhost-container
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:13:55 +0000 UTC - event for affinity-clusterip-timeout: {endpoint-controller } FailedToUpdateEndpoint: Failed to update endpoint services-8259/affinity-clusterip-timeout: Operation cannot be fulfilled on endpoints "affinity-clusterip-timeout": the object has been modified; please apply your changes to the latest version and try again
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:13:55 +0000 UTC - event for affinity-clusterip-timeout-9287z: {kubelet 06998c1b-9fed-44d7-827f-f702404ff383} Killing: Stopping container affinity-clusterip-timeout
Jun 22 13:14:04.017: INFO: At 2021-06-22 13:13:55 +0000 UTC - event for affinity-clusterip-timeout-mbqsr: {kubelet 2352dbd9-b599-409b-9a0b-5bade7a216ea} Killing: Stopping container affinity-clusterip-timeout
Jun 22 13:14:04.018: INFO: At 2021-06-22 13:13:55 +0000 UTC - event for affinity-clusterip-timeout-wqhnb: {kubelet 1d96d19c-1b78-44d8-b822-ba104bc5daa5} Killing: Stopping container affinity-clusterip-timeout
Jun 22 13:14:04.020: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Jun 22 13:14:04.020: INFO: 
Jun 22 13:14:04.026: INFO: 
Logging node info for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.029: INFO: Node Info: &Node{ObjectMeta:{06998c1b-9fed-44d7-827f-f702404ff383   /api/v1/nodes/06998c1b-9fed-44d7-827f-f702404ff383 d6e824ee-0f24-4194-ba8c-f2897d500b67 170498 0 2021-06-21 23:48:31 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:8b60bac6-7336-4a89-974a-c41f2875f963 bosh.zone:WL-OLT-Dev-02 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-02 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.4 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.4] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {e2e.test Update v1 2021-06-22 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}}} {kubelet Update v1 2021-06-22 12:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e034f-72fb-1ea5-cc8c-80ef6bea7bb1,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:52 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:52 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:52 +0000 UTC,LastTransitionTime:2021-06-21 23:48:31 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 13:13:52 +0000 UTC,LastTransitionTime:2021-06-21 23:48:41 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.4,},NodeAddress{Type:InternalIP,Address:11.0.1.4,},NodeAddress{Type:Hostname,Address:11.0.1.4,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:c5dcfa0ab1ae479d98fa8e37e017524d,SystemUUID:420E034F-72FB-1EA5-CC8C-80EF6BEA7BB1,BootID:0d938454-6d4a-4970-b6fd-51fd6a99bee7,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 13:14:04.029: INFO: 
Logging kubelet events for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.033: INFO: 
Logging pods the kubelet thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.048: INFO: node-exporter-j9gvz started at 2021-06-22 12:47:11 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.048: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:14:04.048: INFO: fluent-bit-r2rws started at 2021-06-22 12:47:11 +0000 UTC (1+2 container statuses recorded)
Jun 22 13:14:04.048: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 13:14:04.048: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:14:04.048: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:14:04.048: INFO: sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.048: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 13:14:04.048: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 13:14:04.048: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 22 13:14:04.048: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:14:04.048: INFO: telegraf-7gq7r started at 2021-06-22 12:47:11 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.048: INFO: 	Container telegraf ready: true, restart count 0
W0622 13:14:04.053012      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:14:04.053029      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:14:04.053035      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:14:04.074: INFO: 
Latency metrics for node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:14:04.074: INFO: 
Logging node info for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:14:04.077: INFO: Node Info: &Node{ObjectMeta:{1d96d19c-1b78-44d8-b822-ba104bc5daa5   /api/v1/nodes/1d96d19c-1b78-44d8-b822-ba104bc5daa5 3382db90-e9d9-4d99-92c9-eed5527dcd67 170481 0 2021-06-21 23:42:33 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:38c4eb74-265f-4dd6-a9f0-1bac74785fba bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.3 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.3] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420eaaab-5adc-eb3a-9a4f-3b73f38acf24,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364527616 0} {<nil>} 8168484Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259670016 0} {<nil>} 8066084Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:46 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:46 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:46 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 13:13:46 +0000 UTC,LastTransitionTime:2021-06-21 23:42:33 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.3,},NodeAddress{Type:InternalIP,Address:11.0.1.3,},NodeAddress{Type:Hostname,Address:11.0.1.3,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:fd3f65ecda8a4cb3bd31bfb34c08ea2c,SystemUUID:420EAAAB-5ADC-EB3A-9A4F-3B73F38ACF24,BootID:8812dfc4-73c0-4087-8280-61b5eb4cbf39,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 13:14:04.078: INFO: 
Logging kubelet events for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:14:04.081: INFO: 
Logging pods the kubelet thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:14:04.111: INFO: sink-controller-f6bc7f774-zprjs started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 13:14:04.111: INFO: telegraf-j56d5 started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:14:04.111: INFO: node-exporter-w6bbv started at 2021-06-21 23:42:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:14:04.111: INFO: observability-manager-6cf797f97-8nqdm started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 13:14:04.111: INFO: metric-controller-7f5cb8ff6d-7npkg started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 13:14:04.111: INFO: metrics-server-7d476fdfbd-md64k started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.111: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 13:14:04.111: INFO: coredns-645ccbcd68-7dlmn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.112: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:14:04.112: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 13:14:04.112: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 22 13:14:04.112: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:14:04.112: INFO: fluent-bit-8rhpl started at 2021-06-21 23:47:44 +0000 UTC (1+2 container statuses recorded)
Jun 22 13:14:04.112: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 13:14:04.112: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:14:04.112: INFO: 	Container ghostunnel ready: true, restart count 0
W0622 13:14:04.120679      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:14:04.120810      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:14:04.120932      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:14:04.144: INFO: 
Latency metrics for node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:14:04.144: INFO: 
Logging node info for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:14:04.147: INFO: Node Info: &Node{ObjectMeta:{2352dbd9-b599-409b-9a0b-5bade7a216ea   /api/v1/nodes/2352dbd9-b599-409b-9a0b-5bade7a216ea 4ffa18f4-da34-4eb7-8052-e83631b76176 170487 0 2021-06-21 23:52:14 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:975f3130-3bff-4253-88c9-75060b9b5a34 bosh.zone:WL-ZHH-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-ZHH-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.7 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.7] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:52:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e7dbb-3f7c-5670-16cd-e87a1cc6ae33,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:48 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:48 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:48 +0000 UTC,LastTransitionTime:2021-06-21 23:52:14 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 13:13:48 +0000 UTC,LastTransitionTime:2021-06-21 23:52:24 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.7,},NodeAddress{Type:InternalIP,Address:11.0.1.7,},NodeAddress{Type:Hostname,Address:11.0.1.7,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:90e66be501cd48dc83f0fad01e6c038c,SystemUUID:420E7DBB-3F7C-5670-16CD-E87A1CC6AE33,BootID:eaba17bb-8b7e-4b67-ac5d-85bb9e5c744d,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 13:14:04.147: INFO: 
Logging kubelet events for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:14:04.150: INFO: 
Logging pods the kubelet thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:14:04.171: INFO: fluent-bit-vtnl7 started at 2021-06-21 23:52:24 +0000 UTC (1+2 container statuses recorded)
Jun 22 13:14:04.171: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 13:14:04.171: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:14:04.171: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:14:04.171: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 13:14:04.171: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 22 13:14:04.171: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:14:04.171: INFO: node-exporter-82sxc started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.172: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:14:04.172: INFO: telemetry-agent-8444c9c47b-7xllf started at 2021-06-21 23:55:50 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.172: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 13:14:04.172: INFO: telegraf-dwvlw started at 2021-06-21 23:52:24 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.172: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:14:04.172: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 13:14:04.172: INFO: 	Container e2e ready: true, restart count 0
Jun 22 13:14:04.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
W0622 13:14:04.176373      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:14:04.176390      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:14:04.176397      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:14:04.202: INFO: 
Latency metrics for node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:14:04.202: INFO: 
Logging node info for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:14:04.205: INFO: Node Info: &Node{ObjectMeta:{54b7d611-8263-481b-bda6-56b40bff2a2f   /api/v1/nodes/54b7d611-8263-481b-bda6-56b40bff2a2f d071e496-861f-42ba-a9f8-529501f196ca 170474 0 2021-06-21 23:46:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux bosh.id:bd75bf6d-fb7a-49e2-a49b-8b225c259383 bosh.zone:WL-OLT-Dev-01 failure-domain.beta.kubernetes.io/zone:WL-OLT-Dev-01 kubernetes.io/arch:amd64 kubernetes.io/hostname:11.0.1.5 kubernetes.io/os:linux pks-system/cluster.name:e2e5247b-a9ad-41da-9919-fa9bccbdd539 pks-system/cluster.uuid:service-instance_8c302d16-8348-4c3b-9ba5-9299864faf94 spec.ip:11.0.1.5] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-06-21 23:46:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}}}} {kubelet Update v1 2021-06-22 12:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:bosh.id":{},"f:bosh.zone":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:pks-system/cluster.name":{},"f:pks-system/cluster.uuid":{},"f:spec.ip":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:vsphere://420e2237-ebbd-7ba1-c525-758ac4c56ba2,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33684307968 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8364535808 0} {<nil>} 8168492Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{30315877122 0} {<nil>} 30315877122 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8259678208 0} {<nil>} 8066092Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:44 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:44 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-06-22 13:13:44 +0000 UTC,LastTransitionTime:2021-06-21 23:46:12 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-06-22 13:13:44 +0000 UTC,LastTransitionTime:2021-06-21 23:46:22 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:ExternalIP,Address:11.0.1.5,},NodeAddress{Type:InternalIP,Address:11.0.1.5,},NodeAddress{Type:Hostname,Address:11.0.1.5,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:20304c03bce241438791dd1ad3ceb11a,SystemUUID:420E2237-EBBD-7BA1-C525-758AC4C56BA2,BootID:880882cc-1831-4c1a-bc3f-4b0766aa256e,KernelVersion:4.15.0-142-generic,OSImage:Ubuntu 16.04.7 LTS,ContainerRuntimeVersion:docker://19.3.14,KubeletVersion:v1.19.9+vmware.1,KubeProxyVersion:v1.19.9+vmware.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:2.0],SizeBytes:2676286024,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/cuda-vector-add:1.0],SizeBytes:2016985663,},ContainerImage{Names:[perl:5.26],SizeBytes:853285759,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1],SizeBytes:751946873,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0],SizeBytes:414402477,},ContainerImage{Names:[k8s.gcr.io/sig-storage/nfs-provisioner:v2.2.2],SizeBytes:391772778,},ContainerImage{Names:[gluster/glusterdynamic-provisioner:v1.0],SizeBytes:373281573,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0],SizeBytes:332011484,},ContainerImage{Names:[sonobuoy/systemd-logs:v0.3],SizeBytes:297365055,},ContainerImage{Names:[k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance:v1.19.9],SizeBytes:229700446,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0],SizeBytes:225358913,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/telegraf:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:191406783,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/cert-generator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:189057098,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/validator:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:188444401,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/fluent-bit-out-syslog:e8890cf1e4c9fa65f3eac365474017336c3dbc4c],SizeBytes:168859229,},ContainerImage{Names:[pkstelemetrybot/telemetry-agent:latest],SizeBytes:135481903,},ContainerImage{Names:[httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/observability-manager:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:122053764,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/event-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:103297031,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/metric-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:100670682,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/sink-controller:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:99518332,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/node-exporter:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:80753097,},ContainerImage{Names:[epks-docker-local.artifactory.eng.vmware.com/oratos/ghostunnel:e484f2435fc830429fc9747bd002fcda56dd053e],SizeBytes:79124007,},ContainerImage{Names:[k8s.gcr.io/build-image/debian-iptables:v12.1.2],SizeBytes:78881069,},ContainerImage{Names:[k8s.gcr.io/prometheus-dummy-exporter:v0.1.0],SizeBytes:64832470,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/sd-dummy-exporter:v0.2.0],SizeBytes:58293544,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5],SizeBytes:49818305,},ContainerImage{Names:[cnabu-docker-local.artifactory.eng.vmware.com/k8s/metrics-server-amd64:v0.3.6-vmware-0001],SizeBytes:44669035,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonroot:1.0],SizeBytes:42321438,},ContainerImage{Names:[projects.registry.vmware.com/tkg/coredns:v1.7.0_vmware.8],SizeBytes:42010567,},ContainerImage{Names:[k8s.gcr.io/prometheus-to-sd:v0.5.0],SizeBytes:41861013,},ContainerImage{Names:[sonobuoy/sonobuoy:v0.51.0],SizeBytes:31863175,},ContainerImage{Names:[redis:5.0.5-alpine],SizeBytes:29331594,},ContainerImage{Names:[simple-server:latest],SizeBytes:25289579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/echoserver:2.2],SizeBytes:21692741,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/regression-issue-74839-amd64:1.0],SizeBytes:19227369,},ContainerImage{Names:[nginx:1.15-alpine],SizeBytes:16087791,},ContainerImage{Names:[nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0],SizeBytes:13090050,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0],SizeBytes:10039224,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2],SizeBytes:5124686,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/kitten:1.0],SizeBytes:4747037,},ContainerImage{Names:[busybox:1.29],SizeBytes:1154361,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jun 22 13:14:04.206: INFO: 
Logging kubelet events for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:14:04.209: INFO: 
Logging pods the kubelet thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:14:04.223: INFO: coredns-645ccbcd68-pbmnn started at 2021-06-21 23:47:34 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.223: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:14:04.223: INFO: validator-69f557d7c6-bl8gf started at 2021-06-21 23:47:39 +0000 UTC (1+1 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Init container patch-ca ready: true, restart count 0
Jun 22 13:14:04.224: INFO: 	Container validator ready: true, restart count 0
Jun 22 13:14:04.224: INFO: coredns-645ccbcd68-s8fjw started at 2021-06-21 23:47:33 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:14:04.224: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm started at 2021-06-22 11:57:05 +0000 UTC (0+2 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 22 13:14:04.224: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:14:04.224: INFO: telegraf-9z5xm started at 2021-06-21 23:47:39 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:14:04.224: INFO: node-exporter-8qrc5 started at 2021-06-21 23:46:22 +0000 UTC (0+1 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:14:04.224: INFO: event-controller-85d6bb4d4c-fz5nk started at 2021-06-21 23:47:39 +0000 UTC (1+2 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 13:14:04.224: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 13:14:04.224: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:14:04.224: INFO: fluent-bit-f425q started at 2021-06-21 23:48:02 +0000 UTC (1+2 container statuses recorded)
Jun 22 13:14:04.224: INFO: 	Init container concat-keystore ready: true, restart count 0
Jun 22 13:14:04.224: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:14:04.224: INFO: 	Container ghostunnel ready: true, restart count 0
W0622 13:14:04.228361      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:14:04.228380      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:14:04.228385      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:14:04.247: INFO: 
Latency metrics for node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:14:04.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8259" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• Failure [152.145 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  Jun 22 13:13:54.709: Affinity should hold but didn't.

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241
------------------------------
{"msg":"FAILED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":174,"skipped":3182,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:04.259: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 22 13:14:04.403: INFO: Waiting up to 5m0s for pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1" in namespace "downward-api-4033" to be "Succeeded or Failed"
Jun 22 13:14:04.413: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.753041ms
Jun 22 13:14:06.417: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014300643s
Jun 22 13:14:08.421: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01859233s
Jun 22 13:14:10.425: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022386169s
Jun 22 13:14:12.429: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02653459s
Jun 22 13:14:14.434: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031137962s
STEP: Saw pod success
Jun 22 13:14:14.434: INFO: Pod "downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1" satisfied condition "Succeeded or Failed"
Jun 22 13:14:14.437: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1 container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:14:14.454: INFO: Waiting for pod downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1 to disappear
Jun 22 13:14:14.458: INFO: Pod downward-api-05e6550a-c8ac-49d8-998e-52386f2779f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:14.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4033" for this suite.

• [SLOW TEST:10.209 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":3248,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 13:14:26.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 13:14:26.669: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 13:14:28.670: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 13:14:28.674: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 13:14:30.670: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 13:14:30.674: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:30.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3161" for this suite.

• [SLOW TEST:16.217 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":3270,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:30.687: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:14:31.051: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 13:14:33.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964471, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964471, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964471, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964471, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:14:36.073: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:14:36.076: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:37.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3714" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.571 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":177,"skipped":3281,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:37.258: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0622 13:14:38.062999      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:14:38.063128      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:14:38.063188      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:14:38.063: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:38.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4199" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":178,"skipped":3294,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:38.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8361" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":179,"skipped":3302,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:38.270: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 13:14:38.414: INFO: Waiting up to 5m0s for pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a" in namespace "emptydir-3049" to be "Succeeded or Failed"
Jun 22 13:14:38.427: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.895998ms
Jun 22 13:14:40.433: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018664078s
Jun 22 13:14:42.436: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022349108s
Jun 22 13:14:44.441: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026794309s
Jun 22 13:14:46.445: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Running", Reason="", readiness=true. Elapsed: 8.0313715s
Jun 22 13:14:48.449: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035208113s
STEP: Saw pod success
Jun 22 13:14:48.449: INFO: Pod "pod-11fbb582-5992-40b9-8eb8-7e307b97c92a" satisfied condition "Succeeded or Failed"
Jun 22 13:14:48.453: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-11fbb582-5992-40b9-8eb8-7e307b97c92a container test-container: <nil>
STEP: delete the pod
Jun 22 13:14:48.471: INFO: Waiting for pod pod-11fbb582-5992-40b9-8eb8-7e307b97c92a to disappear
Jun 22 13:14:48.474: INFO: Pod pod-11fbb582-5992-40b9-8eb8-7e307b97c92a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:48.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3049" for this suite.

• [SLOW TEST:10.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":180,"skipped":3302,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:48.485: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9795.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9795.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9795.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9795.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9795.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9795.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 13:14:58.697: INFO: DNS probes using dns-9795/dns-test-c45f637b-8d9c-4c10-978f-796ee038f04f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:14:58.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9795" for this suite.

• [SLOW TEST:10.274 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":181,"skipped":3327,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:14:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5460
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:14:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 13:15:02.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-5460 --namespace=crd-publish-openapi-5460 create -f -'
Jun 22 13:15:03.445: INFO: stderr: ""
Jun 22 13:15:03.445: INFO: stdout: "e2e-test-crd-publish-openapi-5566-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 13:15:03.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-5460 --namespace=crd-publish-openapi-5460 delete e2e-test-crd-publish-openapi-5566-crds test-cr'
Jun 22 13:15:03.533: INFO: stderr: ""
Jun 22 13:15:03.533: INFO: stdout: "e2e-test-crd-publish-openapi-5566-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 22 13:15:03.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-5460 --namespace=crd-publish-openapi-5460 apply -f -'
Jun 22 13:15:03.727: INFO: stderr: ""
Jun 22 13:15:03.727: INFO: stdout: "e2e-test-crd-publish-openapi-5566-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 13:15:03.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-5460 --namespace=crd-publish-openapi-5460 delete e2e-test-crd-publish-openapi-5566-crds test-cr'
Jun 22 13:15:03.816: INFO: stderr: ""
Jun 22 13:15:03.817: INFO: stdout: "e2e-test-crd-publish-openapi-5566-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 13:15:03.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-5460 explain e2e-test-crd-publish-openapi-5566-crds'
Jun 22 13:15:03.995: INFO: stderr: ""
Jun 22 13:15:03.995: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5566-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:15:07.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5460" for this suite.

• [SLOW TEST:8.793 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":182,"skipped":3346,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:15:07.555: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 13:15:19.754: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 13:15:19.758: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 13:15:21.758: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 13:15:21.762: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 13:15:23.758: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 13:15:23.762: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:15:23.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-128" for this suite.

• [SLOW TEST:16.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3352,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:15:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1032
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4265
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:15:30.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6858" for this suite.
STEP: Destroying namespace "nsdeletetest-1032" for this suite.
Jun 22 13:15:30.213: INFO: Namespace nsdeletetest-1032 was already deleted
STEP: Destroying namespace "nsdeletetest-4265" for this suite.

• [SLOW TEST:6.443 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":184,"skipped":3365,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:15:30.217: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3329
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:15:30.355: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 13:15:33.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3329 --namespace=crd-publish-openapi-3329 create -f -'
Jun 22 13:15:34.412: INFO: stderr: ""
Jun 22 13:15:34.412: INFO: stdout: "e2e-test-crd-publish-openapi-2340-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 13:15:34.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3329 --namespace=crd-publish-openapi-3329 delete e2e-test-crd-publish-openapi-2340-crds test-cr'
Jun 22 13:15:34.503: INFO: stderr: ""
Jun 22 13:15:34.503: INFO: stdout: "e2e-test-crd-publish-openapi-2340-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 22 13:15:34.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3329 --namespace=crd-publish-openapi-3329 apply -f -'
Jun 22 13:15:34.693: INFO: stderr: ""
Jun 22 13:15:34.693: INFO: stdout: "e2e-test-crd-publish-openapi-2340-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 13:15:34.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3329 --namespace=crd-publish-openapi-3329 delete e2e-test-crd-publish-openapi-2340-crds test-cr'
Jun 22 13:15:34.772: INFO: stderr: ""
Jun 22 13:15:34.772: INFO: stdout: "e2e-test-crd-publish-openapi-2340-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 22 13:15:34.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-3329 explain e2e-test-crd-publish-openapi-2340-crds'
Jun 22 13:15:34.955: INFO: stderr: ""
Jun 22 13:15:34.955: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2340-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:15:38.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3329" for this suite.

• [SLOW TEST:8.298 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":185,"skipped":3370,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:15:38.516: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t2jc7 in namespace proxy-1474
I0622 13:15:38.671587      20 runners.go:190] Created replication controller with name: proxy-service-t2jc7, namespace: proxy-1474, replica count: 1
I0622 13:15:39.721954      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:40.722186      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:41.722351      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:42.722526      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:43.722757      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:44.722998      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:45.723222      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:15:46.723465      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 13:15:47.723745      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 13:15:48.723903      20 runners.go:190] proxy-service-t2jc7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:15:48.727: INFO: setup took 10.073648181s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 22 13:15:48.733: INFO: (0) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 6.237932ms)
Jun 22 13:15:48.741: INFO: (0) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 14.183153ms)
Jun 22 13:15:48.743: INFO: (0) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 16.128611ms)
Jun 22 13:15:48.745: INFO: (0) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 17.572564ms)
Jun 22 13:15:48.745: INFO: (0) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 18.478124ms)
Jun 22 13:15:48.746: INFO: (0) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 18.745727ms)
Jun 22 13:15:48.746: INFO: (0) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 19.108924ms)
Jun 22 13:15:48.746: INFO: (0) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 19.352011ms)
Jun 22 13:15:48.748: INFO: (0) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 20.847329ms)
Jun 22 13:15:48.748: INFO: (0) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 21.128986ms)
Jun 22 13:15:48.748: INFO: (0) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 21.007294ms)
Jun 22 13:15:48.748: INFO: (0) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 21.316845ms)
Jun 22 13:15:48.749: INFO: (0) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 21.807335ms)
Jun 22 13:15:48.750: INFO: (0) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 22.499533ms)
Jun 22 13:15:48.750: INFO: (0) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 22.649471ms)
Jun 22 13:15:48.753: INFO: (0) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 26.095552ms)
Jun 22 13:15:48.760: INFO: (1) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 6.595754ms)
Jun 22 13:15:48.762: INFO: (1) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 8.070947ms)
Jun 22 13:15:48.762: INFO: (1) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 7.396817ms)
Jun 22 13:15:48.762: INFO: (1) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 8.455372ms)
Jun 22 13:15:48.762: INFO: (1) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 8.678367ms)
Jun 22 13:15:48.764: INFO: (1) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 10.833769ms)
Jun 22 13:15:48.764: INFO: (1) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 10.695267ms)
Jun 22 13:15:48.764: INFO: (1) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 10.582287ms)
Jun 22 13:15:48.765: INFO: (1) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.072225ms)
Jun 22 13:15:48.765: INFO: (1) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 10.98622ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 12.363166ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 11.661993ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 12.132772ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 12.065579ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 12.049363ms)
Jun 22 13:15:48.766: INFO: (1) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 12.992628ms)
Jun 22 13:15:48.771: INFO: (2) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 4.126358ms)
Jun 22 13:15:48.775: INFO: (2) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 8.540088ms)
Jun 22 13:15:48.776: INFO: (2) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 9.02891ms)
Jun 22 13:15:48.776: INFO: (2) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 8.953855ms)
Jun 22 13:15:48.776: INFO: (2) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 9.457184ms)
Jun 22 13:15:48.776: INFO: (2) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 9.811959ms)
Jun 22 13:15:48.777: INFO: (2) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 10.01838ms)
Jun 22 13:15:48.777: INFO: (2) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 10.205933ms)
Jun 22 13:15:48.779: INFO: (2) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.191413ms)
Jun 22 13:15:48.780: INFO: (2) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 12.830458ms)
Jun 22 13:15:48.780: INFO: (2) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 13.444494ms)
Jun 22 13:15:48.780: INFO: (2) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 13.17601ms)
Jun 22 13:15:48.780: INFO: (2) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 13.548413ms)
Jun 22 13:15:48.781: INFO: (2) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.930712ms)
Jun 22 13:15:48.781: INFO: (2) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 14.052624ms)
Jun 22 13:15:48.781: INFO: (2) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 14.336807ms)
Jun 22 13:15:48.790: INFO: (3) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 8.955815ms)
Jun 22 13:15:48.792: INFO: (3) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 10.052983ms)
Jun 22 13:15:48.792: INFO: (3) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 10.36237ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 13.81682ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.465872ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 14.291208ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 15.010013ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.627808ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 14.182978ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 14.726662ms)
Jun 22 13:15:48.796: INFO: (3) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 14.958487ms)
Jun 22 13:15:48.797: INFO: (3) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 14.285387ms)
Jun 22 13:15:48.797: INFO: (3) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 14.514653ms)
Jun 22 13:15:48.797: INFO: (3) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 14.873614ms)
Jun 22 13:15:48.797: INFO: (3) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 14.821237ms)
Jun 22 13:15:48.798: INFO: (3) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 15.522264ms)
Jun 22 13:15:48.814: INFO: (4) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 16.16353ms)
Jun 22 13:15:48.814: INFO: (4) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 16.554809ms)
Jun 22 13:15:48.815: INFO: (4) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 16.655047ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 18.898222ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 18.983683ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 19.054657ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 19.117064ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 19.173869ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 19.298611ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 19.592976ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 19.361023ms)
Jun 22 13:15:48.817: INFO: (4) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 19.49175ms)
Jun 22 13:15:48.818: INFO: (4) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 19.61956ms)
Jun 22 13:15:48.818: INFO: (4) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 19.82003ms)
Jun 22 13:15:48.818: INFO: (4) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 19.771173ms)
Jun 22 13:15:48.818: INFO: (4) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 19.860919ms)
Jun 22 13:15:48.822: INFO: (5) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 3.768985ms)
Jun 22 13:15:48.823: INFO: (5) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 4.593462ms)
Jun 22 13:15:48.824: INFO: (5) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 5.949015ms)
Jun 22 13:15:48.825: INFO: (5) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 6.825864ms)
Jun 22 13:15:48.826: INFO: (5) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 8.120917ms)
Jun 22 13:15:48.826: INFO: (5) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 8.242368ms)
Jun 22 13:15:48.827: INFO: (5) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 8.892451ms)
Jun 22 13:15:48.828: INFO: (5) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 9.833726ms)
Jun 22 13:15:48.829: INFO: (5) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 10.479429ms)
Jun 22 13:15:48.832: INFO: (5) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 14.061641ms)
Jun 22 13:15:48.833: INFO: (5) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 13.994604ms)
Jun 22 13:15:48.833: INFO: (5) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.227902ms)
Jun 22 13:15:48.833: INFO: (5) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.15669ms)
Jun 22 13:15:48.833: INFO: (5) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 14.378832ms)
Jun 22 13:15:48.833: INFO: (5) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 14.732352ms)
Jun 22 13:15:48.834: INFO: (5) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 15.420577ms)
Jun 22 13:15:48.845: INFO: (6) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 11.070167ms)
Jun 22 13:15:48.845: INFO: (6) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 11.070353ms)
Jun 22 13:15:48.845: INFO: (6) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.202174ms)
Jun 22 13:15:48.846: INFO: (6) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 11.139445ms)
Jun 22 13:15:48.846: INFO: (6) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 12.221158ms)
Jun 22 13:15:48.846: INFO: (6) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 11.924405ms)
Jun 22 13:15:48.846: INFO: (6) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 12.297379ms)
Jun 22 13:15:48.847: INFO: (6) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 13.039615ms)
Jun 22 13:15:48.847: INFO: (6) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.026199ms)
Jun 22 13:15:48.848: INFO: (6) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 13.329032ms)
Jun 22 13:15:48.848: INFO: (6) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 13.37173ms)
Jun 22 13:15:48.849: INFO: (6) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 14.5494ms)
Jun 22 13:15:48.849: INFO: (6) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 14.558071ms)
Jun 22 13:15:48.849: INFO: (6) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.354775ms)
Jun 22 13:15:48.849: INFO: (6) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 14.659865ms)
Jun 22 13:15:48.849: INFO: (6) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.712178ms)
Jun 22 13:15:48.857: INFO: (7) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 7.469731ms)
Jun 22 13:15:48.858: INFO: (7) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 8.617877ms)
Jun 22 13:15:48.858: INFO: (7) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 8.523664ms)
Jun 22 13:15:48.859: INFO: (7) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 9.504654ms)
Jun 22 13:15:48.859: INFO: (7) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 9.870845ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 11.849196ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 11.870863ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 11.751955ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 12.131777ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 12.566155ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.32583ms)
Jun 22 13:15:48.862: INFO: (7) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 13.04028ms)
Jun 22 13:15:48.863: INFO: (7) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 12.900831ms)
Jun 22 13:15:48.863: INFO: (7) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 13.862833ms)
Jun 22 13:15:48.864: INFO: (7) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 13.705573ms)
Jun 22 13:15:48.864: INFO: (7) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 14.235709ms)
Jun 22 13:15:48.871: INFO: (8) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 7.239217ms)
Jun 22 13:15:48.873: INFO: (8) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 8.592691ms)
Jun 22 13:15:48.874: INFO: (8) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 9.954758ms)
Jun 22 13:15:48.874: INFO: (8) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 10.262601ms)
Jun 22 13:15:48.876: INFO: (8) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 12.48125ms)
Jun 22 13:15:48.877: INFO: (8) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 12.74965ms)
Jun 22 13:15:48.877: INFO: (8) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 12.609197ms)
Jun 22 13:15:48.877: INFO: (8) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.528553ms)
Jun 22 13:15:48.877: INFO: (8) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 13.156952ms)
Jun 22 13:15:48.877: INFO: (8) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 12.572671ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 14.270526ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.259175ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.590065ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 14.007341ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 14.628993ms)
Jun 22 13:15:48.878: INFO: (8) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 14.0817ms)
Jun 22 13:15:48.890: INFO: (9) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 10.744875ms)
Jun 22 13:15:48.890: INFO: (9) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.171714ms)
Jun 22 13:15:48.893: INFO: (9) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 14.775411ms)
Jun 22 13:15:48.893: INFO: (9) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.765852ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 15.679424ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 15.778967ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 16.018828ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 16.144896ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 16.186267ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 16.480848ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 16.435086ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 16.318492ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 16.718793ms)
Jun 22 13:15:48.895: INFO: (9) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 16.620144ms)
Jun 22 13:15:48.896: INFO: (9) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 16.637586ms)
Jun 22 13:15:48.896: INFO: (9) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 16.973943ms)
Jun 22 13:15:48.904: INFO: (10) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 8.602726ms)
Jun 22 13:15:48.908: INFO: (10) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 11.638417ms)
Jun 22 13:15:48.908: INFO: (10) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 11.855753ms)
Jun 22 13:15:48.909: INFO: (10) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 12.408906ms)
Jun 22 13:15:48.909: INFO: (10) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.391012ms)
Jun 22 13:15:48.909: INFO: (10) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.799437ms)
Jun 22 13:15:48.909: INFO: (10) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.077077ms)
Jun 22 13:15:48.909: INFO: (10) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 12.989877ms)
Jun 22 13:15:48.910: INFO: (10) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 13.555372ms)
Jun 22 13:15:48.911: INFO: (10) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.984843ms)
Jun 22 13:15:48.911: INFO: (10) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 14.53314ms)
Jun 22 13:15:48.911: INFO: (10) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 15.116347ms)
Jun 22 13:15:48.912: INFO: (10) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 15.410287ms)
Jun 22 13:15:48.912: INFO: (10) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 15.353082ms)
Jun 22 13:15:48.912: INFO: (10) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 16.114245ms)
Jun 22 13:15:48.912: INFO: (10) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 15.555596ms)
Jun 22 13:15:48.917: INFO: (11) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 4.401072ms)
Jun 22 13:15:48.918: INFO: (11) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 6.136815ms)
Jun 22 13:15:48.919: INFO: (11) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 6.262969ms)
Jun 22 13:15:48.920: INFO: (11) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 7.799867ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 12.916233ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 12.91087ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 13.036935ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 13.272141ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.44555ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 13.310209ms)
Jun 22 13:15:48.926: INFO: (11) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 13.521724ms)
Jun 22 13:15:48.927: INFO: (11) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 13.605447ms)
Jun 22 13:15:48.927: INFO: (11) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 14.354838ms)
Jun 22 13:15:48.927: INFO: (11) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.469998ms)
Jun 22 13:15:48.927: INFO: (11) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 14.915989ms)
Jun 22 13:15:48.927: INFO: (11) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.968438ms)
Jun 22 13:15:48.939: INFO: (12) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 10.956179ms)
Jun 22 13:15:48.939: INFO: (12) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 11.13754ms)
Jun 22 13:15:48.940: INFO: (12) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.431692ms)
Jun 22 13:15:48.940: INFO: (12) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 12.362509ms)
Jun 22 13:15:48.941: INFO: (12) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 12.661024ms)
Jun 22 13:15:48.943: INFO: (12) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 15.250491ms)
Jun 22 13:15:48.943: INFO: (12) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 15.365884ms)
Jun 22 13:15:48.944: INFO: (12) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 16.486721ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 16.654643ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 16.772533ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 17.109464ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 16.904471ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 17.263238ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 17.323488ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 17.251518ms)
Jun 22 13:15:48.945: INFO: (12) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 17.108386ms)
Jun 22 13:15:48.951: INFO: (13) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 5.706896ms)
Jun 22 13:15:48.951: INFO: (13) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 5.607183ms)
Jun 22 13:15:48.963: INFO: (13) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 17.442945ms)
Jun 22 13:15:48.963: INFO: (13) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 17.599668ms)
Jun 22 13:15:48.963: INFO: (13) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 17.234771ms)
Jun 22 13:15:48.963: INFO: (13) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 17.155242ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 17.906152ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 18.062448ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 18.192999ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 19.01249ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 18.879759ms)
Jun 22 13:15:48.964: INFO: (13) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 18.070125ms)
Jun 22 13:15:48.965: INFO: (13) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 18.138907ms)
Jun 22 13:15:48.965: INFO: (13) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 19.150341ms)
Jun 22 13:15:48.967: INFO: (13) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 20.866588ms)
Jun 22 13:15:48.968: INFO: (13) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 21.890317ms)
Jun 22 13:15:48.976: INFO: (14) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 8.188529ms)
Jun 22 13:15:48.977: INFO: (14) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 8.294552ms)
Jun 22 13:15:48.977: INFO: (14) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 8.358169ms)
Jun 22 13:15:48.978: INFO: (14) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 9.328115ms)
Jun 22 13:15:48.981: INFO: (14) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 12.456334ms)
Jun 22 13:15:48.981: INFO: (14) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 12.642483ms)
Jun 22 13:15:48.981: INFO: (14) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 12.842099ms)
Jun 22 13:15:48.981: INFO: (14) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 12.929857ms)
Jun 22 13:15:48.982: INFO: (14) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 13.512075ms)
Jun 22 13:15:48.983: INFO: (14) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.394698ms)
Jun 22 13:15:48.984: INFO: (14) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 15.486011ms)
Jun 22 13:15:48.984: INFO: (14) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 15.75077ms)
Jun 22 13:15:48.984: INFO: (14) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 15.746396ms)
Jun 22 13:15:48.985: INFO: (14) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 16.858994ms)
Jun 22 13:15:48.986: INFO: (14) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 17.153782ms)
Jun 22 13:15:48.986: INFO: (14) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 17.559362ms)
Jun 22 13:15:48.993: INFO: (15) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 6.633137ms)
Jun 22 13:15:48.993: INFO: (15) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 6.661659ms)
Jun 22 13:15:48.993: INFO: (15) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 7.010199ms)
Jun 22 13:15:48.996: INFO: (15) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 9.691935ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 13.267041ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 13.294794ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 13.358955ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 13.605997ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 13.515541ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 14.091317ms)
Jun 22 13:15:49.000: INFO: (15) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.890084ms)
Jun 22 13:15:49.001: INFO: (15) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 14.084229ms)
Jun 22 13:15:49.001: INFO: (15) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 14.130604ms)
Jun 22 13:15:49.001: INFO: (15) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 14.355577ms)
Jun 22 13:15:49.001: INFO: (15) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 14.711141ms)
Jun 22 13:15:49.001: INFO: (15) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 14.413375ms)
Jun 22 13:15:49.008: INFO: (16) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 6.9883ms)
Jun 22 13:15:49.011: INFO: (16) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 10.045008ms)
Jun 22 13:15:49.011: INFO: (16) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 10.278823ms)
Jun 22 13:15:49.011: INFO: (16) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 10.270779ms)
Jun 22 13:15:49.011: INFO: (16) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 10.212852ms)
Jun 22 13:15:49.011: INFO: (16) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 10.178855ms)
Jun 22 13:15:49.012: INFO: (16) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 10.615542ms)
Jun 22 13:15:49.012: INFO: (16) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 10.832627ms)
Jun 22 13:15:49.012: INFO: (16) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.154799ms)
Jun 22 13:15:49.012: INFO: (16) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 10.955962ms)
Jun 22 13:15:49.014: INFO: (16) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 12.698508ms)
Jun 22 13:15:49.014: INFO: (16) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 12.545218ms)
Jun 22 13:15:49.014: INFO: (16) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 12.97049ms)
Jun 22 13:15:49.014: INFO: (16) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 12.986829ms)
Jun 22 13:15:49.014: INFO: (16) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 13.097931ms)
Jun 22 13:15:49.015: INFO: (16) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 13.425298ms)
Jun 22 13:15:49.023: INFO: (17) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 8.356432ms)
Jun 22 13:15:49.025: INFO: (17) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 10.43583ms)
Jun 22 13:15:49.027: INFO: (17) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 12.203797ms)
Jun 22 13:15:49.027: INFO: (17) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 12.331399ms)
Jun 22 13:15:49.027: INFO: (17) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.481139ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 12.555069ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.700483ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 12.597876ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 12.67997ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 13.001282ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 12.916852ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 13.196293ms)
Jun 22 13:15:49.028: INFO: (17) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 13.285634ms)
Jun 22 13:15:49.029: INFO: (17) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 13.811147ms)
Jun 22 13:15:49.029: INFO: (17) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 14.150608ms)
Jun 22 13:15:49.029: INFO: (17) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 14.300747ms)
Jun 22 13:15:49.036: INFO: (18) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 7.146704ms)
Jun 22 13:15:49.037: INFO: (18) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 7.127046ms)
Jun 22 13:15:49.038: INFO: (18) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 8.90244ms)
Jun 22 13:15:49.038: INFO: (18) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 9.160216ms)
Jun 22 13:15:49.039: INFO: (18) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 9.718616ms)
Jun 22 13:15:49.041: INFO: (18) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.78112ms)
Jun 22 13:15:49.041: INFO: (18) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 11.912066ms)
Jun 22 13:15:49.042: INFO: (18) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 12.011421ms)
Jun 22 13:15:49.042: INFO: (18) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 12.281419ms)
Jun 22 13:15:49.042: INFO: (18) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 12.425285ms)
Jun 22 13:15:49.042: INFO: (18) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 12.536322ms)
Jun 22 13:15:49.043: INFO: (18) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 13.540953ms)
Jun 22 13:15:49.043: INFO: (18) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 13.501187ms)
Jun 22 13:15:49.043: INFO: (18) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 13.747469ms)
Jun 22 13:15:49.043: INFO: (18) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 13.715636ms)
Jun 22 13:15:49.043: INFO: (18) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 13.771946ms)
Jun 22 13:15:49.051: INFO: (19) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z/proxy/rewriteme">test</a> (200; 7.249953ms)
Jun 22 13:15:49.051: INFO: (19) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 7.82932ms)
Jun 22 13:15:49.051: INFO: (19) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">test<... (200; 7.677684ms)
Jun 22 13:15:49.052: INFO: (19) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:443/proxy/tlsrewritem... (200; 7.998162ms)
Jun 22 13:15:49.052: INFO: (19) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 8.089109ms)
Jun 22 13:15:49.052: INFO: (19) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:162/proxy/: bar (200; 8.197602ms)
Jun 22 13:15:49.056: INFO: (19) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:462/proxy/: tls qux (200; 11.773389ms)
Jun 22 13:15:49.056: INFO: (19) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname1/proxy/: foo (200; 12.240685ms)
Jun 22 13:15:49.056: INFO: (19) /api/v1/namespaces/proxy-1474/pods/proxy-service-t2jc7-6hc8z:160/proxy/: foo (200; 11.910165ms)
Jun 22 13:15:49.056: INFO: (19) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname2/proxy/: bar (200; 12.738769ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname1/proxy/: tls baz (200; 12.639339ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/pods/https:proxy-service-t2jc7-6hc8z:460/proxy/: tls baz (200; 12.473527ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1474/pods/http:proxy-service-t2jc7-6hc8z:1080/proxy/rewriteme">... (200; 12.626136ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/services/https:proxy-service-t2jc7:tlsportname2/proxy/: tls qux (200; 12.881705ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/services/proxy-service-t2jc7:portname1/proxy/: foo (200; 13.598063ms)
Jun 22 13:15:49.057: INFO: (19) /api/v1/namespaces/proxy-1474/services/http:proxy-service-t2jc7:portname2/proxy/: bar (200; 13.178374ms)
STEP: deleting ReplicationController proxy-service-t2jc7 in namespace proxy-1474, will wait for the garbage collector to delete the pods
Jun 22 13:15:49.116: INFO: Deleting ReplicationController proxy-service-t2jc7 took: 5.138565ms
Jun 22 13:15:49.717: INFO: Terminating ReplicationController proxy-service-t2jc7 pods took: 600.279467ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:15:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1474" for this suite.

• [SLOW TEST:13.911 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":186,"skipped":3385,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:15:52.427: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 13:15:52.573: INFO: Waiting up to 5m0s for pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2" in namespace "emptydir-2259" to be "Succeeded or Failed"
Jun 22 13:15:52.583: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.19669ms
Jun 22 13:15:54.587: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014333782s
Jun 22 13:15:56.591: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017989005s
Jun 22 13:15:58.595: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022172346s
Jun 22 13:16:00.600: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02690118s
STEP: Saw pod success
Jun 22 13:16:00.600: INFO: Pod "pod-21261cbd-a262-4027-a629-313c41f6e9f2" satisfied condition "Succeeded or Failed"
Jun 22 13:16:00.603: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-21261cbd-a262-4027-a629-313c41f6e9f2 container test-container: <nil>
STEP: delete the pod
Jun 22 13:16:00.619: INFO: Waiting for pod pod-21261cbd-a262-4027-a629-313c41f6e9f2 to disappear
Jun 22 13:16:00.624: INFO: Pod pod-21261cbd-a262-4027-a629-313c41f6e9f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:00.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2259" for this suite.

• [SLOW TEST:8.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3403,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:08.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6391" for this suite.

• [SLOW TEST:8.175 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3446,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 22 13:16:08.952: INFO: Waiting up to 5m0s for pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8" in namespace "downward-api-523" to be "Succeeded or Failed"
Jun 22 13:16:08.955: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935262ms
Jun 22 13:16:10.963: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010706261s
Jun 22 13:16:12.967: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015423526s
Jun 22 13:16:14.971: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01929462s
Jun 22 13:16:16.976: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023865052s
Jun 22 13:16:18.979: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027305997s
STEP: Saw pod success
Jun 22 13:16:18.979: INFO: Pod "downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8" satisfied condition "Succeeded or Failed"
Jun 22 13:16:18.984: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8 container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:16:19.004: INFO: Waiting for pod downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8 to disappear
Jun 22 13:16:19.008: INFO: Pod downward-api-d492d334-079a-48b0-81fd-dc7621ab55d8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:19.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-523" for this suite.

• [SLOW TEST:10.209 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3463,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:19.020: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-ade2d610-669a-46c2-a271-b174d45dfe99
STEP: Creating a pod to test consume secrets
Jun 22 13:16:19.219: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08" in namespace "projected-2209" to be "Succeeded or Failed"
Jun 22 13:16:19.224: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.091358ms
Jun 22 13:16:21.228: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008735375s
Jun 22 13:16:23.232: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012539006s
Jun 22 13:16:25.235: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016013559s
Jun 22 13:16:27.240: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020762477s
Jun 22 13:16:29.245: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026294402s
STEP: Saw pod success
Jun 22 13:16:29.246: INFO: Pod "pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08" satisfied condition "Succeeded or Failed"
Jun 22 13:16:29.250: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 13:16:29.268: INFO: Waiting for pod pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08 to disappear
Jun 22 13:16:29.271: INFO: Pod pod-projected-secrets-ee3a880f-f13e-40e6-8f19-c3a913b68d08 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:29.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2209" for this suite.

• [SLOW TEST:10.265 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3504,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:29.292: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 13:16:29.434: INFO: Waiting up to 5m0s for pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767" in namespace "emptydir-1007" to be "Succeeded or Failed"
Jun 22 13:16:29.452: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010618ms
Jun 22 13:16:31.456: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021900216s
Jun 22 13:16:33.459: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025143372s
Jun 22 13:16:35.463: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029099747s
Jun 22 13:16:37.468: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033338456s
Jun 22 13:16:39.471: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.037106524s
STEP: Saw pod success
Jun 22 13:16:39.471: INFO: Pod "pod-ec2771bc-6d45-45e9-8842-6b294a833767" satisfied condition "Succeeded or Failed"
Jun 22 13:16:39.474: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-ec2771bc-6d45-45e9-8842-6b294a833767 container test-container: <nil>
STEP: delete the pod
Jun 22 13:16:39.496: INFO: Waiting for pod pod-ec2771bc-6d45-45e9-8842-6b294a833767 to disappear
Jun 22 13:16:39.499: INFO: Pod pod-ec2771bc-6d45-45e9-8842-6b294a833767 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:39.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1007" for this suite.

• [SLOW TEST:10.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3553,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:39.509: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-11b42bbb-dd0c-4184-a94b-e1b7139df4d2
STEP: Creating a pod to test consume configMaps
Jun 22 13:16:39.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a" in namespace "configmap-4311" to be "Succeeded or Failed"
Jun 22 13:16:39.668: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113122ms
Jun 22 13:16:41.672: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014304742s
Jun 22 13:16:43.676: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018599471s
Jun 22 13:16:45.680: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022083313s
Jun 22 13:16:47.684: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026283759s
STEP: Saw pod success
Jun 22 13:16:47.684: INFO: Pod "pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a" satisfied condition "Succeeded or Failed"
Jun 22 13:16:47.687: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:16:47.711: INFO: Waiting for pod pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a to disappear
Jun 22 13:16:47.713: INFO: Pod pod-configmaps-821b34ed-017f-4c9d-8308-2a16964e4a0a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:47.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4311" for this suite.

• [SLOW TEST:8.221 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3555,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:47.731: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jun 22 13:16:47.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4710 create -f -'
Jun 22 13:16:48.116: INFO: stderr: ""
Jun 22 13:16:48.116: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 22 13:16:48.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4710 diff -f -'
Jun 22 13:16:48.402: INFO: rc: 1
Jun 22 13:16:48.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-4710 delete -f -'
Jun 22 13:16:48.478: INFO: stderr: ""
Jun 22 13:16:48.478: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:48.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4710" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":193,"skipped":3555,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:48.492: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-7952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 22 13:16:48.655: INFO: starting watch
STEP: patching
STEP: updating
Jun 22 13:16:48.669: INFO: waiting for watch events with expected annotations
Jun 22 13:16:48.669: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:48.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7952" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":194,"skipped":3592,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:48.702: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:16:48.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-892" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":195,"skipped":3602,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:16:48.854: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3263
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 22 13:16:48.988: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:16:52.578: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:17:06.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3263" for this suite.

• [SLOW TEST:17.428 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":196,"skipped":3631,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:17:06.283: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jun 22 13:17:36.456: INFO: Pod pod-hostip-ca1bd8c4-2436-431b-b73a-a5f44b34a944 has hostIP: 11.0.1.4
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:17:36.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3977" for this suite.

• [SLOW TEST:30.184 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3633,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:17:36.467: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 13:17:36.634: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 13:17:41.637: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:17:41.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3684" for this suite.

• [SLOW TEST:5.231 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":198,"skipped":3641,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:17:41.699: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 22 13:17:41.857: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jun 22 13:17:42.688: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 13:17:44.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964662, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964662, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964662, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759964662, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:17:47.796: INFO: Waited 1.028563439s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:17:48.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4326" for this suite.

• [SLOW TEST:6.938 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":199,"skipped":3646,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:17:48.637: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 13:17:48.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2928 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 22 13:17:48.862: INFO: stderr: ""
Jun 22 13:17:48.862: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 22 13:17:48.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2928 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jun 22 13:17:49.123: INFO: stderr: ""
Jun 22 13:17:49.123: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun 22 13:17:49.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-2928 delete pods e2e-test-httpd-pod'
Jun 22 13:17:52.919: INFO: stderr: ""
Jun 22 13:17:52.919: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:17:52.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2928" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":200,"skipped":3647,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:17:52.930: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7220, will wait for the garbage collector to delete the pods
Jun 22 13:18:03.183: INFO: Deleting Job.batch foo took: 6.386257ms
Jun 22 13:18:03.283: INFO: Terminating Job.batch foo pods took: 100.196713ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:18:41.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7220" for this suite.

• [SLOW TEST:48.967 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":201,"skipped":3665,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:18:41.897: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:18:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8876" for this suite.

• [SLOW TEST:5.307 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":202,"skipped":3708,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:18:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-7240
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7240
STEP: Deleting pre-stop pod
Jun 22 13:19:06.384: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:19:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7240" for this suite.

• [SLOW TEST:19.206 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":203,"skipped":3713,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:19:06.413: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 13:19:06.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 13:19:06.556: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 13:19:06.560: INFO: 
Logging pods the apiserver thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383 before test
Jun 22 13:19:06.575: INFO: fluent-bit-r2rws from pks-system started at 2021-06-22 12:47:11 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.575: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:19:06.575: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:19:06.576: INFO: node-exporter-j9gvz from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.576: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:19:06.576: INFO: telegraf-7gq7r from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.576: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:19:06.576: INFO: tester from prestop-7240 started at 2021-06-22 13:18:57 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.576: INFO: 	Container tester ready: true, restart count 0
Jun 22 13:19:06.576: INFO: sonobuoy from sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.576: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 13:19:06.576: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.576: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jun 22 13:19:06.576: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:19:06.576: INFO: 
Logging pods the apiserver thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 before test
Jun 22 13:19:06.592: INFO: coredns-645ccbcd68-7dlmn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.592: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:19:06.592: INFO: metrics-server-7d476fdfbd-md64k from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 13:19:06.593: INFO: fluent-bit-8rhpl from pks-system started at 2021-06-21 23:47:44 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:19:06.593: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:19:06.593: INFO: metric-controller-7f5cb8ff6d-7npkg from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 13:19:06.593: INFO: node-exporter-w6bbv from pks-system started at 2021-06-21 23:42:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:19:06.593: INFO: observability-manager-6cf797f97-8nqdm from pks-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 13:19:06.593: INFO: sink-controller-f6bc7f774-zprjs from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.593: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 13:19:06.594: INFO: telegraf-j56d5 from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.594: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:19:06.594: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.594: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jun 22 13:19:06.594: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:19:06.594: INFO: 
Logging pods the apiserver thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea before test
Jun 22 13:19:06.604: INFO: fluent-bit-vtnl7 from pks-system started at 2021-06-21 23:52:24 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:19:06.604: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:19:06.604: INFO: node-exporter-82sxc from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:19:06.604: INFO: telegraf-dwvlw from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:19:06.604: INFO: telemetry-agent-8444c9c47b-7xllf from pks-system started at 2021-06-21 23:55:50 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 13:19:06.604: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container e2e ready: true, restart count 0
Jun 22 13:19:06.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 13:19:06.604: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.604: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jun 22 13:19:06.604: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:19:06.604: INFO: 
Logging pods the apiserver thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f before test
Jun 22 13:19:06.614: INFO: coredns-645ccbcd68-pbmnn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.614: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:19:06.614: INFO: coredns-645ccbcd68-s8fjw from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.614: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:19:06.615: INFO: event-controller-85d6bb4d4c-fz5nk from pks-system started at 2021-06-21 23:47:39 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 13:19:06.615: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:19:06.615: INFO: fluent-bit-f425q from pks-system started at 2021-06-21 23:48:02 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:19:06.615: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:19:06.615: INFO: node-exporter-8qrc5 from pks-system started at 2021-06-21 23:46:22 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:19:06.615: INFO: telegraf-9z5xm from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:19:06.615: INFO: validator-69f557d7c6-bl8gf from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container validator ready: true, restart count 0
Jun 22 13:19:06.615: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:19:06.615: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Jun 22 13:19:06.615: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bbfa1000-db1c-4d13-bb23-099a51be6285 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-bbfa1000-db1c-4d13-bb23-099a51be6285 off the node 06998c1b-9fed-44d7-827f-f702404ff383
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bbfa1000-db1c-4d13-bb23-099a51be6285
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:19:28.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7230" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:22.334 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":204,"skipped":3714,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:19:28.747: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-2e1ba2c3-9d2e-4b44-be9e-fbd28db78923 in namespace container-probe-4218
Jun 22 13:19:36.894: INFO: Started pod busybox-2e1ba2c3-9d2e-4b44-be9e-fbd28db78923 in namespace container-probe-4218
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 13:19:36.897: INFO: Initial restart count of pod busybox-2e1ba2c3-9d2e-4b44-be9e-fbd28db78923 is 0
Jun 22 13:20:24.987: INFO: Restart count of pod container-probe-4218/busybox-2e1ba2c3-9d2e-4b44-be9e-fbd28db78923 is now 1 (48.090234038s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:20:25.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4218" for this suite.

• [SLOW TEST:56.272 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3715,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:20:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7803
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7803
I0622 13:20:25.232463      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7803, replica count: 2
I0622 13:20:28.282780      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:20:31.282998      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:20:34.283: INFO: Creating new exec pod
I0622 13:20:34.283199      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:20:39.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7803 exec execpodnbx2b -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 13:20:39.504: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 13:20:39.504: INFO: stdout: ""
Jun 22 13:20:39.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7803 exec execpodnbx2b -- /bin/sh -x -c nc -zv -t -w 2 10.100.193.82 80'
Jun 22 13:20:39.669: INFO: stderr: "+ nc -zv -t -w 2 10.100.193.82 80\nConnection to 10.100.193.82 80 port [tcp/http] succeeded!\n"
Jun 22 13:20:39.669: INFO: stdout: ""
Jun 22 13:20:39.669: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:20:39.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7803" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.687 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":206,"skipped":3720,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:20:39.707: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 22 13:20:39.843: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:21:09.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9183" for this suite.

• [SLOW TEST:30.103 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":207,"skipped":3726,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:21:09.810: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-856
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-856
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-856
Jun 22 13:21:09.974: INFO: Found 0 stateful pods, waiting for 1
Jun 22 13:21:19.978: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 22 13:21:19.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:21:20.161: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:21:20.161: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:21:20.161: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:21:20.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 13:21:30.169: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:21:30.169: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:21:30.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999783s
Jun 22 13:21:31.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99243997s
Jun 22 13:21:32.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987626725s
Jun 22 13:21:33.202: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98283097s
Jun 22 13:21:34.207: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97911698s
Jun 22 13:21:35.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974815139s
Jun 22 13:21:36.216: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969982676s
Jun 22 13:21:37.220: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965469834s
Jun 22 13:21:38.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960914328s
Jun 22 13:21:39.229: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.655503ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-856
Jun 22 13:21:40.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:21:40.422: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:21:40.422: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:21:40.422: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:21:40.425: INFO: Found 1 stateful pods, waiting for 3
Jun 22 13:21:50.433: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:21:50.433: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:21:50.433: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 22 13:21:50.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:21:50.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:21:50.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:21:50.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:21:50.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:21:50.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:21:50.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:21:50.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:21:50.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:21:51.027: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:21:51.027: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:21:51.027: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:21:51.027: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:21:51.031: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 22 13:22:01.038: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:22:01.039: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:22:01.039: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:22:01.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999757s
Jun 22 13:22:02.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993071427s
Jun 22 13:22:03.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988266332s
Jun 22 13:22:04.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983675218s
Jun 22 13:22:05.072: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9795098s
Jun 22 13:22:06.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974324055s
Jun 22 13:22:07.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969613027s
Jun 22 13:22:08.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964744069s
Jun 22 13:22:09.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960096004s
Jun 22 13:22:10.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.927031ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-856
Jun 22 13:22:11.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:22:11.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:22:11.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:22:11.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:22:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:22:11.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:22:11.454: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:22:11.454: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:22:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-856 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:22:11.633: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:22:11.633: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:22:11.633: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:22:11.633: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 13:22:41.650: INFO: Deleting all statefulset in ns statefulset-856
Jun 22 13:22:41.653: INFO: Scaling statefulset ss to 0
Jun 22 13:22:41.663: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:22:41.666: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:22:41.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-856" for this suite.

• [SLOW TEST:91.888 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":208,"skipped":3764,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:22:41.700: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-991fc064-0192-4458-ab90-5edfb025f35d in namespace container-probe-7548
Jun 22 13:22:51.857: INFO: Started pod liveness-991fc064-0192-4458-ab90-5edfb025f35d in namespace container-probe-7548
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 13:22:51.860: INFO: Initial restart count of pod liveness-991fc064-0192-4458-ab90-5edfb025f35d is 0
Jun 22 13:23:15.914: INFO: Restart count of pod container-probe-7548/liveness-991fc064-0192-4458-ab90-5edfb025f35d is now 1 (24.054193286s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:23:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7548" for this suite.

• [SLOW TEST:34.243 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3772,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:23:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8653
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-9b75c6d9-960a-4767-965b-24fb5756206a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9b75c6d9-960a-4767-965b-24fb5756206a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:23:26.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8653" for this suite.

• [SLOW TEST:10.205 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3778,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:23:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-6c5c390f-ba86-48f0-b8c8-eb9cf8c8a91f in namespace container-probe-331
Jun 22 13:23:36.353: INFO: Started pod busybox-6c5c390f-ba86-48f0-b8c8-eb9cf8c8a91f in namespace container-probe-331
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 13:23:36.357: INFO: Initial restart count of pod busybox-6c5c390f-ba86-48f0-b8c8-eb9cf8c8a91f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:27:36.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-331" for this suite.

• [SLOW TEST:250.769 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3790,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:27:36.918: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:27:37.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4" in namespace "projected-4391" to be "Succeeded or Failed"
Jun 22 13:27:37.065: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076513ms
Jun 22 13:27:39.069: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007497362s
Jun 22 13:27:41.074: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012299444s
Jun 22 13:27:43.078: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016181254s
Jun 22 13:27:45.083: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021063876s
Jun 22 13:27:47.087: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025526789s
Jun 22 13:27:49.091: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029702406s
Jun 22 13:27:51.095: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033497732s
Jun 22 13:27:53.103: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041209167s
Jun 22 13:27:55.108: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.045906987s
Jun 22 13:27:57.112: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.050304122s
Jun 22 13:27:59.117: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.054955801s
Jun 22 13:28:01.121: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.059580023s
Jun 22 13:28:03.126: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.063939149s
Jun 22 13:28:05.131: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.0692471s
Jun 22 13:28:07.136: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.073940148s
Jun 22 13:28:09.139: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.077463089s
Jun 22 13:28:11.144: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.082149973s
STEP: Saw pod success
Jun 22 13:28:11.144: INFO: Pod "downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4" satisfied condition "Succeeded or Failed"
Jun 22 13:28:11.147: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4 container client-container: <nil>
STEP: delete the pod
Jun 22 13:28:11.190: INFO: Waiting for pod downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4 to disappear
Jun 22 13:28:11.193: INFO: Pod downwardapi-volume-88306ddc-4574-4e8f-af26-b1291cb78bc4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:28:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4391" for this suite.

• [SLOW TEST:34.285 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3798,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:28:11.208: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jun 22 13:28:11.355: INFO: Waiting up to 5m0s for pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7" in namespace "var-expansion-6482" to be "Succeeded or Failed"
Jun 22 13:28:11.363: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.274136ms
Jun 22 13:28:13.368: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034869s
Jun 22 13:28:15.373: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018344621s
Jun 22 13:28:17.377: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022036596s
Jun 22 13:28:19.381: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025804858s
STEP: Saw pod success
Jun 22 13:28:19.381: INFO: Pod "var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7" satisfied condition "Succeeded or Failed"
Jun 22 13:28:19.384: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7 container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:28:19.402: INFO: Waiting for pod var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7 to disappear
Jun 22 13:28:19.405: INFO: Pod var-expansion-4e98a1d9-7c68-447e-b84c-7d41029d96a7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:28:19.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6482" for this suite.

• [SLOW TEST:8.207 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3834,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:28:19.418: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 13:28:27.600: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:28:27.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5839" for this suite.

• [SLOW TEST:8.220 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3870,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:28:27.638: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:28:27.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5136" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":215,"skipped":3871,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}

------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:28:27.785: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:28:27.928: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 13:28:32.932: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 13:28:36.939: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 13:28:38.943: INFO: Creating deployment "test-rollover-deployment"
Jun 22 13:28:38.952: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 13:28:40.959: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 13:28:40.966: INFO: Ensure that both replica sets have 1 created replica
Jun 22 13:28:40.973: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 22 13:28:40.995: INFO: Updating deployment test-rollover-deployment
Jun 22 13:28:40.995: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 13:28:43.005: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 13:28:43.012: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 13:28:43.018: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:43.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965321, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:45.025: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:45.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965324, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:47.025: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:47.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965324, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:49.026: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:49.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965324, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:51.025: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:51.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965324, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:53.025: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 13:28:53.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965324, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965318, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:28:55.025: INFO: 
Jun 22 13:28:55.025: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 22 13:28:55.033: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8080 /apis/apps/v1/namespaces/deployment-8080/deployments/test-rollover-deployment 720f3062-c53b-4668-8e63-1d8c43f5a17f 175115 2 2021-06-22 13:28:38 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-22 13:28:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 13:28:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00778d438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-22 13:28:38 +0000 UTC,LastTransitionTime:2021-06-22 13:28:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-06-22 13:28:54 +0000 UTC,LastTransitionTime:2021-06-22 13:28:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 13:28:55.037: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-8080 /apis/apps/v1/namespaces/deployment-8080/replicasets/test-rollover-deployment-5797c7764 71a542ac-6ff1-413e-9e54-a385a966fb53 175104 2 2021-06-22 13:28:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 720f3062-c53b-4668-8e63-1d8c43f5a17f 0xc00778d960 0xc00778d961}] []  [{kube-controller-manager Update apps/v1 2021-06-22 13:28:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"720f3062-c53b-4668-8e63-1d8c43f5a17f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00778d9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:28:55.037: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 13:28:55.037: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8080 /apis/apps/v1/namespaces/deployment-8080/replicasets/test-rollover-controller b6c8a92b-7c01-4a75-85cf-6cda5773e13e 175114 2 2021-06-22 13:28:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 720f3062-c53b-4668-8e63-1d8c43f5a17f 0xc00778d857 0xc00778d858}] []  [{e2e.test Update apps/v1 2021-06-22 13:28:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 13:28:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"720f3062-c53b-4668-8e63-1d8c43f5a17f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00778d8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:28:55.037: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8080 /apis/apps/v1/namespaces/deployment-8080/replicasets/test-rollover-deployment-78bc8b888c 2ad97cbf-58c6-41ac-87f6-35adcbf4c5ad 175045 2 2021-06-22 13:28:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 720f3062-c53b-4668-8e63-1d8c43f5a17f 0xc00778da47 0xc00778da48}] []  [{kube-controller-manager Update apps/v1 2021-06-22 13:28:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"720f3062-c53b-4668-8e63-1d8c43f5a17f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00778dad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:28:55.041: INFO: Pod "test-rollover-deployment-5797c7764-grpqm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-grpqm test-rollover-deployment-5797c7764- deployment-8080 /api/v1/namespaces/deployment-8080/pods/test-rollover-deployment-5797c7764-grpqm 4f2f6b90-459e-4957-bed3-7345c9e506a0 175064 0 2021-06-22 13:28:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 71a542ac-6ff1-413e-9e54-a385a966fb53 0xc003ed0e40 0xc003ed0e41}] []  [{kube-controller-manager Update v1 2021-06-22 13:28:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71a542ac-6ff1-413e-9e54-a385a966fb53\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 13:28:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.9.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5h7z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5h7z7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5h7z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:28:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:28:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:11.32.9.4,StartTime:2021-06-22 13:28:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 13:28:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://90f5320b416ac9ab0cebc805d18b2a987d38390cd856ed2c406cdfc46096c4a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.9.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:28:55.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8080" for this suite.

• [SLOW TEST:27.270 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":216,"skipped":3871,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:28:55.056: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 13:29:04.229: INFO: DNS probes using dns-test-b4317a6d-f847-4c63-aa50-c26bebc60af7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 13:29:08.286: INFO: File wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:08.292: INFO: File jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:08.292: INFO: Lookups using dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 failed for: [wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local]

Jun 22 13:29:13.297: INFO: File wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:13.301: INFO: File jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:13.301: INFO: Lookups using dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 failed for: [wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local]

Jun 22 13:29:18.298: INFO: File wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:18.301: INFO: Lookups using dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 failed for: [wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local]

Jun 22 13:29:23.297: INFO: File wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:23.300: INFO: Lookups using dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 failed for: [wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local]

Jun 22 13:29:28.297: INFO: File wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local from pod  dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 13:29:28.301: INFO: Lookups using dns-9962/dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 failed for: [wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local]

Jun 22 13:29:33.300: INFO: DNS probes using dns-test-f7208dfd-4817-4b08-b0bb-c11ff9d0ac06 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9962.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9962.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 13:29:39.415: INFO: DNS probes using dns-test-004b670c-1411-4051-9f3b-c900459eafcd succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:29:39.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9962" for this suite.

• [SLOW TEST:44.405 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":217,"skipped":3890,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:29:39.462: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:29:40.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:29:42.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:44.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:46.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:48.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:50.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:52.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:54.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:56.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:29:58.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:30:00.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:30:02.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965380, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:30:05.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 22 13:30:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:05.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5569" for this suite.
STEP: Destroying namespace "webhook-5569-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:26.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":218,"skipped":3912,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:05.798: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6203
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 22 13:30:05.933: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6203" for this suite.

• [SLOW TEST:30.527 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":219,"skipped":3929,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:36.326: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-8595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jun 22 13:30:36.466: INFO: Major version: 1
STEP: Confirm minor version
Jun 22 13:30:36.466: INFO: cleanMinorVersion: 19
Jun 22 13:30:36.466: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:36.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8595" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":220,"skipped":3930,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:36.476: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:30:36.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-8131 version'
Jun 22 13:30:36.693: INFO: stderr: ""
Jun 22 13:30:36.693: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9\", GitCommit:\"9dd794e454ac32d97cde41ae10be801ae98f75df\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:09:28Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9+vmware.1\", GitCommit:\"f856d899461199c512c21d0fdc67d49cc70a8963\", GitTreeState:\"clean\", BuildDate:\"2021-03-19T23:57:11Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:36.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8131" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":221,"skipped":3960,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:36.703: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:36.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3965" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":222,"skipped":3978,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-4ce75766-9da3-4b4e-8f49-1c82f1d343e0
STEP: Creating a pod to test consume configMaps
Jun 22 13:30:37.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15" in namespace "projected-2037" to be "Succeeded or Failed"
Jun 22 13:30:37.049: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174992ms
Jun 22 13:30:39.053: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008441119s
Jun 22 13:30:41.058: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012926012s
Jun 22 13:30:43.062: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017022003s
Jun 22 13:30:45.067: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021847986s
Jun 22 13:30:47.070: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025246122s
STEP: Saw pod success
Jun 22 13:30:47.070: INFO: Pod "pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15" satisfied condition "Succeeded or Failed"
Jun 22 13:30:47.073: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:30:47.103: INFO: Waiting for pod pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15 to disappear
Jun 22 13:30:47.108: INFO: Pod pod-projected-configmaps-ee8d7f0e-058b-4656-a5b3-6db9d27fef15 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:47.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2037" for this suite.

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3999,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:47.127: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 13:30:47.264: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 13:30:47.273: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 13:30:47.276: INFO: 
Logging pods the apiserver thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383 before test
Jun 22 13:30:47.287: INFO: fluent-bit-r2rws from pks-system started at 2021-06-22 12:47:11 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.287: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:30:47.287: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:30:47.287: INFO: node-exporter-j9gvz from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.287: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:30:47.287: INFO: telegraf-7gq7r from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.287: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:30:47.287: INFO: sonobuoy from sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.287: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 13:30:47.287: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.287: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun 22 13:30:47.287: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:30:47.287: INFO: 
Logging pods the apiserver thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 before test
Jun 22 13:30:47.296: INFO: coredns-645ccbcd68-7dlmn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.296: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:30:47.296: INFO: metrics-server-7d476fdfbd-md64k from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.296: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 13:30:47.296: INFO: fluent-bit-8rhpl from pks-system started at 2021-06-21 23:47:44 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.296: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:30:47.296: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:30:47.296: INFO: metric-controller-7f5cb8ff6d-7npkg from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 13:30:47.297: INFO: node-exporter-w6bbv from pks-system started at 2021-06-21 23:42:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:30:47.297: INFO: observability-manager-6cf797f97-8nqdm from pks-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 13:30:47.297: INFO: sink-controller-f6bc7f774-zprjs from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 13:30:47.297: INFO: telegraf-j56d5 from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:30:47.297: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.297: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun 22 13:30:47.297: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:30:47.297: INFO: 
Logging pods the apiserver thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea before test
Jun 22 13:30:47.307: INFO: fluent-bit-vtnl7 from pks-system started at 2021-06-21 23:52:24 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:30:47.307: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:30:47.307: INFO: node-exporter-82sxc from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:30:47.307: INFO: telegraf-dwvlw from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:30:47.307: INFO: telemetry-agent-8444c9c47b-7xllf from pks-system started at 2021-06-21 23:55:50 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 13:30:47.307: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container e2e ready: true, restart count 0
Jun 22 13:30:47.307: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 13:30:47.307: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.307: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun 22 13:30:47.307: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:30:47.307: INFO: 
Logging pods the apiserver thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f before test
Jun 22 13:30:47.318: INFO: coredns-645ccbcd68-pbmnn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:30:47.318: INFO: coredns-645ccbcd68-s8fjw from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:30:47.318: INFO: event-controller-85d6bb4d4c-fz5nk from pks-system started at 2021-06-21 23:47:39 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 13:30:47.318: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:30:47.318: INFO: fluent-bit-f425q from pks-system started at 2021-06-21 23:48:02 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:30:47.318: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:30:47.318: INFO: node-exporter-8qrc5 from pks-system started at 2021-06-21 23:46:22 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:30:47.318: INFO: telegraf-9z5xm from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:30:47.318: INFO: validator-69f557d7c6-bl8gf from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container validator ready: true, restart count 0
Jun 22 13:30:47.318: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:30:47.318: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun 22 13:30:47.318: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3ff6f590-ef59-4e48-bd77-50e8e5460b9a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-3ff6f590-ef59-4e48-bd77-50e8e5460b9a off the node 06998c1b-9fed-44d7-827f-f702404ff383
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3ff6f590-ef59-4e48-bd77-50e8e5460b9a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:30:59.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9098" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:12.286 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":224,"skipped":4063,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:30:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:30:59.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:31:01.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965459, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965459, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965459, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965459, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:31:04.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:05.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1287" for this suite.
STEP: Destroying namespace "webhook-1287-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.649 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":225,"skipped":4080,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:05.064: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:31:05.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:31:07.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965465, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965465, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965465, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965465, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:31:10.799: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:10.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3014" for this suite.
STEP: Destroying namespace "webhook-3014-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":226,"skipped":4081,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:10.887: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-9c21d1fa-31a5-4fef-b4dd-adfc81bad322-7598
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8254" for this suite.
STEP: Destroying namespace "nspatchtest-9c21d1fa-31a5-4fef-b4dd-adfc81bad322-7598" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":227,"skipped":4119,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-83e3f8ee-da83-4a81-9680-d8b02c4a67f3
STEP: Creating a pod to test consume secrets
Jun 22 13:31:11.332: INFO: Waiting up to 5m0s for pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d" in namespace "secrets-9359" to be "Succeeded or Failed"
Jun 22 13:31:11.341: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.821151ms
Jun 22 13:31:13.345: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012347601s
Jun 22 13:31:15.350: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017238983s
Jun 22 13:31:17.353: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021069314s
Jun 22 13:31:19.357: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024258232s
Jun 22 13:31:21.361: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029033835s
STEP: Saw pod success
Jun 22 13:31:21.362: INFO: Pod "pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d" satisfied condition "Succeeded or Failed"
Jun 22 13:31:21.364: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d container secret-env-test: <nil>
STEP: delete the pod
Jun 22 13:31:21.382: INFO: Waiting for pod pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d to disappear
Jun 22 13:31:21.386: INFO: Pod pod-secrets-88177061-1e16-4651-abbc-7d60c9bff49d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:21.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9359" for this suite.

• [SLOW TEST:10.208 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":4131,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:21.397: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-48b648ee-8f3c-4651-a312-9f2c7a8acf68
STEP: Creating a pod to test consume secrets
Jun 22 13:31:21.553: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22" in namespace "projected-2408" to be "Succeeded or Failed"
Jun 22 13:31:21.555: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863931ms
Jun 22 13:31:23.559: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00672779s
Jun 22 13:31:25.564: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011621975s
Jun 22 13:31:27.569: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016179351s
Jun 22 13:31:29.574: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021043375s
STEP: Saw pod success
Jun 22 13:31:29.574: INFO: Pod "pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22" satisfied condition "Succeeded or Failed"
Jun 22 13:31:29.577: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 13:31:29.594: INFO: Waiting for pod pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22 to disappear
Jun 22 13:31:29.598: INFO: Pod pod-projected-secrets-066da503-a94b-4f20-830e-dbbd3dd60d22 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:29.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2408" for this suite.

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":4131,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:29.609: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:31:39.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6137" for this suite.

• [SLOW TEST:10.213 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":230,"skipped":4141,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:31:39.824: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-249
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 22 13:31:39.982: INFO: Found 0 stateful pods, waiting for 3
Jun 22 13:31:49.986: INFO: Found 2 stateful pods, waiting for 3
Jun 22 13:31:59.986: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:31:59.986: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:31:59.986: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 13:32:00.051: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 22 13:32:10.090: INFO: Updating stateful set ss2
Jun 22 13:32:10.113: INFO: Waiting for Pod statefulset-249/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 22 13:32:20.227: INFO: Found 2 stateful pods, waiting for 3
Jun 22 13:32:30.233: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:32:30.233: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:32:30.233: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 22 13:32:30.260: INFO: Updating stateful set ss2
Jun 22 13:32:30.283: INFO: Waiting for Pod statefulset-249/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:32:40.310: INFO: Updating stateful set ss2
Jun 22 13:32:40.332: INFO: Waiting for StatefulSet statefulset-249/ss2 to complete update
Jun 22 13:32:40.332: INFO: Waiting for Pod statefulset-249/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:32:50.342: INFO: Waiting for StatefulSet statefulset-249/ss2 to complete update
Jun 22 13:32:50.342: INFO: Waiting for Pod statefulset-249/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 13:33:00.340: INFO: Deleting all statefulset in ns statefulset-249
Jun 22 13:33:00.343: INFO: Scaling statefulset ss2 to 0
Jun 22 13:33:30.360: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:33:30.363: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:33:30.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-249" for this suite.

• [SLOW TEST:110.569 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":231,"skipped":4145,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:33:30.393: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-83
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-83
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 22 13:33:30.550: INFO: Found 0 stateful pods, waiting for 3
Jun 22 13:33:40.555: INFO: Found 2 stateful pods, waiting for 3
Jun 22 13:33:50.556: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:33:50.556: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:33:50.556: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:33:50.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-83 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:33:51.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:33:51.502: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:33:51.502: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 13:34:01.534: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 22 13:34:11.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-83 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:34:11.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:34:11.728: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:34:11.728: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:34:21.747: INFO: Waiting for StatefulSet statefulset-83/ss2 to complete update
Jun 22 13:34:21.747: INFO: Waiting for Pod statefulset-83/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:21.747: INFO: Waiting for Pod statefulset-83/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:21.747: INFO: Waiting for Pod statefulset-83/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:31.754: INFO: Waiting for StatefulSet statefulset-83/ss2 to complete update
Jun 22 13:34:31.754: INFO: Waiting for Pod statefulset-83/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:31.754: INFO: Waiting for Pod statefulset-83/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:41.754: INFO: Waiting for StatefulSet statefulset-83/ss2 to complete update
Jun 22 13:34:41.754: INFO: Waiting for Pod statefulset-83/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 13:34:51.754: INFO: Waiting for StatefulSet statefulset-83/ss2 to complete update
Jun 22 13:34:51.754: INFO: Waiting for Pod statefulset-83/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 22 13:35:01.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-83 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:35:01.944: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:35:01.944: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:35:01.944: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:35:11.978: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 22 13:35:22.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-83 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:35:22.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:35:22.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:35:22.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:35:42.203: INFO: Waiting for StatefulSet statefulset-83/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 13:35:52.211: INFO: Deleting all statefulset in ns statefulset-83
Jun 22 13:35:52.213: INFO: Scaling statefulset ss2 to 0
Jun 22 13:36:12.230: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:36:12.233: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:36:12.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-83" for this suite.

• [SLOW TEST:161.870 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":232,"skipped":4148,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:36:12.265: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0622 13:36:52.442917      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:36:52.442945      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:36:52.442950      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:36:52.443: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 22 13:36:52.443: INFO: Deleting pod "simpletest.rc-2plxw" in namespace "gc-2087"
Jun 22 13:36:52.457: INFO: Deleting pod "simpletest.rc-774s8" in namespace "gc-2087"
Jun 22 13:36:52.477: INFO: Deleting pod "simpletest.rc-8h7gn" in namespace "gc-2087"
Jun 22 13:36:52.504: INFO: Deleting pod "simpletest.rc-8hc4z" in namespace "gc-2087"
Jun 22 13:36:52.527: INFO: Deleting pod "simpletest.rc-9lxqg" in namespace "gc-2087"
Jun 22 13:36:52.546: INFO: Deleting pod "simpletest.rc-fvqnf" in namespace "gc-2087"
Jun 22 13:36:52.566: INFO: Deleting pod "simpletest.rc-ph4hj" in namespace "gc-2087"
Jun 22 13:36:52.578: INFO: Deleting pod "simpletest.rc-qj4rc" in namespace "gc-2087"
Jun 22 13:36:52.602: INFO: Deleting pod "simpletest.rc-qvm85" in namespace "gc-2087"
Jun 22 13:36:52.626: INFO: Deleting pod "simpletest.rc-s2sjf" in namespace "gc-2087"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:36:52.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2087" for this suite.

• [SLOW TEST:40.396 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":233,"skipped":4155,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:36:52.661: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-snlw
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 13:36:52.816: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-snlw" in namespace "subpath-9919" to be "Succeeded or Failed"
Jun 22 13:36:52.819: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.101074ms
Jun 22 13:36:54.824: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007891651s
Jun 22 13:36:56.828: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011828676s
Jun 22 13:36:58.834: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018020674s
Jun 22 13:37:00.838: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 8.022198119s
Jun 22 13:37:02.842: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 10.026404175s
Jun 22 13:37:04.846: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 12.030501454s
Jun 22 13:37:06.851: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 14.03464643s
Jun 22 13:37:08.855: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 16.038895162s
Jun 22 13:37:10.859: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 18.043297774s
Jun 22 13:37:12.863: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 20.047316904s
Jun 22 13:37:14.867: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 22.0505681s
Jun 22 13:37:16.871: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 24.054814035s
Jun 22 13:37:18.875: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Running", Reason="", readiness=true. Elapsed: 26.05877392s
Jun 22 13:37:20.879: INFO: Pod "pod-subpath-test-configmap-snlw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.062786881s
STEP: Saw pod success
Jun 22 13:37:20.879: INFO: Pod "pod-subpath-test-configmap-snlw" satisfied condition "Succeeded or Failed"
Jun 22 13:37:20.882: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-subpath-test-configmap-snlw container test-container-subpath-configmap-snlw: <nil>
STEP: delete the pod
Jun 22 13:37:20.913: INFO: Waiting for pod pod-subpath-test-configmap-snlw to disappear
Jun 22 13:37:20.917: INFO: Pod pod-subpath-test-configmap-snlw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-snlw
Jun 22 13:37:20.917: INFO: Deleting pod "pod-subpath-test-configmap-snlw" in namespace "subpath-9919"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:37:20.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9919" for this suite.

• [SLOW TEST:28.269 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":234,"skipped":4160,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:37:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-795.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-795.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-795.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-795.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 13:37:31.125: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.128: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.136: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.146: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.149: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.152: INFO: Unable to read jessie_udp@dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:31.162: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:37:36.166: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:36.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:36.191: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:36.195: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:36.210: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:37:41.167: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:41.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:41.187: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:41.190: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:41.203: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:37:46.167: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:46.171: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:46.187: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:46.191: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:46.204: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:37:51.168: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:51.171: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:51.194: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:51.198: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:51.212: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:37:56.167: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:56.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:56.187: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:56.191: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local from pod dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b: the server could not find the requested resource (get pods dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b)
Jun 22 13:37:56.204: INFO: Lookups using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-795.svc.cluster.local]

Jun 22 13:38:01.210: INFO: DNS probes using dns-795/dns-test-444515ce-fc7c-4e52-8180-6e2812884a6b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:38:01.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-795" for this suite.

• [SLOW TEST:40.341 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":235,"skipped":4195,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:38:01.275: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 13:38:01.413: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 13:38:01.422: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 13:38:01.425: INFO: 
Logging pods the apiserver thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383 before test
Jun 22 13:38:01.435: INFO: fluent-bit-r2rws from pks-system started at 2021-06-22 12:47:11 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.435: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:38:01.435: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:38:01.435: INFO: node-exporter-j9gvz from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.435: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:38:01.436: INFO: telegraf-7gq7r from pks-system started at 2021-06-22 12:47:11 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.436: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:38:01.436: INFO: sonobuoy from sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.436: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 13:38:01.436: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.436: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 22 13:38:01.436: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:38:01.436: INFO: 
Logging pods the apiserver thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 before test
Jun 22 13:38:01.446: INFO: coredns-645ccbcd68-7dlmn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.446: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:38:01.446: INFO: metrics-server-7d476fdfbd-md64k from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.446: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 13:38:01.446: INFO: fluent-bit-8rhpl from pks-system started at 2021-06-21 23:47:44 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.446: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:38:01.447: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:38:01.447: INFO: metric-controller-7f5cb8ff6d-7npkg from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.447: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 13:38:01.447: INFO: node-exporter-w6bbv from pks-system started at 2021-06-21 23:42:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.447: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:38:01.447: INFO: observability-manager-6cf797f97-8nqdm from pks-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.447: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 13:38:01.448: INFO: sink-controller-f6bc7f774-zprjs from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.448: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 13:38:01.448: INFO: telegraf-j56d5 from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.448: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:38:01.448: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.448: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 22 13:38:01.448: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:38:01.449: INFO: 
Logging pods the apiserver thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea before test
Jun 22 13:38:01.459: INFO: fluent-bit-vtnl7 from pks-system started at 2021-06-21 23:52:24 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.459: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:38:01.459: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:38:01.459: INFO: node-exporter-82sxc from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.459: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:38:01.459: INFO: telegraf-dwvlw from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.459: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:38:01.459: INFO: telemetry-agent-8444c9c47b-7xllf from pks-system started at 2021-06-21 23:55:50 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.460: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 13:38:01.460: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.460: INFO: 	Container e2e ready: true, restart count 0
Jun 22 13:38:01.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 13:38:01.460: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.460: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 22 13:38:01.460: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:38:01.460: INFO: 
Logging pods the apiserver thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f before test
Jun 22 13:38:01.471: INFO: coredns-645ccbcd68-pbmnn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.471: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:38:01.471: INFO: coredns-645ccbcd68-s8fjw from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.471: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:38:01.471: INFO: event-controller-85d6bb4d4c-fz5nk from pks-system started at 2021-06-21 23:47:39 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.471: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 13:38:01.471: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:38:01.472: INFO: fluent-bit-f425q from pks-system started at 2021-06-21 23:48:02 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.472: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:38:01.472: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:38:01.472: INFO: node-exporter-8qrc5 from pks-system started at 2021-06-21 23:46:22 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.472: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:38:01.472: INFO: telegraf-9z5xm from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.472: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:38:01.472: INFO: validator-69f557d7c6-bl8gf from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:38:01.472: INFO: 	Container validator ready: true, restart count 0
Jun 22 13:38:01.472: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:38:01.472: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 22 13:38:01.472: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 06998c1b-9fed-44d7-827f-f702404ff383
STEP: verifying the node has the label node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
STEP: verifying the node has the label node 2352dbd9-b599-409b-9a0b-5bade7a216ea
STEP: verifying the node has the label node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod coredns-645ccbcd68-7dlmn requesting resource cpu=100m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod coredns-645ccbcd68-pbmnn requesting resource cpu=100m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod coredns-645ccbcd68-s8fjw requesting resource cpu=100m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod metrics-server-7d476fdfbd-md64k requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod event-controller-85d6bb4d4c-fz5nk requesting resource cpu=0m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod fluent-bit-8rhpl requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod fluent-bit-f425q requesting resource cpu=0m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod fluent-bit-r2rws requesting resource cpu=0m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.541: INFO: Pod fluent-bit-vtnl7 requesting resource cpu=0m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.541: INFO: Pod metric-controller-7f5cb8ff6d-7npkg requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod node-exporter-82sxc requesting resource cpu=10m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.541: INFO: Pod node-exporter-8qrc5 requesting resource cpu=10m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod node-exporter-j9gvz requesting resource cpu=10m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.541: INFO: Pod node-exporter-w6bbv requesting resource cpu=10m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod observability-manager-6cf797f97-8nqdm requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod sink-controller-f6bc7f774-zprjs requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod telegraf-7gq7r requesting resource cpu=0m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.541: INFO: Pod telegraf-9z5xm requesting resource cpu=0m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod telegraf-dwvlw requesting resource cpu=0m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.541: INFO: Pod telegraf-j56d5 requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod telemetry-agent-8444c9c47b-7xllf requesting resource cpu=0m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.541: INFO: Pod validator-69f557d7c6-bl8gf requesting resource cpu=0m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod sonobuoy requesting resource cpu=0m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.541: INFO: Pod sonobuoy-e2e-job-60cfc378f7374ff2 requesting resource cpu=0m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.541: INFO: Pod sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz requesting resource cpu=0m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.541: INFO: Pod sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 requesting resource cpu=0m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.541: INFO: Pod sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm requesting resource cpu=0m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
Jun 22 13:38:01.541: INFO: Pod sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp requesting resource cpu=0m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
STEP: Starting Pods to consume most of the cluster CPU.
Jun 22 13:38:01.541: INFO: Creating a pod which consumes cpu=1393m on Node 06998c1b-9fed-44d7-827f-f702404ff383
Jun 22 13:38:01.550: INFO: Creating a pod which consumes cpu=1323m on Node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
Jun 22 13:38:01.556: INFO: Creating a pod which consumes cpu=1393m on Node 2352dbd9-b599-409b-9a0b-5bade7a216ea
Jun 22 13:38:01.563: INFO: Creating a pod which consumes cpu=1253m on Node 54b7d611-8263-481b-bda6-56b40bff2a2f
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca.168aeb12cad3ab99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8541/filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca to 1d96d19c-1b78-44d8-b822-ba104bc5daa5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca.168aeb14909e99f3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca.168aeb1493d0db1f], Reason = [Created], Message = [Created container filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca.168aeb149c3361db], Reason = [Started], Message = [Started container filler-pod-39bfd915-745b-4180-ae77-ccd8097559ca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491.168aeb12ca024dfe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8541/filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491 to 06998c1b-9fed-44d7-827f-f702404ff383]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491.168aeb14af33be6f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491.168aeb14b1a48f1b], Reason = [Created], Message = [Created container filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491.168aeb14b8f74290], Reason = [Started], Message = [Started container filler-pod-40f76aab-40c9-411a-b324-9aaed76e7491]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b.168aeb12cbe76eb0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8541/filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b to 54b7d611-8263-481b-bda6-56b40bff2a2f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b.168aeb147cab5d64], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b.168aeb147f80faf5], Reason = [Created], Message = [Created container filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b.168aeb14873c6194], Reason = [Started], Message = [Started container filler-pod-6a8c6352-7c56-488e-a436-1fd02d8fd65b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe.168aeb12cb4c3f39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8541/filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe to 2352dbd9-b599-409b-9a0b-5bade7a216ea]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe.168aeb146eb5c05b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe.168aeb1471c2da4a], Reason = [Created], Message = [Created container filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe.168aeb147a30f8f9], Reason = [Started], Message = [Started container filler-pod-a68e0071-9b34-48b1-b670-2fd44401fdbe]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168aeb1521c79e26], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168aeb15223fa26a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node 1d96d19c-1b78-44d8-b822-ba104bc5daa5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 2352dbd9-b599-409b-9a0b-5bade7a216ea
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 54b7d611-8263-481b-bda6-56b40bff2a2f
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 06998c1b-9fed-44d7-827f-f702404ff383
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:38:12.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8541" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:11.403 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":236,"skipped":4208,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:38:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 22 13:38:12.823: INFO: Waiting up to 5m0s for pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c" in namespace "downward-api-6300" to be "Succeeded or Failed"
Jun 22 13:38:12.833: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.281181ms
Jun 22 13:38:14.837: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013115155s
Jun 22 13:38:16.841: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017092085s
Jun 22 13:38:18.860: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036910851s
Jun 22 13:38:20.866: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Running", Reason="", readiness=true. Elapsed: 8.042059798s
Jun 22 13:38:22.871: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.047125597s
STEP: Saw pod success
Jun 22 13:38:22.871: INFO: Pod "downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c" satisfied condition "Succeeded or Failed"
Jun 22 13:38:22.874: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:38:22.890: INFO: Waiting for pod downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c to disappear
Jun 22 13:38:22.895: INFO: Pod downward-api-2d1a92ce-ac9a-4732-9746-c50881f3f58c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:38:22.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6300" for this suite.

• [SLOW TEST:10.225 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":4247,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:38:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1454
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 22 13:38:23.050: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:38:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:38:40.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1454" for this suite.

• [SLOW TEST:17.351 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":238,"skipped":4249,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:38:40.258: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-e1aab193-92d9-4f24-93c7-70d1bbb282ac
STEP: Creating a pod to test consume configMaps
Jun 22 13:38:40.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e" in namespace "configmap-5038" to be "Succeeded or Failed"
Jun 22 13:38:40.407: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034979ms
Jun 22 13:38:42.411: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008596678s
Jun 22 13:38:44.416: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01286177s
Jun 22 13:38:46.420: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017245452s
Jun 22 13:38:48.425: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022112708s
Jun 22 13:38:50.430: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026900771s
STEP: Saw pod success
Jun 22 13:38:50.430: INFO: Pod "pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e" satisfied condition "Succeeded or Failed"
Jun 22 13:38:50.433: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 13:38:50.457: INFO: Waiting for pod pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e to disappear
Jun 22 13:38:50.460: INFO: Pod pod-configmaps-1b80d279-5e30-4af7-afba-d537b9105d1e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:38:50.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5038" for this suite.

• [SLOW TEST:10.212 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":4249,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:38:50.471: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:38:50.670: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc" in namespace "security-context-test-6050" to be "Succeeded or Failed"
Jun 22 13:38:50.673: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157363ms
Jun 22 13:38:52.678: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008023826s
Jun 22 13:38:54.682: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01262898s
Jun 22 13:38:56.687: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016902927s
Jun 22 13:38:58.691: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020897884s
Jun 22 13:39:00.695: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025349255s
Jun 22 13:39:00.695: INFO: Pod "busybox-readonly-false-0dd6b242-6ef5-436d-b29a-c21022df65cc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:39:00.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6050" for this suite.

• [SLOW TEST:10.235 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":4259,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:39:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jun 22 13:39:00.853: INFO: Waiting up to 5m0s for pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584" in namespace "var-expansion-6632" to be "Succeeded or Failed"
Jun 22 13:39:00.863: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Pending", Reason="", readiness=false. Elapsed: 9.832693ms
Jun 22 13:39:02.867: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01429206s
Jun 22 13:39:04.871: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018198635s
Jun 22 13:39:06.876: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022637203s
Jun 22 13:39:08.880: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Running", Reason="", readiness=true. Elapsed: 8.02720966s
Jun 22 13:39:10.884: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031454742s
STEP: Saw pod success
Jun 22 13:39:10.884: INFO: Pod "var-expansion-ce381e06-1744-43d3-b818-a9312b49b584" satisfied condition "Succeeded or Failed"
Jun 22 13:39:10.888: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod var-expansion-ce381e06-1744-43d3-b818-a9312b49b584 container dapi-container: <nil>
STEP: delete the pod
Jun 22 13:39:10.905: INFO: Waiting for pod var-expansion-ce381e06-1744-43d3-b818-a9312b49b584 to disappear
Jun 22 13:39:10.908: INFO: Pod var-expansion-ce381e06-1744-43d3-b818-a9312b49b584 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:39:10.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6632" for this suite.

• [SLOW TEST:10.213 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":4261,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:39:10.920: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:39:11.346: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:39:13.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965951, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965951, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965951, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965951, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:39:16.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:39:16.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5166" for this suite.
STEP: Destroying namespace "webhook-5166-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":242,"skipped":4261,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:39:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:39:16.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:39:19.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965956, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965956, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965957, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965956, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:39:22.017: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:39:22.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-776" for this suite.
STEP: Destroying namespace "webhook-776-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.641 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":243,"skipped":4267,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:39:22.136: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:39:22.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:39:24.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965962, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965962, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965962, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759965962, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:39:27.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:39:27.940: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9677-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:39:29.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8849" for this suite.
STEP: Destroying namespace "webhook-8849-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.055 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":244,"skipped":4287,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:39:29.191: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun 22 13:39:29.332: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 13:40:29.364: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:40:29.367: INFO: Starting informer...
STEP: Starting pod...
Jun 22 13:40:29.583: INFO: Pod is running on 06998c1b-9fed-44d7-827f-f702404ff383. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 22 13:40:29.601: INFO: Pod wasn't evicted. Proceeding
Jun 22 13:40:29.601: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 22 13:41:44.675: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:41:44.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1680" for this suite.

• [SLOW TEST:135.495 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":245,"skipped":4293,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:41:44.687: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0622 13:41:54.952450      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:41:54.952571      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:41:54.952599      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:41:54.952: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 22 13:41:54.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-2svhm" in namespace "gc-9118"
Jun 22 13:41:54.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p462" in namespace "gc-9118"
Jun 22 13:41:54.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bgfc" in namespace "gc-9118"
Jun 22 13:41:54.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-854rb" in namespace "gc-9118"
Jun 22 13:41:55.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-btj7r" in namespace "gc-9118"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:41:55.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9118" for this suite.

• [SLOW TEST:10.351 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":246,"skipped":4326,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:41:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7758
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b8f8b263-3381-4264-9c80-fe03051b78e4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b8f8b263-3381-4264-9c80-fe03051b78e4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:43:29.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7758" for this suite.

• [SLOW TEST:94.634 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4346,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:43:29.674: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1424
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 22 13:43:29.834: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:43:49.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1424" for this suite.

• [SLOW TEST:19.636 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":248,"skipped":4362,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:43:49.314: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0622 13:43:59.492357      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0622 13:43:59.492379      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0622 13:43:59.492384      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 22 13:43:59.492: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:43:59.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-781" for this suite.

• [SLOW TEST:10.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":249,"skipped":4386,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:43:59.503: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-827
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 13:43:59.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 22 13:43:59.707: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:44:01.711: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:44:03.711: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:44:05.711: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:44:07.711: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 13:44:09.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:11.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:13.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:15.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:17.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:19.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:21.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:23.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:25.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 13:44:27.711: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 22 13:44:27.717: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 22 13:44:27.723: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 22 13:44:27.729: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 13:44:29.733: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 13:44:31.734: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 13:44:33.733: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 22 13:44:35.733: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun 22 13:44:39.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.7.6:8080/dial?request=hostname&protocol=http&host=11.32.7.2&port=8080&tries=1'] Namespace:pod-network-test-827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:44:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:44:39.896: INFO: Waiting for responses: map[]
Jun 22 13:44:39.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.7.6:8080/dial?request=hostname&protocol=http&host=11.32.7.3&port=8080&tries=1'] Namespace:pod-network-test-827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:44:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:44:39.982: INFO: Waiting for responses: map[]
Jun 22 13:44:39.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.7.6:8080/dial?request=hostname&protocol=http&host=11.32.7.4&port=8080&tries=1'] Namespace:pod-network-test-827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:44:39.984: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:44:40.075: INFO: Waiting for responses: map[]
Jun 22 13:44:40.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.32.7.6:8080/dial?request=hostname&protocol=http&host=11.32.7.5&port=8080&tries=1'] Namespace:pod-network-test-827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 13:44:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:44:40.174: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:44:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-827" for this suite.

• [SLOW TEST:40.682 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":4387,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:44:40.189: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9699
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-323b6d44-68b5-4462-bde8-f99f48dd4b61
STEP: Creating secret with name s-test-opt-upd-1ed7f07e-e562-4491-b16a-49e9b434d6da
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-323b6d44-68b5-4462-bde8-f99f48dd4b61
STEP: Updating secret s-test-opt-upd-1ed7f07e-e562-4491-b16a-49e9b434d6da
STEP: Creating secret with name s-test-opt-create-dfe173cb-b62b-4bc6-a2f8-e946d7ed8bb2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:44:54.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9699" for this suite.

• [SLOW TEST:14.275 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":4422,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:44:54.464: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:44:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:45:04.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6437" for this suite.

• [SLOW TEST:10.202 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":4422,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:45:04.666: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:45:04.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f" in namespace "downward-api-8571" to be "Succeeded or Failed"
Jun 22 13:45:04.841: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21583ms
Jun 22 13:45:06.845: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008030206s
Jun 22 13:45:08.849: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012030333s
Jun 22 13:45:10.853: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015829666s
Jun 22 13:45:12.858: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020891194s
Jun 22 13:45:14.862: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024734999s
STEP: Saw pod success
Jun 22 13:45:14.862: INFO: Pod "downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f" satisfied condition "Succeeded or Failed"
Jun 22 13:45:14.865: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f container client-container: <nil>
STEP: delete the pod
Jun 22 13:45:14.884: INFO: Waiting for pod downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f to disappear
Jun 22 13:45:14.888: INFO: Pod downwardapi-volume-27a5fcac-08d4-4a2e-bae2-1430f013b04f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:45:14.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8571" for this suite.

• [SLOW TEST:10.234 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4428,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:45:14.900: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6950
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6950
STEP: creating replication controller externalsvc in namespace services-6950
I0622 13:45:15.083224      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6950, replica count: 2
I0622 13:45:18.133484      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:21.133680      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:24.133877      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:27.134054      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:30.134228      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:33.134451      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:36.134698      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:39.134912      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:42.135069      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:45.135258      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:48.135435      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:51.135663      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:45:54.135825      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 22 13:45:54.155: INFO: Creating new exec pod
Jun 22 13:45:58.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-6950 exec execpod2mhnw -- /bin/sh -x -c nslookup clusterip-service.services-6950.svc.cluster.local'
Jun 22 13:45:59.103: INFO: stderr: "+ nslookup clusterip-service.services-6950.svc.cluster.local\n"
Jun 22 13:45:59.103: INFO: stdout: "Server:\t\t10.100.192.2\nAddress:\t10.100.192.2#53\n\nclusterip-service.services-6950.svc.cluster.local\tcanonical name = externalsvc.services-6950.svc.cluster.local.\nName:\texternalsvc.services-6950.svc.cluster.local\nAddress: 10.100.203.108\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6950, will wait for the garbage collector to delete the pods
Jun 22 13:45:59.163: INFO: Deleting ReplicationController externalsvc took: 6.647472ms
Jun 22 13:45:59.764: INFO: Terminating ReplicationController externalsvc pods took: 600.147667ms
Jun 22 13:46:11.976: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:46:11.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6950" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:57.110 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":254,"skipped":4437,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:46:12.010: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:46:12.189: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8987dca2-08d7-4888-a79a-a9e23c4de8b2", Controller:(*bool)(0xc0055a6622), BlockOwnerDeletion:(*bool)(0xc0055a6623)}}
Jun 22 13:46:12.207: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3ec13ae6-2be2-4a87-bb31-859c830e58da", Controller:(*bool)(0xc0055a685a), BlockOwnerDeletion:(*bool)(0xc0055a685b)}}
Jun 22 13:46:12.214: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"06c880cc-132b-483e-bf84-0d2cc2b55a08", Controller:(*bool)(0xc0055a6a8a), BlockOwnerDeletion:(*bool)(0xc0055a6a8b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:46:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7444" for this suite.

• [SLOW TEST:5.235 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":255,"skipped":4440,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:46:17.249: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:46:17.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07" in namespace "projected-2057" to be "Succeeded or Failed"
Jun 22 13:46:17.452: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Pending", Reason="", readiness=false. Elapsed: 9.057073ms
Jun 22 13:46:19.456: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013312939s
Jun 22 13:46:21.461: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018166723s
Jun 22 13:46:23.466: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023001579s
Jun 22 13:46:25.470: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026975336s
Jun 22 13:46:27.475: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031693566s
STEP: Saw pod success
Jun 22 13:46:27.475: INFO: Pod "downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07" satisfied condition "Succeeded or Failed"
Jun 22 13:46:27.478: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07 container client-container: <nil>
STEP: delete the pod
Jun 22 13:46:27.495: INFO: Waiting for pod downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07 to disappear
Jun 22 13:46:27.499: INFO: Pod downwardapi-volume-d54376ae-22cd-43a4-8a80-c6654e8c1a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:46:27.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2057" for this suite.

• [SLOW TEST:10.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4461,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:46:27.516: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 13:46:36.700: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:46:36.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6827" for this suite.

• [SLOW TEST:9.219 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4511,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:46:36.734: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 13:46:47.416: INFO: Successfully updated pod "pod-update-c27f2bf8-804f-4571-b7e4-f1b502abc043"
STEP: verifying the updated pod is in kubernetes
Jun 22 13:46:47.421: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:46:47.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6140" for this suite.

• [SLOW TEST:10.698 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4523,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:46:47.432: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6848
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6848
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6848
Jun 22 13:46:47.597: INFO: Found 0 stateful pods, waiting for 1
Jun 22 13:46:57.602: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 22 13:46:57.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:46:57.787: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:46:57.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:46:57.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:46:57.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 13:47:07.794: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:47:07.794: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:47:07.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999796s
Jun 22 13:47:08.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981027965s
Jun 22 13:47:09.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976215385s
Jun 22 13:47:10.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972160954s
Jun 22 13:47:11.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967728755s
Jun 22 13:47:12.850: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962281103s
Jun 22 13:47:13.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95637825s
Jun 22 13:47:14.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.952311435s
Jun 22 13:47:15.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947194872s
Jun 22 13:47:16.869: INFO: Verifying statefulset ss doesn't scale past 3 for another 942.441741ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6848
Jun 22 13:47:17.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:47:18.084: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 13:47:18.084: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:47:18.084: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:47:18.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:47:18.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 13:47:18.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:47:18.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:47:18.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 13:47:18.443: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 13:47:18.443: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 13:47:18.443: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 13:47:18.448: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 22 13:47:28.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:47:28.454: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 13:47:28.454: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 22 13:47:28.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:47:28.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:47:28.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:47:28.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:47:28.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:47:28.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:47:28.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:47:28.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:47:28.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=statefulset-6848 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 13:47:28.990: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 13:47:28.990: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 13:47:28.990: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 13:47:28.990: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:47:28.993: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 22 13:47:39.000: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:47:39.000: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:47:39.000: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 13:47:39.012: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:39.012: INFO: ss-0  06998c1b-9fed-44d7-827f-f702404ff383  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  }]
Jun 22 13:47:39.012: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:39.012: INFO: ss-2  1d96d19c-1b78-44d8-b822-ba104bc5daa5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:39.012: INFO: 
Jun 22 13:47:39.012: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 13:47:40.016: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:40.016: INFO: ss-0  06998c1b-9fed-44d7-827f-f702404ff383  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  }]
Jun 22 13:47:40.017: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:40.017: INFO: ss-2  1d96d19c-1b78-44d8-b822-ba104bc5daa5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:40.017: INFO: 
Jun 22 13:47:40.017: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 13:47:41.021: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:41.021: INFO: ss-0  06998c1b-9fed-44d7-827f-f702404ff383  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  }]
Jun 22 13:47:41.021: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:41.021: INFO: ss-2  1d96d19c-1b78-44d8-b822-ba104bc5daa5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:41.021: INFO: 
Jun 22 13:47:41.021: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 13:47:42.026: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:42.026: INFO: ss-0  06998c1b-9fed-44d7-827f-f702404ff383  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:46:47 +0000 UTC  }]
Jun 22 13:47:42.026: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:42.026: INFO: ss-2  1d96d19c-1b78-44d8-b822-ba104bc5daa5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:42.026: INFO: 
Jun 22 13:47:42.026: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 13:47:43.030: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:43.030: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:43.030: INFO: ss-2  1d96d19c-1b78-44d8-b822-ba104bc5daa5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:43.030: INFO: 
Jun 22 13:47:43.030: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 13:47:44.034: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jun 22 13:47:44.034: INFO: ss-1  2352dbd9-b599-409b-9a0b-5bade7a216ea  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-22 13:47:07 +0000 UTC  }]
Jun 22 13:47:44.034: INFO: 
Jun 22 13:47:44.034: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 22 13:47:45.038: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974588161s
Jun 22 13:47:46.042: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970461678s
Jun 22 13:47:47.046: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.96666501s
Jun 22 13:47:48.050: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.770971ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6848
Jun 22 13:47:49.054: INFO: Scaling statefulset ss to 0
Jun 22 13:47:49.066: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 22 13:47:49.069: INFO: Deleting all statefulset in ns statefulset-6848
Jun 22 13:47:49.072: INFO: Scaling statefulset ss to 0
Jun 22 13:47:49.083: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 13:47:49.086: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:47:49.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6848" for this suite.

• [SLOW TEST:61.690 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":259,"skipped":4542,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:47:49.123: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:49:49.323: INFO: Deleting pod "var-expansion-067867a9-be97-4cd2-92f9-9932516e7b35" in namespace "var-expansion-2108"
Jun 22 13:49:49.330: INFO: Wait up to 5m0s for pod "var-expansion-067867a9-be97-4cd2-92f9-9932516e7b35" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:49:51.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2108" for this suite.

• [SLOW TEST:122.225 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":260,"skipped":4565,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:49:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:49:51.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe" in namespace "downward-api-9399" to be "Succeeded or Failed"
Jun 22 13:49:51.503: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.787272ms
Jun 22 13:49:53.507: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011415288s
Jun 22 13:49:55.511: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015191965s
Jun 22 13:49:57.516: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020144244s
Jun 22 13:49:59.520: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024366367s
STEP: Saw pod success
Jun 22 13:49:59.520: INFO: Pod "downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe" satisfied condition "Succeeded or Failed"
Jun 22 13:49:59.523: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe container client-container: <nil>
STEP: delete the pod
Jun 22 13:49:59.553: INFO: Waiting for pod downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe to disappear
Jun 22 13:49:59.556: INFO: Pod downwardapi-volume-532e8804-199e-4338-a892-e4f00c07c7fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:49:59.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9399" for this suite.

• [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4596,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:49:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-21e00c75-177d-44d9-b585-136b6250eb7d
STEP: Creating secret with name secret-projected-all-test-volume-0f64bb29-91e0-436e-85ae-a71c92d073fa
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 13:49:59.716: INFO: Waiting up to 5m0s for pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b" in namespace "projected-9633" to be "Succeeded or Failed"
Jun 22 13:49:59.719: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08986ms
Jun 22 13:50:01.723: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007059525s
Jun 22 13:50:03.727: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011197242s
Jun 22 13:50:05.732: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015519715s
Jun 22 13:50:07.736: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020267409s
Jun 22 13:50:09.741: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024592546s
STEP: Saw pod success
Jun 22 13:50:09.741: INFO: Pod "projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b" satisfied condition "Succeeded or Failed"
Jun 22 13:50:09.744: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 13:50:09.760: INFO: Waiting for pod projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b to disappear
Jun 22 13:50:09.765: INFO: Pod projected-volume-80ce9d73-9baf-4d64-9a33-a8087c7ae47b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:50:09.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9633" for this suite.

• [SLOW TEST:10.208 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4601,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:50:09.775: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-t2n7
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 13:50:09.928: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t2n7" in namespace "subpath-1919" to be "Succeeded or Failed"
Jun 22 13:50:09.934: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.394728ms
Jun 22 13:50:11.938: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010460807s
Jun 22 13:50:13.943: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015476114s
Jun 22 13:50:15.947: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019623246s
Jun 22 13:50:17.952: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024553217s
Jun 22 13:50:19.956: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 10.028565677s
Jun 22 13:50:21.961: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 12.033157308s
Jun 22 13:50:23.965: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 14.037531538s
Jun 22 13:50:25.970: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 16.042046677s
Jun 22 13:50:27.974: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 18.046594902s
Jun 22 13:50:29.979: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 20.051078478s
Jun 22 13:50:31.983: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 22.055435608s
Jun 22 13:50:33.987: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 24.059713728s
Jun 22 13:50:35.992: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 26.064064238s
Jun 22 13:50:37.996: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Running", Reason="", readiness=true. Elapsed: 28.068565694s
Jun 22 13:50:40.001: INFO: Pod "pod-subpath-test-configmap-t2n7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.073822579s
STEP: Saw pod success
Jun 22 13:50:40.002: INFO: Pod "pod-subpath-test-configmap-t2n7" satisfied condition "Succeeded or Failed"
Jun 22 13:50:40.005: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-subpath-test-configmap-t2n7 container test-container-subpath-configmap-t2n7: <nil>
STEP: delete the pod
Jun 22 13:50:40.024: INFO: Waiting for pod pod-subpath-test-configmap-t2n7 to disappear
Jun 22 13:50:40.028: INFO: Pod pod-subpath-test-configmap-t2n7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t2n7
Jun 22 13:50:40.028: INFO: Deleting pod "pod-subpath-test-configmap-t2n7" in namespace "subpath-1919"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:50:40.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1919" for this suite.

• [SLOW TEST:30.265 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":263,"skipped":4606,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:50:40.042: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:50:51.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7980" for this suite.

• [SLOW TEST:11.186 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":264,"skipped":4612,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:50:51.229: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:50:51.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3" in namespace "projected-4109" to be "Succeeded or Failed"
Jun 22 13:50:51.391: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.621924ms
Jun 22 13:50:53.395: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020175885s
Jun 22 13:50:55.399: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024120262s
Jun 22 13:50:57.403: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027829394s
Jun 22 13:50:59.408: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032642613s
STEP: Saw pod success
Jun 22 13:50:59.408: INFO: Pod "downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3" satisfied condition "Succeeded or Failed"
Jun 22 13:50:59.411: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3 container client-container: <nil>
STEP: delete the pod
Jun 22 13:50:59.428: INFO: Waiting for pod downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3 to disappear
Jun 22 13:50:59.432: INFO: Pod downwardapi-volume-2c127ca8-4dda-45d1-9c15-3957e640fbb3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:50:59.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4109" for this suite.

• [SLOW TEST:8.213 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4663,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:50:59.443: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 13:50:59.581: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:51:11.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6799" for this suite.

• [SLOW TEST:12.444 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4681,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:51:11.887: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:51:45.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6676" for this suite.

• [SLOW TEST:33.405 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4687,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:51:45.293: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 22 13:51:45.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182558 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:51:45.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182559 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:51:45.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182560 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 22 13:51:55.476: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182612 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:51:55.476: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182613 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 13:51:55.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8929 /api/v1/namespaces/watch-8929/configmaps/e2e-watch-test-label-changed c3d0dd70-78f0-409f-ae90-34b1c8ad29cc 182614 0 2021-06-22 13:51:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-22 13:51:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:51:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8929" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":268,"skipped":4705,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:51:55.492: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jun 22 13:51:55.638: INFO: Waiting up to 5m0s for pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314" in namespace "containers-8720" to be "Succeeded or Failed"
Jun 22 13:51:55.647: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314": Phase="Pending", Reason="", readiness=false. Elapsed: 9.7339ms
Jun 22 13:51:57.652: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014574174s
Jun 22 13:51:59.658: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019826539s
Jun 22 13:52:01.661: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023624979s
Jun 22 13:52:03.665: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027290802s
STEP: Saw pod success
Jun 22 13:52:03.665: INFO: Pod "client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314" satisfied condition "Succeeded or Failed"
Jun 22 13:52:03.668: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314 container test-container: <nil>
STEP: delete the pod
Jun 22 13:52:03.683: INFO: Waiting for pod client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314 to disappear
Jun 22 13:52:03.688: INFO: Pod client-containers-bd1ae036-5854-4d1b-b763-ce54aaafd314 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:52:03.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8720" for this suite.

• [SLOW TEST:8.208 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4731,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:52:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 22 13:52:12.379: INFO: Successfully updated pod "annotationupdate2bee94b6-1a0d-4d54-b9a7-eecc37b10eb5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:52:16.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7112" for this suite.

• [SLOW TEST:12.712 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4742,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:52:16.416: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-8jml
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 13:52:16.568: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8jml" in namespace "subpath-6939" to be "Succeeded or Failed"
Jun 22 13:52:16.591: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Pending", Reason="", readiness=false. Elapsed: 22.258672ms
Jun 22 13:52:18.599: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030796633s
Jun 22 13:52:20.604: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035425337s
Jun 22 13:52:22.608: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039243498s
Jun 22 13:52:24.612: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 8.043182651s
Jun 22 13:52:26.616: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 10.047855856s
Jun 22 13:52:28.621: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 12.052890222s
Jun 22 13:52:30.627: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 14.057921621s
Jun 22 13:52:32.632: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 16.063010884s
Jun 22 13:52:34.636: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 18.067833602s
Jun 22 13:52:36.640: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 20.071840566s
Jun 22 13:52:38.645: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 22.076441913s
Jun 22 13:52:40.650: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 24.081198945s
Jun 22 13:52:42.655: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Running", Reason="", readiness=true. Elapsed: 26.086364729s
Jun 22 13:52:44.659: INFO: Pod "pod-subpath-test-downwardapi-8jml": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.090452602s
STEP: Saw pod success
Jun 22 13:52:44.659: INFO: Pod "pod-subpath-test-downwardapi-8jml" satisfied condition "Succeeded or Failed"
Jun 22 13:52:44.662: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-subpath-test-downwardapi-8jml container test-container-subpath-downwardapi-8jml: <nil>
STEP: delete the pod
Jun 22 13:52:44.681: INFO: Waiting for pod pod-subpath-test-downwardapi-8jml to disappear
Jun 22 13:52:44.683: INFO: Pod pod-subpath-test-downwardapi-8jml no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8jml
Jun 22 13:52:44.684: INFO: Deleting pod "pod-subpath-test-downwardapi-8jml" in namespace "subpath-6939"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:52:44.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6939" for this suite.

• [SLOW TEST:28.281 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":271,"skipped":4742,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:52:44.698: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:52:51.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1052" for this suite.

• [SLOW TEST:7.171 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":272,"skipped":4773,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:52:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:52:52.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:52:54.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:52:56.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:52:58.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:53:00.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:53:02.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:53:04.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966772, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:53:07.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:53:07.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7364" for this suite.
STEP: Destroying namespace "webhook-7364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":273,"skipped":4774,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:53:07.383: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 13:53:08.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 13:53:10.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966788, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966788, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966788, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966788, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 13:53:13.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 22 13:53:17.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=webhook-4220 attach --namespace=webhook-4220 to-be-attached-pod -i -c=container1'
Jun 22 13:53:17.188: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:53:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4220" for this suite.
STEP: Destroying namespace "webhook-4220-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.869 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":274,"skipped":4775,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:53:17.252: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:53:27.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2793" for this suite.

• [SLOW TEST:10.197 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4799,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:53:27.452: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jun 22 13:53:27.604: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 22 13:53:27.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:27.867: INFO: stderr: ""
Jun 22 13:53:27.867: INFO: stdout: "service/agnhost-replica created\n"
Jun 22 13:53:27.867: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 22 13:53:27.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:28.079: INFO: stderr: ""
Jun 22 13:53:28.079: INFO: stdout: "service/agnhost-primary created\n"
Jun 22 13:53:28.079: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 13:53:28.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:28.260: INFO: stderr: ""
Jun 22 13:53:28.260: INFO: stdout: "service/frontend created\n"
Jun 22 13:53:28.261: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 22 13:53:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:28.446: INFO: stderr: ""
Jun 22 13:53:28.446: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 13:53:28.446: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 13:53:28.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:28.669: INFO: stderr: ""
Jun 22 13:53:28.669: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 22 13:53:28.669: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 13:53:28.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 create -f -'
Jun 22 13:53:28.867: INFO: stderr: ""
Jun 22 13:53:28.867: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 22 13:53:28.867: INFO: Waiting for all frontend pods to be Running.
Jun 22 13:53:33.918: INFO: Waiting for frontend to serve content.
Jun 22 13:53:33.931: INFO: Trying to add a new entry to the guestbook.
Jun 22 13:53:33.942: INFO: Verifying that added entry can be retrieved.
Jun 22 13:53:33.949: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jun 22 13:53:38.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.063: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.063: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 13:53:39.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.171: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.171: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 13:53:39.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.256: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.256: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 13:53:39.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.334: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.334: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 13:53:39.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.445: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.445: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 13:53:39.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=kubectl-1596 delete --grace-period=0 --force -f -'
Jun 22 13:53:39.578: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 13:53:39.578: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:53:39.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1596" for this suite.

• [SLOW TEST:12.168 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":276,"skipped":4800,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:53:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:53:39.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a" in namespace "downward-api-3599" to be "Succeeded or Failed"
Jun 22 13:53:39.824: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761678ms
Jun 22 13:53:41.827: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009099194s
Jun 22 13:53:43.831: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012741745s
Jun 22 13:53:45.834: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01617695s
Jun 22 13:53:47.838: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019821965s
Jun 22 13:53:49.842: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023415927s
STEP: Saw pod success
Jun 22 13:53:49.842: INFO: Pod "downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a" satisfied condition "Succeeded or Failed"
Jun 22 13:53:49.844: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a container client-container: <nil>
STEP: delete the pod
Jun 22 13:53:49.864: INFO: Waiting for pod downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a to disappear
Jun 22 13:53:49.866: INFO: Pod downwardapi-volume-1dd7961e-427c-4af4-92f0-c2af6fec6b3a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:53:49.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3599" for this suite.

• [SLOW TEST:10.256 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4805,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:53:49.878: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 22 13:53:50.023: INFO: PodSpec: initContainers in spec.initContainers
Jun 22 13:54:43.410: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9f6715ba-4139-40f4-9722-61355fb72541", GenerateName:"", Namespace:"init-container-8240", SelfLink:"/api/v1/namespaces/init-container-8240/pods/pod-init-9f6715ba-4139-40f4-9722-61355fb72541", UID:"e1a67e5d-fd3c-42dc-b00d-203a326f3a15", ResourceVersion:"183692", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759966830, loc:(*time.Location)(0x77148e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"23010848"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002f56ae0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f56b00)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002f56b20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f56b40)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lfp4l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00274aa40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lfp4l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lfp4l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lfp4l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0032847b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"06998c1b-9fed-44d7-827f-f702404ff383", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00335c4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003284830)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003284850)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003284858), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00328485c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001e00170), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966830, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966830, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966830, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966830, loc:(*time.Location)(0x77148e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"11.0.1.4", PodIP:"11.32.8.2", PodIPs:[]v1.PodIP{v1.PodIP{IP:"11.32.8.2"}}, StartTime:(*v1.Time)(0xc002f56ba0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00335c5b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00335c620)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://cc7030b9b5543935118050cdfcab9877cd538ff7f81f623dca4368678d02d558", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f56c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f56c60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0032848bc)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:54:43.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8240" for this suite.

• [SLOW TEST:53.546 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":278,"skipped":4810,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:54:43.424: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7616
Jun 22 13:54:45.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 22 13:54:45.765: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 22 13:54:45.765: INFO: stdout: "iptables"
Jun 22 13:54:45.765: INFO: proxyMode: iptables
Jun 22 13:54:45.773: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:54:45.776: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:54:47.776: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:54:47.780: INFO: Pod kube-proxy-mode-detector still exists
Jun 22 13:54:49.776: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 13:54:49.779: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7616
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7616
I0622 13:54:49.799240      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7616, replica count: 3
I0622 13:54:52.849543      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:54:55.849714      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:54:58.849875      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:01.850031      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:04.850175      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:07.850336      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:10.850510      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:13.850655      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:16.850825      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:19.850970      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:55:22.851072      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 13:55:22.863: INFO: Creating new exec pod
Jun 22 13:55:27.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun 22 13:55:28.071: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 22 13:55:28.071: INFO: stdout: ""
Jun 22 13:55:28.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 10.100.204.123 80'
Jun 22 13:55:28.272: INFO: stderr: "+ nc -zv -t -w 2 10.100.204.123 80\nConnection to 10.100.204.123 80 port [tcp/http] succeeded!\n"
Jun 22 13:55:28.272: INFO: stdout: ""
Jun 22 13:55:28.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 32249'
Jun 22 13:55:28.441: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 32249\nConnection to 11.0.1.4 32249 port [tcp/32249] succeeded!\n"
Jun 22 13:55:28.441: INFO: stdout: ""
Jun 22 13:55:28.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 32249'
Jun 22 13:55:28.613: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 32249\nConnection to 11.0.1.3 32249 port [tcp/32249] succeeded!\n"
Jun 22 13:55:28.613: INFO: stdout: ""
Jun 22 13:55:28.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 32249'
Jun 22 13:55:28.776: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 32249\nConnection to 11.0.1.4 32249 port [tcp/32249] succeeded!\n"
Jun 22 13:55:28.776: INFO: stdout: ""
Jun 22 13:55:28.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 32249'
Jun 22 13:55:28.942: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 32249\nConnection to 11.0.1.3 32249 port [tcp/32249] succeeded!\n"
Jun 22 13:55:28.942: INFO: stdout: ""
Jun 22 13:55:28.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.1.4:32249/ ; done'
Jun 22 13:55:29.190: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n"
Jun 22 13:55:29.190: INFO: stdout: "\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw\naffinity-nodeport-timeout-nqptw"
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Received response from host: affinity-nodeport-timeout-nqptw
Jun 22 13:55:29.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://11.0.1.4:32249/'
Jun 22 13:55:29.360: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n"
Jun 22 13:55:29.360: INFO: stdout: "affinity-nodeport-timeout-nqptw"
Jun 22 13:55:44.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-7616 exec execpod-affinitypv5pr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://11.0.1.4:32249/'
Jun 22 13:55:44.551: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://11.0.1.4:32249/\n"
Jun 22 13:55:44.551: INFO: stdout: "affinity-nodeport-timeout-s4jqs"
Jun 22 13:55:44.551: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7616, will wait for the garbage collector to delete the pods
Jun 22 13:55:44.622: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.059951ms
Jun 22 13:55:45.222: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.104347ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:55:53.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7616" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:70.534 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":279,"skipped":4812,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:55:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 13:56:03.148: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:03.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7979" for this suite.

• [SLOW TEST:9.213 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4815,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:03.173: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5609
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:56:03.306: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5609" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":281,"skipped":4817,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}

------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:03.858: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8156
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-d6bd452f-bc48-4459-8bc4-d67d08105c92
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:14.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8156" for this suite.

• [SLOW TEST:10.222 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":282,"skipped":4817,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:14.082: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 13:56:14.219: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 13:56:14.228: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 13:56:14.231: INFO: 
Logging pods the apiserver thinks is on node 06998c1b-9fed-44d7-827f-f702404ff383 before test
Jun 22 13:56:14.243: INFO: pod-configmaps-de03100e-6fae-4a72-af88-21d42dcc4d8d from configmap-8156 started at 2021-06-22 13:56:04 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.243: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Jun 22 13:56:14.243: INFO: 	Container configmap-volume-data-test ready: true, restart count 0
Jun 22 13:56:14.243: INFO: fluent-bit-sh7kc from pks-system started at 2021-06-22 13:40:41 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.243: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:56:14.243: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:56:14.243: INFO: node-exporter-zch8d from pks-system started at 2021-06-22 13:40:41 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.243: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:56:14.243: INFO: telegraf-cmjpr from pks-system started at 2021-06-22 13:40:41 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.243: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:56:14.243: INFO: sonobuoy from sonobuoy started at 2021-06-22 11:56:55 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.243: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 13:56:14.243: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-2r7sz from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.244: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Jun 22 13:56:14.244: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:56:14.244: INFO: 
Logging pods the apiserver thinks is on node 1d96d19c-1b78-44d8-b822-ba104bc5daa5 before test
Jun 22 13:56:14.255: INFO: coredns-645ccbcd68-7dlmn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:56:14.255: INFO: metrics-server-7d476fdfbd-md64k from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 13:56:14.255: INFO: fluent-bit-8rhpl from pks-system started at 2021-06-21 23:47:44 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:56:14.255: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:56:14.255: INFO: metric-controller-7f5cb8ff6d-7npkg from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container metric-controller ready: true, restart count 0
Jun 22 13:56:14.255: INFO: node-exporter-w6bbv from pks-system started at 2021-06-21 23:42:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:56:14.255: INFO: observability-manager-6cf797f97-8nqdm from pks-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container observability-manager ready: true, restart count 0
Jun 22 13:56:14.255: INFO: sink-controller-f6bc7f774-zprjs from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container sink-controller ready: true, restart count 0
Jun 22 13:56:14.255: INFO: telegraf-j56d5 from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:56:14.255: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-4wdd9 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.255: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Jun 22 13:56:14.255: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:56:14.255: INFO: 
Logging pods the apiserver thinks is on node 2352dbd9-b599-409b-9a0b-5bade7a216ea before test
Jun 22 13:56:14.269: INFO: fluent-bit-vtnl7 from pks-system started at 2021-06-21 23:52:24 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:56:14.269: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:56:14.269: INFO: node-exporter-82sxc from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:56:14.269: INFO: telegraf-dwvlw from pks-system started at 2021-06-21 23:52:24 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:56:14.269: INFO: telemetry-agent-8444c9c47b-7xllf from pks-system started at 2021-06-21 23:55:50 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jun 22 13:56:14.269: INFO: sonobuoy-e2e-job-60cfc378f7374ff2 from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container e2e ready: true, restart count 0
Jun 22 13:56:14.269: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 13:56:14.269: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-t2zwp from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.269: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Jun 22 13:56:14.269: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 13:56:14.269: INFO: 
Logging pods the apiserver thinks is on node 54b7d611-8263-481b-bda6-56b40bff2a2f before test
Jun 22 13:56:14.283: INFO: coredns-645ccbcd68-pbmnn from kube-system started at 2021-06-21 23:47:34 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.283: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:56:14.283: INFO: coredns-645ccbcd68-s8fjw from kube-system started at 2021-06-21 23:47:33 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container coredns ready: true, restart count 0
Jun 22 13:56:14.284: INFO: event-controller-85d6bb4d4c-fz5nk from pks-system started at 2021-06-21 23:47:39 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container event-controller ready: true, restart count 0
Jun 22 13:56:14.284: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:56:14.284: INFO: fluent-bit-f425q from pks-system started at 2021-06-21 23:48:02 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 22 13:56:14.284: INFO: 	Container ghostunnel ready: true, restart count 0
Jun 22 13:56:14.284: INFO: node-exporter-8qrc5 from pks-system started at 2021-06-21 23:46:22 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 22 13:56:14.284: INFO: telegraf-9z5xm from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container telegraf ready: true, restart count 0
Jun 22 13:56:14.284: INFO: validator-69f557d7c6-bl8gf from pks-system started at 2021-06-21 23:47:39 +0000 UTC (1 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container validator ready: true, restart count 0
Jun 22 13:56:14.284: INFO: sonobuoy-systemd-logs-daemon-set-7775baf607bc4189-6t6tm from sonobuoy started at 2021-06-22 11:57:05 +0000 UTC (2 container statuses recorded)
Jun 22 13:56:14.284: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Jun 22 13:56:14.284: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168aec1137f07a15], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168aec11388ea690], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:15.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1843" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":283,"skipped":4826,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7174
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:56:15.469: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 22 13:56:18.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 create -f -'
Jun 22 13:56:19.521: INFO: stderr: ""
Jun 22 13:56:19.521: INFO: stdout: "e2e-test-crd-publish-openapi-5991-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 13:56:19.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 delete e2e-test-crd-publish-openapi-5991-crds test-foo'
Jun 22 13:56:19.611: INFO: stderr: ""
Jun 22 13:56:19.611: INFO: stdout: "e2e-test-crd-publish-openapi-5991-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 22 13:56:19.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 apply -f -'
Jun 22 13:56:19.794: INFO: stderr: ""
Jun 22 13:56:19.794: INFO: stdout: "e2e-test-crd-publish-openapi-5991-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 13:56:19.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 delete e2e-test-crd-publish-openapi-5991-crds test-foo'
Jun 22 13:56:19.878: INFO: stderr: ""
Jun 22 13:56:19.878: INFO: stdout: "e2e-test-crd-publish-openapi-5991-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 22 13:56:19.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 create -f -'
Jun 22 13:56:20.043: INFO: rc: 1
Jun 22 13:56:20.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 apply -f -'
Jun 22 13:56:20.227: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 22 13:56:20.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 create -f -'
Jun 22 13:56:20.415: INFO: rc: 1
Jun 22 13:56:20.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 --namespace=crd-publish-openapi-7174 apply -f -'
Jun 22 13:56:20.584: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 22 13:56:20.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 explain e2e-test-crd-publish-openapi-5991-crds'
Jun 22 13:56:20.756: INFO: stderr: ""
Jun 22 13:56:20.756: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5991-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 22 13:56:20.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 explain e2e-test-crd-publish-openapi-5991-crds.metadata'
Jun 22 13:56:20.931: INFO: stderr: ""
Jun 22 13:56:20.931: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5991-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 22 13:56:20.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 explain e2e-test-crd-publish-openapi-5991-crds.spec'
Jun 22 13:56:21.103: INFO: stderr: ""
Jun 22 13:56:21.103: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5991-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 22 13:56:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 explain e2e-test-crd-publish-openapi-5991-crds.spec.bars'
Jun 22 13:56:21.304: INFO: stderr: ""
Jun 22 13:56:21.304: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5991-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 22 13:56:21.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=crd-publish-openapi-7174 explain e2e-test-crd-publish-openapi-5991-crds.spec.bars2'
Jun 22 13:56:21.471: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:25.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7174" for this suite.

• [SLOW TEST:9.695 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":284,"skipped":4829,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:25.027: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:56:25.165: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 13:56:25.179: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 13:56:35.198: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 13:56:35.203: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 13:56:35.212: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 13:56:37.219: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 13:56:37.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966995, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966995, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966995, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63759966995, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 13:56:39.227: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 22 13:56:39.236: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7823 /apis/apps/v1/namespaces/deployment-7823/deployments/test-rolling-update-deployment 3b973575-d578-4750-9b75-3b4068a652cb 184367 1 2021-06-22 13:56:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-06-22 13:56:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 13:56:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f45628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-22 13:56:35 +0000 UTC,LastTransitionTime:2021-06-22 13:56:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-06-22 13:56:37 +0000 UTC,LastTransitionTime:2021-06-22 13:56:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 13:56:39.239: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-7823 /apis/apps/v1/namespaces/deployment-7823/replicasets/test-rolling-update-deployment-c4cb8d6d9 85c4baef-22fc-462a-8bb5-354973d55f9c 184356 1 2021-06-22 13:56:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3b973575-d578-4750-9b75-3b4068a652cb 0xc004f45d40 0xc004f45d41}] []  [{kube-controller-manager Update apps/v1 2021-06-22 13:56:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b973575-d578-4750-9b75-3b4068a652cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f45de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:56:39.239: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 13:56:39.239: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7823 /apis/apps/v1/namespaces/deployment-7823/replicasets/test-rolling-update-controller a209cba4-8296-4616-bce5-e4aac378b228 184366 2 2021-06-22 13:56:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3b973575-d578-4750-9b75-3b4068a652cb 0xc004f45bd7 0xc004f45bd8}] []  [{e2e.test Update apps/v1 2021-06-22 13:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-22 13:56:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b973575-d578-4750-9b75-3b4068a652cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004f45c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 13:56:39.242: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-rwjvw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-rwjvw test-rolling-update-deployment-c4cb8d6d9- deployment-7823 /api/v1/namespaces/deployment-7823/pods/test-rolling-update-deployment-c4cb8d6d9-rwjvw a96a7558-f6da-4e14-b582-32543cd3c619 184355 0 2021-06-22 13:56:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 85c4baef-22fc-462a-8bb5-354973d55f9c 0xc004f9c480 0xc004f9c481}] []  [{kube-controller-manager Update v1 2021-06-22 13:56:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85c4baef-22fc-462a-8bb5-354973d55f9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-22 13:56:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.32.7.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b4pxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b4pxb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b4pxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:06998c1b-9fed-44d7-827f-f702404ff383,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:56:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:56:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:56:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-22 13:56:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.1.4,PodIP:11.32.7.3,StartTime:2021-06-22 13:56:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-22 13:56:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://79ce1708416bf4c09a49e55b4970d388fc013bcc1563a66ebc18320af739b8a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.32.7.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:39.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7823" for this suite.

• [SLOW TEST:14.227 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":285,"skipped":4842,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:39.254: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:56:39.389: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:49.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5878" for this suite.

• [SLOW TEST:10.252 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4842,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:49.506: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-0269774e-76ee-44c9-aa60-a266b6791789
STEP: Creating a pod to test consume secrets
Jun 22 13:56:49.649: INFO: Waiting up to 5m0s for pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328" in namespace "secrets-5606" to be "Succeeded or Failed"
Jun 22 13:56:49.655: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082477ms
Jun 22 13:56:51.660: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010622237s
Jun 22 13:56:53.664: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01471464s
Jun 22 13:56:55.668: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018623159s
Jun 22 13:56:57.672: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023206496s
Jun 22 13:56:59.680: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031214653s
STEP: Saw pod success
Jun 22 13:56:59.681: INFO: Pod "pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328" satisfied condition "Succeeded or Failed"
Jun 22 13:56:59.683: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 13:56:59.706: INFO: Waiting for pod pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328 to disappear
Jun 22 13:56:59.710: INFO: Pod pod-secrets-7ba063d3-2c07-4ca8-82a8-55d935b62328 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:56:59.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5606" for this suite.

• [SLOW TEST:10.219 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4843,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:56:59.726: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9960
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 22 13:56:59.870: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 22 13:57:12.644: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
Jun 22 13:57:16.208: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:57:29.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9960" for this suite.

• [SLOW TEST:30.211 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":288,"skipped":4917,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:57:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 22 13:57:30.084: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7" in namespace "downward-api-6386" to be "Succeeded or Failed"
Jun 22 13:57:30.093: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.478147ms
Jun 22 13:57:32.102: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018025416s
Jun 22 13:57:34.106: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02212714s
Jun 22 13:57:36.110: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026595271s
Jun 22 13:57:38.115: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030928202s
Jun 22 13:57:40.119: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035365534s
STEP: Saw pod success
Jun 22 13:57:40.119: INFO: Pod "downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7" satisfied condition "Succeeded or Failed"
Jun 22 13:57:40.122: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7 container client-container: <nil>
STEP: delete the pod
Jun 22 13:57:40.141: INFO: Waiting for pod downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7 to disappear
Jun 22 13:57:40.145: INFO: Pod downwardapi-volume-44023ed6-5299-406e-8f8d-13f06205c2f7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:57:40.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6386" for this suite.

• [SLOW TEST:10.217 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4924,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:57:40.156: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-8298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 22 13:57:40.786: INFO: starting watch
STEP: patching
STEP: updating
Jun 22 13:57:40.798: INFO: waiting for watch events with expected annotations
Jun 22 13:57:40.798: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:57:40.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8298" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":290,"skipped":4934,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:57:40.870: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:57:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3246" for this suite.

• [SLOW TEST:11.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":291,"skipped":4946,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:57:52.067: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:57:52.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8883" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":292,"skipped":4968,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:57:52.252: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 22 13:58:26.903: INFO: Successfully updated pod "adopt-release-c7dk5"
STEP: Checking that the Job readopts the Pod
Jun 22 13:58:26.903: INFO: Waiting up to 15m0s for pod "adopt-release-c7dk5" in namespace "job-1138" to be "adopted"
Jun 22 13:58:26.907: INFO: Pod "adopt-release-c7dk5": Phase="Running", Reason="", readiness=true. Elapsed: 4.196752ms
Jun 22 13:58:28.912: INFO: Pod "adopt-release-c7dk5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008319247s
Jun 22 13:58:28.912: INFO: Pod "adopt-release-c7dk5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 22 13:58:29.425: INFO: Successfully updated pod "adopt-release-c7dk5"
STEP: Checking that the Job releases the Pod
Jun 22 13:58:29.425: INFO: Waiting up to 15m0s for pod "adopt-release-c7dk5" in namespace "job-1138" to be "released"
Jun 22 13:58:29.433: INFO: Pod "adopt-release-c7dk5": Phase="Running", Reason="", readiness=true. Elapsed: 8.363974ms
Jun 22 13:58:29.433: INFO: Pod "adopt-release-c7dk5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:58:29.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1138" for this suite.

• [SLOW TEST:37.203 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":293,"skipped":5015,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:58:29.456: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:58:29.634: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 22 13:58:29.643: INFO: Number of nodes with available pods: 0
Jun 22 13:58:29.643: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 22 13:58:29.671: INFO: Number of nodes with available pods: 0
Jun 22 13:58:29.671: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:30.675: INFO: Number of nodes with available pods: 0
Jun 22 13:58:30.675: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:31.674: INFO: Number of nodes with available pods: 0
Jun 22 13:58:31.674: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:32.675: INFO: Number of nodes with available pods: 0
Jun 22 13:58:32.675: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:33.674: INFO: Number of nodes with available pods: 0
Jun 22 13:58:33.674: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:34.674: INFO: Number of nodes with available pods: 0
Jun 22 13:58:34.674: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:35.675: INFO: Number of nodes with available pods: 0
Jun 22 13:58:35.675: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:36.676: INFO: Number of nodes with available pods: 0
Jun 22 13:58:36.676: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:37.675: INFO: Number of nodes with available pods: 1
Jun 22 13:58:37.675: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 22 13:58:37.697: INFO: Number of nodes with available pods: 0
Jun 22 13:58:37.697: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 22 13:58:37.715: INFO: Number of nodes with available pods: 0
Jun 22 13:58:37.715: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:38.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:38.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:39.718: INFO: Number of nodes with available pods: 0
Jun 22 13:58:39.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:40.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:40.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:41.718: INFO: Number of nodes with available pods: 0
Jun 22 13:58:41.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:42.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:42.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:43.718: INFO: Number of nodes with available pods: 0
Jun 22 13:58:43.718: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:44.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:44.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:45.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:45.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:46.720: INFO: Number of nodes with available pods: 0
Jun 22 13:58:46.720: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:47.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:47.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:48.718: INFO: Number of nodes with available pods: 0
Jun 22 13:58:48.718: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:49.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:49.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:50.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:50.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:51.718: INFO: Number of nodes with available pods: 0
Jun 22 13:58:51.718: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:52.719: INFO: Number of nodes with available pods: 0
Jun 22 13:58:52.719: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:53.720: INFO: Number of nodes with available pods: 0
Jun 22 13:58:53.720: INFO: Node 06998c1b-9fed-44d7-827f-f702404ff383 is running more than one daemon pod
Jun 22 13:58:54.719: INFO: Number of nodes with available pods: 1
Jun 22 13:58:54.719: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4452, will wait for the garbage collector to delete the pods
Jun 22 13:58:54.784: INFO: Deleting DaemonSet.extensions daemon-set took: 5.791576ms
Jun 22 13:58:55.384: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.187218ms
Jun 22 13:58:58.287: INFO: Number of nodes with available pods: 0
Jun 22 13:58:58.288: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 13:58:58.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4452/daemonsets","resourceVersion":"185109"},"items":null}

Jun 22 13:58:58.293: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4452/pods","resourceVersion":"185109"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:58:58.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4452" for this suite.

• [SLOW TEST:28.871 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":294,"skipped":5039,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:58:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 22 13:58:58.467: INFO: Creating ReplicaSet my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf
Jun 22 13:58:58.477: INFO: Pod name my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf: Found 0 pods out of 1
Jun 22 13:59:03.482: INFO: Pod name my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf: Found 1 pods out of 1
Jun 22 13:59:03.482: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf" is running
Jun 22 13:59:17.489: INFO: Pod "my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf-bpxrt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 13:58:58 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 13:58:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 13:58:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-22 13:58:58 +0000 UTC Reason: Message:}])
Jun 22 13:59:17.490: INFO: Trying to dial the pod
Jun 22 13:59:22.505: INFO: Controller my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf: Got expected result from replica 1 [my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf-bpxrt]: "my-hostname-basic-c49eebff-1041-45c5-8374-6bf5f494f1cf-bpxrt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:59:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5108" for this suite.

• [SLOW TEST:24.189 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":295,"skipped":5074,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:59:22.520: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 13:59:22.666: INFO: Waiting up to 5m0s for pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df" in namespace "emptydir-7301" to be "Succeeded or Failed"
Jun 22 13:59:22.676: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.152878ms
Jun 22 13:59:24.681: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014519966s
Jun 22 13:59:26.685: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019465085s
Jun 22 13:59:28.690: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023847086s
Jun 22 13:59:30.694: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027768368s
Jun 22 13:59:32.698: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.032375991s
STEP: Saw pod success
Jun 22 13:59:32.698: INFO: Pod "pod-1612ceb0-21c5-4333-bffe-29c28571e7df" satisfied condition "Succeeded or Failed"
Jun 22 13:59:32.701: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-1612ceb0-21c5-4333-bffe-29c28571e7df container test-container: <nil>
STEP: delete the pod
Jun 22 13:59:32.726: INFO: Waiting for pod pod-1612ceb0-21c5-4333-bffe-29c28571e7df to disappear
Jun 22 13:59:32.729: INFO: Pod pod-1612ceb0-21c5-4333-bffe-29c28571e7df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:59:32.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7301" for this suite.

• [SLOW TEST:10.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":5093,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:59:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 13:59:45.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-704" for this suite.

• [SLOW TEST:13.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":297,"skipped":5099,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 13:59:45.987: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4766
STEP: creating service affinity-nodeport-transition in namespace services-4766
STEP: creating replication controller affinity-nodeport-transition in namespace services-4766
I0622 13:59:46.151509      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4766, replica count: 3
I0622 13:59:49.201788      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:59:52.201963      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:59:55.202118      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 13:59:58.202264      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:01.202437      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:04.202634      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:07.202922      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:10.203142      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:13.203295      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:16.203455      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:19.203610      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:22.203772      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 14:00:25.203968      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 14:00:25.215: INFO: Creating new exec pod
Jun 22 14:00:30.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun 22 14:00:30.429: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 22 14:00:30.430: INFO: stdout: ""
Jun 22 14:00:30.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 10.100.206.186 80'
Jun 22 14:00:30.602: INFO: stderr: "+ nc -zv -t -w 2 10.100.206.186 80\nConnection to 10.100.206.186 80 port [tcp/http] succeeded!\n"
Jun 22 14:00:30.602: INFO: stdout: ""
Jun 22 14:00:30.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 31138'
Jun 22 14:00:30.779: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 31138\nConnection to 11.0.1.3 31138 port [tcp/31138] succeeded!\n"
Jun 22 14:00:30.779: INFO: stdout: ""
Jun 22 14:00:30.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 31138'
Jun 22 14:00:30.940: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 31138\nConnection to 11.0.1.4 31138 port [tcp/31138] succeeded!\n"
Jun 22 14:00:30.940: INFO: stdout: ""
Jun 22 14:00:30.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.3 31138'
Jun 22 14:00:31.112: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.3 31138\nConnection to 11.0.1.3 31138 port [tcp/31138] succeeded!\n"
Jun 22 14:00:31.112: INFO: stdout: ""
Jun 22 14:00:31.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c nc -zv -t -w 2 11.0.1.4 31138'
Jun 22 14:00:31.278: INFO: stderr: "+ nc -zv -t -w 2 11.0.1.4 31138\nConnection to 11.0.1.4 31138 port [tcp/31138] succeeded!\n"
Jun 22 14:00:31.278: INFO: stdout: ""
Jun 22 14:00:31.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.1.4:31138/ ; done'
Jun 22 14:00:31.568: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n"
Jun 22 14:00:31.568: INFO: stdout: "\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-sz7bc\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-7f4px\naffinity-nodeport-transition-sz7bc\naffinity-nodeport-transition-7f4px\naffinity-nodeport-transition-7f4px\naffinity-nodeport-transition-7f4px\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-sz7bc\naffinity-nodeport-transition-sz7bc\naffinity-nodeport-transition-7f4px\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-sz7bc"
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-sz7bc
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-7f4px
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-sz7bc
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-7f4px
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-7f4px
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-7f4px
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-sz7bc
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-sz7bc
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-7f4px
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.568: INFO: Received response from host: affinity-nodeport-transition-sz7bc
Jun 22 14:00:31.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-240813907 --namespace=services-4766 exec execpod-affinityfczff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.1.4:31138/ ; done'
Jun 22 14:00:31.836: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.1.4:31138/\n"
Jun 22 14:00:31.836: INFO: stdout: "\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7\naffinity-nodeport-transition-z6wt7"
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Received response from host: affinity-nodeport-transition-z6wt7
Jun 22 14:00:31.836: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4766, will wait for the garbage collector to delete the pods
Jun 22 14:00:31.916: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.892877ms
Jun 22 14:00:32.516: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.123258ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 14:00:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4766" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:58.654 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":298,"skipped":5112,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 14:00:44.641: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-4cb7c025-69da-4afa-b137-f1597ad6778b in namespace container-probe-6359
Jun 22 14:00:54.797: INFO: Started pod test-webserver-4cb7c025-69da-4afa-b137-f1597ad6778b in namespace container-probe-6359
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 14:00:54.799: INFO: Initial restart count of pod test-webserver-4cb7c025-69da-4afa-b137-f1597ad6778b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 14:04:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6359" for this suite.

• [SLOW TEST:250.682 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":5114,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 14:04:55.325: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-2f663400-5b56-4baa-824b-0ad640e69355
STEP: Creating a pod to test consume configMaps
Jun 22 14:04:55.471: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd" in namespace "projected-9739" to be "Succeeded or Failed"
Jun 22 14:04:55.489: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.785586ms
Jun 22 14:04:57.493: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02236108s
Jun 22 14:04:59.497: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025929579s
Jun 22 14:05:01.504: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033203228s
Jun 22 14:05:03.509: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0383747s
Jun 22 14:05:05.514: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.042889072s
STEP: Saw pod success
Jun 22 14:05:05.514: INFO: Pod "pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd" satisfied condition "Succeeded or Failed"
Jun 22 14:05:05.517: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 14:05:05.541: INFO: Waiting for pod pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd to disappear
Jun 22 14:05:05.545: INFO: Pod pod-projected-configmaps-4c29eb28-ba4a-4a97-b18f-d4e040657ddd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 14:05:05.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9739" for this suite.

• [SLOW TEST:10.233 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":5140,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 14:05:05.561: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-19d37e87-eb81-46a5-9209-76cc3de12462
STEP: Creating a pod to test consume configMaps
Jun 22 14:05:05.717: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf" in namespace "projected-366" to be "Succeeded or Failed"
Jun 22 14:05:05.727: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.784068ms
Jun 22 14:05:07.732: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015405602s
Jun 22 14:05:09.738: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021272219s
Jun 22 14:05:11.743: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026477397s
Jun 22 14:05:13.747: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030392967s
Jun 22 14:05:15.751: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.033795113s
STEP: Saw pod success
Jun 22 14:05:15.751: INFO: Pod "pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf" satisfied condition "Succeeded or Failed"
Jun 22 14:05:15.754: INFO: Trying to get logs from node 06998c1b-9fed-44d7-827f-f702404ff383 pod pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 14:05:15.773: INFO: Waiting for pod pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf to disappear
Jun 22 14:05:15.776: INFO: Pod pod-projected-configmaps-7051cd4b-099f-40f1-9fb4-31535d707baf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 14:05:15.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-366" for this suite.

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":301,"skipped":5165,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 22 14:05:15.788: INFO: >>> kubeConfig: /tmp/kubeconfig-240813907
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jun 22 14:05:16.446: INFO: created pod pod-service-account-defaultsa
Jun 22 14:05:16.446: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 14:05:16.459: INFO: created pod pod-service-account-mountsa
Jun 22 14:05:16.459: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 14:05:16.471: INFO: created pod pod-service-account-nomountsa
Jun 22 14:05:16.471: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 14:05:16.478: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 14:05:16.478: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 14:05:16.488: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 14:05:16.488: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 14:05:16.498: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 14:05:16.498: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 14:05:16.509: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 14:05:16.510: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 14:05:16.518: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 14:05:16.518: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 14:05:16.529: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 14:05:16.529: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 22 14:05:16.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9929" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":302,"skipped":5175,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSJun 22 14:05:16.557: INFO: Running AfterSuite actions on all nodes
Jun 22 14:05:16.557: INFO: Running AfterSuite actions on node 1
Jun 22 14:05:16.558: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":302,"skipped":5179,"failed":3,"failures":["[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}


Summarizing 3 Failures:

[Fail] [sig-network] Services [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241

[Fail] [sig-network] Services [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241

[Fail] [sig-network] Services [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:241

Ran 305 of 5484 Specs in 7686.877 seconds
FAIL! -- 302 Passed | 3 Failed | 0 Pending | 5179 Skipped
--- FAIL: TestE2E (7686.93s)
FAIL

Ginkgo ran 1 suite in 2h8m8.409053892s
Test Suite Failed
