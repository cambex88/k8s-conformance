I0712 06:24:15.325254      24 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-439874929
I0712 06:24:15.325269      24 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0712 06:24:15.325352      24 e2e.go:129] Starting e2e run "b811cf48-c30b-4b3b-a060-cb9b76a73b8a" on Ginkgo node 1
{"msg":"Test Suite starting","total":303,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626071054 - Will randomize all specs
Will run 303 of 5234 specs

Jul 12 06:24:15.427: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 06:24:15.431: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 12 06:24:15.461: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 12 06:24:15.554: INFO: 66 / 66 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 12 06:24:15.554: INFO: expected 55 pod replicas in namespace 'kube-system', 55 are Running and Ready.
Jul 12 06:24:15.554: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 12 06:24:15.576: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 12 06:24:15.576: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cube-logstash' (0 seconds elapsed)
Jul 12 06:24:15.576: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gpumonopoly-device-plugin-daemonset' (0 seconds elapsed)
Jul 12 06:24:15.576: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gpushare-device-plugin-daemonset' (0 seconds elapsed)
Jul 12 06:24:15.576: INFO: e2e test version: v1.19.4
Jul 12 06:24:15.578: INFO: kube-apiserver version: v1.19.4-20210401+5f6a22c5cc915c14a0ec51f07cf1bcb0d86dd53e
Jul 12 06:24:15.578: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 06:24:15.584: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:24:15.584: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
Jul 12 06:24:15.952: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:24:15.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0" in namespace "projected-6734" to be "Succeeded or Failed"
Jul 12 06:24:15.986: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638529ms
Jul 12 06:24:17.990: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00671687s
Jul 12 06:24:19.994: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010299294s
Jul 12 06:24:21.998: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014213667s
Jul 12 06:24:24.002: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018219644s
Jul 12 06:24:27.147: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163788829s
Jul 12 06:24:29.151: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.167145251s
STEP: Saw pod success
Jul 12 06:24:29.151: INFO: Pod "downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0" satisfied condition "Succeeded or Failed"
Jul 12 06:24:29.177: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0 container client-container: <nil>
STEP: delete the pod
Jul 12 06:24:29.382: INFO: Waiting for pod downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0 to disappear
Jul 12 06:24:29.385: INFO: Pod downwardapi-volume-889d4bed-84aa-4463-8c58-82334b5143b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:24:29.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6734" for this suite.

• [SLOW TEST:13.849 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":1,"skipped":16,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:24:29.433: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-96b9a531-f1e2-4282-b42f-e9a216baf6a5
STEP: Creating a pod to test consume configMaps
Jul 12 06:24:29.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e" in namespace "projected-4250" to be "Succeeded or Failed"
Jul 12 06:24:29.905: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832616ms
Jul 12 06:24:34.502: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.601994782s
Jul 12 06:24:36.713: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.812950839s
Jul 12 06:24:38.724: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.823963524s
Jul 12 06:24:40.728: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.827431762s
Jul 12 06:24:47.648: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.748234603s
Jul 12 06:24:49.653: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.752719614s
Jul 12 06:24:51.657: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.756537095s
STEP: Saw pod success
Jul 12 06:24:51.657: INFO: Pod "pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e" satisfied condition "Succeeded or Failed"
Jul 12 06:24:51.660: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:24:51.785: INFO: Waiting for pod pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e to disappear
Jul 12 06:24:51.788: INFO: Pod pod-projected-configmaps-6b82a701-b139-4828-9e47-c64564742e9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:24:51.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4250" for this suite.

• [SLOW TEST:22.375 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":2,"skipped":46,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:24:51.809: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-f766fc02-6959-40e1-a317-157e013ecfc0 in namespace container-probe-8888
Jul 12 06:25:06.013: INFO: Started pod liveness-f766fc02-6959-40e1-a317-157e013ecfc0 in namespace container-probe-8888
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 06:25:06.016: INFO: Initial restart count of pod liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is 0
Jul 12 06:25:22.711: INFO: Restart count of pod container-probe-8888/liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is now 1 (16.694537711s elapsed)
Jul 12 06:25:45.531: INFO: Restart count of pod container-probe-8888/liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is now 2 (39.514872015s elapsed)
Jul 12 06:26:01.563: INFO: Restart count of pod container-probe-8888/liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is now 3 (55.54692823s elapsed)
Jul 12 06:26:22.282: INFO: Restart count of pod container-probe-8888/liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is now 4 (1m16.265688194s elapsed)
Jul 12 06:27:30.190: INFO: Restart count of pod container-probe-8888/liveness-f766fc02-6959-40e1-a317-157e013ecfc0 is now 5 (2m24.17431803s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:27:30.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8888" for this suite.

• [SLOW TEST:158.531 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":303,"completed":3,"skipped":51,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:27:30.341: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 12 06:29:32.120: INFO: Successfully updated pod "var-expansion-f0420fc9-3d17-4162-991b-8cab1e893dab"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 12 06:29:48.163: INFO: Deleting pod "var-expansion-f0420fc9-3d17-4162-991b-8cab1e893dab" in namespace "var-expansion-5387"
Jul 12 06:29:48.215: INFO: Wait up to 5m0s for pod "var-expansion-f0420fc9-3d17-4162-991b-8cab1e893dab" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:30:24.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5387" for this suite.

• [SLOW TEST:173.901 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":303,"completed":4,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:30:24.242: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jul 12 06:30:24.346: INFO: Waiting up to 5m0s for pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580" in namespace "containers-8208" to be "Succeeded or Failed"
Jul 12 06:30:24.348: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214572ms
Jul 12 06:30:26.352: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005794214s
Jul 12 06:30:28.356: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009736195s
Jul 12 06:30:30.359: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013084245s
Jul 12 06:30:32.364: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01770177s
Jul 12 06:30:34.367: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02101778s
Jul 12 06:30:36.370: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.024178379s
STEP: Saw pod success
Jul 12 06:30:36.370: INFO: Pod "client-containers-4eeba818-2201-455d-817d-aa82a62da580" satisfied condition "Succeeded or Failed"
Jul 12 06:30:36.377: INFO: Trying to get logs from node 10.32.0.100 pod client-containers-4eeba818-2201-455d-817d-aa82a62da580 container test-container: <nil>
STEP: delete the pod
Jul 12 06:30:36.435: INFO: Waiting for pod client-containers-4eeba818-2201-455d-817d-aa82a62da580 to disappear
Jul 12 06:30:36.762: INFO: Pod client-containers-4eeba818-2201-455d-817d-aa82a62da580 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:30:36.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8208" for this suite.

• [SLOW TEST:12.629 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":303,"completed":5,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:30:36.872: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jul 12 06:30:37.038: INFO: Waiting up to 5m0s for pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1" in namespace "containers-5820" to be "Succeeded or Failed"
Jul 12 06:30:37.040: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349895ms
Jul 12 06:30:39.044: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00650716s
Jul 12 06:30:41.059: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021820039s
Jul 12 06:30:43.102: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064041168s
Jul 12 06:30:45.105: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067032874s
Jul 12 06:30:47.108: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070781247s
Jul 12 06:30:49.112: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.074150718s
STEP: Saw pod success
Jul 12 06:30:49.112: INFO: Pod "client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1" satisfied condition "Succeeded or Failed"
Jul 12 06:30:49.117: INFO: Trying to get logs from node 10.32.0.100 pod client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1 container test-container: <nil>
STEP: delete the pod
Jul 12 06:30:49.195: INFO: Waiting for pod client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1 to disappear
Jul 12 06:30:49.197: INFO: Pod client-containers-c532b891-6cb8-490b-9dd9-c94c286507a1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:30:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5820" for this suite.

• [SLOW TEST:12.339 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":303,"completed":6,"skipped":109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:30:49.212: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 12 06:30:49.712: INFO: Waiting up to 5m0s for pod "pod-8041af53-f66f-4a41-a847-7d010e328be2" in namespace "emptydir-5906" to be "Succeeded or Failed"
Jul 12 06:30:49.714: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041004ms
Jul 12 06:30:51.717: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004863377s
Jul 12 06:30:53.729: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01774096s
Jul 12 06:30:55.733: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020942442s
Jul 12 06:30:57.738: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026750179s
Jul 12 06:31:00.086: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.374766103s
Jul 12 06:31:02.114: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.402122416s
STEP: Saw pod success
Jul 12 06:31:02.114: INFO: Pod "pod-8041af53-f66f-4a41-a847-7d010e328be2" satisfied condition "Succeeded or Failed"
Jul 12 06:31:02.116: INFO: Trying to get logs from node 10.32.0.100 pod pod-8041af53-f66f-4a41-a847-7d010e328be2 container test-container: <nil>
STEP: delete the pod
Jul 12 06:31:02.805: INFO: Waiting for pod pod-8041af53-f66f-4a41-a847-7d010e328be2 to disappear
Jul 12 06:31:02.810: INFO: Pod pod-8041af53-f66f-4a41-a847-7d010e328be2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:31:02.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5906" for this suite.

• [SLOW TEST:13.618 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":7,"skipped":160,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:31:02.830: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9672
STEP: creating service affinity-nodeport in namespace services-9672
STEP: creating replication controller affinity-nodeport in namespace services-9672
I0712 06:31:04.214198      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9672, replica count: 3
I0712 06:31:07.265147      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:31:10.265446      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:31:13.265707      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:31:16.265948      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:31:19.266128      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 06:31:19.284: INFO: Creating new exec pod
Jul 12 06:31:30.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-9672 execpod-affinitywk9jd -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jul 12 06:31:33.600: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 12 06:31:33.600: INFO: stdout: ""
Jul 12 06:31:33.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-9672 execpod-affinitywk9jd -- /bin/sh -x -c nc -zv -t -w 2 10.254.101.131 80'
Jul 12 06:31:33.847: INFO: stderr: "+ nc -zv -t -w 2 10.254.101.131 80\nConnection to 10.254.101.131 80 port [tcp/http] succeeded!\n"
Jul 12 06:31:33.847: INFO: stdout: ""
Jul 12 06:31:33.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-9672 execpod-affinitywk9jd -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.102 30851'
Jul 12 06:31:34.092: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.102 30851\nConnection to 10.32.0.102 30851 port [tcp/30851] succeeded!\n"
Jul 12 06:31:34.092: INFO: stdout: ""
Jul 12 06:31:34.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-9672 execpod-affinitywk9jd -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.100 30851'
Jul 12 06:31:34.346: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.100 30851\nConnection to 10.32.0.100 30851 port [tcp/30851] succeeded!\n"
Jul 12 06:31:34.346: INFO: stdout: ""
Jul 12 06:31:34.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-9672 execpod-affinitywk9jd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.100:30851/ ; done'
Jul 12 06:31:34.817: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30851/\n"
Jul 12 06:31:34.817: INFO: stdout: "\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm\naffinity-nodeport-s4cnm"
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Received response from host: affinity-nodeport-s4cnm
Jul 12 06:31:34.817: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9672, will wait for the garbage collector to delete the pods
Jul 12 06:31:35.109: INFO: Deleting ReplicationController affinity-nodeport took: 159.052015ms
Jul 12 06:31:35.309: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.219045ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:31:49.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9672" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:46.848 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":8,"skipped":162,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:31:49.678: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-688/configmap-test-062014a1-59a7-4615-93ca-926c2387659f
STEP: Creating a pod to test consume configMaps
Jul 12 06:31:49.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8" in namespace "configmap-688" to be "Succeeded or Failed"
Jul 12 06:31:49.980: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869443ms
Jul 12 06:31:51.983: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004913451s
Jul 12 06:31:53.988: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009865322s
Jul 12 06:31:55.992: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013618264s
Jul 12 06:31:57.995: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016938759s
Jul 12 06:32:00.012: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033719113s
Jul 12 06:32:02.060: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.081694489s
STEP: Saw pod success
Jul 12 06:32:02.060: INFO: Pod "pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8" satisfied condition "Succeeded or Failed"
Jul 12 06:32:02.138: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8 container env-test: <nil>
STEP: delete the pod
Jul 12 06:32:02.197: INFO: Waiting for pod pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8 to disappear
Jul 12 06:32:02.254: INFO: Pod pod-configmaps-c6b6a29b-95b9-4490-bf87-e76912ec63f8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:32:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-688" for this suite.

• [SLOW TEST:12.626 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":303,"completed":9,"skipped":170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:32:02.307: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0712 06:32:04.816738      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 06:32:04.816766      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 06:32:04.816777      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 06:32:04.816: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:32:04.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7437" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":303,"completed":10,"skipped":263,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:32:04.847: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-1325/configmap-test-bf493071-a4d2-4cd0-b475-5653c865bb2c
STEP: Creating a pod to test consume configMaps
Jul 12 06:32:05.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41" in namespace "configmap-1325" to be "Succeeded or Failed"
Jul 12 06:32:05.210: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204209ms
Jul 12 06:32:08.072: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866459687s
Jul 12 06:32:10.093: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887399067s
Jul 12 06:32:12.125: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919726418s
Jul 12 06:32:14.381: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Pending", Reason="", readiness=false. Elapsed: 9.175901751s
Jul 12 06:32:16.384: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.178977018s
STEP: Saw pod success
Jul 12 06:32:16.384: INFO: Pod "pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41" satisfied condition "Succeeded or Failed"
Jul 12 06:32:16.409: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41 container env-test: <nil>
STEP: delete the pod
Jul 12 06:32:16.539: INFO: Waiting for pod pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41 to disappear
Jul 12 06:32:16.549: INFO: Pod pod-configmaps-59e35ee1-5f42-4d25-aec2-236fa9138d41 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:32:16.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1325" for this suite.

• [SLOW TEST:11.716 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":11,"skipped":277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:32:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-788f82ee-da81-4ed9-be2f-add5c26f301b in namespace container-probe-8672
Jul 12 06:32:29.872: INFO: Started pod liveness-788f82ee-da81-4ed9-be2f-add5c26f301b in namespace container-probe-8672
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 06:32:29.875: INFO: Initial restart count of pod liveness-788f82ee-da81-4ed9-be2f-add5c26f301b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:36:30.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8672" for this suite.

• [SLOW TEST:253.750 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":303,"completed":12,"skipped":321,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:36:30.314: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:36:30.715: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:36:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4884" for this suite.

• [SLOW TEST:12.660 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":303,"completed":13,"skipped":335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:36:42.974: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jul 12 06:36:43.228: INFO: created test-event-1
Jul 12 06:36:43.249: INFO: created test-event-2
Jul 12 06:36:43.305: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 12 06:36:43.308: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 12 06:36:43.508: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:36:43.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3587" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":303,"completed":14,"skipped":364,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:36:43.549: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:36:43.635: INFO: Creating deployment "test-recreate-deployment"
Jul 12 06:36:43.683: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 12 06:36:43.732: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 12 06:36:45.739: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 12 06:36:45.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:36:47.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:36:49.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:36:51.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:36:53.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668603, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:36:55.757: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 12 06:36:55.793: INFO: Updating deployment test-recreate-deployment
Jul 12 06:36:55.793: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 12 06:36:56.838: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3720 /apis/apps/v1/namespaces/deployment-3720/deployments/test-recreate-deployment 8c0ae939-b961-471a-a3ce-4914e7264c85 2797080 2 2021-07-12 06:36:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 06:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 06:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004449df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-12 06:36:56 +0000 UTC,LastTransitionTime:2021-07-12 06:36:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-07-12 06:36:56 +0000 UTC,LastTransitionTime:2021-07-12 06:36:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 12 06:36:57.075: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3720 /apis/apps/v1/namespaces/deployment-3720/replicasets/test-recreate-deployment-f79dd4667 fb4299fd-de2b-468c-b8d7-a9837dea204e 2797077 1 2021-07-12 06:36:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8c0ae939-b961-471a-a3ce-4914e7264c85 0xc002d69a10 0xc002d69a11}] []  [{kube-controller-manager Update apps/v1 2021-07-12 06:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c0ae939-b961-471a-a3ce-4914e7264c85\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d69a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 06:36:57.075: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 12 06:36:57.075: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-3720 /apis/apps/v1/namespaces/deployment-3720/replicasets/test-recreate-deployment-c96cf48f ad1efa17-efcc-42eb-bc5a-ec36149a45e1 2797066 2 2021-07-12 06:36:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8c0ae939-b961-471a-a3ce-4914e7264c85 0xc002d6991f 0xc002d69930}] []  [{kube-controller-manager Update apps/v1 2021-07-12 06:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c0ae939-b961-471a-a3ce-4914e7264c85\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d699a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 06:36:57.094: INFO: Pod "test-recreate-deployment-f79dd4667-kngzh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-kngzh test-recreate-deployment-f79dd4667- deployment-3720 /api/v1/namespaces/deployment-3720/pods/test-recreate-deployment-f79dd4667-kngzh faffec3a-9c9b-416f-a316-c15bdb2adb74 2797071 0 2021-07-12 06:36:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 fb4299fd-de2b-468c-b8d7-a9837dea204e 0xc002d69f60 0xc002d69f61}] []  [{kube-controller-manager Update v1 2021-07-12 06:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4299fd-de2b-468c-b8d7-a9837dea204e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6kn7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6kn7f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6kn7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 06:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:36:57.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3720" for this suite.

• [SLOW TEST:14.086 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":15,"skipped":368,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:36:57.636: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 06:36:58.144: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 06:36:58.185: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 06:36:58.188: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.100 before test
Jul 12 06:36:58.206: INFO: csi-rbdplugin-8rsp7 from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.206: INFO: test-recreate-deployment-f79dd4667-kngzh from deployment-3720 started at 2021-07-12 06:36:56 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container httpd ready: false, restart count 0
Jul 12 06:36:58.206: INFO: calico-node-mrt5l from kube-system started at 2021-07-12 01:23:30 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 06:36:58.206: INFO: coredns-7699c68bdc-nrw9s from kube-system started at 2021-07-12 06:02:49 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container coredns ready: true, restart count 0
Jul 12 06:36:58.206: INFO: cube-logstash-sztvc from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-core-8448fcd4dd-hsvz2 from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container core ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-exporter-5d8b88579-7bpfn from kube-system started at 2021-07-12 06:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container exporter ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-jobservice-5c7d4566ff-lp4hp from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-nginx-86f8877965-s48pm from kube-system started at 2021-07-12 06:03:01 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container nginx ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-portal-76856b56c7-p6zqk from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container portal ready: true, restart count 0
Jul 12 06:36:58.206: INFO: harbor-app-harbor-registry-6bc47dd67f-rcrrl from kube-system started at 2021-07-12 06:02:49 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container registry ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 06:36:58.206: INFO: stolon-app-keeper-0 from kube-system started at 2021-07-12 06:02:21 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.206: INFO: stolon-app-proxy-59967b544f-8cssc from kube-system started at 2021-07-12 06:02:40 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.206: INFO: stolon-app-sentinel-5fc7bf8cf8-sfjnr from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.206: INFO: alertmanager-main-2 from monitoring started at 2021-07-12 06:02:32 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 06:36:58.206: INFO: cubenode-exporter-p2bg8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 06:36:58.206: INFO: node-exporter-nvbt2 from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 06:36:58.206: INFO: pod-exec-websocket-75c94814-9a01-4f29-9470-40f5f65cbdf8 from pods-4884 started at 2021-07-12 06:36:30 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container main ready: true, restart count 0
Jul 12 06:36:58.206: INFO: sonobuoy from sonobuoy started at 2021-07-12 06:23:52 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 06:36:58.206: INFO: sonobuoy-e2e-job-c08e481580ac4ebf from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container e2e ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 06:36:58.206: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 06:36:58.206: INFO: 	Container systemd-logs ready: false, restart count 6
Jul 12 06:36:58.206: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.102 before test
Jul 12 06:36:58.228: INFO: captain-controller-manager-56dd98c4dd-cbspp from captain-system started at 2021-07-09 02:45:32 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container manager ready: true, restart count 6
Jul 12 06:36:58.228: INFO: csi-rbdplugin-46rrc from ceph-csi started at 2021-07-09 02:46:27 +0000 UTC (3 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.228: INFO: csi-rbdplugin-provisioner-74d4496bf7-22rlt from ceph-csi started at 2021-07-12 06:00:59 +0000 UTC (6 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.228: INFO: csi-rbdplugin-provisioner-74d4496bf7-xzzbm from ceph-csi started at 2021-07-09 02:45:32 +0000 UTC (6 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 06:36:58.228: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 06:36:58.228: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.228: INFO: alpine-595b7c4b74-xvrn8 from default started at 2021-07-10 02:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container alpine ready: true, restart count 0
Jul 12 06:36:58.228: INFO: dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde from dns-1249 started at 2021-07-09 07:18:28 +0000 UTC (3 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container jessie-querier ready: true, restart count 373
Jul 12 06:36:58.228: INFO: 	Container querier ready: true, restart count 391
Jul 12 06:36:58.228: INFO: 	Container webserver ready: true, restart count 0
Jul 12 06:36:58.228: INFO: pod-047c84e7-4461-47fd-ad72-3e49a5479adb from emptydir-6449 started at 2021-07-12 02:54:00 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container test-container ready: false, restart count 0
Jul 12 06:36:58.228: INFO: ipsection-controllers-6d8496c6bd-vd86c from ipsection-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container ipsection-controllers ready: true, restart count 0
Jul 12 06:36:58.228: INFO: calico-kube-controllers-668c8b44f6-vfp6k from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 06:36:58.228: INFO: calico-node-x7txv from kube-system started at 2021-07-12 01:23:01 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 06:36:58.228: INFO: coredns-7699c68bdc-bzzjr from kube-system started at 2021-07-10 06:56:57 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container coredns ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-kong-b86d5bf9c-pkln4 from kube-system started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-logging-58f8c874b8-964j2 from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-logstash-xbhqg from kube-system started at 2021-07-09 02:46:27 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-networking-68749499f7-7vqt4 from kube-system started at 2021-07-12 06:01:00 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-node-65789b66c-tglb7 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-openapi-v1-658dd7f87b-nhzqv from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-ops-webapp-85c8594b7b-krvgz from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-storage-ceph-access-5f4db88cdd-hh7xq from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-storage-ceph-access ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-storage-f6786ff45-6q6b2 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-ticket-7955cf677f-scx9z from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cube-webapp-57c694b676-pscrq from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 06:36:58.228: INFO: gpushare-schd-extender-9b5766d8c-xx5n2 from kube-system started at 2021-07-12 06:00:52 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container gpushare-schd-extender ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-core-8448fcd4dd-cvk8h from kube-system started at 2021-07-09 02:46:56 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container core ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-exporter-5d8b88579-jsq2l from kube-system started at 2021-07-09 02:46:40 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-jobservice-5c7d4566ff-6brx2 from kube-system started at 2021-07-09 02:46:36 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-nginx-86f8877965-8qcgn from kube-system started at 2021-07-09 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container nginx ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-portal-76856b56c7-b2p7w from kube-system started at 2021-07-09 02:46:30 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container portal ready: true, restart count 0
Jul 12 06:36:58.228: INFO: harbor-app-harbor-registry-6bc47dd67f-dwjmv from kube-system started at 2021-07-09 02:46:39 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container registry ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 06:36:58.228: INFO: kubernetes-dashboard-749844f89d-xflsb from kube-system started at 2021-07-12 06:00:56 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 12 06:36:58.228: INFO: local-path-provisioner-67d77c895b-ptflh from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jul 12 06:36:58.228: INFO: stolon-app-keeper-1 from kube-system started at 2021-07-09 02:46:17 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.228: INFO: stolon-app-proxy-59967b544f-t57n4 from kube-system started at 2021-07-09 02:46:19 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.228: INFO: stolon-app-sentinel-5fc7bf8cf8-5fkrt from kube-system started at 2021-07-09 02:46:06 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.228: INFO: alertmanager-main-1 from monitoring started at 2021-07-09 02:46:01 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 06:36:58.228: INFO: cubenode-exporter-dnhsc from monitoring started at 2021-07-09 02:46:21 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: mysqld-exporter-1-5d6b99c586-bsghg from monitoring started at 2021-07-12 06:01:01 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: mysqld-exporter-2-5f45bbf595-stzdj from monitoring started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: node-exporter-dgd55 from monitoring started at 2021-07-08 11:02:46 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: prometheus-k8s-0 from monitoring started at 2021-07-09 02:46:08 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container prometheus ready: true, restart count 1
Jul 12 06:36:58.228: INFO: prometheus-operator-6786cb4fc5-t8456 from monitoring started at 2021-07-09 02:45:33 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 12 06:36:58.228: INFO: redis-exporter-1-5877697f69-86xfb from monitoring started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 06:36:58.228: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 06:36:58.228: INFO: 	Container systemd-logs ready: false, restart count 6
Jul 12 06:36:58.228: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.3 before test
Jul 12 06:36:58.256: INFO: csi-rbdplugin-provisioner-74d4496bf7-ch8g6 from ceph-csi started at 2021-07-08 11:06:48 +0000 UTC (6 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container csi-attacher ready: true, restart count 4
Jul 12 06:36:58.256: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 06:36:58.256: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container csi-resizer ready: true, restart count 6
Jul 12 06:36:58.256: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 06:36:58.256: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.256: INFO: csi-rbdplugin-zcqsm from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 06:36:58.256: INFO: calico-kube-controllers-668c8b44f6-7vr9t from kube-system started at 2021-07-08 09:44:33 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 06:36:58.256: INFO: calico-kube-controllers-668c8b44f6-vcfkc from kube-system started at 2021-07-08 09:44:32 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 06:36:58.256: INFO: calico-node-zmmxk from kube-system started at 2021-07-12 01:24:00 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 06:36:58.256: INFO: coredns-7699c68bdc-klbx8 from kube-system started at 2021-07-10 06:57:49 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container coredns ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-appstore-0 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-appstore ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-ha-rabbitmq-0 from kube-system started at 2021-07-08 11:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container rabbitmq ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-kong-b86d5bf9c-4vtvh from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-logging-58f8c874b8-mjz4v from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-logstash-wfm47 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-networking-68749499f7-rt2dc from kube-system started at 2021-07-08 11:02:00 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-node-65789b66c-6tmls from kube-system started at 2021-07-09 02:45:30 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-openapi-v1-658dd7f87b-psc7x from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-ops-webapp-85c8594b7b-b544h from kube-system started at 2021-07-09 02:45:31 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-storage-f6786ff45-7649l from kube-system started at 2021-07-08 11:06:26 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-ticket-7955cf677f-jvn8v from kube-system started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cube-webapp-57c694b676-9qpnc from kube-system started at 2021-07-08 11:02:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-core-8448fcd4dd-h5fdq from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container core ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-exporter-5d8b88579-9wf79 from kube-system started at 2021-07-08 10:36:46 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container exporter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-jobservice-5c7d4566ff-fsx5c from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-nginx-86f8877965-4rcpv from kube-system started at 2021-07-08 10:36:45 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container nginx ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-portal-76856b56c7-8nczp from kube-system started at 2021-07-08 10:36:44 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container portal ready: true, restart count 0
Jul 12 06:36:58.256: INFO: harbor-app-harbor-registry-6bc47dd67f-7kjrn from kube-system started at 2021-07-08 10:36:43 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container registry ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 06:36:58.256: INFO: kube-mini-chartmuseum-55d66cd548-2b8x2 from kube-system started at 2021-07-08 09:45:51 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container kube-mini-chartmuseum ready: true, restart count 0
Jul 12 06:36:58.256: INFO: logdir-admission-webhook-deployment-5b8587f876-dlgl5 from kube-system started at 2021-07-08 11:06:20 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container logdir-admission-webhook ready: true, restart count 0
Jul 12 06:36:58.256: INFO: metrics-server-86d56f4667-s8nqj from kube-system started at 2021-07-08 11:08:32 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 06:36:58.256: INFO: stolon-app-keeper-2 from kube-system started at 2021-07-08 10:34:49 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.256: INFO: stolon-app-proxy-59967b544f-ww9rx from kube-system started at 2021-07-08 10:33:36 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.256: INFO: stolon-app-sentinel-5fc7bf8cf8-grbwv from kube-system started at 2021-07-08 10:33:33 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container stolon ready: true, restart count 0
Jul 12 06:36:58.256: INFO: alertmanager-main-0 from monitoring started at 2021-07-08 11:02:21 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 06:36:58.256: INFO: cubenode-exporter-dvvm8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: grafana-5cd649d575-q2cdw from monitoring started at 2021-07-08 11:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container grafana ready: true, restart count 0
Jul 12 06:36:58.256: INFO: influxdb-exporter-1-7599cc4587-8f6tn from monitoring started at 2021-07-08 11:08:17 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container influxdb-exporter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: kube-state-metrics-5d5d4d46c-g5zl8 from monitoring started at 2021-07-08 11:02:42 +0000 UTC (3 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 12 06:36:58.256: INFO: node-exporter-d42br from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: prometheus-adapter-84fbb7d77b-4qbgb from monitoring started at 2021-07-08 11:02:56 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: redis-exporter-2-6d778f7cf8-6kvb5 from monitoring started at 2021-07-08 11:08:25 +0000 UTC (1 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 06:36:58.256: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 06:36:58.256: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 06:36:58.256: INFO: 	Container systemd-logs ready: false, restart count 6
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-86ae1343-99b5-4e83-a63e-0b1e237f9fe4 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-86ae1343-99b5-4e83-a63e-0b1e237f9fe4 off the node 10.32.0.100
STEP: verifying the node doesn't have the label kubernetes.io/e2e-86ae1343-99b5-4e83-a63e-0b1e237f9fe4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:37:47.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6590" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:49.604 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":303,"completed":16,"skipped":376,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:37:47.240: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:37:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-574" for this suite.

• [SLOW TEST:12.506 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":303,"completed":17,"skipped":376,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:37:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:37:59.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2" in namespace "downward-api-3578" to be "Succeeded or Failed"
Jul 12 06:37:59.869: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829373ms
Jul 12 06:38:01.872: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004447s
Jul 12 06:38:03.875: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007845584s
Jul 12 06:38:05.879: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011015992s
Jul 12 06:38:07.882: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014390771s
Jul 12 06:38:11.376: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.508061608s
Jul 12 06:38:13.379: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.511531037s
STEP: Saw pod success
Jul 12 06:38:13.379: INFO: Pod "downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2" satisfied condition "Succeeded or Failed"
Jul 12 06:38:13.383: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2 container client-container: <nil>
STEP: delete the pod
Jul 12 06:38:16.264: INFO: Waiting for pod downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2 to disappear
Jul 12 06:38:16.267: INFO: Pod downwardapi-volume-7307d6c4-ed86-4967-851c-ca771f7795e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:38:16.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3578" for this suite.

• [SLOW TEST:16.563 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":303,"completed":18,"skipped":391,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:38:16.310: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:38:32.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2029" for this suite.

• [SLOW TEST:16.546 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":303,"completed":19,"skipped":408,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:38:32.856: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-69b4b2f6-ebf6-44ea-9a47-46ccb8b5b613
STEP: Creating a pod to test consume configMaps
Jul 12 06:38:33.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96" in namespace "projected-4459" to be "Succeeded or Failed"
Jul 12 06:38:33.192: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Pending", Reason="", readiness=false. Elapsed: 5.98912ms
Jul 12 06:38:35.199: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012343396s
Jul 12 06:38:37.202: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015901786s
Jul 12 06:38:39.208: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022103945s
Jul 12 06:38:41.295: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108636312s
Jul 12 06:38:43.298: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112067667s
STEP: Saw pod success
Jul 12 06:38:43.298: INFO: Pod "pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96" satisfied condition "Succeeded or Failed"
Jul 12 06:38:43.306: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:38:43.387: INFO: Waiting for pod pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96 to disappear
Jul 12 06:38:43.392: INFO: Pod pod-projected-configmaps-b3e211e9-ca5e-4654-ad98-801b3e8a6c96 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:38:43.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4459" for this suite.

• [SLOW TEST:10.582 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":20,"skipped":423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:38:43.439: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 12 06:38:43.700: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 06:39:44.232: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 12 06:39:44.330: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 12 06:39:44.386: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul 12 06:39:45.913: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:40:12.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-200" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:89.456 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":303,"completed":21,"skipped":468,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:40:12.896: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 06:40:13.173: INFO: Number of nodes with available pods: 0
Jul 12 06:40:13.173: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:14.181: INFO: Number of nodes with available pods: 0
Jul 12 06:40:14.181: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:16.031: INFO: Number of nodes with available pods: 0
Jul 12 06:40:16.032: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:16.230: INFO: Number of nodes with available pods: 0
Jul 12 06:40:16.230: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:17.180: INFO: Number of nodes with available pods: 0
Jul 12 06:40:17.180: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:18.182: INFO: Number of nodes with available pods: 0
Jul 12 06:40:18.182: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:19.181: INFO: Number of nodes with available pods: 0
Jul 12 06:40:19.181: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:20.750: INFO: Number of nodes with available pods: 0
Jul 12 06:40:20.750: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:21.424: INFO: Number of nodes with available pods: 0
Jul 12 06:40:21.424: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:22.203: INFO: Number of nodes with available pods: 0
Jul 12 06:40:22.203: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:23.525: INFO: Number of nodes with available pods: 1
Jul 12 06:40:23.525: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:24.181: INFO: Number of nodes with available pods: 1
Jul 12 06:40:24.181: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 06:40:25.714: INFO: Number of nodes with available pods: 2
Jul 12 06:40:25.714: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 06:40:26.181: INFO: Number of nodes with available pods: 2
Jul 12 06:40:26.181: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 06:40:27.184: INFO: Number of nodes with available pods: 2
Jul 12 06:40:27.184: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 06:40:28.182: INFO: Number of nodes with available pods: 3
Jul 12 06:40:28.182: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 12 06:40:28.234: INFO: Number of nodes with available pods: 2
Jul 12 06:40:28.234: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:29.243: INFO: Number of nodes with available pods: 2
Jul 12 06:40:29.243: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:30.255: INFO: Number of nodes with available pods: 2
Jul 12 06:40:30.255: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:31.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:31.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:32.243: INFO: Number of nodes with available pods: 2
Jul 12 06:40:32.243: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:33.251: INFO: Number of nodes with available pods: 2
Jul 12 06:40:33.251: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:34.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:34.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:35.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:35.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:36.243: INFO: Number of nodes with available pods: 2
Jul 12 06:40:36.243: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:37.243: INFO: Number of nodes with available pods: 2
Jul 12 06:40:37.243: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:38.246: INFO: Number of nodes with available pods: 2
Jul 12 06:40:38.246: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:39.242: INFO: Number of nodes with available pods: 2
Jul 12 06:40:39.242: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:40.247: INFO: Number of nodes with available pods: 2
Jul 12 06:40:40.247: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:41.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:41.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:42.285: INFO: Number of nodes with available pods: 2
Jul 12 06:40:42.285: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:43.242: INFO: Number of nodes with available pods: 2
Jul 12 06:40:43.242: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:44.252: INFO: Number of nodes with available pods: 2
Jul 12 06:40:44.252: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:45.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:45.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:46.262: INFO: Number of nodes with available pods: 2
Jul 12 06:40:46.262: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:47.244: INFO: Number of nodes with available pods: 2
Jul 12 06:40:47.244: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 06:40:48.244: INFO: Number of nodes with available pods: 3
Jul 12 06:40:48.244: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-811, will wait for the garbage collector to delete the pods
Jul 12 06:40:48.373: INFO: Deleting DaemonSet.extensions daemon-set took: 14.597242ms
Jul 12 06:40:48.674: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.309775ms
Jul 12 06:40:59.481: INFO: Number of nodes with available pods: 0
Jul 12 06:40:59.481: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 06:40:59.493: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-811/daemonsets","resourceVersion":"2799532"},"items":null}

Jul 12 06:40:59.499: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-811/pods","resourceVersion":"2799532"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:40:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-811" for this suite.

• [SLOW TEST:46.662 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":303,"completed":22,"skipped":479,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:40:59.558: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 12 06:40:59.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6746'
Jul 12 06:40:59.988: INFO: stderr: ""
Jul 12 06:40:59.988: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 12 06:40:59.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pod e2e-test-httpd-pod -o json --namespace=kubectl-6746'
Jul 12 06:41:00.126: INFO: stderr: ""
Jul 12 06:41:00.126: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-07-12T06:40:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-12T06:40:59Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6746\",\n        \"resourceVersion\": \"2799547\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6746/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b845e23b-66d2-469b-b62b-857a4865ef2d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-b8l8f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.32.0.100\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-b8l8f\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-b8l8f\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T06:40:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\"\n    }\n}\n"
Jul 12 06:41:00.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 replace -f - --dry-run server --namespace=kubectl-6746'
Jul 12 06:41:00.665: INFO: stderr: "W0712 06:41:00.191574     253 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Jul 12 06:41:00.665: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jul 12 06:41:00.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete pods e2e-test-httpd-pod --namespace=kubectl-6746'
Jul 12 06:41:03.023: INFO: stderr: ""
Jul 12 06:41:03.023: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:41:03.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6746" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":303,"completed":23,"skipped":484,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:41:03.050: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:42:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8119" for this suite.

• [SLOW TEST:60.196 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":303,"completed":24,"skipped":486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:42:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 12 06:42:03.530: INFO: Waiting up to 5m0s for pod "pod-ef620941-251c-48d5-8324-51e3731cde6f" in namespace "emptydir-9361" to be "Succeeded or Failed"
Jul 12 06:42:03.596: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Pending", Reason="", readiness=false. Elapsed: 66.283285ms
Jul 12 06:42:05.600: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069638794s
Jul 12 06:42:07.604: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074066317s
Jul 12 06:42:09.608: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078199333s
Jul 12 06:42:11.637: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106664129s
Jul 12 06:42:13.641: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111013261s
STEP: Saw pod success
Jul 12 06:42:13.641: INFO: Pod "pod-ef620941-251c-48d5-8324-51e3731cde6f" satisfied condition "Succeeded or Failed"
Jul 12 06:42:13.646: INFO: Trying to get logs from node 10.32.0.100 pod pod-ef620941-251c-48d5-8324-51e3731cde6f container test-container: <nil>
STEP: delete the pod
Jul 12 06:42:13.821: INFO: Waiting for pod pod-ef620941-251c-48d5-8324-51e3731cde6f to disappear
Jul 12 06:42:13.827: INFO: Pod pod-ef620941-251c-48d5-8324-51e3731cde6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:42:13.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9361" for this suite.

• [SLOW TEST:10.627 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":25,"skipped":526,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:42:13.874: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 12 06:42:14.518: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jul 12 06:42:16.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:42:18.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:42:20.687: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:42:22.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:42:24.687: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:42:26.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668935, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761668934, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 06:42:29.733: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:42:29.737: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:42:36.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9775" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:22.677 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":303,"completed":26,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:42:36.551: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jul 12 06:42:36.913: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 12 06:42:36.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:41.563: INFO: stderr: ""
Jul 12 06:42:41.563: INFO: stdout: "service/agnhost-replica created\n"
Jul 12 06:42:41.563: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 12 06:42:41.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:42.124: INFO: stderr: ""
Jul 12 06:42:42.124: INFO: stdout: "service/agnhost-primary created\n"
Jul 12 06:42:42.124: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 12 06:42:42.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:42.599: INFO: stderr: ""
Jul 12 06:42:42.599: INFO: stdout: "service/frontend created\n"
Jul 12 06:42:42.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 12 06:42:42.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:42.989: INFO: stderr: ""
Jul 12 06:42:42.989: INFO: stdout: "deployment.apps/frontend created\n"
Jul 12 06:42:42.989: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 12 06:42:42.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:43.524: INFO: stderr: ""
Jul 12 06:42:43.524: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 12 06:42:43.525: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 12 06:42:43.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-3532'
Jul 12 06:42:43.953: INFO: stderr: ""
Jul 12 06:42:43.953: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 12 06:42:43.953: INFO: Waiting for all frontend pods to be Running.
Jul 12 06:42:59.004: INFO: Waiting for frontend to serve content.
Jul 12 06:42:59.059: INFO: Trying to add a new entry to the guestbook.
Jul 12 06:42:59.098: INFO: Verifying that added entry can be retrieved.
Jul 12 06:42:59.104: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jul 12 06:43:04.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:04.263: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:04.263: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 06:43:04.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:04.509: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:04.509: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 06:43:04.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:04.685: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:04.685: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 06:43:04.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:04.792: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:04.792: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 06:43:04.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:05.173: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:05.173: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 06:43:05.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-3532'
Jul 12 06:43:06.566: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:43:06.566: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:43:06.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3532" for this suite.

• [SLOW TEST:30.317 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":303,"completed":27,"skipped":556,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:43:06.869: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:43:07.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943" in namespace "downward-api-1461" to be "Succeeded or Failed"
Jul 12 06:43:07.572: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 52.910507ms
Jul 12 06:43:09.879: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358985087s
Jul 12 06:43:11.996: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476149532s
Jul 12 06:43:13.999: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 6.479151051s
Jul 12 06:43:16.003: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 8.483159337s
Jul 12 06:43:18.007: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 10.487173586s
Jul 12 06:43:20.030: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 12.510141154s
Jul 12 06:43:22.033: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Pending", Reason="", readiness=false. Elapsed: 14.513520469s
Jul 12 06:43:24.036: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.516572289s
STEP: Saw pod success
Jul 12 06:43:24.036: INFO: Pod "downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943" satisfied condition "Succeeded or Failed"
Jul 12 06:43:24.041: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943 container client-container: <nil>
STEP: delete the pod
Jul 12 06:43:25.270: INFO: Waiting for pod downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943 to disappear
Jul 12 06:43:25.274: INFO: Pod downwardapi-volume-0dd6ef0d-8cea-4657-b4b7-781444f11943 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:43:25.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1461" for this suite.

• [SLOW TEST:18.421 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":28,"skipped":565,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:43:25.290: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c50fe45a-184d-4e6b-b73f-79c17d1b5e64
STEP: Creating a pod to test consume configMaps
Jul 12 06:43:25.545: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff" in namespace "projected-3811" to be "Succeeded or Failed"
Jul 12 06:43:25.569: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 23.757834ms
Jul 12 06:43:27.572: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026972582s
Jul 12 06:43:29.606: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060248023s
Jul 12 06:43:31.628: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082206253s
Jul 12 06:43:33.631: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08567263s
Jul 12 06:43:35.646: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.1008718s
Jul 12 06:43:37.669: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.123888818s
Jul 12 06:43:39.679: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.133396086s
STEP: Saw pod success
Jul 12 06:43:39.679: INFO: Pod "pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff" satisfied condition "Succeeded or Failed"
Jul 12 06:43:39.682: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:43:39.759: INFO: Waiting for pod pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff to disappear
Jul 12 06:43:39.762: INFO: Pod pod-projected-configmaps-27124712-0d7b-4280-ad39-ab126503a8ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:43:39.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3811" for this suite.

• [SLOW TEST:14.484 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":29,"skipped":570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:43:39.775: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:43:40.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a" in namespace "downward-api-9487" to be "Succeeded or Failed"
Jul 12 06:43:40.025: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.875192ms
Jul 12 06:43:42.034: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011481743s
Jul 12 06:43:44.355: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332866717s
Jul 12 06:43:46.359: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.336772531s
Jul 12 06:43:48.362: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.340016208s
Jul 12 06:43:50.392: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.369332807s
Jul 12 06:43:52.401: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.378664171s
STEP: Saw pod success
Jul 12 06:43:52.401: INFO: Pod "downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a" satisfied condition "Succeeded or Failed"
Jul 12 06:43:52.405: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a container client-container: <nil>
STEP: delete the pod
Jul 12 06:43:52.542: INFO: Waiting for pod downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a to disappear
Jul 12 06:43:52.544: INFO: Pod downwardapi-volume-3eef495d-7fb1-4d33-8548-c5b1338bde9a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:43:52.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9487" for this suite.

• [SLOW TEST:12.872 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":30,"skipped":598,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:43:52.647: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 12 06:44:06.287: INFO: Successfully updated pod "annotationupdate219ada01-9560-4751-b63b-d80d37ad3fca"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:44:08.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4898" for this suite.

• [SLOW TEST:15.748 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":31,"skipped":600,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:44:08.396: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:46:08.529: INFO: Deleting pod "var-expansion-e6f436ca-f084-498d-9c86-69c1d6c6a8a8" in namespace "var-expansion-3006"
Jul 12 06:46:08.564: INFO: Wait up to 5m0s for pod "var-expansion-e6f436ca-f084-498d-9c86-69c1d6c6a8a8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:46:12.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3006" for this suite.

• [SLOW TEST:124.269 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":303,"completed":32,"skipped":607,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:46:12.665: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-1934
STEP: creating replication controller nodeport-test in namespace services-1934
I0712 06:46:12.964636      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1934, replica count: 2
I0712 06:46:16.014893      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:46:19.015074      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:46:22.015315      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 06:46:25.015: INFO: Creating new exec pod
I0712 06:46:25.015517      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 06:46:38.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1934 execpod8l6xw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 12 06:46:38.471: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 12 06:46:38.471: INFO: stdout: ""
Jul 12 06:46:38.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1934 execpod8l6xw -- /bin/sh -x -c nc -zv -t -w 2 10.254.40.201 80'
Jul 12 06:46:38.886: INFO: stderr: "+ nc -zv -t -w 2 10.254.40.201 80\nConnection to 10.254.40.201 80 port [tcp/http] succeeded!\n"
Jul 12 06:46:38.886: INFO: stdout: ""
Jul 12 06:46:38.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1934 execpod8l6xw -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.100 31484'
Jul 12 06:46:39.133: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.100 31484\nConnection to 10.32.0.100 31484 port [tcp/31484] succeeded!\n"
Jul 12 06:46:39.133: INFO: stdout: ""
Jul 12 06:46:39.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1934 execpod8l6xw -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.3 31484'
Jul 12 06:46:39.364: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.3 31484\nConnection to 10.32.0.3 31484 port [tcp/31484] succeeded!\n"
Jul 12 06:46:39.364: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:46:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1934" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.795 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":303,"completed":33,"skipped":624,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:46:39.461: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 06:46:40.346: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 06:46:42.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:46:44.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:46:46.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:46:48.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:46:50.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:46:52.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669200, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 06:46:55.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:46:55.484: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8702-crds.webhook.example.com via the AdmissionRegistration API
Jul 12 06:47:01.097: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:47:02.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3009" for this suite.
STEP: Destroying namespace "webhook-3009-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:23.462 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":303,"completed":34,"skipped":629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:47:02.924: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:47:03.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6282" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":303,"completed":35,"skipped":664,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:47:03.322: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-d4932f3f-8e28-4fdb-a5a1-61d58d015ee8
STEP: Creating a pod to test consume configMaps
Jul 12 06:47:05.556: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6" in namespace "projected-361" to be "Succeeded or Failed"
Jul 12 06:47:05.582: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.391459ms
Jul 12 06:47:07.598: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04253743s
Jul 12 06:47:09.602: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046048999s
Jul 12 06:47:11.605: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049611311s
Jul 12 06:47:13.609: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052949839s
Jul 12 06:47:15.612: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056367564s
Jul 12 06:47:17.616: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.060240492s
STEP: Saw pod success
Jul 12 06:47:17.616: INFO: Pod "pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6" satisfied condition "Succeeded or Failed"
Jul 12 06:47:17.651: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:47:17.717: INFO: Waiting for pod pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6 to disappear
Jul 12 06:47:17.719: INFO: Pod pod-projected-configmaps-56416e3e-8f5c-41ec-ae8c-a6ea5762aaa6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:47:17.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-361" for this suite.

• [SLOW TEST:14.502 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":36,"skipped":675,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:47:17.825: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 06:47:18.880: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 06:47:20.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669239, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:47:22.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669239, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:47:24.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669239, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:47:26.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669239, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 06:47:28.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669239, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761669238, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 06:47:31.943: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Jul 12 06:47:32.094: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:47:32.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6328" for this suite.
STEP: Destroying namespace "webhook-6328-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.900 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":303,"completed":37,"skipped":689,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:47:32.725: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 12 06:47:33.057: INFO: Created pod &Pod{ObjectMeta:{dns-8713  dns-8713 /api/v1/namespaces/dns-8713/pods/dns-8713 b6cf590a-f09f-47b3-b1cc-e5b902dd4cdf 2803774 0 2021-07-12 06:47:33 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-07-12 06:47:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n6jlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n6jlk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n6jlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 06:47:33.117: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:35.277: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:37.120: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:39.120: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:41.120: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:43.151: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:45.120: INFO: The status of Pod dns-8713 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 06:47:47.137: INFO: The status of Pod dns-8713 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 12 06:47:47.137: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8713 PodName:dns-8713 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 06:47:47.137: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Verifying customized DNS server is configured on pod...
Jul 12 06:47:47.299: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8713 PodName:dns-8713 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 06:47:47.299: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 06:47:47.467: INFO: Deleting pod dns-8713...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:47:47.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8713" for this suite.

• [SLOW TEST:14.836 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":303,"completed":38,"skipped":697,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:47:47.562: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 12 06:47:48.252: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 06:48:48.348: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:48:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 12 06:49:00.610: INFO: found a healthy node: 10.32.0.100
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:49:49.111: INFO: pods created so far: [1 1 1]
Jul 12 06:49:49.111: INFO: length of pods created so far: 3
Jul 12 06:49:59.135: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:06.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-181" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9190" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:139.300 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":303,"completed":39,"skipped":712,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:50:06.862: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-3bb9728e-0f9f-4cb0-b596-da884791c9f6
STEP: Creating a pod to test consume configMaps
Jul 12 06:50:06.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a" in namespace "projected-6350" to be "Succeeded or Failed"
Jul 12 06:50:07.001: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145699ms
Jul 12 06:50:09.040: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041404666s
Jul 12 06:50:11.045: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046069968s
Jul 12 06:50:13.049: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049713274s
Jul 12 06:50:15.098: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099011603s
Jul 12 06:50:17.101: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102628276s
Jul 12 06:50:19.104: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.105650106s
STEP: Saw pod success
Jul 12 06:50:19.105: INFO: Pod "pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a" satisfied condition "Succeeded or Failed"
Jul 12 06:50:19.157: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:50:19.220: INFO: Waiting for pod pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a to disappear
Jul 12 06:50:19.230: INFO: Pod pod-projected-configmaps-c07371f6-a64f-441f-9e6e-9a259918371a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:19.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6350" for this suite.

• [SLOW TEST:12.386 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":40,"skipped":720,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:50:19.250: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:50:20.370: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1" in namespace "projected-2407" to be "Succeeded or Failed"
Jul 12 06:50:20.499: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 128.856817ms
Jul 12 06:50:22.503: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132374743s
Jul 12 06:50:24.508: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138032513s
Jul 12 06:50:26.513: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142256872s
Jul 12 06:50:28.540: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.170116594s
Jul 12 06:50:30.740: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36956244s
Jul 12 06:50:32.774: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.403876469s
STEP: Saw pod success
Jul 12 06:50:32.774: INFO: Pod "downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1" satisfied condition "Succeeded or Failed"
Jul 12 06:50:32.778: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1 container client-container: <nil>
STEP: delete the pod
Jul 12 06:50:32.981: INFO: Waiting for pod downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1 to disappear
Jul 12 06:50:32.992: INFO: Pod downwardapi-volume-8fb97f0a-cf1b-4cfe-9a96-d102661652f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:32.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2407" for this suite.

• [SLOW TEST:13.756 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":41,"skipped":724,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:50:33.006: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9e8e3867-cbf4-469f-8b2f-59f2e14721c6
STEP: Creating a pod to test consume secrets
Jul 12 06:50:33.350: INFO: Waiting up to 5m0s for pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c" in namespace "secrets-7334" to be "Succeeded or Failed"
Jul 12 06:50:33.441: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.806929ms
Jul 12 06:50:35.444: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094546452s
Jul 12 06:50:37.448: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09824287s
Jul 12 06:50:39.451: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101300737s
Jul 12 06:50:41.455: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.104971108s
Jul 12 06:50:43.460: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.109677844s
Jul 12 06:50:45.535: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.18474278s
STEP: Saw pod success
Jul 12 06:50:45.535: INFO: Pod "pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c" satisfied condition "Succeeded or Failed"
Jul 12 06:50:45.537: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 06:50:45.670: INFO: Waiting for pod pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c to disappear
Jul 12 06:50:45.691: INFO: Pod pod-secrets-961e201d-5dca-4749-8911-dd708e0e014c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:45.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7334" for this suite.
STEP: Destroying namespace "secret-namespace-99" for this suite.

• [SLOW TEST:12.817 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":303,"completed":42,"skipped":732,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:50:45.824: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 12 06:50:56.739: INFO: Successfully updated pod "labelsupdatee6399b34-5fa7-4f5a-ac66-5080f615a88f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:50:58.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8323" for this suite.

• [SLOW TEST:12.985 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":43,"skipped":742,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:50:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Jul 12 06:50:58.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-843'
Jul 12 06:50:59.347: INFO: stderr: ""
Jul 12 06:50:59.347: INFO: stdout: "pod/pause created\n"
Jul 12 06:50:59.347: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 12 06:50:59.347: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-843" to be "running and ready"
Jul 12 06:50:59.351: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554478ms
Jul 12 06:51:01.359: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012229949s
Jul 12 06:51:03.880: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.533234989s
Jul 12 06:51:06.061: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714727191s
Jul 12 06:51:08.453: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.106734377s
Jul 12 06:51:10.458: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 11.111009165s
Jul 12 06:51:10.458: INFO: Pod "pause" satisfied condition "running and ready"
Jul 12 06:51:10.458: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 12 06:51:10.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 label pods pause testing-label=testing-label-value --namespace=kubectl-843'
Jul 12 06:51:10.568: INFO: stderr: ""
Jul 12 06:51:10.568: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 12 06:51:10.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pod pause -L testing-label --namespace=kubectl-843'
Jul 12 06:51:10.652: INFO: stderr: ""
Jul 12 06:51:10.652: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 12 06:51:10.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 label pods pause testing-label- --namespace=kubectl-843'
Jul 12 06:51:10.747: INFO: stderr: ""
Jul 12 06:51:10.747: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 12 06:51:10.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pod pause -L testing-label --namespace=kubectl-843'
Jul 12 06:51:10.829: INFO: stderr: ""
Jul 12 06:51:10.829: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jul 12 06:51:10.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-843'
Jul 12 06:51:10.958: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:51:10.958: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 12 06:51:10.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get rc,svc -l name=pause --no-headers --namespace=kubectl-843'
Jul 12 06:51:11.077: INFO: stderr: "No resources found in kubectl-843 namespace.\n"
Jul 12 06:51:11.077: INFO: stdout: ""
Jul 12 06:51:11.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -l name=pause --namespace=kubectl-843 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 06:51:11.154: INFO: stderr: ""
Jul 12 06:51:11.154: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:51:11.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-843" for this suite.

• [SLOW TEST:12.461 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":303,"completed":44,"skipped":751,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:51:11.271: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:51:11.925: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662" in namespace "security-context-test-9352" to be "Succeeded or Failed"
Jul 12 06:51:11.927: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 1.899659ms
Jul 12 06:51:13.964: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039759769s
Jul 12 06:51:15.982: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057354298s
Jul 12 06:51:17.987: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062133478s
Jul 12 06:51:19.991: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066177344s
Jul 12 06:51:22.001: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Pending", Reason="", readiness=false. Elapsed: 10.075952704s
Jul 12 06:51:24.208: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.283291174s
Jul 12 06:51:24.208: INFO: Pod "busybox-readonly-false-b491f099-7156-41b1-a77e-827c3bd43662" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:51:24.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9352" for this suite.

• [SLOW TEST:13.000 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":303,"completed":45,"skipped":752,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:51:24.271: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 06:51:39.308: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63: the server could not find the requested resource (get pods dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63)
Jul 12 06:51:39.310: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63: the server could not find the requested resource (get pods dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63)
Jul 12 06:51:39.324: INFO: Unable to read jessie_udp@PodARecord from pod dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63: the server could not find the requested resource (get pods dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63)
Jul 12 06:51:39.329: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63: the server could not find the requested resource (get pods dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63)
Jul 12 06:51:39.329: INFO: Lookups using dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 06:51:44.408: INFO: DNS probes using dns-7889/dns-test-066185bf-a66f-4bd4-9261-8fa23bfc5d63 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:51:44.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7889" for this suite.

• [SLOW TEST:20.210 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":303,"completed":46,"skipped":761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:51:44.482: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:51:44.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9521" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":303,"completed":47,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:51:44.799: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 12 06:51:45.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-7754'
Jul 12 06:51:46.043: INFO: stderr: ""
Jul 12 06:51:46.043: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 06:51:46.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:51:46.274: INFO: stderr: ""
Jul 12 06:51:46.274: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jul 12 06:51:51.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:51:51.384: INFO: stderr: ""
Jul 12 06:51:51.384: INFO: stdout: "update-demo-nautilus-jhtr9 update-demo-nautilus-mq22g "
Jul 12 06:51:51.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-jhtr9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:51:51.477: INFO: stderr: ""
Jul 12 06:51:51.477: INFO: stdout: ""
Jul 12 06:51:51.477: INFO: update-demo-nautilus-jhtr9 is created but not running
Jul 12 06:51:56.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:51:56.621: INFO: stderr: ""
Jul 12 06:51:56.621: INFO: stdout: "update-demo-nautilus-jhtr9 update-demo-nautilus-mq22g "
Jul 12 06:51:56.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-jhtr9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:51:56.704: INFO: stderr: ""
Jul 12 06:51:56.704: INFO: stdout: "true"
Jul 12 06:51:56.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-jhtr9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:51:56.786: INFO: stderr: ""
Jul 12 06:51:56.786: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 06:51:56.786: INFO: validating pod update-demo-nautilus-jhtr9
Jul 12 06:51:56.804: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 06:51:56.804: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 06:51:56.804: INFO: update-demo-nautilus-jhtr9 is verified up and running
Jul 12 06:51:56.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:51:56.890: INFO: stderr: ""
Jul 12 06:51:56.890: INFO: stdout: "true"
Jul 12 06:51:56.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:51:56.970: INFO: stderr: ""
Jul 12 06:51:56.970: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 06:51:56.970: INFO: validating pod update-demo-nautilus-mq22g
Jul 12 06:51:57.000: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 06:51:57.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 06:51:57.000: INFO: update-demo-nautilus-mq22g is verified up and running
STEP: scaling down the replication controller
Jul 12 06:51:57.002: INFO: scanned /root for discovery docs: <nil>
Jul 12 06:51:57.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7754'
Jul 12 06:51:58.131: INFO: stderr: ""
Jul 12 06:51:58.131: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 06:51:58.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:51:58.222: INFO: stderr: ""
Jul 12 06:51:58.222: INFO: stdout: "update-demo-nautilus-jhtr9 update-demo-nautilus-mq22g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 12 06:52:03.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:03.309: INFO: stderr: ""
Jul 12 06:52:03.309: INFO: stdout: "update-demo-nautilus-jhtr9 update-demo-nautilus-mq22g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 12 06:52:08.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:08.410: INFO: stderr: ""
Jul 12 06:52:08.410: INFO: stdout: "update-demo-nautilus-jhtr9 update-demo-nautilus-mq22g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 12 06:52:13.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:13.499: INFO: stderr: ""
Jul 12 06:52:13.499: INFO: stdout: "update-demo-nautilus-mq22g "
Jul 12 06:52:13.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:13.589: INFO: stderr: ""
Jul 12 06:52:13.589: INFO: stdout: "true"
Jul 12 06:52:13.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:13.668: INFO: stderr: ""
Jul 12 06:52:13.668: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 06:52:13.668: INFO: validating pod update-demo-nautilus-mq22g
Jul 12 06:52:13.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 06:52:13.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 06:52:13.671: INFO: update-demo-nautilus-mq22g is verified up and running
STEP: scaling up the replication controller
Jul 12 06:52:13.673: INFO: scanned /root for discovery docs: <nil>
Jul 12 06:52:13.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7754'
Jul 12 06:52:14.856: INFO: stderr: ""
Jul 12 06:52:14.856: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 06:52:14.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:15.388: INFO: stderr: ""
Jul 12 06:52:15.388: INFO: stdout: "update-demo-nautilus-4lfp6 update-demo-nautilus-mq22g "
Jul 12 06:52:15.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4lfp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:15.474: INFO: stderr: ""
Jul 12 06:52:15.474: INFO: stdout: ""
Jul 12 06:52:15.474: INFO: update-demo-nautilus-4lfp6 is created but not running
Jul 12 06:52:20.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:20.774: INFO: stderr: ""
Jul 12 06:52:20.774: INFO: stdout: "update-demo-nautilus-4lfp6 update-demo-nautilus-mq22g "
Jul 12 06:52:20.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4lfp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:20.857: INFO: stderr: ""
Jul 12 06:52:20.857: INFO: stdout: ""
Jul 12 06:52:20.857: INFO: update-demo-nautilus-4lfp6 is created but not running
Jul 12 06:52:25.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7754'
Jul 12 06:52:25.954: INFO: stderr: ""
Jul 12 06:52:25.954: INFO: stdout: "update-demo-nautilus-4lfp6 update-demo-nautilus-mq22g "
Jul 12 06:52:25.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4lfp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:26.044: INFO: stderr: ""
Jul 12 06:52:26.044: INFO: stdout: "true"
Jul 12 06:52:26.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4lfp6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:26.121: INFO: stderr: ""
Jul 12 06:52:26.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 06:52:26.121: INFO: validating pod update-demo-nautilus-4lfp6
Jul 12 06:52:26.124: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 06:52:26.124: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 06:52:26.124: INFO: update-demo-nautilus-4lfp6 is verified up and running
Jul 12 06:52:26.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:26.210: INFO: stderr: ""
Jul 12 06:52:26.210: INFO: stdout: "true"
Jul 12 06:52:26.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-mq22g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7754'
Jul 12 06:52:26.291: INFO: stderr: ""
Jul 12 06:52:26.291: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 06:52:26.291: INFO: validating pod update-demo-nautilus-mq22g
Jul 12 06:52:26.294: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 06:52:26.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 06:52:26.294: INFO: update-demo-nautilus-mq22g is verified up and running
STEP: using delete to clean up resources
Jul 12 06:52:26.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-7754'
Jul 12 06:52:26.394: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 06:52:26.394: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 12 06:52:26.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7754'
Jul 12 06:52:26.575: INFO: stderr: "No resources found in kubectl-7754 namespace.\n"
Jul 12 06:52:26.575: INFO: stdout: ""
Jul 12 06:52:26.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -l name=update-demo --namespace=kubectl-7754 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 06:52:26.661: INFO: stderr: ""
Jul 12 06:52:26.661: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:52:26.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7754" for this suite.

• [SLOW TEST:41.879 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":303,"completed":48,"skipped":864,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:52:26.678: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:52:37.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1393" for this suite.

• [SLOW TEST:10.759 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":49,"skipped":876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:52:37.440: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 06:52:37.598: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1527
I0712 06:52:37.684040      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1527, replica count: 1
I0712 06:52:38.734411      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:39.734564      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:40.734709      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:41.734913      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:42.735115      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:43.735374      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:44.735644      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:45.735970      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:46.736230      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 06:52:47.736441      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 06:52:47.954: INFO: Created: latency-svc-qqxcs
Jul 12 06:52:47.964: INFO: Got endpoints: latency-svc-qqxcs [127.380934ms]
Jul 12 06:52:48.023: INFO: Created: latency-svc-rjg9d
Jul 12 06:52:48.129: INFO: Got endpoints: latency-svc-rjg9d [165.364231ms]
Jul 12 06:52:48.138: INFO: Created: latency-svc-hx6xr
Jul 12 06:52:48.173: INFO: Got endpoints: latency-svc-hx6xr [208.950136ms]
Jul 12 06:52:48.219: INFO: Created: latency-svc-tl4pc
Jul 12 06:52:48.231: INFO: Got endpoints: latency-svc-tl4pc [267.088156ms]
Jul 12 06:52:48.339: INFO: Created: latency-svc-cvbbp
Jul 12 06:52:48.398: INFO: Got endpoints: latency-svc-cvbbp [433.864486ms]
Jul 12 06:52:48.438: INFO: Created: latency-svc-d4zvs
Jul 12 06:52:49.705: INFO: Got endpoints: latency-svc-d4zvs [1.741477977s]
Jul 12 06:52:49.821: INFO: Created: latency-svc-rqvxv
Jul 12 06:52:49.832: INFO: Created: latency-svc-d64wt
Jul 12 06:52:49.863: INFO: Got endpoints: latency-svc-rqvxv [1.898762959s]
Jul 12 06:52:49.885: INFO: Got endpoints: latency-svc-d64wt [1.921643453s]
Jul 12 06:52:50.047: INFO: Created: latency-svc-dd6p2
Jul 12 06:52:50.086: INFO: Got endpoints: latency-svc-dd6p2 [2.121859966s]
Jul 12 06:52:50.090: INFO: Created: latency-svc-lmf7l
Jul 12 06:52:50.098: INFO: Got endpoints: latency-svc-lmf7l [2.134438621s]
Jul 12 06:52:50.240: INFO: Created: latency-svc-25c57
Jul 12 06:52:50.346: INFO: Got endpoints: latency-svc-25c57 [2.382541914s]
Jul 12 06:52:50.369: INFO: Created: latency-svc-lccll
Jul 12 06:52:50.406: INFO: Got endpoints: latency-svc-lccll [2.442547894s]
Jul 12 06:52:50.424: INFO: Created: latency-svc-dnr44
Jul 12 06:52:50.444: INFO: Got endpoints: latency-svc-dnr44 [2.48031508s]
Jul 12 06:52:50.920: INFO: Created: latency-svc-4rvwr
Jul 12 06:52:50.941: INFO: Got endpoints: latency-svc-4rvwr [2.976995432s]
Jul 12 06:52:50.957: INFO: Created: latency-svc-6nk59
Jul 12 06:52:50.999: INFO: Got endpoints: latency-svc-6nk59 [3.034949227s]
Jul 12 06:52:51.086: INFO: Created: latency-svc-tj747
Jul 12 06:52:51.116: INFO: Got endpoints: latency-svc-tj747 [3.151918577s]
Jul 12 06:52:51.125: INFO: Created: latency-svc-knf8h
Jul 12 06:52:51.134: INFO: Got endpoints: latency-svc-knf8h [3.004414487s]
Jul 12 06:52:51.230: INFO: Created: latency-svc-crdvg
Jul 12 06:52:51.263: INFO: Got endpoints: latency-svc-crdvg [3.090272812s]
Jul 12 06:52:51.341: INFO: Created: latency-svc-5ncbs
Jul 12 06:52:51.389: INFO: Got endpoints: latency-svc-5ncbs [3.157732118s]
Jul 12 06:52:51.458: INFO: Created: latency-svc-jwpsm
Jul 12 06:52:51.466: INFO: Got endpoints: latency-svc-jwpsm [3.068278621s]
Jul 12 06:52:51.597: INFO: Created: latency-svc-2tpfl
Jul 12 06:52:51.711: INFO: Got endpoints: latency-svc-2tpfl [2.00582945s]
Jul 12 06:52:51.716: INFO: Created: latency-svc-wlfss
Jul 12 06:52:51.726: INFO: Got endpoints: latency-svc-wlfss [1.862902771s]
Jul 12 06:52:51.889: INFO: Created: latency-svc-qrmvk
Jul 12 06:52:51.939: INFO: Got endpoints: latency-svc-qrmvk [2.053355497s]
Jul 12 06:52:51.939: INFO: Created: latency-svc-b5gdr
Jul 12 06:52:52.075: INFO: Got endpoints: latency-svc-b5gdr [1.988854057s]
Jul 12 06:52:52.083: INFO: Created: latency-svc-xqkn8
Jul 12 06:52:52.100: INFO: Got endpoints: latency-svc-xqkn8 [2.001148943s]
Jul 12 06:52:52.242: INFO: Created: latency-svc-lq7jz
Jul 12 06:52:52.345: INFO: Got endpoints: latency-svc-lq7jz [1.998851002s]
Jul 12 06:52:52.434: INFO: Created: latency-svc-wjxc8
Jul 12 06:52:52.636: INFO: Created: latency-svc-rgfwg
Jul 12 06:52:52.636: INFO: Got endpoints: latency-svc-rgfwg [2.229380309s]
Jul 12 06:52:52.658: INFO: Got endpoints: latency-svc-wjxc8 [2.21373532s]
Jul 12 06:52:52.725: INFO: Created: latency-svc-8855m
Jul 12 06:52:52.780: INFO: Got endpoints: latency-svc-8855m [1.839423359s]
Jul 12 06:52:52.786: INFO: Created: latency-svc-g9449
Jul 12 06:52:52.850: INFO: Got endpoints: latency-svc-g9449 [1.850988592s]
Jul 12 06:52:52.887: INFO: Created: latency-svc-d8zz8
Jul 12 06:52:52.896: INFO: Got endpoints: latency-svc-d8zz8 [1.780434717s]
Jul 12 06:52:53.076: INFO: Created: latency-svc-27xdj
Jul 12 06:52:53.400: INFO: Got endpoints: latency-svc-27xdj [2.266566433s]
Jul 12 06:52:53.469: INFO: Created: latency-svc-84fzx
Jul 12 06:52:53.492: INFO: Got endpoints: latency-svc-84fzx [2.228502805s]
Jul 12 06:52:53.500: INFO: Created: latency-svc-d7xhp
Jul 12 06:52:53.508: INFO: Got endpoints: latency-svc-d7xhp [2.119789394s]
Jul 12 06:52:53.620: INFO: Created: latency-svc-8dqmd
Jul 12 06:52:53.656: INFO: Got endpoints: latency-svc-8dqmd [2.189948914s]
Jul 12 06:52:53.678: INFO: Created: latency-svc-2cpnm
Jul 12 06:52:53.767: INFO: Got endpoints: latency-svc-2cpnm [2.055826355s]
Jul 12 06:52:53.787: INFO: Created: latency-svc-6mpfc
Jul 12 06:52:53.792: INFO: Created: latency-svc-nhb9p
Jul 12 06:52:53.806: INFO: Got endpoints: latency-svc-6mpfc [2.080603977s]
Jul 12 06:52:53.906: INFO: Got endpoints: latency-svc-nhb9p [1.967710053s]
Jul 12 06:52:53.915: INFO: Created: latency-svc-jlhkk
Jul 12 06:52:53.959: INFO: Got endpoints: latency-svc-jlhkk [1.884555702s]
Jul 12 06:52:53.960: INFO: Created: latency-svc-tl4r5
Jul 12 06:52:53.978: INFO: Got endpoints: latency-svc-tl4r5 [1.878663768s]
Jul 12 06:52:54.176: INFO: Created: latency-svc-ftjk7
Jul 12 06:52:54.299: INFO: Got endpoints: latency-svc-ftjk7 [1.953328194s]
Jul 12 06:52:54.315: INFO: Created: latency-svc-lgnmn
Jul 12 06:52:54.356: INFO: Got endpoints: latency-svc-lgnmn [1.720231565s]
Jul 12 06:52:54.462: INFO: Created: latency-svc-pp9pt
Jul 12 06:52:54.473: INFO: Got endpoints: latency-svc-pp9pt [1.814969144s]
Jul 12 06:52:54.534: INFO: Created: latency-svc-dh5fh
Jul 12 06:52:54.623: INFO: Got endpoints: latency-svc-dh5fh [1.842913245s]
Jul 12 06:52:54.645: INFO: Created: latency-svc-hddhz
Jul 12 06:52:54.756: INFO: Got endpoints: latency-svc-hddhz [1.906517774s]
Jul 12 06:52:54.868: INFO: Created: latency-svc-5nnm6
Jul 12 06:52:54.882: INFO: Created: latency-svc-z5qxc
Jul 12 06:52:54.937: INFO: Got endpoints: latency-svc-z5qxc [1.536237464s]
Jul 12 06:52:55.015: INFO: Got endpoints: latency-svc-5nnm6 [2.118633466s]
Jul 12 06:52:55.036: INFO: Created: latency-svc-mt7ww
Jul 12 06:52:55.137: INFO: Got endpoints: latency-svc-mt7ww [1.645004775s]
Jul 12 06:52:55.230: INFO: Created: latency-svc-lgqf8
Jul 12 06:52:55.260: INFO: Created: latency-svc-gvcrm
Jul 12 06:52:55.268: INFO: Got endpoints: latency-svc-lgqf8 [1.759497087s]
Jul 12 06:52:55.276: INFO: Got endpoints: latency-svc-gvcrm [1.620161707s]
Jul 12 06:52:55.358: INFO: Created: latency-svc-kxjwn
Jul 12 06:52:55.416: INFO: Got endpoints: latency-svc-kxjwn [1.648584388s]
Jul 12 06:52:55.443: INFO: Created: latency-svc-gzdqs
Jul 12 06:52:55.471: INFO: Got endpoints: latency-svc-gzdqs [1.664427127s]
Jul 12 06:52:55.569: INFO: Created: latency-svc-gtdzv
Jul 12 06:52:55.587: INFO: Got endpoints: latency-svc-gtdzv [1.680583612s]
Jul 12 06:52:55.691: INFO: Created: latency-svc-5jvct
Jul 12 06:52:55.718: INFO: Got endpoints: latency-svc-5jvct [1.759091927s]
Jul 12 06:52:55.744: INFO: Created: latency-svc-bnwv8
Jul 12 06:52:55.751: INFO: Got endpoints: latency-svc-bnwv8 [1.772978733s]
Jul 12 06:52:55.841: INFO: Created: latency-svc-27x8b
Jul 12 06:52:55.885: INFO: Got endpoints: latency-svc-27x8b [1.586292612s]
Jul 12 06:52:55.958: INFO: Created: latency-svc-ngdsf
Jul 12 06:52:56.021: INFO: Created: latency-svc-jq7vs
Jul 12 06:52:56.053: INFO: Got endpoints: latency-svc-jq7vs [1.580378333s]
Jul 12 06:52:56.083: INFO: Created: latency-svc-zkn8l
Jul 12 06:52:56.087: INFO: Got endpoints: latency-svc-ngdsf [1.731099826s]
Jul 12 06:52:56.133: INFO: Got endpoints: latency-svc-zkn8l [1.509941194s]
Jul 12 06:52:56.142: INFO: Created: latency-svc-fqwz2
Jul 12 06:52:56.353: INFO: Got endpoints: latency-svc-fqwz2 [1.595997039s]
Jul 12 06:52:56.462: INFO: Created: latency-svc-c948l
Jul 12 06:52:56.475: INFO: Created: latency-svc-md7gx
Jul 12 06:52:56.491: INFO: Got endpoints: latency-svc-md7gx [1.476254209s]
Jul 12 06:52:56.535: INFO: Got endpoints: latency-svc-c948l [1.598630714s]
Jul 12 06:52:56.605: INFO: Created: latency-svc-6dj46
Jul 12 06:52:56.675: INFO: Got endpoints: latency-svc-6dj46 [1.538136902s]
Jul 12 06:52:56.688: INFO: Created: latency-svc-g9njq
Jul 12 06:52:56.759: INFO: Created: latency-svc-wtntd
Jul 12 06:52:56.792: INFO: Got endpoints: latency-svc-g9njq [1.523652316s]
Jul 12 06:52:56.792: INFO: Got endpoints: latency-svc-wtntd [1.515603632s]
Jul 12 06:52:56.829: INFO: Created: latency-svc-tz78r
Jul 12 06:52:56.895: INFO: Got endpoints: latency-svc-tz78r [1.478759394s]
Jul 12 06:52:56.938: INFO: Created: latency-svc-2cd6s
Jul 12 06:52:56.978: INFO: Created: latency-svc-rv5qp
Jul 12 06:52:56.986: INFO: Got endpoints: latency-svc-2cd6s [1.514938356s]
Jul 12 06:52:57.000: INFO: Got endpoints: latency-svc-rv5qp [1.413303639s]
Jul 12 06:52:57.084: INFO: Created: latency-svc-j4r67
Jul 12 06:52:57.145: INFO: Got endpoints: latency-svc-j4r67 [1.426698997s]
Jul 12 06:52:57.220: INFO: Created: latency-svc-lnzw8
Jul 12 06:52:57.234: INFO: Created: latency-svc-2c94x
Jul 12 06:52:57.292: INFO: Got endpoints: latency-svc-lnzw8 [1.540646285s]
Jul 12 06:52:57.300: INFO: Created: latency-svc-jlw8k
Jul 12 06:52:57.445: INFO: Got endpoints: latency-svc-2c94x [1.560328527s]
Jul 12 06:52:57.479: INFO: Got endpoints: latency-svc-jlw8k [1.425356669s]
Jul 12 06:52:57.525: INFO: Created: latency-svc-xl596
Jul 12 06:52:57.611: INFO: Created: latency-svc-hzl8z
Jul 12 06:52:57.642: INFO: Got endpoints: latency-svc-xl596 [1.554447164s]
Jul 12 06:52:57.688: INFO: Got endpoints: latency-svc-hzl8z [1.554521253s]
Jul 12 06:52:57.695: INFO: Created: latency-svc-p5cgl
Jul 12 06:52:57.754: INFO: Got endpoints: latency-svc-p5cgl [1.4015125s]
Jul 12 06:52:57.801: INFO: Created: latency-svc-w2zqr
Jul 12 06:52:57.836: INFO: Got endpoints: latency-svc-w2zqr [1.344746438s]
Jul 12 06:52:57.928: INFO: Created: latency-svc-6lbvl
Jul 12 06:52:57.986: INFO: Got endpoints: latency-svc-6lbvl [1.451016276s]
Jul 12 06:52:58.203: INFO: Created: latency-svc-6ggbp
Jul 12 06:52:58.215: INFO: Got endpoints: latency-svc-6ggbp [1.540445223s]
Jul 12 06:52:58.262: INFO: Created: latency-svc-7zvtq
Jul 12 06:52:58.353: INFO: Created: latency-svc-kpwkj
Jul 12 06:52:58.364: INFO: Got endpoints: latency-svc-kpwkj [1.572354823s]
Jul 12 06:52:58.379: INFO: Got endpoints: latency-svc-7zvtq [1.587066664s]
Jul 12 06:52:58.398: INFO: Created: latency-svc-n52fl
Jul 12 06:52:58.763: INFO: Got endpoints: latency-svc-n52fl [1.868050452s]
Jul 12 06:52:58.777: INFO: Created: latency-svc-l265m
Jul 12 06:52:58.777: INFO: Got endpoints: latency-svc-l265m [1.791004556s]
Jul 12 06:52:58.845: INFO: Created: latency-svc-ntghq
Jul 12 06:52:58.887: INFO: Got endpoints: latency-svc-ntghq [1.886976767s]
Jul 12 06:52:58.980: INFO: Created: latency-svc-d4snk
Jul 12 06:52:59.034: INFO: Got endpoints: latency-svc-d4snk [1.888954409s]
Jul 12 06:52:59.110: INFO: Created: latency-svc-5lw5s
Jul 12 06:52:59.110: INFO: Got endpoints: latency-svc-5lw5s [1.817668295s]
Jul 12 06:52:59.154: INFO: Created: latency-svc-9kmr9
Jul 12 06:52:59.221: INFO: Got endpoints: latency-svc-9kmr9 [1.775349002s]
Jul 12 06:52:59.259: INFO: Created: latency-svc-gjkcv
Jul 12 06:52:59.304: INFO: Got endpoints: latency-svc-gjkcv [1.82529043s]
Jul 12 06:52:59.326: INFO: Created: latency-svc-cgtqn
Jul 12 06:52:59.437: INFO: Created: latency-svc-q4kgl
Jul 12 06:52:59.459: INFO: Got endpoints: latency-svc-cgtqn [1.81758631s]
Jul 12 06:52:59.495: INFO: Got endpoints: latency-svc-q4kgl [1.807449152s]
Jul 12 06:52:59.568: INFO: Created: latency-svc-ddrkk
Jul 12 06:52:59.654: INFO: Got endpoints: latency-svc-ddrkk [1.900158283s]
Jul 12 06:52:59.738: INFO: Created: latency-svc-6x96j
Jul 12 06:53:00.735: INFO: Got endpoints: latency-svc-6x96j [2.898765697s]
Jul 12 06:53:00.779: INFO: Created: latency-svc-7w688
Jul 12 06:53:00.829: INFO: Created: latency-svc-hmtpd
Jul 12 06:53:00.860: INFO: Got endpoints: latency-svc-7w688 [2.644525244s]
Jul 12 06:53:00.912: INFO: Got endpoints: latency-svc-hmtpd [2.926028567s]
Jul 12 06:53:00.989: INFO: Created: latency-svc-q82gd
Jul 12 06:53:01.028: INFO: Got endpoints: latency-svc-q82gd [2.663674254s]
Jul 12 06:53:01.090: INFO: Created: latency-svc-v4rhf
Jul 12 06:53:01.148: INFO: Got endpoints: latency-svc-v4rhf [2.768886048s]
Jul 12 06:53:01.160: INFO: Created: latency-svc-dtz2w
Jul 12 06:53:01.238: INFO: Got endpoints: latency-svc-dtz2w [2.475240889s]
Jul 12 06:53:01.307: INFO: Created: latency-svc-gztsl
Jul 12 06:53:01.327: INFO: Got endpoints: latency-svc-gztsl [2.549931004s]
Jul 12 06:53:01.463: INFO: Created: latency-svc-9v7vw
Jul 12 06:53:01.471: INFO: Got endpoints: latency-svc-9v7vw [2.583483478s]
Jul 12 06:53:01.622: INFO: Created: latency-svc-bzbp9
Jul 12 06:53:01.671: INFO: Got endpoints: latency-svc-bzbp9 [2.637326387s]
Jul 12 06:53:01.707: INFO: Created: latency-svc-4m5tt
Jul 12 06:53:01.721: INFO: Got endpoints: latency-svc-4m5tt [2.611631295s]
Jul 12 06:53:01.830: INFO: Created: latency-svc-qjsqz
Jul 12 06:53:01.848: INFO: Got endpoints: latency-svc-qjsqz [2.627423254s]
Jul 12 06:53:01.939: INFO: Created: latency-svc-hqc6l
Jul 12 06:53:01.997: INFO: Got endpoints: latency-svc-hqc6l [2.69295147s]
Jul 12 06:53:02.005: INFO: Created: latency-svc-55xll
Jul 12 06:53:02.013: INFO: Got endpoints: latency-svc-55xll [2.553368449s]
Jul 12 06:53:02.127: INFO: Created: latency-svc-snwj8
Jul 12 06:53:02.160: INFO: Created: latency-svc-2kbjs
Jul 12 06:53:02.189: INFO: Got endpoints: latency-svc-2kbjs [2.534399513s]
Jul 12 06:53:02.202: INFO: Got endpoints: latency-svc-snwj8 [2.706706745s]
Jul 12 06:53:02.365: INFO: Created: latency-svc-6dm9z
Jul 12 06:53:02.371: INFO: Got endpoints: latency-svc-6dm9z [1.636563764s]
Jul 12 06:53:02.453: INFO: Created: latency-svc-vspwc
Jul 12 06:53:02.497: INFO: Got endpoints: latency-svc-vspwc [1.636862741s]
Jul 12 06:53:02.561: INFO: Created: latency-svc-l5rms
Jul 12 06:53:02.739: INFO: Created: latency-svc-jpwc4
Jul 12 06:53:02.747: INFO: Got endpoints: latency-svc-l5rms [1.834990216s]
Jul 12 06:53:02.755: INFO: Got endpoints: latency-svc-jpwc4 [1.727368432s]
Jul 12 06:53:02.872: INFO: Created: latency-svc-dvg69
Jul 12 06:53:02.880: INFO: Got endpoints: latency-svc-dvg69 [1.732267795s]
Jul 12 06:53:03.017: INFO: Created: latency-svc-w8t7p
Jul 12 06:53:03.022: INFO: Got endpoints: latency-svc-w8t7p [1.783958547s]
Jul 12 06:53:03.099: INFO: Created: latency-svc-zs9wm
Jul 12 06:53:03.164: INFO: Created: latency-svc-57bxs
Jul 12 06:53:03.178: INFO: Got endpoints: latency-svc-zs9wm [1.851490493s]
Jul 12 06:53:03.223: INFO: Created: latency-svc-dt8ds
Jul 12 06:53:03.269: INFO: Got endpoints: latency-svc-57bxs [1.798339526s]
Jul 12 06:53:03.306: INFO: Got endpoints: latency-svc-dt8ds [1.634030693s]
Jul 12 06:53:03.431: INFO: Created: latency-svc-f2j5r
Jul 12 06:53:03.472: INFO: Got endpoints: latency-svc-f2j5r [1.75080104s]
Jul 12 06:53:03.481: INFO: Created: latency-svc-gmvqg
Jul 12 06:53:03.572: INFO: Got endpoints: latency-svc-gmvqg [1.724157081s]
Jul 12 06:53:03.587: INFO: Created: latency-svc-5w9t7
Jul 12 06:53:03.624: INFO: Got endpoints: latency-svc-5w9t7 [1.626842972s]
Jul 12 06:53:03.781: INFO: Created: latency-svc-z2v2w
Jul 12 06:53:03.814: INFO: Got endpoints: latency-svc-z2v2w [1.801650151s]
Jul 12 06:53:03.853: INFO: Created: latency-svc-pt94z
Jul 12 06:53:03.920: INFO: Created: latency-svc-hxk47
Jul 12 06:53:03.989: INFO: Got endpoints: latency-svc-pt94z [1.800460653s]
Jul 12 06:53:04.032: INFO: Got endpoints: latency-svc-hxk47 [1.830248945s]
Jul 12 06:53:04.033: INFO: Created: latency-svc-gtqt9
Jul 12 06:53:04.039: INFO: Created: latency-svc-97vfn
Jul 12 06:53:04.047: INFO: Got endpoints: latency-svc-97vfn [1.550564613s]
Jul 12 06:53:04.148: INFO: Got endpoints: latency-svc-gtqt9 [1.776286053s]
Jul 12 06:53:04.162: INFO: Created: latency-svc-8fss9
Jul 12 06:53:04.207: INFO: Got endpoints: latency-svc-8fss9 [1.459767938s]
Jul 12 06:53:04.323: INFO: Created: latency-svc-cvshb
Jul 12 06:53:04.381: INFO: Got endpoints: latency-svc-cvshb [1.625886494s]
Jul 12 06:53:04.465: INFO: Created: latency-svc-kwhck
Jul 12 06:53:04.516: INFO: Got endpoints: latency-svc-kwhck [1.635757344s]
Jul 12 06:53:04.528: INFO: Created: latency-svc-snrql
Jul 12 06:53:04.547: INFO: Got endpoints: latency-svc-snrql [1.525520401s]
Jul 12 06:53:04.620: INFO: Created: latency-svc-dd9mr
Jul 12 06:53:04.645: INFO: Got endpoints: latency-svc-dd9mr [1.466474856s]
Jul 12 06:53:04.653: INFO: Created: latency-svc-xhzvf
Jul 12 06:53:04.687: INFO: Created: latency-svc-jsgzr
Jul 12 06:53:04.718: INFO: Got endpoints: latency-svc-xhzvf [1.448445166s]
Jul 12 06:53:04.798: INFO: Got endpoints: latency-svc-jsgzr [1.49209022s]
Jul 12 06:53:04.883: INFO: Created: latency-svc-rc76b
Jul 12 06:53:04.915: INFO: Created: latency-svc-9c8n5
Jul 12 06:53:04.941: INFO: Got endpoints: latency-svc-9c8n5 [1.368808316s]
Jul 12 06:53:05.012: INFO: Got endpoints: latency-svc-rc76b [1.539484149s]
Jul 12 06:53:05.128: INFO: Created: latency-svc-xn7sb
Jul 12 06:53:05.181: INFO: Got endpoints: latency-svc-xn7sb [1.557032633s]
Jul 12 06:53:05.237: INFO: Created: latency-svc-mrtdz
Jul 12 06:53:05.248: INFO: Created: latency-svc-q6mjc
Jul 12 06:53:05.257: INFO: Got endpoints: latency-svc-q6mjc [1.267739097s]
Jul 12 06:53:05.314: INFO: Got endpoints: latency-svc-mrtdz [1.499784413s]
Jul 12 06:53:05.329: INFO: Created: latency-svc-w6qgk
Jul 12 06:53:05.373: INFO: Got endpoints: latency-svc-w6qgk [1.340301349s]
Jul 12 06:53:05.608: INFO: Created: latency-svc-cwmln
Jul 12 06:53:05.647: INFO: Got endpoints: latency-svc-cwmln [1.599995094s]
Jul 12 06:53:05.663: INFO: Created: latency-svc-zk2d4
Jul 12 06:53:05.749: INFO: Got endpoints: latency-svc-zk2d4 [1.60078835s]
Jul 12 06:53:05.798: INFO: Created: latency-svc-m7gv5
Jul 12 06:53:05.838: INFO: Got endpoints: latency-svc-m7gv5 [1.630335173s]
Jul 12 06:53:05.958: INFO: Created: latency-svc-9ndvj
Jul 12 06:53:05.999: INFO: Got endpoints: latency-svc-9ndvj [1.617666853s]
Jul 12 06:53:06.159: INFO: Created: latency-svc-549p7
Jul 12 06:53:06.784: INFO: Got endpoints: latency-svc-549p7 [2.26831314s]
Jul 12 06:53:06.875: INFO: Created: latency-svc-fgp7b
Jul 12 06:53:06.914: INFO: Created: latency-svc-rjm44
Jul 12 06:53:06.914: INFO: Got endpoints: latency-svc-rjm44 [2.366540692s]
Jul 12 06:53:06.954: INFO: Got endpoints: latency-svc-fgp7b [2.30937265s]
Jul 12 06:53:07.058: INFO: Created: latency-svc-wmg2w
Jul 12 06:53:07.066: INFO: Got endpoints: latency-svc-wmg2w [2.348382759s]
Jul 12 06:53:07.129: INFO: Created: latency-svc-5rq4q
Jul 12 06:53:07.149: INFO: Got endpoints: latency-svc-5rq4q [2.350872656s]
Jul 12 06:53:07.313: INFO: Created: latency-svc-fvzwn
Jul 12 06:53:07.349: INFO: Created: latency-svc-lrhzd
Jul 12 06:53:07.367: INFO: Got endpoints: latency-svc-fvzwn [2.425542988s]
Jul 12 06:53:07.388: INFO: Got endpoints: latency-svc-lrhzd [2.376349232s]
Jul 12 06:53:07.479: INFO: Created: latency-svc-2cbzf
Jul 12 06:53:07.521: INFO: Got endpoints: latency-svc-2cbzf [2.340103227s]
Jul 12 06:53:07.529: INFO: Created: latency-svc-gmkrk
Jul 12 06:53:07.573: INFO: Got endpoints: latency-svc-gmkrk [2.315931672s]
Jul 12 06:53:07.630: INFO: Created: latency-svc-2pnm4
Jul 12 06:53:07.633: INFO: Got endpoints: latency-svc-2pnm4 [2.318681809s]
Jul 12 06:53:07.709: INFO: Created: latency-svc-75vtk
Jul 12 06:53:07.717: INFO: Created: latency-svc-hc5b2
Jul 12 06:53:07.725: INFO: Got endpoints: latency-svc-75vtk [2.352347997s]
Jul 12 06:53:07.817: INFO: Got endpoints: latency-svc-hc5b2 [2.169425535s]
Jul 12 06:53:07.856: INFO: Created: latency-svc-57dm7
Jul 12 06:53:07.856: INFO: Got endpoints: latency-svc-57dm7 [2.107777843s]
Jul 12 06:53:07.962: INFO: Created: latency-svc-7lpxn
Jul 12 06:53:08.026: INFO: Got endpoints: latency-svc-7lpxn [2.187978578s]
Jul 12 06:53:08.034: INFO: Created: latency-svc-gcnhd
Jul 12 06:53:08.192: INFO: Got endpoints: latency-svc-gcnhd [2.193527368s]
Jul 12 06:53:08.315: INFO: Created: latency-svc-66bm8
Jul 12 06:53:08.363: INFO: Got endpoints: latency-svc-66bm8 [1.578761312s]
Jul 12 06:53:08.367: INFO: Created: latency-svc-hvqnb
Jul 12 06:53:08.417: INFO: Got endpoints: latency-svc-hvqnb [1.503058198s]
Jul 12 06:53:08.451: INFO: Created: latency-svc-4bm44
Jul 12 06:53:08.459: INFO: Got endpoints: latency-svc-4bm44 [1.504629365s]
Jul 12 06:53:08.597: INFO: Created: latency-svc-xbn4p
Jul 12 06:53:08.655: INFO: Got endpoints: latency-svc-xbn4p [1.588821658s]
Jul 12 06:53:08.737: INFO: Created: latency-svc-tp28p
Jul 12 06:53:08.785: INFO: Got endpoints: latency-svc-tp28p [1.636077445s]
Jul 12 06:53:08.801: INFO: Created: latency-svc-8rd99
Jul 12 06:53:08.809: INFO: Got endpoints: latency-svc-8rd99 [1.442219013s]
Jul 12 06:53:08.954: INFO: Created: latency-svc-sh6v9
Jul 12 06:53:08.988: INFO: Got endpoints: latency-svc-sh6v9 [1.599495243s]
Jul 12 06:53:09.056: INFO: Created: latency-svc-zs7g8
Jul 12 06:53:09.056: INFO: Got endpoints: latency-svc-zs7g8 [1.53479976s]
Jul 12 06:53:09.073: INFO: Created: latency-svc-89987
Jul 12 06:53:09.110: INFO: Got endpoints: latency-svc-89987 [1.536636299s]
Jul 12 06:53:09.155: INFO: Created: latency-svc-l95p9
Jul 12 06:53:09.190: INFO: Got endpoints: latency-svc-l95p9 [1.556796231s]
Jul 12 06:53:09.296: INFO: Created: latency-svc-mgmbk
Jul 12 06:53:09.346: INFO: Created: latency-svc-mpm62
Jul 12 06:53:09.501: INFO: Got endpoints: latency-svc-mgmbk [1.775762494s]
Jul 12 06:53:09.526: INFO: Got endpoints: latency-svc-mpm62 [1.709163233s]
Jul 12 06:53:09.878: INFO: Created: latency-svc-rshns
Jul 12 06:53:09.918: INFO: Created: latency-svc-2v2kb
Jul 12 06:53:09.926: INFO: Got endpoints: latency-svc-2v2kb [1.900345732s]
Jul 12 06:53:09.960: INFO: Got endpoints: latency-svc-rshns [2.103087459s]
Jul 12 06:53:09.998: INFO: Created: latency-svc-jfjkv
Jul 12 06:53:09.998: INFO: Got endpoints: latency-svc-jfjkv [1.80574718s]
Jul 12 06:53:10.098: INFO: Created: latency-svc-96kks
Jul 12 06:53:10.164: INFO: Got endpoints: latency-svc-96kks [1.801265764s]
Jul 12 06:53:10.229: INFO: Created: latency-svc-gzc2w
Jul 12 06:53:10.276: INFO: Created: latency-svc-944w9
Jul 12 06:53:10.285: INFO: Got endpoints: latency-svc-gzc2w [1.867388663s]
Jul 12 06:53:10.293: INFO: Got endpoints: latency-svc-944w9 [1.833940927s]
Jul 12 06:53:10.489: INFO: Created: latency-svc-zj6j9
Jul 12 06:53:10.524: INFO: Got endpoints: latency-svc-zj6j9 [1.86859422s]
Jul 12 06:53:10.576: INFO: Created: latency-svc-g92qw
Jul 12 06:53:10.627: INFO: Created: latency-svc-cqc2d
Jul 12 06:53:10.635: INFO: Got endpoints: latency-svc-g92qw [1.849999227s]
Jul 12 06:53:10.651: INFO: Got endpoints: latency-svc-cqc2d [1.842249081s]
Jul 12 06:53:10.806: INFO: Created: latency-svc-xqzgn
Jul 12 06:53:10.839: INFO: Got endpoints: latency-svc-xqzgn [1.851138677s]
Jul 12 06:53:10.895: INFO: Created: latency-svc-xspv8
Jul 12 06:53:10.952: INFO: Created: latency-svc-sm2fw
Jul 12 06:53:10.960: INFO: Got endpoints: latency-svc-xspv8 [1.904150224s]
Jul 12 06:53:10.968: INFO: Got endpoints: latency-svc-sm2fw [1.858313386s]
Jul 12 06:53:11.148: INFO: Created: latency-svc-pjzp2
Jul 12 06:53:11.181: INFO: Got endpoints: latency-svc-pjzp2 [1.990707851s]
Jul 12 06:53:11.293: INFO: Created: latency-svc-5h7qv
Jul 12 06:53:11.339: INFO: Created: latency-svc-2ppb5
Jul 12 06:53:11.351: INFO: Got endpoints: latency-svc-5h7qv [1.850193402s]
Jul 12 06:53:11.376: INFO: Got endpoints: latency-svc-2ppb5 [1.849980715s]
Jul 12 06:53:11.443: INFO: Created: latency-svc-n8fd4
Jul 12 06:53:11.526: INFO: Got endpoints: latency-svc-n8fd4 [1.599824918s]
Jul 12 06:53:11.546: INFO: Created: latency-svc-x5ltj
Jul 12 06:53:11.693: INFO: Got endpoints: latency-svc-x5ltj [1.732989428s]
Jul 12 06:53:11.697: INFO: Created: latency-svc-sc82w
Jul 12 06:53:11.701: INFO: Got endpoints: latency-svc-sc82w [1.70301243s]
Jul 12 06:53:11.818: INFO: Created: latency-svc-2pm5k
Jul 12 06:53:11.931: INFO: Got endpoints: latency-svc-2pm5k [1.76718007s]
Jul 12 06:53:11.948: INFO: Created: latency-svc-v99cq
Jul 12 06:53:12.027: INFO: Got endpoints: latency-svc-v99cq [1.742006764s]
Jul 12 06:53:12.056: INFO: Created: latency-svc-rbw7z
Jul 12 06:53:12.060: INFO: Got endpoints: latency-svc-rbw7z [1.767225799s]
Jul 12 06:53:12.202: INFO: Created: latency-svc-tvcpt
Jul 12 06:53:12.256: INFO: Got endpoints: latency-svc-tvcpt [1.731855392s]
Jul 12 06:53:12.264: INFO: Created: latency-svc-j6m6g
Jul 12 06:53:12.339: INFO: Created: latency-svc-j2k8j
Jul 12 06:53:12.356: INFO: Got endpoints: latency-svc-j6m6g [1.720930704s]
Jul 12 06:53:12.373: INFO: Got endpoints: latency-svc-j2k8j [1.721379409s]
Jul 12 06:53:12.435: INFO: Created: latency-svc-ptz89
Jul 12 06:53:12.443: INFO: Got endpoints: latency-svc-ptz89 [1.604257058s]
Jul 12 06:53:12.535: INFO: Created: latency-svc-x68ll
Jul 12 06:53:12.570: INFO: Got endpoints: latency-svc-x68ll [1.609588518s]
Jul 12 06:53:12.657: INFO: Created: latency-svc-ghrcf
Jul 12 06:53:12.657: INFO: Got endpoints: latency-svc-ghrcf [1.688745634s]
Jul 12 06:53:12.865: INFO: Created: latency-svc-ql9kw
Jul 12 06:53:12.910: INFO: Got endpoints: latency-svc-ql9kw [1.729068011s]
Jul 12 06:53:12.943: INFO: Created: latency-svc-9sfqw
Jul 12 06:53:13.002: INFO: Created: latency-svc-v5c9p
Jul 12 06:53:13.027: INFO: Got endpoints: latency-svc-v5c9p [1.650639585s]
Jul 12 06:53:13.042: INFO: Got endpoints: latency-svc-9sfqw [1.690409179s]
Jul 12 06:53:13.130: INFO: Created: latency-svc-j8ccg
Jul 12 06:53:13.215: INFO: Got endpoints: latency-svc-j8ccg [1.689179981s]
Jul 12 06:53:13.243: INFO: Created: latency-svc-64rqv
Jul 12 06:53:13.277: INFO: Got endpoints: latency-svc-64rqv [1.584281046s]
Jul 12 06:53:13.285: INFO: Created: latency-svc-q2n29
Jul 12 06:53:13.294: INFO: Got endpoints: latency-svc-q2n29 [1.592357479s]
Jul 12 06:53:13.773: INFO: Created: latency-svc-h9mmk
Jul 12 06:53:13.807: INFO: Got endpoints: latency-svc-h9mmk [1.875758229s]
Jul 12 06:53:13.819: INFO: Created: latency-svc-tqwjh
Jul 12 06:53:13.832: INFO: Got endpoints: latency-svc-tqwjh [1.804995756s]
Jul 12 06:53:13.832: INFO: Latencies: [165.364231ms 208.950136ms 267.088156ms 433.864486ms 1.267739097s 1.340301349s 1.344746438s 1.368808316s 1.4015125s 1.413303639s 1.425356669s 1.426698997s 1.442219013s 1.448445166s 1.451016276s 1.459767938s 1.466474856s 1.476254209s 1.478759394s 1.49209022s 1.499784413s 1.503058198s 1.504629365s 1.509941194s 1.514938356s 1.515603632s 1.523652316s 1.525520401s 1.53479976s 1.536237464s 1.536636299s 1.538136902s 1.539484149s 1.540445223s 1.540646285s 1.550564613s 1.554447164s 1.554521253s 1.556796231s 1.557032633s 1.560328527s 1.572354823s 1.578761312s 1.580378333s 1.584281046s 1.586292612s 1.587066664s 1.588821658s 1.592357479s 1.595997039s 1.598630714s 1.599495243s 1.599824918s 1.599995094s 1.60078835s 1.604257058s 1.609588518s 1.617666853s 1.620161707s 1.625886494s 1.626842972s 1.630335173s 1.634030693s 1.635757344s 1.636077445s 1.636563764s 1.636862741s 1.645004775s 1.648584388s 1.650639585s 1.664427127s 1.680583612s 1.688745634s 1.689179981s 1.690409179s 1.70301243s 1.709163233s 1.720231565s 1.720930704s 1.721379409s 1.724157081s 1.727368432s 1.729068011s 1.731099826s 1.731855392s 1.732267795s 1.732989428s 1.741477977s 1.742006764s 1.75080104s 1.759091927s 1.759497087s 1.76718007s 1.767225799s 1.772978733s 1.775349002s 1.775762494s 1.776286053s 1.780434717s 1.783958547s 1.791004556s 1.798339526s 1.800460653s 1.801265764s 1.801650151s 1.804995756s 1.80574718s 1.807449152s 1.814969144s 1.81758631s 1.817668295s 1.82529043s 1.830248945s 1.833940927s 1.834990216s 1.839423359s 1.842249081s 1.842913245s 1.849980715s 1.849999227s 1.850193402s 1.850988592s 1.851138677s 1.851490493s 1.858313386s 1.862902771s 1.867388663s 1.868050452s 1.86859422s 1.875758229s 1.878663768s 1.884555702s 1.886976767s 1.888954409s 1.898762959s 1.900158283s 1.900345732s 1.904150224s 1.906517774s 1.921643453s 1.953328194s 1.967710053s 1.988854057s 1.990707851s 1.998851002s 2.001148943s 2.00582945s 2.053355497s 2.055826355s 2.080603977s 2.103087459s 2.107777843s 2.118633466s 2.119789394s 2.121859966s 2.134438621s 2.169425535s 2.187978578s 2.189948914s 2.193527368s 2.21373532s 2.228502805s 2.229380309s 2.266566433s 2.26831314s 2.30937265s 2.315931672s 2.318681809s 2.340103227s 2.348382759s 2.350872656s 2.352347997s 2.366540692s 2.376349232s 2.382541914s 2.425542988s 2.442547894s 2.475240889s 2.48031508s 2.534399513s 2.549931004s 2.553368449s 2.583483478s 2.611631295s 2.627423254s 2.637326387s 2.644525244s 2.663674254s 2.69295147s 2.706706745s 2.768886048s 2.898765697s 2.926028567s 2.976995432s 3.004414487s 3.034949227s 3.068278621s 3.090272812s 3.151918577s 3.157732118s]
Jul 12 06:53:13.832: INFO: 50 %ile: 1.791004556s
Jul 12 06:53:13.832: INFO: 90 %ile: 2.549931004s
Jul 12 06:53:13.832: INFO: 99 %ile: 3.151918577s
Jul 12 06:53:13.832: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:53:13.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1527" for this suite.

• [SLOW TEST:36.504 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":303,"completed":50,"skipped":910,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:53:13.944: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-f30bd07b-4d08-4a97-a697-5fc8a137099d in namespace container-probe-2211
Jul 12 06:53:24.150: INFO: Started pod test-webserver-f30bd07b-4d08-4a97-a697-5fc8a137099d in namespace container-probe-2211
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 06:53:24.153: INFO: Initial restart count of pod test-webserver-f30bd07b-4d08-4a97-a697-5fc8a137099d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:57:25.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2211" for this suite.

• [SLOW TEST:251.912 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":51,"skipped":924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:57:25.857: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8df57348-2a2f-4758-bc11-28a05ab05a96
STEP: Creating a pod to test consume secrets
Jul 12 06:57:26.348: INFO: Waiting up to 5m0s for pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595" in namespace "secrets-8182" to be "Succeeded or Failed"
Jul 12 06:57:26.370: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Pending", Reason="", readiness=false. Elapsed: 22.142145ms
Jul 12 06:57:28.374: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025688862s
Jul 12 06:57:30.377: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028663186s
Jul 12 06:57:32.380: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032293415s
Jul 12 06:57:34.392: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043612472s
Jul 12 06:57:36.395: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.046645409s
STEP: Saw pod success
Jul 12 06:57:36.395: INFO: Pod "pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595" satisfied condition "Succeeded or Failed"
Jul 12 06:57:36.398: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 06:57:36.569: INFO: Waiting for pod pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595 to disappear
Jul 12 06:57:36.652: INFO: Pod pod-secrets-e2ce749e-e788-4575-8b45-ca8415424595 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:57:36.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8182" for this suite.

• [SLOW TEST:10.815 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":52,"skipped":954,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:57:36.672: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 12 06:57:49.699: INFO: Successfully updated pod "labelsupdate8430df3c-9e47-4c85-a698-f1c14e1a659b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:57:51.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1645" for this suite.

• [SLOW TEST:15.091 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":53,"skipped":960,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:57:51.763: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0712 06:57:53.081187      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 06:57:53.081205      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 06:57:53.081210      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 06:57:53.081: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:57:53.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6083" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":303,"completed":54,"skipped":976,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:57:53.092: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 12 06:57:54.191: INFO: Waiting up to 5m0s for pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916" in namespace "downward-api-9193" to be "Succeeded or Failed"
Jul 12 06:57:54.285: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Pending", Reason="", readiness=false. Elapsed: 94.023887ms
Jul 12 06:57:56.289: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098182299s
Jul 12 06:57:58.295: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103619733s
Jul 12 06:58:00.298: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Pending", Reason="", readiness=false. Elapsed: 6.106757883s
Jul 12 06:58:02.300: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109461558s
Jul 12 06:58:04.303: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111897889s
STEP: Saw pod success
Jul 12 06:58:04.303: INFO: Pod "downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916" satisfied condition "Succeeded or Failed"
Jul 12 06:58:04.329: INFO: Trying to get logs from node 10.32.0.100 pod downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916 container dapi-container: <nil>
STEP: delete the pod
Jul 12 06:58:04.451: INFO: Waiting for pod downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916 to disappear
Jul 12 06:58:04.478: INFO: Pod downward-api-0acfc310-6fd3-499d-aee5-72988b0b0916 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:58:04.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9193" for this suite.

• [SLOW TEST:11.416 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":303,"completed":55,"skipped":985,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:58:04.509: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:58:05.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7946" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":303,"completed":56,"skipped":993,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:58:05.659: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 12 06:58:05.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2865'
Jul 12 06:58:09.714: INFO: stderr: ""
Jul 12 06:58:09.714: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 12 06:58:19.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pod e2e-test-httpd-pod --namespace=kubectl-2865 -o json'
Jul 12 06:58:19.854: INFO: stderr: ""
Jul 12 06:58:19.854: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"calicoIP\": \"192.168.205.210/26\",\n            \"cni.projectcalico.org/podIP\": \"192.168.205.210/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.205.210/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.205.210\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2021-07-12T06:58:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-12T06:58:09Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:calicoIP\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-12T06:58:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-12T06:58:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.205.210\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-12T06:58:18Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2865\",\n        \"resourceVersion\": \"2811444\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2865/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8fb0e3ed-9600-4c13-ac7c-f2a8fb8305dd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qrvqk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.32.0.100\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qrvqk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qrvqk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T06:58:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T06:58:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T06:58:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T06:58:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://61147d527a880517926f9fa33fc92db9e64688b85704474b4b13ce1e12bcfc58\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-12T06:58:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.32.0.100\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.205.210\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.205.210\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-12T06:58:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 12 06:58:19.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 replace -f - --namespace=kubectl-2865'
Jul 12 06:58:20.398: INFO: stderr: ""
Jul 12 06:58:20.398: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Jul 12 06:58:20.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete pods e2e-test-httpd-pod --namespace=kubectl-2865'
Jul 12 06:58:25.715: INFO: stderr: ""
Jul 12 06:58:25.715: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:58:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2865" for this suite.

• [SLOW TEST:20.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":303,"completed":57,"skipped":994,"failed":0}
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:58:25.729: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 12 06:58:25.865: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:58:40.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5125" for this suite.

• [SLOW TEST:14.818 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":303,"completed":58,"skipped":994,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:58:40.547: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a983f5fe-e9af-417b-9d9c-0ec64e84131f
STEP: Creating a pod to test consume configMaps
Jul 12 06:58:40.864: INFO: Waiting up to 5m0s for pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee" in namespace "configmap-2924" to be "Succeeded or Failed"
Jul 12 06:58:40.867: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155196ms
Jul 12 06:58:42.871: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006420774s
Jul 12 06:58:44.875: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010236673s
Jul 12 06:58:46.919: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054124219s
Jul 12 06:58:48.933: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068075818s
Jul 12 06:58:50.941: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076549136s
Jul 12 06:58:52.945: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.080450219s
STEP: Saw pod success
Jul 12 06:58:52.945: INFO: Pod "pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee" satisfied condition "Succeeded or Failed"
Jul 12 06:58:52.948: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 06:58:53.059: INFO: Waiting for pod pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee to disappear
Jul 12 06:58:53.069: INFO: Pod pod-configmaps-53952213-f4d7-45b4-89b5-9173087cafee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:58:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2924" for this suite.

• [SLOW TEST:12.660 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":59,"skipped":998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:58:53.207: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:58:53.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e" in namespace "downward-api-6886" to be "Succeeded or Failed"
Jul 12 06:58:53.538: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433111ms
Jul 12 06:58:55.542: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00618757s
Jul 12 06:58:57.546: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009698605s
Jul 12 06:58:59.549: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013378225s
Jul 12 06:59:01.653: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116853886s
Jul 12 06:59:03.671: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.135399654s
Jul 12 06:59:05.675: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.13894416s
STEP: Saw pod success
Jul 12 06:59:05.675: INFO: Pod "downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e" satisfied condition "Succeeded or Failed"
Jul 12 06:59:05.684: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e container client-container: <nil>
STEP: delete the pod
Jul 12 06:59:05.784: INFO: Waiting for pod downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e to disappear
Jul 12 06:59:05.787: INFO: Pod downwardapi-volume-0ff53cb7-07de-41c7-bb79-0aa156805a1e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:59:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6886" for this suite.

• [SLOW TEST:12.603 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":60,"skipped":1025,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:59:05.811: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 12 06:59:05.934: INFO: Waiting up to 5m0s for pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942" in namespace "emptydir-7423" to be "Succeeded or Failed"
Jul 12 06:59:05.939: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90865ms
Jul 12 06:59:07.942: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008087859s
Jul 12 06:59:09.997: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062219854s
Jul 12 06:59:12.029: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094665803s
Jul 12 06:59:14.033: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098438407s
Jul 12 06:59:16.036: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102143208s
Jul 12 06:59:18.040: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.105491686s
STEP: Saw pod success
Jul 12 06:59:18.040: INFO: Pod "pod-686ad1c3-c420-4a68-9e6f-345074bf9942" satisfied condition "Succeeded or Failed"
Jul 12 06:59:18.195: INFO: Trying to get logs from node 10.32.0.100 pod pod-686ad1c3-c420-4a68-9e6f-345074bf9942 container test-container: <nil>
STEP: delete the pod
Jul 12 06:59:18.293: INFO: Waiting for pod pod-686ad1c3-c420-4a68-9e6f-345074bf9942 to disappear
Jul 12 06:59:18.296: INFO: Pod pod-686ad1c3-c420-4a68-9e6f-345074bf9942 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:59:18.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7423" for this suite.

• [SLOW TEST:12.631 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":61,"skipped":1044,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:59:18.442: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 12 06:59:43.446: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 06:59:43.500: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 06:59:45.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 06:59:45.543: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 06:59:47.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 06:59:47.504: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:59:47.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-586" for this suite.

• [SLOW TEST:29.104 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":303,"completed":62,"skipped":1055,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:59:47.546: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 12 06:59:47.777: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 12 06:59:52.789: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 06:59:53.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7641" for this suite.

• [SLOW TEST:6.472 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":303,"completed":63,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 06:59:54.019: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 06:59:54.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c" in namespace "projected-4426" to be "Succeeded or Failed"
Jul 12 06:59:54.487: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138475ms
Jul 12 06:59:56.490: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005178212s
Jul 12 06:59:58.561: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076610259s
Jul 12 07:00:00.567: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082690485s
Jul 12 07:00:02.584: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099413498s
Jul 12 07:00:04.590: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105365351s
Jul 12 07:00:06.593: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.108874097s
STEP: Saw pod success
Jul 12 07:00:06.593: INFO: Pod "downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c" satisfied condition "Succeeded or Failed"
Jul 12 07:00:06.598: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c container client-container: <nil>
STEP: delete the pod
Jul 12 07:00:07.099: INFO: Waiting for pod downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c to disappear
Jul 12 07:00:07.134: INFO: Pod downwardapi-volume-82be8fb9-cd06-46db-aa02-f0b39e60423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:00:07.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4426" for this suite.

• [SLOW TEST:13.221 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":64,"skipped":1079,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:00:07.240: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:00:13.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2336" for this suite.
STEP: Destroying namespace "nsdeletetest-6904" for this suite.
Jul 12 07:00:14.004: INFO: Namespace nsdeletetest-6904 was already deleted
STEP: Destroying namespace "nsdeletetest-1541" for this suite.

• [SLOW TEST:6.823 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":303,"completed":65,"skipped":1095,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:00:14.063: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 12 07:00:14.212: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:00:23.597: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:00:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5121" for this suite.

• [SLOW TEST:31.153 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":303,"completed":66,"skipped":1110,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:00:45.217: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 07:00:45.413: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 07:00:45.450: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 07:00:45.452: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.100 before test
Jul 12 07:00:45.468: INFO: csi-rbdplugin-8rsp7 from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.468: INFO: calico-node-mrt5l from kube-system started at 2021-07-12 01:23:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:00:45.468: INFO: coredns-7699c68bdc-nrw9s from kube-system started at 2021-07-12 06:02:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:00:45.468: INFO: cube-logstash-sztvc from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-core-8448fcd4dd-hsvz2 from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container core ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-exporter-5d8b88579-7bpfn from kube-system started at 2021-07-12 06:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-jobservice-5c7d4566ff-lp4hp from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-nginx-86f8877965-s48pm from kube-system started at 2021-07-12 06:03:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-portal-76856b56c7-p6zqk from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:00:45.468: INFO: harbor-app-harbor-registry-6bc47dd67f-rcrrl from kube-system started at 2021-07-12 06:02:49 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:00:45.468: INFO: stolon-app-keeper-0 from kube-system started at 2021-07-12 06:02:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.468: INFO: stolon-app-proxy-59967b544f-8cssc from kube-system started at 2021-07-12 06:02:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.468: INFO: stolon-app-sentinel-5fc7bf8cf8-sfjnr from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.468: INFO: alertmanager-main-2 from monitoring started at 2021-07-12 06:02:32 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:00:45.468: INFO: cubenode-exporter-p2bg8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:00:45.468: INFO: node-exporter-nvbt2 from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:00:45.468: INFO: sonobuoy from sonobuoy started at 2021-07-12 06:23:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 07:00:45.468: INFO: sonobuoy-e2e-job-c08e481580ac4ebf from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container e2e ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:00:45.468: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:00:45.468: INFO: 	Container systemd-logs ready: true, restart count 11
Jul 12 07:00:45.468: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.102 before test
Jul 12 07:00:45.537: INFO: captain-controller-manager-56dd98c4dd-cbspp from captain-system started at 2021-07-09 02:45:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container manager ready: true, restart count 6
Jul 12 07:00:45.537: INFO: csi-rbdplugin-46rrc from ceph-csi started at 2021-07-09 02:46:27 +0000 UTC (3 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.537: INFO: csi-rbdplugin-provisioner-74d4496bf7-22rlt from ceph-csi started at 2021-07-12 06:00:59 +0000 UTC (6 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.537: INFO: csi-rbdplugin-provisioner-74d4496bf7-xzzbm from ceph-csi started at 2021-07-09 02:45:32 +0000 UTC (6 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:00:45.537: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:00:45.537: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:00:45.537: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.537: INFO: alpine-595b7c4b74-xvrn8 from default started at 2021-07-10 02:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container alpine ready: true, restart count 0
Jul 12 07:00:45.537: INFO: dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde from dns-1249 started at 2021-07-09 07:18:28 +0000 UTC (3 container statuses recorded)
Jul 12 07:00:45.537: INFO: 	Container jessie-querier ready: true, restart count 375
Jul 12 07:00:45.537: INFO: 	Container querier ready: true, restart count 393
Jul 12 07:00:45.537: INFO: 	Container webserver ready: true, restart count 0
Jul 12 07:00:45.538: INFO: pod-047c84e7-4461-47fd-ad72-3e49a5479adb from emptydir-6449 started at 2021-07-12 02:54:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container test-container ready: false, restart count 0
Jul 12 07:00:45.538: INFO: ipsection-controllers-6d8496c6bd-vd86c from ipsection-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container ipsection-controllers ready: true, restart count 0
Jul 12 07:00:45.538: INFO: calico-kube-controllers-668c8b44f6-vfp6k from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:00:45.538: INFO: calico-node-x7txv from kube-system started at 2021-07-12 01:23:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:00:45.538: INFO: coredns-7699c68bdc-bzzjr from kube-system started at 2021-07-10 06:56:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-kong-b86d5bf9c-pkln4 from kube-system started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-logging-58f8c874b8-964j2 from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-logstash-xbhqg from kube-system started at 2021-07-09 02:46:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-networking-68749499f7-7vqt4 from kube-system started at 2021-07-12 06:01:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-node-65789b66c-tglb7 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-openapi-v1-658dd7f87b-nhzqv from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-ops-webapp-85c8594b7b-krvgz from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-storage-ceph-access-5f4db88cdd-hh7xq from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-storage-ceph-access ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-storage-f6786ff45-6q6b2 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-ticket-7955cf677f-scx9z from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cube-webapp-57c694b676-pscrq from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:00:45.538: INFO: gpushare-schd-extender-9b5766d8c-xx5n2 from kube-system started at 2021-07-12 06:00:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container gpushare-schd-extender ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-core-8448fcd4dd-cvk8h from kube-system started at 2021-07-09 02:46:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container core ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-exporter-5d8b88579-jsq2l from kube-system started at 2021-07-09 02:46:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-jobservice-5c7d4566ff-6brx2 from kube-system started at 2021-07-09 02:46:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-nginx-86f8877965-8qcgn from kube-system started at 2021-07-09 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-portal-76856b56c7-b2p7w from kube-system started at 2021-07-09 02:46:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:00:45.538: INFO: harbor-app-harbor-registry-6bc47dd67f-dwjmv from kube-system started at 2021-07-09 02:46:39 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:00:45.538: INFO: kubernetes-dashboard-749844f89d-xflsb from kube-system started at 2021-07-12 06:00:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 12 07:00:45.538: INFO: local-path-provisioner-67d77c895b-ptflh from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jul 12 07:00:45.538: INFO: stolon-app-keeper-1 from kube-system started at 2021-07-09 02:46:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.538: INFO: stolon-app-proxy-59967b544f-t57n4 from kube-system started at 2021-07-09 02:46:19 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.538: INFO: stolon-app-sentinel-5fc7bf8cf8-5fkrt from kube-system started at 2021-07-09 02:46:06 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.538: INFO: alertmanager-main-1 from monitoring started at 2021-07-09 02:46:01 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:00:45.538: INFO: cubenode-exporter-dnhsc from monitoring started at 2021-07-09 02:46:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: mysqld-exporter-1-5d6b99c586-bsghg from monitoring started at 2021-07-12 06:01:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: mysqld-exporter-2-5f45bbf595-stzdj from monitoring started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: node-exporter-dgd55 from monitoring started at 2021-07-08 11:02:46 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: prometheus-k8s-0 from monitoring started at 2021-07-09 02:46:08 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container prometheus ready: true, restart count 1
Jul 12 07:00:45.538: INFO: prometheus-operator-6786cb4fc5-t8456 from monitoring started at 2021-07-09 02:45:33 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 12 07:00:45.538: INFO: redis-exporter-1-5877697f69-86xfb from monitoring started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:00:45.538: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.538: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:00:45.538: INFO: 	Container systemd-logs ready: false, restart count 11
Jul 12 07:00:45.538: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.3 before test
Jul 12 07:00:45.594: INFO: csi-rbdplugin-provisioner-74d4496bf7-ch8g6 from ceph-csi started at 2021-07-08 11:06:48 +0000 UTC (6 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container csi-attacher ready: true, restart count 4
Jul 12 07:00:45.594: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:00:45.594: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container csi-resizer ready: true, restart count 6
Jul 12 07:00:45.594: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:00:45.594: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.594: INFO: csi-rbdplugin-zcqsm from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:00:45.594: INFO: calico-kube-controllers-668c8b44f6-7vr9t from kube-system started at 2021-07-08 09:44:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:00:45.594: INFO: calico-kube-controllers-668c8b44f6-vcfkc from kube-system started at 2021-07-08 09:44:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:00:45.594: INFO: calico-node-zmmxk from kube-system started at 2021-07-12 01:24:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:00:45.594: INFO: coredns-7699c68bdc-klbx8 from kube-system started at 2021-07-10 06:57:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-appstore-0 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-appstore ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-ha-rabbitmq-0 from kube-system started at 2021-07-08 11:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container rabbitmq ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-kong-b86d5bf9c-4vtvh from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-logging-58f8c874b8-mjz4v from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-logstash-wfm47 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-networking-68749499f7-rt2dc from kube-system started at 2021-07-08 11:02:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-node-65789b66c-6tmls from kube-system started at 2021-07-09 02:45:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-openapi-v1-658dd7f87b-psc7x from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-ops-webapp-85c8594b7b-b544h from kube-system started at 2021-07-09 02:45:31 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-storage-f6786ff45-7649l from kube-system started at 2021-07-08 11:06:26 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-ticket-7955cf677f-jvn8v from kube-system started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cube-webapp-57c694b676-9qpnc from kube-system started at 2021-07-08 11:02:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-core-8448fcd4dd-h5fdq from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container core ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-exporter-5d8b88579-9wf79 from kube-system started at 2021-07-08 10:36:46 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-jobservice-5c7d4566ff-fsx5c from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-nginx-86f8877965-4rcpv from kube-system started at 2021-07-08 10:36:45 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-portal-76856b56c7-8nczp from kube-system started at 2021-07-08 10:36:44 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:00:45.594: INFO: harbor-app-harbor-registry-6bc47dd67f-7kjrn from kube-system started at 2021-07-08 10:36:43 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:00:45.594: INFO: kube-mini-chartmuseum-55d66cd548-2b8x2 from kube-system started at 2021-07-08 09:45:51 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container kube-mini-chartmuseum ready: true, restart count 0
Jul 12 07:00:45.594: INFO: logdir-admission-webhook-deployment-5b8587f876-dlgl5 from kube-system started at 2021-07-08 11:06:20 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container logdir-admission-webhook ready: true, restart count 0
Jul 12 07:00:45.594: INFO: metrics-server-86d56f4667-s8nqj from kube-system started at 2021-07-08 11:08:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 07:00:45.594: INFO: stolon-app-keeper-2 from kube-system started at 2021-07-08 10:34:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.594: INFO: stolon-app-proxy-59967b544f-ww9rx from kube-system started at 2021-07-08 10:33:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.594: INFO: stolon-app-sentinel-5fc7bf8cf8-grbwv from kube-system started at 2021-07-08 10:33:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:00:45.594: INFO: alertmanager-main-0 from monitoring started at 2021-07-08 11:02:21 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:00:45.594: INFO: cubenode-exporter-dvvm8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: grafana-5cd649d575-q2cdw from monitoring started at 2021-07-08 11:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container grafana ready: true, restart count 0
Jul 12 07:00:45.594: INFO: influxdb-exporter-1-7599cc4587-8f6tn from monitoring started at 2021-07-08 11:08:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container influxdb-exporter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: kube-state-metrics-5d5d4d46c-g5zl8 from monitoring started at 2021-07-08 11:02:42 +0000 UTC (3 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 12 07:00:45.594: INFO: node-exporter-d42br from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: prometheus-adapter-84fbb7d77b-4qbgb from monitoring started at 2021-07-08 11:02:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: redis-exporter-2-6d778f7cf8-6kvb5 from monitoring started at 2021-07-08 11:08:25 +0000 UTC (1 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:00:45.594: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:00:45.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:00:45.594: INFO: 	Container systemd-logs ready: true, restart count 11
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1690f9006a375691], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1690f9006eb64d00], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:00:46.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9528" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":303,"completed":67,"skipped":1125,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:00:46.717: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:00:46.807: INFO: Waiting up to 5m0s for pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab" in namespace "security-context-test-7756" to be "Succeeded or Failed"
Jul 12 07:00:46.894: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 87.633628ms
Jul 12 07:00:49.566: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759431915s
Jul 12 07:00:51.594: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786751359s
Jul 12 07:00:53.596: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78949684s
Jul 12 07:00:55.600: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.792781175s
Jul 12 07:00:57.603: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.796179715s
Jul 12 07:00:59.611: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.803893217s
Jul 12 07:01:01.627: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.819908821s
Jul 12 07:01:01.627: INFO: Pod "busybox-user-65534-117e968a-9dd7-4ab1-b283-b3db59e62dab" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:01:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7756" for this suite.

• [SLOW TEST:14.937 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":68,"skipped":1139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:01:01.655: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:01:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3833" for this suite.
STEP: Destroying namespace "nspatchtest-a8a06fad-7748-4a3c-b285-7d498d282850-9825" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":303,"completed":69,"skipped":1163,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:01:02.933: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:01:07.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2178" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":303,"completed":70,"skipped":1172,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:01:07.783: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:01:08.434: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 12 07:01:10.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:01:12.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:01:14.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:01:16.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:01:18.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670068, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:01:21.584: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:01:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:01:27.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7737" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:20.381 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":303,"completed":71,"skipped":1181,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:01:28.165: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1405
STEP: creating service affinity-clusterip-transition in namespace services-1405
STEP: creating replication controller affinity-clusterip-transition in namespace services-1405
I0712 07:01:29.408803      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1405, replica count: 3
I0712 07:01:32.459189      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:01:35.459453      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:01:38.459606      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:01:41.459921      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:01:44.460023      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:01:44.490: INFO: Creating new exec pod
Jul 12 07:01:57.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1405 execpod-affinity5wjld -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jul 12 07:01:57.775: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 12 07:01:57.775: INFO: stdout: ""
Jul 12 07:01:57.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1405 execpod-affinity5wjld -- /bin/sh -x -c nc -zv -t -w 2 10.254.223.32 80'
Jul 12 07:01:58.017: INFO: stderr: "+ nc -zv -t -w 2 10.254.223.32 80\nConnection to 10.254.223.32 80 port [tcp/http] succeeded!\n"
Jul 12 07:01:58.017: INFO: stdout: ""
Jul 12 07:01:58.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1405 execpod-affinity5wjld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.223.32:80/ ; done'
Jul 12 07:01:58.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n"
Jul 12 07:01:58.379: INFO: stdout: "\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-zp8rr\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-zp8rr\naffinity-clusterip-transition-zp8rr\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-zp8rr\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-7z28p\naffinity-clusterip-transition-7z28p"
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-zp8rr
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-zp8rr
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-zp8rr
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-zp8rr
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.379: INFO: Received response from host: affinity-clusterip-transition-7z28p
Jul 12 07:01:58.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1405 execpod-affinity5wjld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.223.32:80/ ; done'
Jul 12 07:01:58.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.223.32:80/\n"
Jul 12 07:01:58.848: INFO: stdout: "\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl\naffinity-clusterip-transition-r9thl"
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Received response from host: affinity-clusterip-transition-r9thl
Jul 12 07:01:58.848: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1405, will wait for the garbage collector to delete the pods
Jul 12 07:01:59.139: INFO: Deleting ReplicationController affinity-clusterip-transition took: 86.808347ms
Jul 12 07:01:59.339: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.172561ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:02:09.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1405" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:41.522 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":72,"skipped":1189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:02:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 07:02:21.832: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:02:21.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3523" for this suite.

• [SLOW TEST:12.201 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":73,"skipped":1217,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:02:21.888: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 07:02:22.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595" in namespace "projected-7077" to be "Succeeded or Failed"
Jul 12 07:02:22.345: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412087ms
Jul 12 07:02:24.349: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006126285s
Jul 12 07:02:26.353: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009854327s
Jul 12 07:02:28.361: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017883731s
Jul 12 07:02:30.374: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0308559s
Jul 12 07:02:32.393: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050419391s
Jul 12 07:02:34.396: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.053022546s
STEP: Saw pod success
Jul 12 07:02:34.396: INFO: Pod "downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595" satisfied condition "Succeeded or Failed"
Jul 12 07:02:34.399: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595 container client-container: <nil>
STEP: delete the pod
Jul 12 07:02:34.461: INFO: Waiting for pod downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595 to disappear
Jul 12 07:02:34.497: INFO: Pod downwardapi-volume-abd73cd0-5e03-4d11-9131-11868b12e595 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:02:34.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7077" for this suite.

• [SLOW TEST:12.622 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":74,"skipped":1218,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:02:34.510: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c69dbd61-5911-4cc2-a626-41e3a97ca8e3
STEP: Creating a pod to test consume configMaps
Jul 12 07:02:34.731: INFO: Waiting up to 5m0s for pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1" in namespace "configmap-7384" to be "Succeeded or Failed"
Jul 12 07:02:34.777: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.296743ms
Jul 12 07:02:36.780: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049583887s
Jul 12 07:02:38.784: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052913534s
Jul 12 07:02:40.812: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081389638s
Jul 12 07:02:42.873: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.142611487s
Jul 12 07:02:44.877: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.145852987s
Jul 12 07:02:46.881: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.150005841s
STEP: Saw pod success
Jul 12 07:02:46.881: INFO: Pod "pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1" satisfied condition "Succeeded or Failed"
Jul 12 07:02:46.926: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 07:02:46.999: INFO: Waiting for pod pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1 to disappear
Jul 12 07:02:47.002: INFO: Pod pod-configmaps-66b7d627-055f-4d98-ad29-6bf458d410d1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:02:47.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7384" for this suite.

• [SLOW TEST:12.524 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":75,"skipped":1226,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:02:47.034: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-5d7b4cd0-0602-42f9-9183-b1e5cac83f18
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5d7b4cd0-0602-42f9-9183-b1e5cac83f18
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:02:59.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5768" for this suite.

• [SLOW TEST:12.421 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":76,"skipped":1229,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:02:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jul 12 07:02:59.623: INFO: Waiting up to 5m0s for pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb" in namespace "var-expansion-1623" to be "Succeeded or Failed"
Jul 12 07:02:59.626: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552012ms
Jul 12 07:03:01.629: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005877325s
Jul 12 07:03:03.633: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009800642s
Jul 12 07:03:05.642: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018911517s
Jul 12 07:03:07.646: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022514133s
Jul 12 07:03:09.649: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025704249s
Jul 12 07:03:11.764: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.140477156s
STEP: Saw pod success
Jul 12 07:03:11.764: INFO: Pod "var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb" satisfied condition "Succeeded or Failed"
Jul 12 07:03:11.766: INFO: Trying to get logs from node 10.32.0.100 pod var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:03:11.880: INFO: Waiting for pod var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb to disappear
Jul 12 07:03:11.890: INFO: Pod var-expansion-ffa29f99-41b8-4760-b758-192d16fccceb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:03:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1623" for this suite.

• [SLOW TEST:12.476 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":303,"completed":77,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:03:11.932: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 12 07:03:26.569: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2568 PodName:pod-sharedvolume-9760e55c-80e6-431b-81f9-52c367f8a53c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:03:26.569: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:03:26.827: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:03:26.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2568" for this suite.

• [SLOW TEST:15.382 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":303,"completed":78,"skipped":1272,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:03:27.314: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a18d49b4-cf0a-494c-a4ae-a6d0110b47ea
STEP: Creating a pod to test consume configMaps
Jul 12 07:03:28.320: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f" in namespace "projected-8951" to be "Succeeded or Failed"
Jul 12 07:03:28.364: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.024841ms
Jul 12 07:03:30.503: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182578034s
Jul 12 07:03:32.506: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.186160529s
Jul 12 07:03:34.510: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.189885844s
Jul 12 07:03:36.782: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.46141693s
Jul 12 07:03:38.951: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.630449054s
Jul 12 07:03:40.954: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.633678794s
STEP: Saw pod success
Jul 12 07:03:40.954: INFO: Pod "pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f" satisfied condition "Succeeded or Failed"
Jul 12 07:03:40.956: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 07:03:41.280: INFO: Waiting for pod pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f to disappear
Jul 12 07:03:41.288: INFO: Pod pod-projected-configmaps-46a842b0-985e-4bd5-8dc6-070498c6310f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:03:41.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8951" for this suite.

• [SLOW TEST:14.634 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":79,"skipped":1292,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:03:41.949: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 12 07:03:52.290: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6590 PodName:var-expansion-74fb1fdc-8996-4e4f-8683-d2b13ac6c758 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:03:52.290: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: test for file in mounted path
Jul 12 07:03:52.494: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6590 PodName:var-expansion-74fb1fdc-8996-4e4f-8683-d2b13ac6c758 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:03:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: updating the annotation value
Jul 12 07:03:53.185: INFO: Successfully updated pod "var-expansion-74fb1fdc-8996-4e4f-8683-d2b13ac6c758"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 12 07:03:53.188: INFO: Deleting pod "var-expansion-74fb1fdc-8996-4e4f-8683-d2b13ac6c758" in namespace "var-expansion-6590"
Jul 12 07:03:53.277: INFO: Wait up to 5m0s for pod "var-expansion-74fb1fdc-8996-4e4f-8683-d2b13ac6c758" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:04:39.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6590" for this suite.

• [SLOW TEST:57.355 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":303,"completed":80,"skipped":1304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:04:39.304: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8784
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8784
STEP: creating replication controller externalsvc in namespace services-8784
I0712 07:04:40.346229      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8784, replica count: 2
I0712 07:04:43.396559      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:04:46.396708      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:04:49.396952      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:04:52.397116      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 12 07:04:52.514: INFO: Creating new exec pod
Jul 12 07:05:02.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-8784 execpod57khv -- /bin/sh -x -c nslookup nodeport-service.services-8784.svc.cluster.local'
Jul 12 07:05:02.773: INFO: stderr: "+ nslookup nodeport-service.services-8784.svc.cluster.local\n"
Jul 12 07:05:02.773: INFO: stdout: "Server:\t\t10.254.0.2\nAddress:\t10.254.0.2#53\n\nnodeport-service.services-8784.svc.cluster.local\tcanonical name = externalsvc.services-8784.svc.cluster.local.\nName:\texternalsvc.services-8784.svc.cluster.local\nAddress: 10.254.115.216\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8784, will wait for the garbage collector to delete the pods
Jul 12 07:05:02.881: INFO: Deleting ReplicationController externalsvc took: 47.681632ms
Jul 12 07:05:02.981: INFO: Terminating ReplicationController externalsvc pods took: 100.1848ms
Jul 12 07:05:08.734: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:05:08.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8784" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.656 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":303,"completed":81,"skipped":1335,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:05:08.960: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1812.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1812.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1812.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1812.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1812.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1812.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:05:25.189: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.192: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.195: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.197: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.199: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.202: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.204: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.207: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.210: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.213: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1812.svc.cluster.local from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.216: INFO: Unable to read jessie_udp@PodARecord from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.218: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6: the server could not find the requested resource (get pods dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6)
Jul 12 07:05:25.219: INFO: Lookups using dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1812.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1812.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1812.svc.cluster.local jessie_udp@dns-test-service-2.dns-1812.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1812.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 07:05:30.250: INFO: DNS probes using dns-1812/dns-test-e42f02d7-50c8-4dba-9b65-4ab0c123c9d6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:05:30.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1812" for this suite.

• [SLOW TEST:21.943 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":303,"completed":82,"skipped":1346,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:05:30.904: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4423
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-4423
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4423
Jul 12 07:05:31.085: INFO: Found 0 stateful pods, waiting for 1
Jul 12 07:05:41.088: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:05:51.089: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 12 07:05:51.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:05:51.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:05:51.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:05:51.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 07:05:53.061: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 12 07:06:03.136: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 07:06:03.136: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:06:03.157: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:03.157: INFO: ss-0  10.32.0.100  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:03.157: INFO: 
Jul 12 07:06:03.157: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 12 07:06:04.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99766289s
Jul 12 07:06:05.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.517195216s
Jul 12 07:06:06.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.510159458s
Jul 12 07:06:07.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.507261978s
Jul 12 07:06:08.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.451916754s
Jul 12 07:06:09.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.444045815s
Jul 12 07:06:10.720: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.438140989s
Jul 12 07:06:11.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.433190443s
Jul 12 07:06:12.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 428.985764ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4423
Jul 12 07:06:13.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:13.983: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 07:06:13.983: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 07:06:13.983: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 07:06:13.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:14.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 12 07:06:14.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 07:06:14.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 07:06:14.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:14.844: INFO: rc: 1
Jul 12 07:06:14.844: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 12 07:06:24.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:25.096: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 12 07:06:25.096: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 07:06:25.096: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 07:06:25.101: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:06:25.101: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:06:25.101: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 12 07:06:25.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:06:25.344: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:06:25.344: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:06:25.344: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 07:06:25.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:06:25.606: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:06:25.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:06:25.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 07:06:25.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:06:25.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:06:25.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:06:25.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 07:06:25.849: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:06:25.902: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 12 07:06:35.944: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 07:06:35.944: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 07:06:35.944: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 07:06:35.962: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:35.962: INFO: ss-0  10.32.0.100  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:35.962: INFO: ss-1  10.32.0.3    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:35.962: INFO: ss-2  10.32.0.102  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:35.962: INFO: 
Jul 12 07:06:35.962: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 07:06:37.520: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:37.520: INFO: ss-0  10.32.0.100  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:37.520: INFO: ss-1  10.32.0.3    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:37.520: INFO: ss-2  10.32.0.102  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:37.520: INFO: 
Jul 12 07:06:37.520: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 07:06:38.523: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:38.523: INFO: ss-0  10.32.0.100  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:38.523: INFO: ss-1  10.32.0.3    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:38.524: INFO: ss-2  10.32.0.102  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:38.524: INFO: 
Jul 12 07:06:38.524: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 07:06:39.527: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:39.527: INFO: ss-0  10.32.0.100  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:39.527: INFO: ss-1  10.32.0.3    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:39.527: INFO: ss-2  10.32.0.102  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:39.527: INFO: 
Jul 12 07:06:39.527: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 07:06:40.531: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:40.531: INFO: ss-0  10.32.0.100  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:05:31 +0000 UTC  }]
Jul 12 07:06:40.531: INFO: ss-1  10.32.0.3    Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:40.531: INFO: ss-2  10.32.0.102  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:40.531: INFO: 
Jul 12 07:06:40.531: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 07:06:41.534: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 12 07:06:41.534: INFO: ss-1  10.32.0.3    Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:41.534: INFO: ss-2  10.32.0.102  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:41.534: INFO: 
Jul 12 07:06:41.534: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 12 07:06:42.543: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jul 12 07:06:42.543: INFO: ss-1  10.32.0.3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:42.543: INFO: 
Jul 12 07:06:42.543: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 12 07:06:43.547: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jul 12 07:06:43.547: INFO: ss-1  10.32.0.3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:43.547: INFO: 
Jul 12 07:06:43.547: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 12 07:06:44.552: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jul 12 07:06:44.552: INFO: ss-1  10.32.0.3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:44.552: INFO: 
Jul 12 07:06:44.552: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 12 07:06:45.558: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jul 12 07:06:45.558: INFO: ss-1  10.32.0.3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 07:06:03 +0000 UTC  }]
Jul 12 07:06:45.559: INFO: 
Jul 12 07:06:45.559: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4423
Jul 12 07:06:46.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:46.695: INFO: rc: 1
Jul 12 07:06:46.695: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 12 07:06:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:06:56.906: INFO: rc: 1
Jul 12 07:06:56.906: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:06.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:06.992: INFO: rc: 1
Jul 12 07:07:06.992: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:16.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:17.077: INFO: rc: 1
Jul 12 07:07:17.077: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:27.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:27.162: INFO: rc: 1
Jul 12 07:07:27.162: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:37.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:37.244: INFO: rc: 1
Jul 12 07:07:37.244: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:47.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:47.421: INFO: rc: 1
Jul 12 07:07:47.421: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:07:57.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:07:57.510: INFO: rc: 1
Jul 12 07:07:57.510: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:08:07.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:08:09.584: INFO: rc: 1
Jul 12 07:08:09.584: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:08:19.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:08:20.603: INFO: rc: 1
Jul 12 07:08:20.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:08:30.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:08:30.690: INFO: rc: 1
Jul 12 07:08:30.690: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:08:40.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:08:40.781: INFO: rc: 1
Jul 12 07:08:40.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:08:50.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:08:50.898: INFO: rc: 1
Jul 12 07:08:50.898: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:00.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:01.239: INFO: rc: 1
Jul 12 07:09:01.239: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:11.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:11.372: INFO: rc: 1
Jul 12 07:09:11.372: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:21.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:21.461: INFO: rc: 1
Jul 12 07:09:21.461: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:31.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:31.554: INFO: rc: 1
Jul 12 07:09:31.554: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:41.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:41.658: INFO: rc: 1
Jul 12 07:09:41.658: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:09:51.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:09:51.754: INFO: rc: 1
Jul 12 07:09:51.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:01.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:01.854: INFO: rc: 1
Jul 12 07:10:01.854: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:11.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:11.953: INFO: rc: 1
Jul 12 07:10:11.954: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:21.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:22.039: INFO: rc: 1
Jul 12 07:10:22.039: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:32.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:32.130: INFO: rc: 1
Jul 12 07:10:32.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:42.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:42.225: INFO: rc: 1
Jul 12 07:10:42.225: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:10:52.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:10:52.311: INFO: rc: 1
Jul 12 07:10:52.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:02.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:02.400: INFO: rc: 1
Jul 12 07:11:02.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:12.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:12.491: INFO: rc: 1
Jul 12 07:11:12.491: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:22.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:22.740: INFO: rc: 1
Jul 12 07:11:22.740: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:32.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:32.839: INFO: rc: 1
Jul 12 07:11:32.839: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:42.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:42.955: INFO: rc: 1
Jul 12 07:11:42.956: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 12 07:11:52.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-4423 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:11:53.049: INFO: rc: 1
Jul 12 07:11:53.049: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jul 12 07:11:53.049: INFO: Scaling statefulset ss to 0
Jul 12 07:11:53.079: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 07:11:53.083: INFO: Deleting all statefulset in ns statefulset-4423
Jul 12 07:11:53.085: INFO: Scaling statefulset ss to 0
Jul 12 07:11:53.092: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:11:53.094: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:11:53.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4423" for this suite.

• [SLOW TEST:382.264 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":303,"completed":83,"skipped":1352,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:11:53.168: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 12 07:11:53.383: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:11:53.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9536" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":303,"completed":84,"skipped":1355,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:11:53.500: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2519
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 07:11:53.613: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 07:11:53.753: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:11:55.838: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:11:57.769: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:11:59.758: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:12:01.758: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:12:03.760: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:12:05.757: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:12:07.777: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:12:09.757: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:12:11.757: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:12:13.757: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:12:15.758: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:12:17.775: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 07:12:17.781: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:12:19.785: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:12:21.842: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:12:23.784: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:12:25.792: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 07:12:25.797: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 07:12:37.974: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.205.236:8080/dial?request=hostname&protocol=http&host=192.168.205.235&port=8080&tries=1'] Namespace:pod-network-test-2519 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:12:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:12:38.186: INFO: Waiting for responses: map[]
Jul 12 07:12:38.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.205.236:8080/dial?request=hostname&protocol=http&host=192.168.167.166&port=8080&tries=1'] Namespace:pod-network-test-2519 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:12:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:12:38.374: INFO: Waiting for responses: map[]
Jul 12 07:12:38.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.205.236:8080/dial?request=hostname&protocol=http&host=192.168.173.160&port=8080&tries=1'] Namespace:pod-network-test-2519 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:12:38.392: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:12:38.549: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:12:38.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2519" for this suite.

• [SLOW TEST:45.071 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":303,"completed":85,"skipped":1374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:12:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 12 07:13:03.070: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:03.073: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:05.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:05.078: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:07.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:07.078: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:09.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:09.077: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:11.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:11.078: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:13.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:13.079: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:15.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:15.129: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:17.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:17.078: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 07:13:19.073: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 07:13:19.077: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:13:19.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6043" for this suite.

• [SLOW TEST:40.567 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":303,"completed":86,"skipped":1396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:13:19.140: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-6162
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6162 to expose endpoints map[]
Jul 12 07:13:19.401: INFO: successfully validated that service multi-endpoint-test in namespace services-6162 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6162
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6162 to expose endpoints map[pod1:[100]]
Jul 12 07:13:23.487: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Jul 12 07:13:28.487: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Jul 12 07:13:29.491: INFO: successfully validated that service multi-endpoint-test in namespace services-6162 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6162
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6162 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 12 07:13:33.575: INFO: Unexpected endpoints: found map[968b0e84-6673-49f9-8e30-ec21ceace22d:[100]], expected map[pod1:[100] pod2:[101]], will retry
Jul 12 07:13:37.566: INFO: successfully validated that service multi-endpoint-test in namespace services-6162 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-6162
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6162 to expose endpoints map[pod2:[101]]
Jul 12 07:13:38.702: INFO: successfully validated that service multi-endpoint-test in namespace services-6162 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6162
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6162 to expose endpoints map[]
Jul 12 07:13:39.799: INFO: successfully validated that service multi-endpoint-test in namespace services-6162 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:13:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6162" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.881 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":303,"completed":87,"skipped":1419,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:13:40.021: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0712 07:14:20.545256      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 07:14:20.545281      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 07:14:20.545288      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 07:14:20.545: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 12 07:14:20.545: INFO: Deleting pod "simpletest.rc-6j48c" in namespace "gc-6696"
Jul 12 07:14:20.659: INFO: Deleting pod "simpletest.rc-9dg98" in namespace "gc-6696"
Jul 12 07:14:20.911: INFO: Deleting pod "simpletest.rc-9vrsg" in namespace "gc-6696"
Jul 12 07:14:20.984: INFO: Deleting pod "simpletest.rc-bhf45" in namespace "gc-6696"
Jul 12 07:14:22.358: INFO: Deleting pod "simpletest.rc-dkcsb" in namespace "gc-6696"
Jul 12 07:14:22.888: INFO: Deleting pod "simpletest.rc-hdjnf" in namespace "gc-6696"
Jul 12 07:14:23.068: INFO: Deleting pod "simpletest.rc-rkfxv" in namespace "gc-6696"
Jul 12 07:14:23.160: INFO: Deleting pod "simpletest.rc-zb44m" in namespace "gc-6696"
Jul 12 07:14:23.299: INFO: Deleting pod "simpletest.rc-zczrj" in namespace "gc-6696"
Jul 12 07:14:24.168: INFO: Deleting pod "simpletest.rc-zd97t" in namespace "gc-6696"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:14:24.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6696" for this suite.

• [SLOW TEST:45.530 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":303,"completed":88,"skipped":1428,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:14:25.552: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:14:26.127: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 12 07:14:35.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 create -f -'
Jul 12 07:14:38.817: INFO: stderr: ""
Jul 12 07:14:38.818: INFO: stdout: "e2e-test-crd-publish-openapi-6664-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 12 07:14:38.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 delete e2e-test-crd-publish-openapi-6664-crds test-foo'
Jul 12 07:14:38.962: INFO: stderr: ""
Jul 12 07:14:38.962: INFO: stdout: "e2e-test-crd-publish-openapi-6664-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 12 07:14:38.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 apply -f -'
Jul 12 07:14:39.415: INFO: stderr: ""
Jul 12 07:14:39.415: INFO: stdout: "e2e-test-crd-publish-openapi-6664-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 12 07:14:39.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 delete e2e-test-crd-publish-openapi-6664-crds test-foo'
Jul 12 07:14:39.572: INFO: stderr: ""
Jul 12 07:14:39.572: INFO: stdout: "e2e-test-crd-publish-openapi-6664-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 12 07:14:39.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 create -f -'
Jul 12 07:14:40.015: INFO: rc: 1
Jul 12 07:14:40.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 apply -f -'
Jul 12 07:14:40.427: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 12 07:14:40.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 create -f -'
Jul 12 07:14:41.371: INFO: rc: 1
Jul 12 07:14:41.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3282 apply -f -'
Jul 12 07:14:41.768: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 12 07:14:41.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6664-crds'
Jul 12 07:14:42.191: INFO: stderr: ""
Jul 12 07:14:42.191: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6664-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 12 07:14:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6664-crds.metadata'
Jul 12 07:14:42.607: INFO: stderr: ""
Jul 12 07:14:42.607: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6664-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 12 07:14:42.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6664-crds.spec'
Jul 12 07:14:43.011: INFO: stderr: ""
Jul 12 07:14:43.011: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6664-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 12 07:14:43.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6664-crds.spec.bars'
Jul 12 07:14:43.434: INFO: stderr: ""
Jul 12 07:14:43.434: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6664-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 12 07:14:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6664-crds.spec.bars2'
Jul 12 07:14:43.867: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:14:48.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3282" for this suite.

• [SLOW TEST:22.887 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":303,"completed":89,"skipped":1444,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:14:48.439: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-9920/secret-test-a3913d07-ba7b-4b02-9f06-b4d6d81001fa
STEP: Creating a pod to test consume secrets
Jul 12 07:14:48.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198" in namespace "secrets-9920" to be "Succeeded or Failed"
Jul 12 07:14:48.827: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Pending", Reason="", readiness=false. Elapsed: 105.928166ms
Jul 12 07:14:50.830: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109483978s
Jul 12 07:14:52.859: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137874178s
Jul 12 07:14:54.863: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142181245s
Jul 12 07:14:56.972: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Pending", Reason="", readiness=false. Elapsed: 8.251525481s
Jul 12 07:14:58.981: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.260655365s
STEP: Saw pod success
Jul 12 07:14:58.981: INFO: Pod "pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198" satisfied condition "Succeeded or Failed"
Jul 12 07:14:58.998: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198 container env-test: <nil>
STEP: delete the pod
Jul 12 07:14:59.855: INFO: Waiting for pod pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198 to disappear
Jul 12 07:15:00.298: INFO: Pod pod-configmaps-ab446e71-5237-46d9-8ee8-8fdb6a2d2198 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:15:00.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9920" for this suite.

• [SLOW TEST:11.995 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":90,"skipped":1451,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:15:00.434: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:15:00.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}}, CollisionCount:(*int32)(nil)}
Jul 12 07:15:03.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:15:05.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:15:07.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:15:09.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:15:11.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670901, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761670900, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:15:14.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 12 07:15:14.086: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:15:14.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9266" for this suite.
STEP: Destroying namespace "webhook-9266-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.992 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":303,"completed":91,"skipped":1453,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:15:14.426: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 07:15:14.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf" in namespace "downward-api-7183" to be "Succeeded or Failed"
Jul 12 07:15:15.810: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.202652202s
Jul 12 07:15:17.813: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205738001s
Jul 12 07:15:19.852: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.24485559s
Jul 12 07:15:21.856: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.249115827s
Jul 12 07:15:23.859: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.252247945s
Jul 12 07:15:25.863: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.255756144s
Jul 12 07:15:27.866: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.259022534s
STEP: Saw pod success
Jul 12 07:15:27.866: INFO: Pod "downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf" satisfied condition "Succeeded or Failed"
Jul 12 07:15:27.870: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf container client-container: <nil>
STEP: delete the pod
Jul 12 07:15:27.949: INFO: Waiting for pod downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf to disappear
Jul 12 07:15:27.952: INFO: Pod downwardapi-volume-aa022d4e-c790-40a4-8d70-9620089462bf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:15:27.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7183" for this suite.

• [SLOW TEST:13.578 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":92,"skipped":1461,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:15:28.005: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jul 12 07:15:28.119: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-439874929 proxy --unix-socket=/tmp/kubectl-proxy-unix231764320/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:15:28.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5902" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":303,"completed":93,"skipped":1464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:15:28.209: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 12 07:15:28.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-2547'
Jul 12 07:15:28.826: INFO: stderr: ""
Jul 12 07:15:28.826: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 07:15:29.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:29.830: INFO: Found 0 / 1
Jul 12 07:15:30.829: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:30.829: INFO: Found 0 / 1
Jul 12 07:15:31.836: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:31.836: INFO: Found 0 / 1
Jul 12 07:15:32.829: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:32.829: INFO: Found 0 / 1
Jul 12 07:15:33.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:33.830: INFO: Found 0 / 1
Jul 12 07:15:34.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:34.830: INFO: Found 0 / 1
Jul 12 07:15:35.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:35.830: INFO: Found 0 / 1
Jul 12 07:15:36.993: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:36.993: INFO: Found 0 / 1
Jul 12 07:15:37.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:37.830: INFO: Found 0 / 1
Jul 12 07:15:38.858: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:38.858: INFO: Found 1 / 1
Jul 12 07:15:38.858: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 12 07:15:38.892: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:38.893: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 07:15:38.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 patch pod agnhost-primary-zjtfd --namespace=kubectl-2547 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 12 07:15:39.010: INFO: stderr: ""
Jul 12 07:15:39.010: INFO: stdout: "pod/agnhost-primary-zjtfd patched\n"
STEP: checking annotations
Jul 12 07:15:39.012: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 07:15:39.012: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:15:39.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2547" for this suite.

• [SLOW TEST:10.817 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1490
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":303,"completed":94,"skipped":1511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:15:39.027: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-84j8
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 07:15:39.349: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-84j8" in namespace "subpath-4245" to be "Succeeded or Failed"
Jul 12 07:15:39.451: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 101.637474ms
Jul 12 07:15:41.455: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105508675s
Jul 12 07:15:43.489: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140048448s
Jul 12 07:15:45.509: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159132461s
Jul 12 07:15:47.595: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.245230303s
Jul 12 07:15:49.597: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.247973734s
Jul 12 07:15:51.600: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 12.250991649s
Jul 12 07:15:53.604: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 14.254473122s
Jul 12 07:15:55.607: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 16.257117929s
Jul 12 07:15:57.633: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 18.283806975s
Jul 12 07:15:59.654: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 20.304521259s
Jul 12 07:16:01.658: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 22.30823742s
Jul 12 07:16:03.669: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 24.319885337s
Jul 12 07:16:05.673: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 26.323383253s
Jul 12 07:16:07.726: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Running", Reason="", readiness=true. Elapsed: 28.376390882s
Jul 12 07:16:09.730: INFO: Pod "pod-subpath-test-secret-84j8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.380273921s
STEP: Saw pod success
Jul 12 07:16:09.730: INFO: Pod "pod-subpath-test-secret-84j8" satisfied condition "Succeeded or Failed"
Jul 12 07:16:09.732: INFO: Trying to get logs from node 10.32.0.100 pod pod-subpath-test-secret-84j8 container test-container-subpath-secret-84j8: <nil>
STEP: delete the pod
Jul 12 07:16:10.205: INFO: Waiting for pod pod-subpath-test-secret-84j8 to disappear
Jul 12 07:16:10.208: INFO: Pod pod-subpath-test-secret-84j8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-84j8
Jul 12 07:16:10.208: INFO: Deleting pod "pod-subpath-test-secret-84j8" in namespace "subpath-4245"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:16:10.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4245" for this suite.

• [SLOW TEST:31.406 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":303,"completed":95,"skipped":1552,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:16:10.433: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:16:11.001: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:16:16.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2891" for this suite.

• [SLOW TEST:6.331 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":303,"completed":96,"skipped":1573,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:16:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-d6174fde-112a-496c-b912-d9fea62fbd7b in namespace container-probe-2127
Jul 12 07:16:27.051: INFO: Started pod liveness-d6174fde-112a-496c-b912-d9fea62fbd7b in namespace container-probe-2127
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 07:16:27.053: INFO: Initial restart count of pod liveness-d6174fde-112a-496c-b912-d9fea62fbd7b is 0
Jul 12 07:16:47.685: INFO: Restart count of pod container-probe-2127/liveness-d6174fde-112a-496c-b912-d9fea62fbd7b is now 1 (20.631370628s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:16:47.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2127" for this suite.

• [SLOW TEST:30.961 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":97,"skipped":1585,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:16:47.727: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2028.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2028.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2028.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2028.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2028.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2028.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:17:02.436: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200: the server could not find the requested resource (get pods dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200)
Jul 12 07:17:02.439: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200: the server could not find the requested resource (get pods dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200)
Jul 12 07:17:02.447: INFO: Unable to read jessie_udp@PodARecord from pod dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200: the server could not find the requested resource (get pods dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200)
Jul 12 07:17:02.449: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200: the server could not find the requested resource (get pods dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200)
Jul 12 07:17:02.449: INFO: Lookups using dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 07:17:07.551: INFO: DNS probes using dns-2028/dns-test-c6fffcb2-c999-43bf-b2d5-15a8a43e4200 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:17:07.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2028" for this suite.

• [SLOW TEST:19.943 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":303,"completed":98,"skipped":1587,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:17:07.670: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8056.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8056.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8056.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8056.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.163.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.163.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.163.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.163.204_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8056.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8056.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8056.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8056.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8056.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8056.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.163.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.163.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.163.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.163.204_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:17:24.663: INFO: Unable to read wheezy_udp@dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.670: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.677: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.679: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.687: INFO: Unable to read jessie_udp@dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.701: INFO: Unable to read jessie_udp@PodARecord from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.732: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f: the server could not find the requested resource (get pods dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f)
Jul 12 07:17:24.736: INFO: Lookups using dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f failed for: [wheezy_udp@dns-test-service.dns-8056.svc.cluster.local wheezy_tcp@dns-test-service.dns-8056.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-8056.svc.cluster.local jessie_tcp@dns-test-service.dns-8056.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8056.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 07:17:29.796: INFO: DNS probes using dns-8056/dns-test-cc8b2fd3-4329-4f71-b41e-2d4a41483d1f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:17:31.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8056" for this suite.

• [SLOW TEST:24.359 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":303,"completed":99,"skipped":1591,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:17:32.030: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jul 12 07:17:41.400: INFO: 0 pods remaining
Jul 12 07:17:41.400: INFO: 0 pods has nil DeletionTimestamp
Jul 12 07:17:41.400: INFO: 
STEP: Gathering metrics
W0712 07:17:42.307778      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 07:17:42.307798      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 07:17:42.307803      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 07:17:42.307: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:17:42.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-946" for this suite.

• [SLOW TEST:10.529 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":303,"completed":100,"skipped":1600,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:17:42.559: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-23e2272a-9b0c-4d81-b45f-39a45cb8f119
STEP: Creating a pod to test consume secrets
Jul 12 07:17:44.403: INFO: Waiting up to 5m0s for pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40" in namespace "secrets-720" to be "Succeeded or Failed"
Jul 12 07:17:44.407: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005142ms
Jul 12 07:17:47.158: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755065984s
Jul 12 07:17:49.161: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.758198649s
Jul 12 07:17:51.164: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.761652155s
Jul 12 07:17:53.168: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.764959929s
Jul 12 07:17:55.171: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.767834593s
Jul 12 07:17:57.174: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.771518833s
STEP: Saw pod success
Jul 12 07:17:57.174: INFO: Pod "pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40" satisfied condition "Succeeded or Failed"
Jul 12 07:17:57.178: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:17:57.302: INFO: Waiting for pod pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40 to disappear
Jul 12 07:17:57.309: INFO: Pod pod-secrets-e68edd17-46e0-45e7-af69-3633f8877a40 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:17:57.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-720" for this suite.

• [SLOW TEST:14.778 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":101,"skipped":1613,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:17:57.337: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jul 12 07:17:57.641: INFO: Waiting up to 5m0s for pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2" in namespace "containers-7229" to be "Succeeded or Failed"
Jul 12 07:17:57.643: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005357ms
Jul 12 07:17:59.646: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00476096s
Jul 12 07:18:01.650: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008616521s
Jul 12 07:18:03.654: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012772776s
Jul 12 07:18:05.659: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017944122s
Jul 12 07:18:07.663: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022008967s
Jul 12 07:18:09.667: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.026284526s
STEP: Saw pod success
Jul 12 07:18:09.667: INFO: Pod "client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2" satisfied condition "Succeeded or Failed"
Jul 12 07:18:09.671: INFO: Trying to get logs from node 10.32.0.100 pod client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2 container test-container: <nil>
STEP: delete the pod
Jul 12 07:18:09.759: INFO: Waiting for pod client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2 to disappear
Jul 12 07:18:09.762: INFO: Pod client-containers-b3f06878-fa2b-41ca-b316-3b1360bff7d2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:18:09.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7229" for this suite.

• [SLOW TEST:12.439 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":303,"completed":102,"skipped":1619,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:18:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-74a984cd-d477-4408-86ca-e7ae029d5a2d
STEP: Creating a pod to test consume configMaps
Jul 12 07:18:10.062: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976" in namespace "projected-1926" to be "Succeeded or Failed"
Jul 12 07:18:10.212: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 150.120705ms
Jul 12 07:18:12.216: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154253598s
Jul 12 07:18:14.221: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159304315s
Jul 12 07:18:16.226: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163805554s
Jul 12 07:18:18.229: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 8.167103142s
Jul 12 07:18:20.314: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Pending", Reason="", readiness=false. Elapsed: 10.251529909s
Jul 12 07:18:22.317: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.254723527s
STEP: Saw pod success
Jul 12 07:18:22.317: INFO: Pod "pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976" satisfied condition "Succeeded or Failed"
Jul 12 07:18:22.320: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 07:18:22.552: INFO: Waiting for pod pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976 to disappear
Jul 12 07:18:22.555: INFO: Pod pod-projected-configmaps-6792f1cc-4200-4ca8-bbe5-429d0c358976 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:18:22.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1926" for this suite.

• [SLOW TEST:12.788 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":103,"skipped":1629,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:18:22.564: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-a58e7feb-a37a-4a39-aea1-b573b63823e2
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:18:22.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1822" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":303,"completed":104,"skipped":1631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:18:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 12 07:18:22.803: INFO: Waiting up to 5m0s for pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d" in namespace "downward-api-110" to be "Succeeded or Failed"
Jul 12 07:18:22.831: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.879969ms
Jul 12 07:18:24.835: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031126806s
Jul 12 07:18:26.838: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034870219s
Jul 12 07:18:28.842: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038390727s
Jul 12 07:18:30.845: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041355242s
Jul 12 07:18:32.986: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182454544s
Jul 12 07:18:34.989: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.185525468s
STEP: Saw pod success
Jul 12 07:18:34.989: INFO: Pod "downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d" satisfied condition "Succeeded or Failed"
Jul 12 07:18:34.993: INFO: Trying to get logs from node 10.32.0.100 pod downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:18:35.174: INFO: Waiting for pod downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d to disappear
Jul 12 07:18:35.221: INFO: Pod downward-api-5007d651-ba36-4ffd-a30c-e251d60cc71d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:18:35.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-110" for this suite.

• [SLOW TEST:12.551 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":303,"completed":105,"skipped":1663,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:18:35.266: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 12 07:18:48.047: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3890 pod-service-account-0430571e-e7da-4497-9ac0-aaa4013a0f84 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 12 07:18:48.308: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3890 pod-service-account-0430571e-e7da-4497-9ac0-aaa4013a0f84 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 12 07:18:48.569: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3890 pod-service-account-0430571e-e7da-4497-9ac0-aaa4013a0f84 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:18:48.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3890" for this suite.

• [SLOW TEST:13.571 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":303,"completed":106,"skipped":1684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:18:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jul 12 07:18:49.042: INFO: Waiting up to 5m0s for pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92" in namespace "var-expansion-8464" to be "Succeeded or Failed"
Jul 12 07:18:49.089: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 46.642097ms
Jul 12 07:18:51.104: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061765693s
Jul 12 07:18:53.121: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079088195s
Jul 12 07:18:55.140: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0980054s
Jul 12 07:18:57.391: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.349150178s
Jul 12 07:18:59.395: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Pending", Reason="", readiness=false. Elapsed: 10.352696381s
Jul 12 07:19:01.398: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.355808592s
STEP: Saw pod success
Jul 12 07:19:01.398: INFO: Pod "var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92" satisfied condition "Succeeded or Failed"
Jul 12 07:19:01.401: INFO: Trying to get logs from node 10.32.0.100 pod var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92 container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:19:02.011: INFO: Waiting for pod var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92 to disappear
Jul 12 07:19:02.034: INFO: Pod var-expansion-6a8b481e-bf8f-431d-ae62-7c1303582f92 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:19:02.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8464" for this suite.

• [SLOW TEST:13.236 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":303,"completed":107,"skipped":1713,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:19:02.074: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0712 07:19:12.450831      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 07:19:12.450856      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 07:19:12.450865      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 07:19:12.450: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:19:12.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9716" for this suite.

• [SLOW TEST:10.431 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":303,"completed":108,"skipped":1729,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:19:12.505: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:19:12.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5792" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":303,"completed":109,"skipped":1731,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:19:12.967: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-a0892ec9-315b-4de0-82eb-1f7f7da2a4d6
STEP: Creating a pod to test consume secrets
Jul 12 07:19:13.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d" in namespace "projected-8687" to be "Succeeded or Failed"
Jul 12 07:19:13.229: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281869ms
Jul 12 07:19:15.232: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005433142s
Jul 12 07:19:17.276: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049214298s
Jul 12 07:19:19.279: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052083117s
Jul 12 07:19:21.282: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054857096s
Jul 12 07:19:23.285: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.058110729s
Jul 12 07:19:25.293: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.06574936s
STEP: Saw pod success
Jul 12 07:19:25.293: INFO: Pod "pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d" satisfied condition "Succeeded or Failed"
Jul 12 07:19:25.296: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:19:25.401: INFO: Waiting for pod pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d to disappear
Jul 12 07:19:25.403: INFO: Pod pod-projected-secrets-8726a639-7f59-4f49-9ff1-3b038395a59d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:19:25.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8687" for this suite.

• [SLOW TEST:12.446 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":110,"skipped":1735,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:19:25.413: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5951
Jul 12 07:19:29.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 12 07:19:29.966: INFO: rc: 7
Jul 12 07:19:29.986: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:19:29.988: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:19:31.988: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:19:32.034: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:19:33.988: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:19:34.007: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:19:35.988: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:19:35.991: INFO: Pod kube-proxy-mode-detector no longer exists
Jul 12 07:19:35.992: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-5951
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5951
I0712 07:19:36.131622      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5951, replica count: 3
I0712 07:19:39.181974      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:19:42.182224      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:19:45.182425      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:19:48.182643      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:19:51.182895      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:19:54.183087      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:19:54.246: INFO: Creating new exec pod
Jul 12 07:20:07.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jul 12 07:20:07.571: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 12 07:20:07.571: INFO: stdout: ""
Jul 12 07:20:07.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c nc -zv -t -w 2 10.254.188.1 80'
Jul 12 07:20:07.807: INFO: stderr: "+ nc -zv -t -w 2 10.254.188.1 80\nConnection to 10.254.188.1 80 port [tcp/http] succeeded!\n"
Jul 12 07:20:07.807: INFO: stdout: ""
Jul 12 07:20:07.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.3 32203'
Jul 12 07:20:08.046: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.3 32203\nConnection to 10.32.0.3 32203 port [tcp/32203] succeeded!\n"
Jul 12 07:20:08.047: INFO: stdout: ""
Jul 12 07:20:08.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.100 32203'
Jul 12 07:20:08.262: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.100 32203\nConnection to 10.32.0.100 32203 port [tcp/32203] succeeded!\n"
Jul 12 07:20:08.262: INFO: stdout: ""
Jul 12 07:20:08.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.100:32203/ ; done'
Jul 12 07:20:08.596: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n"
Jul 12 07:20:08.596: INFO: stdout: "\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs\naffinity-nodeport-timeout-lrsqs"
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Received response from host: affinity-nodeport-timeout-lrsqs
Jul 12 07:20:08.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.32.0.100:32203/'
Jul 12 07:20:08.887: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n"
Jul 12 07:20:08.887: INFO: stdout: "affinity-nodeport-timeout-lrsqs"
Jul 12 07:20:23.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5951 execpod-affinityzdn22 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.32.0.100:32203/'
Jul 12 07:20:24.117: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.32.0.100:32203/\n"
Jul 12 07:20:24.117: INFO: stdout: "affinity-nodeport-timeout-kgrmx"
Jul 12 07:20:24.117: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5951, will wait for the garbage collector to delete the pods
Jul 12 07:20:24.423: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.93869ms
Jul 12 07:20:24.723: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 300.164129ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:20:39.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5951" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:74.478 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":111,"skipped":1751,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:20:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:20:40.240: INFO: Creating ReplicaSet my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216
Jul 12 07:20:40.303: INFO: Pod name my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216: Found 0 pods out of 1
Jul 12 07:20:45.316: INFO: Pod name my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216: Found 1 pods out of 1
Jul 12 07:20:45.316: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216" is running
Jul 12 07:20:51.341: INFO: Pod "my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216-r268v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:20:40 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:20:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:20:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:20:40 +0000 UTC Reason: Message:}])
Jul 12 07:20:51.342: INFO: Trying to dial the pod
Jul 12 07:20:56.354: INFO: Controller my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216: Got expected result from replica 1 [my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216-r268v]: "my-hostname-basic-076a3f14-43d8-4c69-9730-3973990d4216-r268v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:20:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8196" for this suite.

• [SLOW TEST:16.501 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":112,"skipped":1770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:20:56.393: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jul 12 07:20:56.561: INFO: created test-pod-1
Jul 12 07:20:56.574: INFO: created test-pod-2
Jul 12 07:20:56.608: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:20:58.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2844" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":303,"completed":113,"skipped":1826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:20:58.693: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:20:59.800: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 07:21:02.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:21:04.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:21:06.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:21:08.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:21:10.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:21:12.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671260, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671259, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:21:15.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:21:15.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1775" for this suite.
STEP: Destroying namespace "webhook-1775-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":303,"completed":114,"skipped":1856,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:21:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:21:16.092: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 07:21:25.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-6415 create -f -'
Jul 12 07:21:28.742: INFO: stderr: ""
Jul 12 07:21:28.742: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 12 07:21:28.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-6415 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
Jul 12 07:21:28.925: INFO: stderr: ""
Jul 12 07:21:28.925: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 12 07:21:28.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-6415 apply -f -'
Jul 12 07:21:29.375: INFO: stderr: ""
Jul 12 07:21:29.375: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 12 07:21:29.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-6415 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
Jul 12 07:21:29.484: INFO: stderr: ""
Jul 12 07:21:29.484: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 12 07:21:29.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-5296-crds'
Jul 12 07:21:29.840: INFO: stderr: ""
Jul 12 07:21:29.840: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5296-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:21:34.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6415" for this suite.

• [SLOW TEST:18.535 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":303,"completed":115,"skipped":1876,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:21:34.263: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-405
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 12 07:21:34.507: INFO: Found 0 stateful pods, waiting for 3
Jul 12 07:21:44.515: INFO: Found 1 stateful pods, waiting for 3
Jul 12 07:21:54.510: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:21:54.510: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:21:54.510: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:22:04.707: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:04.707: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:04.707: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:22:14.511: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:14.511: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:14.511: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 12 07:22:14.543: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 12 07:22:24.869: INFO: Updating stateful set ss2
Jul 12 07:22:24.912: INFO: Waiting for Pod statefulset-405/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 12 07:22:35.710: INFO: Found 2 stateful pods, waiting for 3
Jul 12 07:22:45.715: INFO: Found 2 stateful pods, waiting for 3
Jul 12 07:22:55.714: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:55.714: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:22:55.714: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:23:05.714: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:23:05.714: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:23:05.714: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 12 07:23:05.749: INFO: Updating stateful set ss2
Jul 12 07:23:05.817: INFO: Waiting for Pod statefulset-405/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:23:15.824: INFO: Waiting for Pod statefulset-405/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:23:25.874: INFO: Updating stateful set ss2
Jul 12 07:23:25.961: INFO: Waiting for StatefulSet statefulset-405/ss2 to complete update
Jul 12 07:23:25.961: INFO: Waiting for Pod statefulset-405/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:23:35.968: INFO: Waiting for StatefulSet statefulset-405/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 07:23:45.969: INFO: Deleting all statefulset in ns statefulset-405
Jul 12 07:23:45.972: INFO: Scaling statefulset ss2 to 0
Jul 12 07:24:25.993: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:24:25.997: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:24:26.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-405" for this suite.

• [SLOW TEST:171.781 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":303,"completed":116,"skipped":1877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:24:26.046: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:24:27.069: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 07:24:29.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:24:31.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:24:33.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:24:35.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:24:37.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671467, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:24:40.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:24:50.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7902" for this suite.
STEP: Destroying namespace "webhook-7902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:25.120 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":303,"completed":117,"skipped":1953,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:24:51.166: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 12 07:25:06.077: INFO: Successfully updated pod "adopt-release-5nqqx"
STEP: Checking that the Job readopts the Pod
Jul 12 07:25:06.077: INFO: Waiting up to 15m0s for pod "adopt-release-5nqqx" in namespace "job-8711" to be "adopted"
Jul 12 07:25:06.079: INFO: Pod "adopt-release-5nqqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.265145ms
Jul 12 07:25:08.082: INFO: Pod "adopt-release-5nqqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005404113s
Jul 12 07:25:08.082: INFO: Pod "adopt-release-5nqqx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 12 07:25:08.612: INFO: Successfully updated pod "adopt-release-5nqqx"
STEP: Checking that the Job releases the Pod
Jul 12 07:25:08.612: INFO: Waiting up to 15m0s for pod "adopt-release-5nqqx" in namespace "job-8711" to be "released"
Jul 12 07:25:08.614: INFO: Pod "adopt-release-5nqqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.126559ms
Jul 12 07:25:10.617: INFO: Pod "adopt-release-5nqqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.004716657s
Jul 12 07:25:10.617: INFO: Pod "adopt-release-5nqqx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:25:10.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8711" for this suite.

• [SLOW TEST:19.501 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":303,"completed":118,"skipped":1955,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:25:10.668: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jul 12 07:25:10.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f -'
Jul 12 07:25:11.274: INFO: stderr: ""
Jul 12 07:25:11.274: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 12 07:25:11.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 diff -f -'
Jul 12 07:25:12.267: INFO: rc: 1
Jul 12 07:25:12.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete -f -'
Jul 12 07:25:12.440: INFO: stderr: ""
Jul 12 07:25:12.440: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:25:12.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-818" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":303,"completed":119,"skipped":1958,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:25:12.550: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 12 07:25:13.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828308 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:25:13.110: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828309 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:25:13.111: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828313 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 12 07:25:23.293: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828428 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:25:23.293: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828430 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:25:23.303: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9925 /api/v1/namespaces/watch-9925/configmaps/e2e-watch-test-label-changed 9306a905-e812-41b7-8286-b8094e16138b 2828431 0 2021-07-12 07:25:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 07:25:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:25:23.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9925" for this suite.

• [SLOW TEST:10.770 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":303,"completed":120,"skipped":1972,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:25:23.320: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd
Jul 12 07:25:23.560: INFO: Pod name my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd: Found 0 pods out of 1
Jul 12 07:25:28.563: INFO: Pod name my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd: Found 1 pods out of 1
Jul 12 07:25:28.563: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd" are running
Jul 12 07:25:34.569: INFO: Pod "my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd-fk4f9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:25:23 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:25:23 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:25:23 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 07:25:23 +0000 UTC Reason: Message:}])
Jul 12 07:25:34.569: INFO: Trying to dial the pod
Jul 12 07:25:39.616: INFO: Controller my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd: Got expected result from replica 1 [my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd-fk4f9]: "my-hostname-basic-ea93dd7a-b767-4a87-b39e-c9d3522157fd-fk4f9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:25:39.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7581" for this suite.

• [SLOW TEST:16.352 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":121,"skipped":1978,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:25:39.672: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:25:39.853: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 12 07:25:39.865: INFO: Number of nodes with available pods: 0
Jul 12 07:25:39.865: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 12 07:25:40.005: INFO: Number of nodes with available pods: 0
Jul 12 07:25:40.005: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:41.007: INFO: Number of nodes with available pods: 0
Jul 12 07:25:41.007: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:42.008: INFO: Number of nodes with available pods: 0
Jul 12 07:25:42.008: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:43.007: INFO: Number of nodes with available pods: 0
Jul 12 07:25:43.007: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:44.033: INFO: Number of nodes with available pods: 0
Jul 12 07:25:44.033: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:45.008: INFO: Number of nodes with available pods: 0
Jul 12 07:25:45.008: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:46.024: INFO: Number of nodes with available pods: 0
Jul 12 07:25:46.024: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:47.007: INFO: Number of nodes with available pods: 0
Jul 12 07:25:47.007: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:48.022: INFO: Number of nodes with available pods: 0
Jul 12 07:25:48.022: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:49.008: INFO: Number of nodes with available pods: 0
Jul 12 07:25:49.008: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:50.538: INFO: Number of nodes with available pods: 0
Jul 12 07:25:50.538: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:51.021: INFO: Number of nodes with available pods: 0
Jul 12 07:25:51.021: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:52.008: INFO: Number of nodes with available pods: 1
Jul 12 07:25:52.008: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 12 07:25:52.067: INFO: Number of nodes with available pods: 1
Jul 12 07:25:52.067: INFO: Number of running nodes: 0, number of available pods: 1
Jul 12 07:25:53.464: INFO: Number of nodes with available pods: 0
Jul 12 07:25:53.464: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 12 07:25:53.522: INFO: Number of nodes with available pods: 0
Jul 12 07:25:53.522: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:54.526: INFO: Number of nodes with available pods: 0
Jul 12 07:25:54.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:55.526: INFO: Number of nodes with available pods: 0
Jul 12 07:25:55.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:56.526: INFO: Number of nodes with available pods: 0
Jul 12 07:25:56.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:57.526: INFO: Number of nodes with available pods: 0
Jul 12 07:25:57.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:58.565: INFO: Number of nodes with available pods: 0
Jul 12 07:25:58.565: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:25:59.526: INFO: Number of nodes with available pods: 0
Jul 12 07:25:59.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:00.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:00.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:01.525: INFO: Number of nodes with available pods: 0
Jul 12 07:26:01.525: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:02.525: INFO: Number of nodes with available pods: 0
Jul 12 07:26:02.525: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:03.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:03.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:04.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:04.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:05.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:05.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:06.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:06.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:07.526: INFO: Number of nodes with available pods: 0
Jul 12 07:26:07.526: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 07:26:08.526: INFO: Number of nodes with available pods: 1
Jul 12 07:26:08.526: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6084, will wait for the garbage collector to delete the pods
Jul 12 07:26:08.607: INFO: Deleting DaemonSet.extensions daemon-set took: 22.559519ms
Jul 12 07:26:08.707: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.316088ms
Jul 12 07:26:18.611: INFO: Number of nodes with available pods: 0
Jul 12 07:26:18.611: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 07:26:18.613: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6084/daemonsets","resourceVersion":"2828999"},"items":null}

Jul 12 07:26:18.615: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6084/pods","resourceVersion":"2828999"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:26:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6084" for this suite.

• [SLOW TEST:39.402 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":303,"completed":122,"skipped":1979,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:26:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-284
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 07:26:19.205: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 07:26:19.457: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:21.479: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:23.460: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:25.461: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:27.470: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:29.888: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:26:31.460: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:33.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:35.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:37.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:39.490: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:41.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:43.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:26:45.461: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 07:26:45.469: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:26:47.482: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:26:49.473: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 07:26:51.472: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 07:26:51.668: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 07:27:02.329: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.205.215:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:27:02.329: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:27:02.569: INFO: Found all expected endpoints: [netserver-0]
Jul 12 07:27:02.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.168.114:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:27:02.572: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:27:02.738: INFO: Found all expected endpoints: [netserver-1]
Jul 12 07:27:02.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.173.168:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:27:02.741: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:27:02.922: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:27:02.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-284" for this suite.

• [SLOW TEST:43.868 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":123,"skipped":1983,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:27:02.942: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 12 07:27:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:27:31.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5124" for this suite.

• [SLOW TEST:28.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":303,"completed":124,"skipped":1995,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:27:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-shctz in namespace proxy-2205
I0712 07:27:31.980113      24 runners.go:190] Created replication controller with name: proxy-service-shctz, namespace: proxy-2205, replica count: 1
I0712 07:27:33.030453      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:34.030626      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:35.030803      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:36.030988      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:37.031172      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:38.031391      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:39.031633      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:40.032002      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:41.032233      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:42.032473      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:27:43.032703      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0712 07:27:44.032963      24 runners.go:190] proxy-service-shctz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:27:44.036: INFO: setup took 12.140501543s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 12 07:27:44.041: INFO: (0) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.743274ms)
Jul 12 07:27:44.041: INFO: (0) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 4.349214ms)
Jul 12 07:27:44.041: INFO: (0) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 4.181469ms)
Jul 12 07:27:44.041: INFO: (0) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 4.262561ms)
Jul 12 07:27:44.042: INFO: (0) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 4.519419ms)
Jul 12 07:27:44.043: INFO: (0) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 5.939889ms)
Jul 12 07:27:44.043: INFO: (0) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 6.003539ms)
Jul 12 07:27:44.043: INFO: (0) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 6.209286ms)
Jul 12 07:27:44.050: INFO: (0) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 13.761108ms)
Jul 12 07:27:44.054: INFO: (0) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 17.168063ms)
Jul 12 07:27:44.107: INFO: (0) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 69.484722ms)
Jul 12 07:27:44.107: INFO: (0) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 70.163205ms)
Jul 12 07:27:44.107: INFO: (0) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 70.053279ms)
Jul 12 07:27:44.109: INFO: (0) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 71.852111ms)
Jul 12 07:27:44.109: INFO: (0) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 71.855902ms)
Jul 12 07:27:44.113: INFO: (0) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 75.465408ms)
Jul 12 07:27:44.115: INFO: (1) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.668687ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.784669ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.923905ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 3.048519ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.440996ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.475965ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.392543ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.418631ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.423155ms)
Jul 12 07:27:44.116: INFO: (1) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.479108ms)
Jul 12 07:27:44.117: INFO: (1) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.874177ms)
Jul 12 07:27:44.117: INFO: (1) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.870497ms)
Jul 12 07:27:44.117: INFO: (1) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.131733ms)
Jul 12 07:27:44.117: INFO: (1) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.670384ms)
Jul 12 07:27:44.118: INFO: (1) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.74286ms)
Jul 12 07:27:44.118: INFO: (1) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 5.23614ms)
Jul 12 07:27:44.120: INFO: (2) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.247297ms)
Jul 12 07:27:44.120: INFO: (2) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.398235ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.627231ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.305238ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.335936ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.848999ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.406789ms)
Jul 12 07:27:44.121: INFO: (2) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.091846ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.921593ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.633001ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.761474ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.698271ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.365001ms)
Jul 12 07:27:44.122: INFO: (2) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 3.872662ms)
Jul 12 07:27:44.123: INFO: (2) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.795145ms)
Jul 12 07:27:44.123: INFO: (2) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.355429ms)
Jul 12 07:27:44.125: INFO: (3) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.04722ms)
Jul 12 07:27:44.125: INFO: (3) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.219731ms)
Jul 12 07:27:44.125: INFO: (3) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.259097ms)
Jul 12 07:27:44.125: INFO: (3) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.451275ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.643389ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.689829ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.798972ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.994994ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.96393ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.97976ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.598697ms)
Jul 12 07:27:44.126: INFO: (3) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.490118ms)
Jul 12 07:27:44.127: INFO: (3) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.750835ms)
Jul 12 07:27:44.127: INFO: (3) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 3.98055ms)
Jul 12 07:27:44.127: INFO: (3) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.052102ms)
Jul 12 07:27:44.127: INFO: (3) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.173734ms)
Jul 12 07:27:44.129: INFO: (4) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.277277ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.368215ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.314729ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.222618ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.364152ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.81712ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.473653ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.647587ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.951106ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.90686ms)
Jul 12 07:27:44.130: INFO: (4) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 2.783811ms)
Jul 12 07:27:44.131: INFO: (4) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.665529ms)
Jul 12 07:27:44.131: INFO: (4) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.513568ms)
Jul 12 07:27:44.131: INFO: (4) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.719808ms)
Jul 12 07:27:44.132: INFO: (4) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.34115ms)
Jul 12 07:27:44.132: INFO: (4) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.175886ms)
Jul 12 07:27:44.134: INFO: (5) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.254849ms)
Jul 12 07:27:44.134: INFO: (5) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.552251ms)
Jul 12 07:27:44.134: INFO: (5) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.533619ms)
Jul 12 07:27:44.134: INFO: (5) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.594657ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.619307ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.952452ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 3.049921ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.17832ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.356056ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.504224ms)
Jul 12 07:27:44.135: INFO: (5) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.63637ms)
Jul 12 07:27:44.136: INFO: (5) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.975482ms)
Jul 12 07:27:44.136: INFO: (5) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 4.029248ms)
Jul 12 07:27:44.136: INFO: (5) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.526714ms)
Jul 12 07:27:44.137: INFO: (5) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.688949ms)
Jul 12 07:27:44.137: INFO: (5) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 5.316053ms)
Jul 12 07:27:44.140: INFO: (6) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.532543ms)
Jul 12 07:27:44.140: INFO: (6) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.715374ms)
Jul 12 07:27:44.140: INFO: (6) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.584155ms)
Jul 12 07:27:44.140: INFO: (6) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.843837ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.231214ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.399223ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.527708ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.558598ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.9726ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.926785ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 4.000176ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 3.914884ms)
Jul 12 07:27:44.141: INFO: (6) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.95346ms)
Jul 12 07:27:44.142: INFO: (6) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.986109ms)
Jul 12 07:27:44.142: INFO: (6) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.051787ms)
Jul 12 07:27:44.142: INFO: (6) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 4.34354ms)
Jul 12 07:27:44.144: INFO: (7) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.451906ms)
Jul 12 07:27:44.144: INFO: (7) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.244592ms)
Jul 12 07:27:44.145: INFO: (7) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.124691ms)
Jul 12 07:27:44.145: INFO: (7) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.190655ms)
Jul 12 07:27:44.145: INFO: (7) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.428297ms)
Jul 12 07:27:44.145: INFO: (7) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.796746ms)
Jul 12 07:27:44.145: INFO: (7) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.698766ms)
Jul 12 07:27:44.146: INFO: (7) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.342852ms)
Jul 12 07:27:44.146: INFO: (7) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.342955ms)
Jul 12 07:27:44.146: INFO: (7) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.097738ms)
Jul 12 07:27:44.146: INFO: (7) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.295919ms)
Jul 12 07:27:44.146: INFO: (7) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.184128ms)
Jul 12 07:27:44.147: INFO: (7) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.271099ms)
Jul 12 07:27:44.147: INFO: (7) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.821863ms)
Jul 12 07:27:44.147: INFO: (7) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.180503ms)
Jul 12 07:27:44.147: INFO: (7) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.953154ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.433288ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.361945ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.479972ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.494353ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.631643ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.84403ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.004117ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.965368ms)
Jul 12 07:27:44.150: INFO: (8) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.231243ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.378729ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.366175ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 3.637179ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.697634ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.804208ms)
Jul 12 07:27:44.151: INFO: (8) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.106085ms)
Jul 12 07:27:44.152: INFO: (8) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.313322ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.23472ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.023035ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.622952ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.923697ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.748818ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.607251ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.708496ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.194023ms)
Jul 12 07:27:44.155: INFO: (9) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.546324ms)
Jul 12 07:27:44.156: INFO: (9) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.244496ms)
Jul 12 07:27:44.156: INFO: (9) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.543058ms)
Jul 12 07:27:44.156: INFO: (9) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 3.48203ms)
Jul 12 07:27:44.156: INFO: (9) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.54157ms)
Jul 12 07:27:44.157: INFO: (9) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.279327ms)
Jul 12 07:27:44.157: INFO: (9) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 4.994849ms)
Jul 12 07:27:44.157: INFO: (9) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.546159ms)
Jul 12 07:27:44.161: INFO: (10) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.847439ms)
Jul 12 07:27:44.161: INFO: (10) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.82385ms)
Jul 12 07:27:44.161: INFO: (10) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 4.091641ms)
Jul 12 07:27:44.161: INFO: (10) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 4.330205ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 4.370631ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 4.474657ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 4.434689ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 4.768551ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 4.843461ms)
Jul 12 07:27:44.162: INFO: (10) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 4.864524ms)
Jul 12 07:27:44.164: INFO: (10) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 6.793735ms)
Jul 12 07:27:44.164: INFO: (10) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 7.278483ms)
Jul 12 07:27:44.167: INFO: (10) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 10.017095ms)
Jul 12 07:27:44.167: INFO: (10) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 10.11174ms)
Jul 12 07:27:44.167: INFO: (10) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 10.306505ms)
Jul 12 07:27:44.168: INFO: (10) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 11.011138ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.52652ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.461871ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.540615ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.640357ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.809061ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.061582ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.096033ms)
Jul 12 07:27:44.171: INFO: (11) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.189867ms)
Jul 12 07:27:44.172: INFO: (11) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.169669ms)
Jul 12 07:27:44.172: INFO: (11) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.200437ms)
Jul 12 07:27:44.172: INFO: (11) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.670695ms)
Jul 12 07:27:44.172: INFO: (11) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 3.802312ms)
Jul 12 07:27:44.172: INFO: (11) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.121157ms)
Jul 12 07:27:44.173: INFO: (11) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 4.476588ms)
Jul 12 07:27:44.173: INFO: (11) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.586191ms)
Jul 12 07:27:44.173: INFO: (11) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 5.08441ms)
Jul 12 07:27:44.176: INFO: (12) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.222311ms)
Jul 12 07:27:44.176: INFO: (12) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.360045ms)
Jul 12 07:27:44.176: INFO: (12) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.586537ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.698174ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.848051ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.544942ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.221723ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.775902ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.311723ms)
Jul 12 07:27:44.177: INFO: (12) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.498295ms)
Jul 12 07:27:44.178: INFO: (12) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.304226ms)
Jul 12 07:27:44.178: INFO: (12) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 3.878764ms)
Jul 12 07:27:44.178: INFO: (12) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 3.887288ms)
Jul 12 07:27:44.179: INFO: (12) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 5.036762ms)
Jul 12 07:27:44.179: INFO: (12) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.938194ms)
Jul 12 07:27:44.179: INFO: (12) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 5.128332ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.650904ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 3.169513ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.246355ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.464758ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.136845ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.563147ms)
Jul 12 07:27:44.183: INFO: (13) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.402612ms)
Jul 12 07:27:44.184: INFO: (13) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.81096ms)
Jul 12 07:27:44.184: INFO: (13) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 4.159849ms)
Jul 12 07:27:44.184: INFO: (13) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.46727ms)
Jul 12 07:27:44.184: INFO: (13) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 4.11555ms)
Jul 12 07:27:44.184: INFO: (13) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.830516ms)
Jul 12 07:27:44.185: INFO: (13) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.710301ms)
Jul 12 07:27:44.185: INFO: (13) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 5.160373ms)
Jul 12 07:27:44.185: INFO: (13) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.789256ms)
Jul 12 07:27:44.186: INFO: (13) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 5.738984ms)
Jul 12 07:27:44.188: INFO: (14) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.101359ms)
Jul 12 07:27:44.188: INFO: (14) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.17771ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.903113ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.981488ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.900386ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.972703ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.042492ms)
Jul 12 07:27:44.189: INFO: (14) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.261955ms)
Jul 12 07:27:44.190: INFO: (14) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.882309ms)
Jul 12 07:27:44.190: INFO: (14) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.000875ms)
Jul 12 07:27:44.190: INFO: (14) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 4.293742ms)
Jul 12 07:27:44.190: INFO: (14) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 4.457828ms)
Jul 12 07:27:44.191: INFO: (14) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 4.522299ms)
Jul 12 07:27:44.191: INFO: (14) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.528892ms)
Jul 12 07:27:44.191: INFO: (14) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 5.089015ms)
Jul 12 07:27:44.191: INFO: (14) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 5.174703ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.376821ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.445984ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.390563ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.544111ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.727495ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.039712ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.099599ms)
Jul 12 07:27:44.194: INFO: (15) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.818308ms)
Jul 12 07:27:44.195: INFO: (15) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.513195ms)
Jul 12 07:27:44.195: INFO: (15) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 3.373452ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 4.240973ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 4.051987ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.101111ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 4.411059ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 4.484198ms)
Jul 12 07:27:44.196: INFO: (15) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.586351ms)
Jul 12 07:27:44.199: INFO: (16) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.054775ms)
Jul 12 07:27:44.199: INFO: (16) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.166398ms)
Jul 12 07:27:44.199: INFO: (16) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.490646ms)
Jul 12 07:27:44.199: INFO: (16) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 3.016971ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.494815ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 3.211121ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.00324ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.675946ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.234136ms)
Jul 12 07:27:44.200: INFO: (16) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 3.356253ms)
Jul 12 07:27:44.201: INFO: (16) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 3.656754ms)
Jul 12 07:27:44.201: INFO: (16) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 4.254722ms)
Jul 12 07:27:44.201: INFO: (16) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 4.176322ms)
Jul 12 07:27:44.201: INFO: (16) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 4.19598ms)
Jul 12 07:27:44.202: INFO: (16) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 5.011587ms)
Jul 12 07:27:44.203: INFO: (16) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 5.799358ms)
Jul 12 07:27:44.205: INFO: (17) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.369558ms)
Jul 12 07:27:44.205: INFO: (17) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 2.198105ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.18515ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.078208ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 1.945577ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.655585ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 2.548042ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 2.731745ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.303663ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 2.403587ms)
Jul 12 07:27:44.206: INFO: (17) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 3.266413ms)
Jul 12 07:27:44.207: INFO: (17) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 3.353942ms)
Jul 12 07:27:44.207: INFO: (17) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 3.64538ms)
Jul 12 07:27:44.208: INFO: (17) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.991237ms)
Jul 12 07:27:44.208: INFO: (17) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 4.294733ms)
Jul 12 07:27:44.209: INFO: (17) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 5.050267ms)
Jul 12 07:27:44.215: INFO: (18) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 5.841485ms)
Jul 12 07:27:44.215: INFO: (18) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 6.515432ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 6.455688ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 6.587801ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 6.587039ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 6.819983ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 6.88048ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 7.18639ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 7.018703ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 7.043987ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 7.347598ms)
Jul 12 07:27:44.216: INFO: (18) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 7.365615ms)
Jul 12 07:27:44.217: INFO: (18) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 7.644201ms)
Jul 12 07:27:44.217: INFO: (18) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 7.767477ms)
Jul 12 07:27:44.217: INFO: (18) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 7.745325ms)
Jul 12 07:27:44.218: INFO: (18) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 8.581743ms)
Jul 12 07:27:44.220: INFO: (19) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:1080/proxy/rewriteme">... (200; 2.108613ms)
Jul 12 07:27:44.220: INFO: (19) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp/proxy/rewriteme">test</a> (200; 2.29571ms)
Jul 12 07:27:44.220: INFO: (19) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.212661ms)
Jul 12 07:27:44.221: INFO: (19) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.10457ms)
Jul 12 07:27:44.221: INFO: (19) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:160/proxy/: foo (200; 2.668732ms)
Jul 12 07:27:44.221: INFO: (19) /api/v1/namespaces/proxy-2205/pods/http:proxy-service-shctz-q2qpp:162/proxy/: bar (200; 2.272845ms)
Jul 12 07:27:44.221: INFO: (19) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:462/proxy/: tls qux (200; 3.333607ms)
Jul 12 07:27:44.222: INFO: (19) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname2/proxy/: tls qux (200; 3.746719ms)
Jul 12 07:27:44.222: INFO: (19) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:460/proxy/: tls baz (200; 3.763086ms)
Jul 12 07:27:44.222: INFO: (19) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname1/proxy/: foo (200; 3.70408ms)
Jul 12 07:27:44.222: INFO: (19) /api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/https:proxy-service-shctz-q2qpp:443/proxy/tlsrewritem... (200; 3.329977ms)
Jul 12 07:27:44.222: INFO: (19) /api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/: <a href="/api/v1/namespaces/proxy-2205/pods/proxy-service-shctz-q2qpp:1080/proxy/rewriteme">test<... (200; 3.921523ms)
Jul 12 07:27:44.248: INFO: (19) /api/v1/namespaces/proxy-2205/services/proxy-service-shctz:portname2/proxy/: bar (200; 29.672938ms)
Jul 12 07:27:44.248: INFO: (19) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname1/proxy/: foo (200; 29.591285ms)
Jul 12 07:27:44.249: INFO: (19) /api/v1/namespaces/proxy-2205/services/https:proxy-service-shctz:tlsportname1/proxy/: tls baz (200; 30.884682ms)
Jul 12 07:27:44.250: INFO: (19) /api/v1/namespaces/proxy-2205/services/http:proxy-service-shctz:portname2/proxy/: bar (200; 31.548737ms)
STEP: deleting ReplicationController proxy-service-shctz in namespace proxy-2205, will wait for the garbage collector to delete the pods
Jul 12 07:27:44.393: INFO: Deleting ReplicationController proxy-service-shctz took: 41.437742ms
Jul 12 07:27:44.493: INFO: Terminating ReplicationController proxy-service-shctz pods took: 100.138445ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:27:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2205" for this suite.

• [SLOW TEST:26.890 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":303,"completed":125,"skipped":1998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:27:58.652: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:27:58.792: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:28:06.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4617" for this suite.

• [SLOW TEST:8.267 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":303,"completed":126,"skipped":2051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:28:06.920: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-b3294f2e-fc25-48f4-93f2-a213c161e2d6
STEP: Creating a pod to test consume secrets
Jul 12 07:28:07.343: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706" in namespace "projected-9332" to be "Succeeded or Failed"
Jul 12 07:28:07.346: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 2.292322ms
Jul 12 07:28:09.349: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005113348s
Jul 12 07:28:11.351: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008022997s
Jul 12 07:28:13.354: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011006968s
Jul 12 07:28:15.358: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014228789s
Jul 12 07:28:17.360: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016995501s
Jul 12 07:28:19.364: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.020719333s
STEP: Saw pod success
Jul 12 07:28:19.364: INFO: Pod "pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706" satisfied condition "Succeeded or Failed"
Jul 12 07:28:19.423: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:28:19.484: INFO: Waiting for pod pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706 to disappear
Jul 12 07:28:19.487: INFO: Pod pod-projected-secrets-2a995707-4d51-48ab-af7d-8ca3d817e706 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:28:19.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9332" for this suite.

• [SLOW TEST:12.581 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":127,"skipped":2106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:28:19.501: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:28:19.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3823" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":128,"skipped":2137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:28:19.818: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 12 07:28:19.923: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 12 07:28:20.005: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 12 07:28:20.005: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 12 07:28:20.059: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 12 07:28:20.059: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 12 07:28:20.233: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 12 07:28:20.233: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 12 07:28:27.388: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:28:27.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-735" for this suite.

• [SLOW TEST:7.626 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":303,"completed":129,"skipped":2185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:28:27.444: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:28:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:29:29.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8031" for this suite.

• [SLOW TEST:62.209 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":303,"completed":130,"skipped":2216,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:29:29.654: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 12 07:29:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jul 12 07:29:30.386: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 12 07:29:32.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:34.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:36.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:38.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:40.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:42.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671770, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:29:45.688: INFO: Waited 1.115591874s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:29:47.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2893" for this suite.

• [SLOW TEST:17.919 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":303,"completed":131,"skipped":2224,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:29:47.573: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:29:47.763: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 07:29:57.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3768 create -f -'
Jul 12 07:30:00.897: INFO: stderr: ""
Jul 12 07:30:00.898: INFO: stdout: "e2e-test-crd-publish-openapi-6681-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 12 07:30:00.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3768 delete e2e-test-crd-publish-openapi-6681-crds test-cr'
Jul 12 07:30:01.038: INFO: stderr: ""
Jul 12 07:30:01.038: INFO: stdout: "e2e-test-crd-publish-openapi-6681-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 12 07:30:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3768 apply -f -'
Jul 12 07:30:01.579: INFO: stderr: ""
Jul 12 07:30:01.579: INFO: stdout: "e2e-test-crd-publish-openapi-6681-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 12 07:30:01.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-3768 delete e2e-test-crd-publish-openapi-6681-crds test-cr'
Jul 12 07:30:01.711: INFO: stderr: ""
Jul 12 07:30:01.711: INFO: stdout: "e2e-test-crd-publish-openapi-6681-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 12 07:30:01.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-6681-crds'
Jul 12 07:30:02.108: INFO: stderr: ""
Jul 12 07:30:02.108: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6681-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:30:06.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3768" for this suite.

• [SLOW TEST:19.019 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":303,"completed":132,"skipped":2227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:30:06.592: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:30:07.134: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 07:30:09.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:30:11.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:30:13.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:30:15.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:30:17.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671807, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:30:20.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:30:33.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1929" for this suite.
STEP: Destroying namespace "webhook-1929-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:26.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":303,"completed":133,"skipped":2268,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:30:33.437: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:30:43.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5449" for this suite.

• [SLOW TEST:10.394 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":303,"completed":134,"skipped":2279,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:30:43.832: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8663
STEP: creating service affinity-clusterip in namespace services-8663
STEP: creating replication controller affinity-clusterip in namespace services-8663
I0712 07:30:44.071087      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-8663, replica count: 3
I0712 07:30:47.121504      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:30:50.121803      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:30:53.122014      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:30:56.122279      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:30:59.122475      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:30:59.131: INFO: Creating new exec pod
Jul 12 07:31:12.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-8663 execpod-affinityrbrx7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jul 12 07:31:12.510: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 12 07:31:12.510: INFO: stdout: ""
Jul 12 07:31:12.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-8663 execpod-affinityrbrx7 -- /bin/sh -x -c nc -zv -t -w 2 10.254.227.86 80'
Jul 12 07:31:12.768: INFO: stderr: "+ nc -zv -t -w 2 10.254.227.86 80\nConnection to 10.254.227.86 80 port [tcp/http] succeeded!\n"
Jul 12 07:31:12.768: INFO: stdout: ""
Jul 12 07:31:12.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-8663 execpod-affinityrbrx7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.227.86:80/ ; done'
Jul 12 07:31:13.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.227.86:80/\n"
Jul 12 07:31:13.073: INFO: stdout: "\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l\naffinity-clusterip-nx84l"
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Received response from host: affinity-clusterip-nx84l
Jul 12 07:31:13.073: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8663, will wait for the garbage collector to delete the pods
Jul 12 07:31:13.403: INFO: Deleting ReplicationController affinity-clusterip took: 137.573588ms
Jul 12 07:31:14.104: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.189625ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:31:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8663" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:39.394 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":135,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:31:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-5fbf7bd2-b5dc-4ff5-a72f-e771be1b72ab
STEP: Creating a pod to test consume configMaps
Jul 12 07:31:23.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30" in namespace "configmap-6348" to be "Succeeded or Failed"
Jul 12 07:31:23.470: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 36.279576ms
Jul 12 07:31:25.472: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039245696s
Jul 12 07:31:27.475: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041952412s
Jul 12 07:31:29.478: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04514522s
Jul 12 07:31:31.485: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051292146s
Jul 12 07:31:33.487: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Pending", Reason="", readiness=false. Elapsed: 10.054057838s
Jul 12 07:31:35.490: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.056759286s
STEP: Saw pod success
Jul 12 07:31:35.490: INFO: Pod "pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30" satisfied condition "Succeeded or Failed"
Jul 12 07:31:35.610: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 07:31:35.668: INFO: Waiting for pod pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30 to disappear
Jul 12 07:31:35.670: INFO: Pod pod-configmaps-001c025b-d536-4c67-bb71-168c8aef2c30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:31:35.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6348" for this suite.

• [SLOW TEST:12.501 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":136,"skipped":2303,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:31:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 07:31:35.987: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 07:31:36.047: INFO: waiting for watch events with expected annotations
Jul 12 07:31:36.047: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:31:36.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6521" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":303,"completed":137,"skipped":2317,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:31:36.344: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9018 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9018;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9018 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9018;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9018.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9018.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9018.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9018.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9018.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9018.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9018.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9018.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9018.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 139.233.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.233.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.233.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.233.139_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9018 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9018;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9018 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9018;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9018.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9018.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9018.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9018.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9018.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9018.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9018.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9018.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9018.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9018.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 139.233.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.233.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.233.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.233.139_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:31:49.172: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.198: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.201: INFO: Unable to read wheezy_udp@dns-test-service.dns-9018 from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.203: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9018 from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.231: INFO: Unable to read wheezy_udp@dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.234: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.236: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.241: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.260: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.263: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.270: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.273: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.299: INFO: Unable to read jessie_udp@dns-test-service.dns-9018 from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-9018 from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.311: INFO: Unable to read jessie_udp@dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9018.svc from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.328: INFO: Unable to read jessie_udp@PodARecord from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.330: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c: the server could not find the requested resource (get pods dns-test-78263eac-345f-46f5-9673-a07cc927418c)
Jul 12 07:31:49.382: INFO: Lookups using dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9018 wheezy_tcp@dns-test-service.dns-9018 wheezy_udp@dns-test-service.dns-9018.svc wheezy_tcp@dns-test-service.dns-9018.svc wheezy_udp@_http._tcp.dns-test-service.dns-9018.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9018.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9018 jessie_tcp@dns-test-service.dns-9018 jessie_udp@dns-test-service.dns-9018.svc jessie_tcp@dns-test-service.dns-9018.svc jessie_udp@_http._tcp.dns-test-service.dns-9018.svc jessie_tcp@_http._tcp.dns-test-service.dns-9018.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 07:31:54.546: INFO: DNS probes using dns-9018/dns-test-78263eac-345f-46f5-9673-a07cc927418c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:31:55.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9018" for this suite.

• [SLOW TEST:19.427 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":303,"completed":138,"skipped":2337,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:31:55.771: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:31:57.535: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 07:31:59.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:32:02.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:32:03.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:32:05.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:32:07.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:32:09.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671917, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761671916, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:32:12.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:32:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:32:19.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7857" for this suite.
STEP: Destroying namespace "webhook-7857-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:23.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":303,"completed":139,"skipped":2348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:32:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jul 12 07:32:19.540: INFO: Waiting up to 5m0s for pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a" in namespace "var-expansion-5536" to be "Succeeded or Failed"
Jul 12 07:32:19.599: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.956363ms
Jul 12 07:32:21.602: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062453671s
Jul 12 07:32:23.605: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065294253s
Jul 12 07:32:25.608: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068093529s
Jul 12 07:32:27.611: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.071009016s
Jul 12 07:32:29.615: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.074596017s
STEP: Saw pod success
Jul 12 07:32:29.615: INFO: Pod "var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a" satisfied condition "Succeeded or Failed"
Jul 12 07:32:29.617: INFO: Trying to get logs from node 10.32.0.100 pod var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:32:29.871: INFO: Waiting for pod var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a to disappear
Jul 12 07:32:29.879: INFO: Pod var-expansion-009803e7-20ba-4901-9c5c-a8ae3320136a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:32:29.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5536" for this suite.

• [SLOW TEST:10.653 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":303,"completed":140,"skipped":2372,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:32:29.936: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:32:30.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 version'
Jul 12 07:32:30.264: INFO: stderr: ""
Jul 12 07:32:30.264: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"\", Minor:\"\", GitVersion:\"v1.19.4-20210401+5f6a22c5cc915c14a0ec51f07cf1bcb0d86dd53e\", GitCommit:\"$Format:%H$\", GitTreeState:\"\", BuildDate:\"1970-01-01T00:00:00Z\", GoVersion:\"go1.15.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:32:30.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-829" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":303,"completed":141,"skipped":2374,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:32:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-a7edf8c5-382d-40ac-b7fe-ebf703667581
STEP: Creating a pod to test consume secrets
Jul 12 07:32:30.533: INFO: Waiting up to 5m0s for pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03" in namespace "secrets-4007" to be "Succeeded or Failed"
Jul 12 07:32:30.535: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 1.790151ms
Jul 12 07:32:32.538: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295974s
Jul 12 07:32:34.554: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020333245s
Jul 12 07:32:36.580: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047074585s
Jul 12 07:32:38.583: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049809091s
Jul 12 07:32:40.615: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Pending", Reason="", readiness=false. Elapsed: 10.081875413s
Jul 12 07:32:42.618: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.084887378s
STEP: Saw pod success
Jul 12 07:32:42.618: INFO: Pod "pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03" satisfied condition "Succeeded or Failed"
Jul 12 07:32:42.702: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:32:42.859: INFO: Waiting for pod pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03 to disappear
Jul 12 07:32:42.861: INFO: Pod pod-secrets-fb935a52-ff40-47e3-914e-618803bf5d03 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:32:42.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4007" for this suite.

• [SLOW TEST:12.593 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":142,"skipped":2385,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:32:42.877: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 12 07:32:43.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2256'
Jul 12 07:32:43.141: INFO: stderr: ""
Jul 12 07:32:43.141: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jul 12 07:32:43.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete pods e2e-test-httpd-pod --namespace=kubectl-2256'
Jul 12 07:32:47.611: INFO: stderr: ""
Jul 12 07:32:47.611: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:32:47.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2256" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":303,"completed":143,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:32:47.628: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6475
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 07:32:47.763: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 07:32:48.046: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:32:50.152: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:32:52.207: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:32:54.071: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:32:56.071: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:32:58.065: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:00.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:02.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:04.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:06.048: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:08.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:10.048: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:12.073: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:14.074: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:16.857: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 07:33:18.049: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 07:33:18.177: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 07:33:18.182: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 07:33:30.251: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.205.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6475 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:33:30.252: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:33:31.423: INFO: Found all expected endpoints: [netserver-0]
Jul 12 07:33:31.425: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.168.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6475 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:33:31.425: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:33:32.614: INFO: Found all expected endpoints: [netserver-1]
Jul 12 07:33:32.625: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.173.170 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6475 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 07:33:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:33:33.822: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:33:33.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6475" for this suite.

• [SLOW TEST:46.220 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":144,"skipped":2435,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:33:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 12 07:33:34.206: INFO: Waiting up to 5m0s for pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b" in namespace "emptydir-7765" to be "Succeeded or Failed"
Jul 12 07:33:34.209: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094791ms
Jul 12 07:33:36.216: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00954853s
Jul 12 07:33:38.220: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013241138s
Jul 12 07:33:40.223: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016199172s
Jul 12 07:33:42.226: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019431362s
Jul 12 07:33:44.230: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023880263s
Jul 12 07:33:46.463: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.256415522s
Jul 12 07:33:48.466: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.259504962s
STEP: Saw pod success
Jul 12 07:33:48.466: INFO: Pod "pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b" satisfied condition "Succeeded or Failed"
Jul 12 07:33:48.468: INFO: Trying to get logs from node 10.32.0.100 pod pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b container test-container: <nil>
STEP: delete the pod
Jul 12 07:33:48.583: INFO: Waiting for pod pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b to disappear
Jul 12 07:33:48.585: INFO: Pod pod-36c334d0-5213-4ec3-a5aa-b1c78648b33b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:33:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7765" for this suite.

• [SLOW TEST:14.801 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":145,"skipped":2438,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:33:48.649: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-608.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-608.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-608.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-608.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-608.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-608.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:34:03.157: INFO: Unable to read wheezy_udp@PodARecord from pod dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391: the server could not find the requested resource (get pods dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391)
Jul 12 07:34:03.159: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391: the server could not find the requested resource (get pods dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391)
Jul 12 07:34:03.168: INFO: Unable to read jessie_udp@PodARecord from pod dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391: the server could not find the requested resource (get pods dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391)
Jul 12 07:34:03.171: INFO: Unable to read jessie_tcp@PodARecord from pod dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391: the server could not find the requested resource (get pods dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391)
Jul 12 07:34:03.171: INFO: Lookups using dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 12 07:34:08.295: INFO: DNS probes using dns-608/dns-test-cdd7969d-8f8f-4c5b-a188-a46d7567c391 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:34:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-608" for this suite.

• [SLOW TEST:21.669 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":303,"completed":146,"skipped":2438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:34:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-7d33fea4-1942-4bad-9c30-1879e5bdec2e
STEP: Creating configMap with name cm-test-opt-upd-85b25cb1-eb63-41af-96b5-f970f2144c2b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7d33fea4-1942-4bad-9c30-1879e5bdec2e
STEP: Updating configmap cm-test-opt-upd-85b25cb1-eb63-41af-96b5-f970f2144c2b
STEP: Creating configMap with name cm-test-opt-create-e43fd518-0939-4923-826a-f954df7c9115
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:35:39.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8625" for this suite.

• [SLOW TEST:89.591 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":147,"skipped":2464,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:35:39.910: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ce74e024-e596-4e88-a0d5-0a03523102cd
STEP: Creating secret with name s-test-opt-upd-153bd7e4-3102-4e90-a586-997c7773539a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ce74e024-e596-4e88-a0d5-0a03523102cd
STEP: Updating secret s-test-opt-upd-153bd7e4-3102-4e90-a586-997c7773539a
STEP: Creating secret with name s-test-opt-create-d9d5e436-510c-4591-9cb6-f340069f3320
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:37:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7224" for this suite.

• [SLOW TEST:80.158 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":148,"skipped":2470,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:37:00.068: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-895
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-895
Jul 12 07:37:00.390: INFO: Found 0 stateful pods, waiting for 1
Jul 12 07:37:10.393: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:37:20.392: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 07:37:20.515: INFO: Deleting all statefulset in ns statefulset-895
Jul 12 07:37:20.618: INFO: Scaling statefulset ss to 0
Jul 12 07:37:30.755: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:37:30.783: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:37:30.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-895" for this suite.

• [SLOW TEST:30.804 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":303,"completed":149,"skipped":2489,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:37:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 12 07:37:31.083: INFO: Waiting up to 5m0s for pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da" in namespace "emptydir-2235" to be "Succeeded or Failed"
Jul 12 07:37:31.104: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 20.933811ms
Jul 12 07:37:33.107: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023915111s
Jul 12 07:37:35.115: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03202167s
Jul 12 07:37:37.129: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045447479s
Jul 12 07:37:39.133: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050181338s
Jul 12 07:37:41.137: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Pending", Reason="", readiness=false. Elapsed: 10.053306063s
Jul 12 07:37:43.139: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.056017559s
STEP: Saw pod success
Jul 12 07:37:43.139: INFO: Pod "pod-d11c9a73-5f2f-455f-a954-4bd74ed692da" satisfied condition "Succeeded or Failed"
Jul 12 07:37:43.142: INFO: Trying to get logs from node 10.32.0.100 pod pod-d11c9a73-5f2f-455f-a954-4bd74ed692da container test-container: <nil>
STEP: delete the pod
Jul 12 07:37:43.364: INFO: Waiting for pod pod-d11c9a73-5f2f-455f-a954-4bd74ed692da to disappear
Jul 12 07:37:43.366: INFO: Pod pod-d11c9a73-5f2f-455f-a954-4bd74ed692da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:37:43.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2235" for this suite.

• [SLOW TEST:12.523 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":150,"skipped":2492,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:37:43.396: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4532
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4532
STEP: creating replication controller externalsvc in namespace services-4532
I0712 07:37:43.771589      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4532, replica count: 2
I0712 07:37:46.821933      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:37:49.822180      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:37:52.822367      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:37:55.822636      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 12 07:37:55.906: INFO: Creating new exec pod
Jul 12 07:38:07.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4532 execpodgxmkg -- /bin/sh -x -c nslookup clusterip-service.services-4532.svc.cluster.local'
Jul 12 07:38:08.290: INFO: stderr: "+ nslookup clusterip-service.services-4532.svc.cluster.local\n"
Jul 12 07:38:08.290: INFO: stdout: "Server:\t\t10.254.0.2\nAddress:\t10.254.0.2#53\n\nclusterip-service.services-4532.svc.cluster.local\tcanonical name = externalsvc.services-4532.svc.cluster.local.\nName:\texternalsvc.services-4532.svc.cluster.local\nAddress: 10.254.184.243\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4532, will wait for the garbage collector to delete the pods
Jul 12 07:38:08.355: INFO: Deleting ReplicationController externalsvc took: 11.931673ms
Jul 12 07:38:08.455: INFO: Terminating ReplicationController externalsvc pods took: 100.163005ms
Jul 12 07:38:18.613: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:38:18.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4532" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:35.361 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":303,"completed":151,"skipped":2494,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:38:18.757: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 12 07:38:18.998: INFO: Waiting up to 5m0s for pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a" in namespace "downward-api-9665" to be "Succeeded or Failed"
Jul 12 07:38:19.015: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.976654ms
Jul 12 07:38:21.018: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020406437s
Jul 12 07:38:23.697: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699393494s
Jul 12 07:38:25.717: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.718931347s
Jul 12 07:38:27.720: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.721922732s
Jul 12 07:38:29.723: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.725455817s
STEP: Saw pod success
Jul 12 07:38:29.723: INFO: Pod "downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a" satisfied condition "Succeeded or Failed"
Jul 12 07:38:29.726: INFO: Trying to get logs from node 10.32.0.100 pod downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:38:29.843: INFO: Waiting for pod downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a to disappear
Jul 12 07:38:29.845: INFO: Pod downward-api-a1bf4cfc-f9fa-47d2-a8a2-26b29688b92a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:38:29.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9665" for this suite.

• [SLOW TEST:11.103 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":303,"completed":152,"skipped":2495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:38:29.860: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-7ee485a6-915e-450d-9982-bac741d2cda8
STEP: Creating a pod to test consume configMaps
Jul 12 07:38:30.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a" in namespace "configmap-3246" to be "Succeeded or Failed"
Jul 12 07:38:30.436: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913342ms
Jul 12 07:38:32.473: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038082272s
Jul 12 07:38:34.486: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050955767s
Jul 12 07:38:36.526: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091182447s
Jul 12 07:38:38.600: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.16584971s
Jul 12 07:38:40.617: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182673717s
Jul 12 07:38:42.621: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.186808987s
STEP: Saw pod success
Jul 12 07:38:42.621: INFO: Pod "pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a" satisfied condition "Succeeded or Failed"
Jul 12 07:38:42.706: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 07:38:43.445: INFO: Waiting for pod pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a to disappear
Jul 12 07:38:43.449: INFO: Pod pod-configmaps-d850cb44-f288-4780-b13c-7023b70a622a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:38:43.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3246" for this suite.

• [SLOW TEST:13.783 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":153,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:38:43.643: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4912
Jul 12 07:38:48.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 12 07:38:48.344: INFO: rc: 7
Jul 12 07:38:48.380: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:49.557: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:38:51.557: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:51.563: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:38:53.557: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:53.581: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:38:55.557: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:55.559: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:38:57.557: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:57.559: INFO: Pod kube-proxy-mode-detector still exists
Jul 12 07:38:59.557: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 07:38:59.560: INFO: Pod kube-proxy-mode-detector no longer exists
Jul 12 07:38:59.560: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-4912
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4912
I0712 07:38:59.667492      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4912, replica count: 3
I0712 07:39:02.717943      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:39:05.718168      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:39:08.718375      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:39:11.718544      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:39:14.718772      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:39:14.747: INFO: Creating new exec pod
Jul 12 07:39:25.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 execpod-affinitygbp65 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 12 07:39:26.081: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 12 07:39:26.081: INFO: stdout: ""
Jul 12 07:39:26.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 execpod-affinitygbp65 -- /bin/sh -x -c nc -zv -t -w 2 10.254.93.61 80'
Jul 12 07:39:26.335: INFO: stderr: "+ nc -zv -t -w 2 10.254.93.61 80\nConnection to 10.254.93.61 80 port [tcp/http] succeeded!\n"
Jul 12 07:39:26.335: INFO: stdout: ""
Jul 12 07:39:26.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 execpod-affinitygbp65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.93.61:80/ ; done'
Jul 12 07:39:26.665: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n"
Jul 12 07:39:26.665: INFO: stdout: "\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485\naffinity-clusterip-timeout-89485"
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Received response from host: affinity-clusterip-timeout-89485
Jul 12 07:39:26.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 execpod-affinitygbp65 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.93.61:80/'
Jul 12 07:39:26.887: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n"
Jul 12 07:39:26.887: INFO: stdout: "affinity-clusterip-timeout-89485"
Jul 12 07:39:41.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-4912 execpod-affinitygbp65 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.93.61:80/'
Jul 12 07:39:42.152: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.93.61:80/\n"
Jul 12 07:39:42.152: INFO: stdout: "affinity-clusterip-timeout-d7r6z"
Jul 12 07:39:42.152: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4912, will wait for the garbage collector to delete the pods
Jul 12 07:39:42.425: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 127.239125ms
Jul 12 07:39:42.625: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.145038ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:39:59.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4912" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:76.226 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":154,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:39:59.870: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:40:00.084: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:40:02.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:40:04.106: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:40:06.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:40:08.098: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 07:40:10.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:12.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:14.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:16.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:18.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:20.482: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:22.092: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:24.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:26.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:28.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:30.091: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:32.087: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = false)
Jul 12 07:40:34.115: INFO: The status of Pod test-webserver-9a67fc13-9cd5-4595-aa56-5e3239dac392 is Running (Ready = true)
Jul 12 07:40:34.118: INFO: Container started at 2021-07-12 07:40:09 +0000 UTC, pod became ready at 2021-07-12 07:40:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:40:34.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-913" for this suite.

• [SLOW TEST:34.271 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":303,"completed":155,"skipped":2603,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:40:34.141: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 12 07:40:47.559: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3bbee87e-27c9-4aca-8773-dcd219265d2d"
Jul 12 07:40:47.559: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3bbee87e-27c9-4aca-8773-dcd219265d2d" in namespace "pods-9897" to be "terminated due to deadline exceeded"
Jul 12 07:40:47.562: INFO: Pod "pod-update-activedeadlineseconds-3bbee87e-27c9-4aca-8773-dcd219265d2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.856986ms
Jul 12 07:40:49.573: INFO: Pod "pod-update-activedeadlineseconds-3bbee87e-27c9-4aca-8773-dcd219265d2d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01375842s
Jul 12 07:40:49.573: INFO: Pod "pod-update-activedeadlineseconds-3bbee87e-27c9-4aca-8773-dcd219265d2d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:40:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9897" for this suite.

• [SLOW TEST:15.451 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":303,"completed":156,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:40:49.593: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:40:51.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6756" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":303,"completed":157,"skipped":2639,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:40:51.652: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 07:41:03.016: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:41:03.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3580" for this suite.

• [SLOW TEST:11.475 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":303,"completed":158,"skipped":2645,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:41:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5352, will wait for the garbage collector to delete the pods
Jul 12 07:41:15.444: INFO: Deleting Job.batch foo took: 19.121225ms
Jul 12 07:41:15.644: INFO: Terminating Job.batch foo pods took: 200.165551ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:41:59.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5352" for this suite.

• [SLOW TEST:55.987 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":303,"completed":159,"skipped":2654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:41:59.114: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:41:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:42:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-972" for this suite.

• [SLOW TEST:6.244 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":303,"completed":160,"skipped":2680,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:42:05.358: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 12 07:42:05.466: INFO: Waiting up to 5m0s for pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd" in namespace "emptydir-2856" to be "Succeeded or Failed"
Jul 12 07:42:05.477: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.792684ms
Jul 12 07:42:07.481: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014086686s
Jul 12 07:42:09.484: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017544268s
Jul 12 07:42:11.516: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049980597s
Jul 12 07:42:13.519: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052676062s
Jul 12 07:42:15.557: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.090088274s
Jul 12 07:42:17.559: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.092773137s
STEP: Saw pod success
Jul 12 07:42:17.559: INFO: Pod "pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd" satisfied condition "Succeeded or Failed"
Jul 12 07:42:17.562: INFO: Trying to get logs from node 10.32.0.100 pod pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd container test-container: <nil>
STEP: delete the pod
Jul 12 07:42:17.700: INFO: Waiting for pod pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd to disappear
Jul 12 07:42:17.718: INFO: Pod pod-1d3fdbb5-38e9-4f02-b0ec-045db8736dbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:42:17.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2856" for this suite.

• [SLOW TEST:12.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":161,"skipped":2696,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:42:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:42:28.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4295" for this suite.

• [SLOW TEST:10.334 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a read only busybox container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":162,"skipped":2703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:42:28.068: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-d054aebd-90cb-4940-9158-eabd92dc3194
STEP: Creating a pod to test consume secrets
Jul 12 07:42:28.306: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0" in namespace "projected-9809" to be "Succeeded or Failed"
Jul 12 07:42:28.316: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36508ms
Jul 12 07:42:30.978: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671994678s
Jul 12 07:42:32.981: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.674887423s
Jul 12 07:42:34.984: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.677950096s
Jul 12 07:42:36.987: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.6809696s
Jul 12 07:42:38.990: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.684053175s
Jul 12 07:42:41.030: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.72357977s
STEP: Saw pod success
Jul 12 07:42:41.030: INFO: Pod "pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0" satisfied condition "Succeeded or Failed"
Jul 12 07:42:41.032: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:42:41.161: INFO: Waiting for pod pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0 to disappear
Jul 12 07:42:41.163: INFO: Pod pod-projected-secrets-dd640842-5522-46fe-86fd-9603b7b3beb0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:42:41.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9809" for this suite.

• [SLOW TEST:13.119 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":163,"skipped":2725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:42:41.188: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:42:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9721" for this suite.

• [SLOW TEST:12.611 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":303,"completed":164,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:42:53.799: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7268
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7268
I0712 07:42:54.207075      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7268, replica count: 2
I0712 07:42:57.257405      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:43:00.257599      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:43:03.257782      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:43:06.258: INFO: Creating new exec pod
I0712 07:43:06.258008      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:43:19.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-7268 execpodh94ks -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 12 07:43:22.411: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 12 07:43:22.411: INFO: stdout: ""
Jul 12 07:43:22.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-7268 execpodh94ks -- /bin/sh -x -c nc -zv -t -w 2 10.254.100.223 80'
Jul 12 07:43:22.654: INFO: stderr: "+ nc -zv -t -w 2 10.254.100.223 80\nConnection to 10.254.100.223 80 port [tcp/http] succeeded!\n"
Jul 12 07:43:22.654: INFO: stdout: ""
Jul 12 07:43:22.654: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:43:22.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7268" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.007 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":303,"completed":165,"skipped":2763,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:43:22.806: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:43:22.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9614" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":303,"completed":166,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:43:22.964: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:43:23.260: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 12 07:43:28.266: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 07:43:36.288: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 12 07:43:36.359: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7064 /apis/apps/v1/namespaces/deployment-7064/deployments/test-cleanup-deployment 6452bbcd-9270-4c9a-9dba-4dbcd41bc62e 2839684 1 2021-07-12 07:43:36 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-07-12 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ac9bb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 12 07:43:36.361: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:43:36.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7064" for this suite.

• [SLOW TEST:13.429 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":303,"completed":167,"skipped":2784,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:43:36.394: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:43:36.965: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 07:43:39.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672616, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:43:41.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672616, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:43:43.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672616, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:43:45.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672616, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:43:47.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672617, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672616, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:43:50.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
Jul 12 07:43:50.144: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:43:50.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4753" for this suite.
STEP: Destroying namespace "webhook-4753-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":303,"completed":168,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:43:50.836: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 07:43:51.133: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 07:43:51.145: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 07:43:51.172: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.100 before test
Jul 12 07:43:51.183: INFO: csi-rbdplugin-8rsp7 from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.183: INFO: calico-node-mrt5l from kube-system started at 2021-07-12 01:23:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:43:51.183: INFO: coredns-7699c68bdc-nrw9s from kube-system started at 2021-07-12 06:02:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:43:51.183: INFO: cube-logstash-sztvc from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-core-8448fcd4dd-hsvz2 from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container core ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-exporter-5d8b88579-7bpfn from kube-system started at 2021-07-12 06:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-jobservice-5c7d4566ff-lp4hp from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-nginx-86f8877965-s48pm from kube-system started at 2021-07-12 06:03:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-portal-76856b56c7-p6zqk from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:43:51.183: INFO: harbor-app-harbor-registry-6bc47dd67f-rcrrl from kube-system started at 2021-07-12 06:02:49 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:43:51.183: INFO: stolon-app-keeper-0 from kube-system started at 2021-07-12 06:02:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.183: INFO: stolon-app-proxy-59967b544f-8cssc from kube-system started at 2021-07-12 06:02:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.183: INFO: stolon-app-sentinel-5fc7bf8cf8-sfjnr from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.183: INFO: alertmanager-main-2 from monitoring started at 2021-07-12 06:02:32 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:43:51.183: INFO: cubenode-exporter-p2bg8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:43:51.183: INFO: node-exporter-nvbt2 from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:43:51.183: INFO: sonobuoy from sonobuoy started at 2021-07-12 06:23:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 07:43:51.183: INFO: sonobuoy-e2e-job-c08e481580ac4ebf from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container e2e ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:43:51.183: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:43:51.183: INFO: 	Container systemd-logs ready: false, restart count 18
Jul 12 07:43:51.183: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.102 before test
Jul 12 07:43:51.204: INFO: captain-controller-manager-56dd98c4dd-cbspp from captain-system started at 2021-07-09 02:45:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container manager ready: true, restart count 6
Jul 12 07:43:51.204: INFO: csi-rbdplugin-46rrc from ceph-csi started at 2021-07-09 02:46:27 +0000 UTC (3 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.204: INFO: csi-rbdplugin-provisioner-74d4496bf7-22rlt from ceph-csi started at 2021-07-12 06:00:59 +0000 UTC (6 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.204: INFO: csi-rbdplugin-provisioner-74d4496bf7-xzzbm from ceph-csi started at 2021-07-09 02:45:32 +0000 UTC (6 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:43:51.204: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:43:51.204: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.204: INFO: alpine-595b7c4b74-xvrn8 from default started at 2021-07-10 02:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container alpine ready: true, restart count 0
Jul 12 07:43:51.204: INFO: dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde from dns-1249 started at 2021-07-09 07:18:28 +0000 UTC (3 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container jessie-querier ready: true, restart count 379
Jul 12 07:43:51.204: INFO: 	Container querier ready: true, restart count 397
Jul 12 07:43:51.204: INFO: 	Container webserver ready: true, restart count 0
Jul 12 07:43:51.204: INFO: pod-047c84e7-4461-47fd-ad72-3e49a5479adb from emptydir-6449 started at 2021-07-12 02:54:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container test-container ready: false, restart count 0
Jul 12 07:43:51.204: INFO: ipsection-controllers-6d8496c6bd-vd86c from ipsection-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container ipsection-controllers ready: true, restart count 0
Jul 12 07:43:51.204: INFO: calico-kube-controllers-668c8b44f6-vfp6k from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:43:51.204: INFO: calico-node-x7txv from kube-system started at 2021-07-12 01:23:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:43:51.204: INFO: coredns-7699c68bdc-bzzjr from kube-system started at 2021-07-10 06:56:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-kong-b86d5bf9c-pkln4 from kube-system started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-logging-58f8c874b8-964j2 from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-logstash-xbhqg from kube-system started at 2021-07-09 02:46:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-networking-68749499f7-7vqt4 from kube-system started at 2021-07-12 06:01:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-node-65789b66c-tglb7 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-openapi-v1-658dd7f87b-nhzqv from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-ops-webapp-85c8594b7b-krvgz from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-storage-ceph-access-5f4db88cdd-hh7xq from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-storage-ceph-access ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-storage-f6786ff45-6q6b2 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-ticket-7955cf677f-scx9z from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cube-webapp-57c694b676-pscrq from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:43:51.204: INFO: gpushare-schd-extender-9b5766d8c-xx5n2 from kube-system started at 2021-07-12 06:00:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container gpushare-schd-extender ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-core-8448fcd4dd-cvk8h from kube-system started at 2021-07-09 02:46:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container core ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-exporter-5d8b88579-jsq2l from kube-system started at 2021-07-09 02:46:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-jobservice-5c7d4566ff-6brx2 from kube-system started at 2021-07-09 02:46:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-nginx-86f8877965-8qcgn from kube-system started at 2021-07-09 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-portal-76856b56c7-b2p7w from kube-system started at 2021-07-09 02:46:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:43:51.204: INFO: harbor-app-harbor-registry-6bc47dd67f-dwjmv from kube-system started at 2021-07-09 02:46:39 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:43:51.204: INFO: kubernetes-dashboard-749844f89d-xflsb from kube-system started at 2021-07-12 06:00:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 12 07:43:51.204: INFO: local-path-provisioner-67d77c895b-ptflh from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jul 12 07:43:51.204: INFO: stolon-app-keeper-1 from kube-system started at 2021-07-09 02:46:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.204: INFO: stolon-app-proxy-59967b544f-t57n4 from kube-system started at 2021-07-09 02:46:19 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.204: INFO: stolon-app-sentinel-5fc7bf8cf8-5fkrt from kube-system started at 2021-07-09 02:46:06 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.204: INFO: alertmanager-main-1 from monitoring started at 2021-07-09 02:46:01 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:43:51.204: INFO: cubenode-exporter-dnhsc from monitoring started at 2021-07-09 02:46:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: mysqld-exporter-1-5d6b99c586-bsghg from monitoring started at 2021-07-12 06:01:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: mysqld-exporter-2-5f45bbf595-stzdj from monitoring started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: node-exporter-dgd55 from monitoring started at 2021-07-08 11:02:46 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: prometheus-k8s-0 from monitoring started at 2021-07-09 02:46:08 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container prometheus ready: true, restart count 1
Jul 12 07:43:51.204: INFO: prometheus-operator-6786cb4fc5-t8456 from monitoring started at 2021-07-09 02:45:33 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 12 07:43:51.204: INFO: redis-exporter-1-5877697f69-86xfb from monitoring started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:43:51.204: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:43:51.204: INFO: 	Container systemd-logs ready: false, restart count 18
Jul 12 07:43:51.204: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.3 before test
Jul 12 07:43:51.255: INFO: csi-rbdplugin-provisioner-74d4496bf7-ch8g6 from ceph-csi started at 2021-07-08 11:06:48 +0000 UTC (6 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container csi-attacher ready: true, restart count 4
Jul 12 07:43:51.255: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:43:51.255: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container csi-resizer ready: true, restart count 6
Jul 12 07:43:51.255: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:43:51.255: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.255: INFO: csi-rbdplugin-zcqsm from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:43:51.255: INFO: calico-kube-controllers-668c8b44f6-7vr9t from kube-system started at 2021-07-08 09:44:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:43:51.255: INFO: calico-kube-controllers-668c8b44f6-vcfkc from kube-system started at 2021-07-08 09:44:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:43:51.255: INFO: calico-node-zmmxk from kube-system started at 2021-07-12 01:24:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:43:51.255: INFO: coredns-7699c68bdc-klbx8 from kube-system started at 2021-07-10 06:57:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-appstore-0 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-appstore ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-ha-rabbitmq-0 from kube-system started at 2021-07-08 11:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container rabbitmq ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-kong-b86d5bf9c-4vtvh from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-logging-58f8c874b8-mjz4v from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-logstash-wfm47 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-networking-68749499f7-rt2dc from kube-system started at 2021-07-08 11:02:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-node-65789b66c-6tmls from kube-system started at 2021-07-09 02:45:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-openapi-v1-658dd7f87b-psc7x from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-ops-webapp-85c8594b7b-b544h from kube-system started at 2021-07-09 02:45:31 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-storage-f6786ff45-7649l from kube-system started at 2021-07-08 11:06:26 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-ticket-7955cf677f-jvn8v from kube-system started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cube-webapp-57c694b676-9qpnc from kube-system started at 2021-07-08 11:02:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-core-8448fcd4dd-h5fdq from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container core ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-exporter-5d8b88579-9wf79 from kube-system started at 2021-07-08 10:36:46 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-jobservice-5c7d4566ff-fsx5c from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-nginx-86f8877965-4rcpv from kube-system started at 2021-07-08 10:36:45 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-portal-76856b56c7-8nczp from kube-system started at 2021-07-08 10:36:44 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:43:51.255: INFO: harbor-app-harbor-registry-6bc47dd67f-7kjrn from kube-system started at 2021-07-08 10:36:43 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:43:51.255: INFO: kube-mini-chartmuseum-55d66cd548-2b8x2 from kube-system started at 2021-07-08 09:45:51 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container kube-mini-chartmuseum ready: true, restart count 0
Jul 12 07:43:51.255: INFO: logdir-admission-webhook-deployment-5b8587f876-dlgl5 from kube-system started at 2021-07-08 11:06:20 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container logdir-admission-webhook ready: true, restart count 0
Jul 12 07:43:51.255: INFO: metrics-server-86d56f4667-s8nqj from kube-system started at 2021-07-08 11:08:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 07:43:51.255: INFO: stolon-app-keeper-2 from kube-system started at 2021-07-08 10:34:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.255: INFO: stolon-app-proxy-59967b544f-ww9rx from kube-system started at 2021-07-08 10:33:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.255: INFO: stolon-app-sentinel-5fc7bf8cf8-grbwv from kube-system started at 2021-07-08 10:33:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:43:51.255: INFO: alertmanager-main-0 from monitoring started at 2021-07-08 11:02:21 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:43:51.255: INFO: cubenode-exporter-dvvm8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: grafana-5cd649d575-q2cdw from monitoring started at 2021-07-08 11:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container grafana ready: true, restart count 0
Jul 12 07:43:51.255: INFO: influxdb-exporter-1-7599cc4587-8f6tn from monitoring started at 2021-07-08 11:08:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container influxdb-exporter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: kube-state-metrics-5d5d4d46c-g5zl8 from monitoring started at 2021-07-08 11:02:42 +0000 UTC (3 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 12 07:43:51.255: INFO: node-exporter-d42br from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: prometheus-adapter-84fbb7d77b-4qbgb from monitoring started at 2021-07-08 11:02:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: redis-exporter-2-6d778f7cf8-6kvb5 from monitoring started at 2021-07-08 11:08:25 +0000 UTC (1 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:43:51.255: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:43:51.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:43:51.255: INFO: 	Container systemd-logs ready: false, restart count 18
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-afa01a67-63ed-47ef-91e6-36b47c70fe9f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-afa01a67-63ed-47ef-91e6-36b47c70fe9f off the node 10.32.0.100
STEP: verifying the node doesn't have the label kubernetes.io/e2e-afa01a67-63ed-47ef-91e6-36b47c70fe9f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:44:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7966" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:25.205 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":303,"completed":169,"skipped":2814,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:44:16.041: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Jul 12 07:44:16.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-2878 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 12 07:44:16.226: INFO: stderr: ""
Jul 12 07:44:16.227: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jul 12 07:44:16.227: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 12 07:44:16.227: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2878" to be "running and ready, or succeeded"
Jul 12 07:44:16.316: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.828853ms
Jul 12 07:44:18.320: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092973305s
Jul 12 07:44:20.323: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09647253s
Jul 12 07:44:22.326: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099602885s
Jul 12 07:44:24.352: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12530724s
Jul 12 07:44:26.356: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.129838504s
Jul 12 07:44:28.364: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 12.137034573s
Jul 12 07:44:28.364: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 12 07:44:28.364: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 12 07:44:28.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878'
Jul 12 07:44:28.489: INFO: stderr: ""
Jul 12 07:44:28.489: INFO: stdout: "I0712 07:44:25.515338       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qsx5 240\nI0712 07:44:25.715474       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/97n 244\nI0712 07:44:25.916083       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rznd 353\nI0712 07:44:26.115469       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8n8 443\nI0712 07:44:26.315446       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/htx 532\nI0712 07:44:26.515490       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/fl8k 563\nI0712 07:44:26.715506       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6q7 330\nI0712 07:44:26.915480       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vjn 415\nI0712 07:44:27.115532       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/wr64 548\nI0712 07:44:27.315469       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/lhm4 211\nI0712 07:44:27.515497       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/r8qg 332\nI0712 07:44:27.715480       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/945 456\nI0712 07:44:27.915461       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/5n2g 546\nI0712 07:44:28.115558       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/rb48 555\nI0712 07:44:28.315456       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/d9bn 420\n"
STEP: limiting log lines
Jul 12 07:44:28.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878 --tail=1'
Jul 12 07:44:28.593: INFO: stderr: ""
Jul 12 07:44:28.593: INFO: stdout: "I0712 07:44:28.515474       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/sbj8 219\n"
Jul 12 07:44:28.593: INFO: got output "I0712 07:44:28.515474       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/sbj8 219\n"
STEP: limiting log bytes
Jul 12 07:44:28.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878 --limit-bytes=1'
Jul 12 07:44:28.756: INFO: stderr: ""
Jul 12 07:44:28.756: INFO: stdout: "I"
Jul 12 07:44:28.756: INFO: got output "I"
STEP: exposing timestamps
Jul 12 07:44:28.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878 --tail=1 --timestamps'
Jul 12 07:44:28.853: INFO: stderr: ""
Jul 12 07:44:28.853: INFO: stdout: "2021-07-12T07:44:28.715555542Z I0712 07:44:28.715454       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/kbtk 500\n"
Jul 12 07:44:28.853: INFO: got output "2021-07-12T07:44:28.715555542Z I0712 07:44:28.715454       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/kbtk 500\n"
STEP: restricting to a time range
Jul 12 07:44:31.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878 --since=1s'
Jul 12 07:44:31.461: INFO: stderr: ""
Jul 12 07:44:31.461: INFO: stdout: "I0712 07:44:30.515480       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/gfnf 294\nI0712 07:44:30.715497       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/srb 266\nI0712 07:44:30.915607       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/d9s 555\nI0712 07:44:31.115534       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/7pf2 451\nI0712 07:44:31.315542       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/qhzb 518\n"
Jul 12 07:44:31.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs logs-generator logs-generator --namespace=kubectl-2878 --since=24h'
Jul 12 07:44:31.553: INFO: stderr: ""
Jul 12 07:44:31.553: INFO: stdout: "I0712 07:44:25.515338       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qsx5 240\nI0712 07:44:25.715474       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/97n 244\nI0712 07:44:25.916083       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rznd 353\nI0712 07:44:26.115469       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8n8 443\nI0712 07:44:26.315446       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/htx 532\nI0712 07:44:26.515490       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/fl8k 563\nI0712 07:44:26.715506       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6q7 330\nI0712 07:44:26.915480       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vjn 415\nI0712 07:44:27.115532       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/wr64 548\nI0712 07:44:27.315469       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/lhm4 211\nI0712 07:44:27.515497       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/r8qg 332\nI0712 07:44:27.715480       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/945 456\nI0712 07:44:27.915461       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/5n2g 546\nI0712 07:44:28.115558       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/rb48 555\nI0712 07:44:28.315456       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/d9bn 420\nI0712 07:44:28.515474       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/sbj8 219\nI0712 07:44:28.715454       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/kbtk 500\nI0712 07:44:28.915450       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/8cdt 293\nI0712 07:44:29.115435       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/phq 526\nI0712 07:44:29.315440       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/r8b 590\nI0712 07:44:29.515443       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/kgq 483\nI0712 07:44:29.715375       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/kmn5 413\nI0712 07:44:29.915511       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/p4l 552\nI0712 07:44:30.115502       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/lwvl 551\nI0712 07:44:30.315467       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/tx8 202\nI0712 07:44:30.515480       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/gfnf 294\nI0712 07:44:30.715497       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/srb 266\nI0712 07:44:30.915607       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/d9s 555\nI0712 07:44:31.115534       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/7pf2 451\nI0712 07:44:31.315542       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/qhzb 518\nI0712 07:44:31.515454       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/xmkr 305\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Jul 12 07:44:31.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete pod logs-generator --namespace=kubectl-2878'
Jul 12 07:44:38.534: INFO: stderr: ""
Jul 12 07:44:38.534: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:44:38.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2878" for this suite.

• [SLOW TEST:22.524 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":303,"completed":170,"skipped":2822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:44:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:44:39.623: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 07:44:41.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:44:43.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:44:45.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:44:47.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:44:49.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672679, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:44:52.731: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
Jul 12 07:44:52.848: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 12 07:45:04.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 attach --namespace=webhook-1340 to-be-attached-pod -i -c=container1'
Jul 12 07:45:05.115: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:45:05.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1340" for this suite.
STEP: Destroying namespace "webhook-1340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:26.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":303,"completed":171,"skipped":2855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:45:05.299: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:45:05.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7321" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":303,"completed":172,"skipped":2885,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:45:05.519: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:45:39.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3346" for this suite.

• [SLOW TEST:34.269 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":303,"completed":173,"skipped":2905,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:45:39.788: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-bd197d3f-905a-4ebf-adcb-e5666ffce9e2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:45:54.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6032" for this suite.

• [SLOW TEST:14.608 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":174,"skipped":2907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:45:54.396: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 07:45:54.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c" in namespace "downward-api-3955" to be "Succeeded or Failed"
Jul 12 07:45:54.578: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242209ms
Jul 12 07:45:56.615: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039153703s
Jul 12 07:45:58.618: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042058812s
Jul 12 07:46:00.621: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045426321s
Jul 12 07:46:02.883: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.307270353s
Jul 12 07:46:04.888: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.312582968s
Jul 12 07:46:06.891: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.315216962s
STEP: Saw pod success
Jul 12 07:46:06.891: INFO: Pod "downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c" satisfied condition "Succeeded or Failed"
Jul 12 07:46:07.556: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c container client-container: <nil>
STEP: delete the pod
Jul 12 07:46:08.559: INFO: Waiting for pod downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c to disappear
Jul 12 07:46:08.743: INFO: Pod downwardapi-volume-1d59de0c-a23b-44d7-8b34-84d1483e415c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:46:08.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3955" for this suite.

• [SLOW TEST:14.405 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":175,"skipped":2985,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:46:08.802: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 12 07:46:31.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 07:46:31.224: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 07:46:33.224: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 07:46:33.227: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 07:46:35.224: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 07:46:35.246: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 07:46:37.224: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 07:46:37.226: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:46:37.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7828" for this suite.

• [SLOW TEST:28.561 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":303,"completed":176,"skipped":2989,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:46:37.363: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:46:38.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}}, CollisionCount:(*int32)(nil)}
Jul 12 07:46:40.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:46:42.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:46:44.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:46:46.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:46:48.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761672798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:46:51.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Jul 12 07:46:51.504: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:46:51.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1043" for this suite.
STEP: Destroying namespace "webhook-1043-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.876 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":303,"completed":177,"skipped":2992,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:46:52.239: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 12 07:46:52.400: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 12 07:47:13.445: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 07:47:22.927: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:47:44.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7444" for this suite.

• [SLOW TEST:52.418 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":303,"completed":178,"skipped":2996,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:47:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 07:47:57.011: INFO: Waiting up to 5m0s for pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e" in namespace "pods-1561" to be "Succeeded or Failed"
Jul 12 07:47:57.014: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779798ms
Jul 12 07:47:59.072: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061063362s
Jul 12 07:48:01.075: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064456062s
Jul 12 07:48:03.080: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069360266s
Jul 12 07:48:05.156: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.144813367s
Jul 12 07:48:07.163: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.152523454s
STEP: Saw pod success
Jul 12 07:48:07.163: INFO: Pod "client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e" satisfied condition "Succeeded or Failed"
Jul 12 07:48:07.172: INFO: Trying to get logs from node 10.32.0.100 pod client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e container env3cont: <nil>
STEP: delete the pod
Jul 12 07:48:07.399: INFO: Waiting for pod client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e to disappear
Jul 12 07:48:07.421: INFO: Pod client-envvars-45a5771f-1603-4a63-ab49-350fd1c49e8e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:48:07.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1561" for this suite.

• [SLOW TEST:22.804 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":303,"completed":179,"skipped":3002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:48:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:48:21.646: INFO: File wheezy_udp@dns-test-service-3.dns-5569.svc.cluster.local from pod  dns-5569/dns-test-fc9d9405-15a1-49e7-9a49-09e8e28056eb contains '' instead of 'foo.example.com.'
Jul 12 07:48:21.648: INFO: Lookups using dns-5569/dns-test-fc9d9405-15a1-49e7-9a49-09e8e28056eb failed for: [wheezy_udp@dns-test-service-3.dns-5569.svc.cluster.local]

Jul 12 07:48:26.661: INFO: DNS probes using dns-test-fc9d9405-15a1-49e7-9a49-09e8e28056eb succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:48:40.778: INFO: DNS probes using dns-test-bae0c3cc-ebe8-4c46-8296-097b823e94dc succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5569.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5569.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 07:48:55.519: INFO: DNS probes using dns-test-f133bea8-0909-451d-875c-61d0365035ea succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:48:55.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5569" for this suite.

• [SLOW TEST:48.401 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":303,"completed":180,"skipped":3030,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:48:55.864: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:49:08.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6324" for this suite.

• [SLOW TEST:12.355 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":303,"completed":181,"skipped":3038,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:49:08.219: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:49:21.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4687" for this suite.

• [SLOW TEST:13.755 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":303,"completed":182,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:49:21.974: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0712 07:49:33.016920      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 07:49:33.016938      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 07:49:33.016943      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 07:49:33.016: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 12 07:49:33.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-crz75" in namespace "gc-9632"
Jul 12 07:49:33.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-g68vf" in namespace "gc-9632"
Jul 12 07:49:33.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbmx" in namespace "gc-9632"
Jul 12 07:49:33.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-ktmd6" in namespace "gc-9632"
Jul 12 07:49:33.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-sjxxj" in namespace "gc-9632"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:49:34.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9632" for this suite.

• [SLOW TEST:13.401 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":303,"completed":183,"skipped":3069,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:49:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-fbqs
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 07:49:36.875: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fbqs" in namespace "subpath-1426" to be "Succeeded or Failed"
Jul 12 07:49:36.947: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 71.701611ms
Jul 12 07:49:38.963: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087803321s
Jul 12 07:49:40.967: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091405435s
Jul 12 07:49:43.123: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247471594s
Jul 12 07:49:45.126: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 8.250670492s
Jul 12 07:49:47.129: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Pending", Reason="", readiness=false. Elapsed: 10.25410789s
Jul 12 07:49:49.133: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 12.25783961s
Jul 12 07:49:51.137: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 14.261490875s
Jul 12 07:49:53.150: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 16.275011157s
Jul 12 07:49:55.154: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 18.278711714s
Jul 12 07:49:57.157: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 20.281699841s
Jul 12 07:49:59.160: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 22.284913269s
Jul 12 07:50:01.321: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 24.446105087s
Jul 12 07:50:03.325: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 26.449670378s
Jul 12 07:50:05.338: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 28.462948926s
Jul 12 07:50:07.341: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Running", Reason="", readiness=true. Elapsed: 30.466288925s
Jul 12 07:50:09.344: INFO: Pod "pod-subpath-test-configmap-fbqs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.46909157s
STEP: Saw pod success
Jul 12 07:50:09.344: INFO: Pod "pod-subpath-test-configmap-fbqs" satisfied condition "Succeeded or Failed"
Jul 12 07:50:09.348: INFO: Trying to get logs from node 10.32.0.100 pod pod-subpath-test-configmap-fbqs container test-container-subpath-configmap-fbqs: <nil>
STEP: delete the pod
Jul 12 07:50:09.414: INFO: Waiting for pod pod-subpath-test-configmap-fbqs to disappear
Jul 12 07:50:09.416: INFO: Pod pod-subpath-test-configmap-fbqs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fbqs
Jul 12 07:50:09.416: INFO: Deleting pod "pod-subpath-test-configmap-fbqs" in namespace "subpath-1426"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:50:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1426" for this suite.

• [SLOW TEST:34.093 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":303,"completed":184,"skipped":3084,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:50:09.469: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:50:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9665" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":303,"completed":185,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:50:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jul 12 07:50:09.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 api-versions'
Jul 12 07:50:10.235: INFO: stderr: ""
Jul 12 07:50:10.235: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.alauda.io/v1alpha1\napp.alauda.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.dahuatech.com/v1\ncrd.projectcalico.org/v1\ndahuatech.com/v1alpha1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nipsection.ipsections/v1\nlocalstorage.dahuatech.com/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:50:10.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8239" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":303,"completed":186,"skipped":3108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:50:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5722
STEP: creating service affinity-nodeport-transition in namespace services-5722
STEP: creating replication controller affinity-nodeport-transition in namespace services-5722
I0712 07:50:10.484521      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5722, replica count: 3
I0712 07:50:13.534851      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:50:16.534957      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:50:19.535100      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:50:22.535265      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 07:50:25.535517      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 07:50:25.577: INFO: Creating new exec pod
Jul 12 07:50:38.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jul 12 07:50:39.035: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 12 07:50:39.035: INFO: stdout: ""
Jul 12 07:50:39.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c nc -zv -t -w 2 10.254.214.189 80'
Jul 12 07:50:39.319: INFO: stderr: "+ nc -zv -t -w 2 10.254.214.189 80\nConnection to 10.254.214.189 80 port [tcp/http] succeeded!\n"
Jul 12 07:50:39.319: INFO: stdout: ""
Jul 12 07:50:39.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.100 30558'
Jul 12 07:50:39.592: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.100 30558\nConnection to 10.32.0.100 30558 port [tcp/30558] succeeded!\n"
Jul 12 07:50:39.592: INFO: stdout: ""
Jul 12 07:50:39.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.3 30558'
Jul 12 07:50:39.853: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.3 30558\nConnection to 10.32.0.3 30558 port [tcp/30558] succeeded!\n"
Jul 12 07:50:39.853: INFO: stdout: ""
Jul 12 07:50:39.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.100:30558/ ; done'
Jul 12 07:50:40.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n"
Jul 12 07:50:40.193: INFO: stdout: "\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-t62c9\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-plhln\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-t62c9\naffinity-nodeport-transition-7pl52"
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-t62c9
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-plhln
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-t62c9
Jul 12 07:50:40.193: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-5722 execpod-affinity26ms8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.100:30558/ ; done'
Jul 12 07:50:40.558: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.100:30558/\n"
Jul 12 07:50:40.558: INFO: stdout: "\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52\naffinity-nodeport-transition-7pl52"
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Received response from host: affinity-nodeport-transition-7pl52
Jul 12 07:50:40.558: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5722, will wait for the garbage collector to delete the pods
Jul 12 07:50:40.811: INFO: Deleting ReplicationController affinity-nodeport-transition took: 41.181063ms
Jul 12 07:50:41.111: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 300.160392ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:50:59.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5722" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:49.482 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":187,"skipped":3133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:50:59.739: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jul 12 07:50:59.888: INFO: created test-podtemplate-1
Jul 12 07:50:59.896: INFO: created test-podtemplate-2
Jul 12 07:50:59.905: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 12 07:50:59.907: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 12 07:51:00.001: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:51:00.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9640" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":303,"completed":188,"skipped":3160,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:51:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:51:00.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 07:51:02.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673060, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:51:04.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673060, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:51:07.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673060, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:51:08.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673061, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673060, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:51:11.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:51:11.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4035" for this suite.
STEP: Destroying namespace "webhook-4035-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":303,"completed":189,"skipped":3181,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:51:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-2tdt
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 07:51:13.130: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2tdt" in namespace "subpath-9783" to be "Succeeded or Failed"
Jul 12 07:51:13.161: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Pending", Reason="", readiness=false. Elapsed: 30.15895ms
Jul 12 07:51:15.272: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.141774319s
Jul 12 07:51:17.430: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299848382s
Jul 12 07:51:19.434: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303235668s
Jul 12 07:51:21.477: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.346456192s
Jul 12 07:51:23.508: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 10.377694931s
Jul 12 07:51:25.511: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 12.380685587s
Jul 12 07:51:27.515: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 14.384835885s
Jul 12 07:51:29.518: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 16.388124596s
Jul 12 07:51:31.538: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 18.40740818s
Jul 12 07:51:33.647: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 20.516482753s
Jul 12 07:51:35.659: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 22.528615845s
Jul 12 07:51:37.661: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 24.531113282s
Jul 12 07:51:39.697: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 26.566304981s
Jul 12 07:51:41.702: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 28.571576455s
Jul 12 07:51:43.705: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 30.574611672s
Jul 12 07:51:45.717: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Running", Reason="", readiness=true. Elapsed: 32.586425234s
Jul 12 07:51:47.726: INFO: Pod "pod-subpath-test-projected-2tdt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.596108407s
STEP: Saw pod success
Jul 12 07:51:47.727: INFO: Pod "pod-subpath-test-projected-2tdt" satisfied condition "Succeeded or Failed"
Jul 12 07:51:47.838: INFO: Trying to get logs from node 10.32.0.100 pod pod-subpath-test-projected-2tdt container test-container-subpath-projected-2tdt: <nil>
STEP: delete the pod
Jul 12 07:51:47.928: INFO: Waiting for pod pod-subpath-test-projected-2tdt to disappear
Jul 12 07:51:47.930: INFO: Pod pod-subpath-test-projected-2tdt no longer exists
STEP: Deleting pod pod-subpath-test-projected-2tdt
Jul 12 07:51:47.930: INFO: Deleting pod "pod-subpath-test-projected-2tdt" in namespace "subpath-9783"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:51:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9783" for this suite.

• [SLOW TEST:35.446 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":303,"completed":190,"skipped":3186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:51:48.036: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-0395d0a8-4369-4a9b-98a8-d632fcf56689
STEP: Creating a pod to test consume secrets
Jul 12 07:51:48.251: INFO: Waiting up to 5m0s for pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b" in namespace "secrets-3532" to be "Succeeded or Failed"
Jul 12 07:51:48.254: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890627ms
Jul 12 07:51:50.470: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21922222s
Jul 12 07:51:52.474: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.223373926s
Jul 12 07:51:54.477: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.226574433s
Jul 12 07:51:56.480: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.229618182s
Jul 12 07:51:58.485: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.234266148s
Jul 12 07:52:00.489: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.23796977s
STEP: Saw pod success
Jul 12 07:52:00.489: INFO: Pod "pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b" satisfied condition "Succeeded or Failed"
Jul 12 07:52:00.492: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 07:52:00.596: INFO: Waiting for pod pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b to disappear
Jul 12 07:52:00.613: INFO: Pod pod-secrets-a76de54b-73ce-41eb-bdd3-451348477a1b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:52:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3532" for this suite.

• [SLOW TEST:12.610 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":191,"skipped":3223,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:52:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9416
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 12 07:52:01.255: INFO: Found 0 stateful pods, waiting for 3
Jul 12 07:52:11.273: INFO: Found 1 stateful pods, waiting for 3
Jul 12 07:52:21.258: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:21.258: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:21.258: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:52:31.258: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:31.258: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:31.258: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 07:52:41.258: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:41.258: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:41.258: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 07:52:41.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-9416 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:52:41.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:52:41.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:52:41.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 12 07:52:41.707: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 12 07:52:51.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-9416 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:52:52.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 07:52:52.146: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 07:52:52.146: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 07:53:02.233: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:53:02.233: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:02.233: INFO: Waiting for Pod statefulset-9416/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:02.233: INFO: Waiting for Pod statefulset-9416/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:12.239: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:53:12.239: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:12.239: INFO: Waiting for Pod statefulset-9416/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:22.255: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:53:22.256: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:22.256: INFO: Waiting for Pod statefulset-9416/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:32.242: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:53:32.242: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:42.239: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:53:42.239: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 12 07:53:52.347: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 12 07:54:03.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-9416 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 07:54:06.299: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 07:54:06.299: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 07:54:06.299: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 07:54:16.484: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 12 07:54:26.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-9416 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 07:54:26.862: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 07:54:26.862: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 07:54:26.862: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 07:54:36.881: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:54:36.881: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:54:36.881: INFO: Waiting for Pod statefulset-9416/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:54:46.887: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:54:46.887: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:54:46.887: INFO: Waiting for Pod statefulset-9416/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:54:57.064: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:54:57.064: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:55:06.887: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
Jul 12 07:55:06.887: INFO: Waiting for Pod statefulset-9416/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 12 07:55:16.901: INFO: Waiting for StatefulSet statefulset-9416/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 07:55:27.601: INFO: Deleting all statefulset in ns statefulset-9416
Jul 12 07:55:27.633: INFO: Scaling statefulset ss2 to 0
Jul 12 07:55:57.838: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 07:55:57.840: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:55:57.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9416" for this suite.

• [SLOW TEST:237.311 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":303,"completed":192,"skipped":3225,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:55:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b629c1e6-e314-48be-99cf-0522526c8207
STEP: Creating a pod to test consume secrets
Jul 12 07:55:58.229: INFO: Waiting up to 5m0s for pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786" in namespace "secrets-8912" to be "Succeeded or Failed"
Jul 12 07:55:58.232: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08413ms
Jul 12 07:56:00.235: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005751312s
Jul 12 07:56:02.237: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008538267s
Jul 12 07:56:04.240: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01117202s
Jul 12 07:56:06.243: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013901131s
Jul 12 07:56:08.396: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Pending", Reason="", readiness=false. Elapsed: 10.166893238s
Jul 12 07:56:10.398: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.16960702s
STEP: Saw pod success
Jul 12 07:56:10.399: INFO: Pod "pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786" satisfied condition "Succeeded or Failed"
Jul 12 07:56:10.405: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786 container secret-env-test: <nil>
STEP: delete the pod
Jul 12 07:56:10.491: INFO: Waiting for pod pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786 to disappear
Jul 12 07:56:10.493: INFO: Pod pod-secrets-3200838c-c161-440b-a2a7-b6400bb19786 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:10.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8912" for this suite.

• [SLOW TEST:12.550 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":303,"completed":193,"skipped":3231,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:10.508: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jul 12 07:56:10.652: INFO: Major version: 
STEP: Confirm minor version
Jul 12 07:56:10.652: INFO: cleanMinorVersion: 
Jul 12 07:56:10.652: INFO: Minor version: 
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:10.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6375" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":303,"completed":194,"skipped":3244,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:10.755: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:10.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9503" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":303,"completed":195,"skipped":3246,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:11.079: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 12 07:56:11.308: INFO: Waiting up to 5m0s for pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59" in namespace "emptydir-7531" to be "Succeeded or Failed"
Jul 12 07:56:11.310: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355284ms
Jul 12 07:56:14.141: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833849722s
Jul 12 07:56:16.144: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.836696736s
Jul 12 07:56:18.147: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.839875602s
Jul 12 07:56:20.151: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Pending", Reason="", readiness=false. Elapsed: 8.843318561s
Jul 12 07:56:22.162: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.854907268s
STEP: Saw pod success
Jul 12 07:56:22.162: INFO: Pod "pod-c65b0b6a-61eb-4667-ab8c-566640d67a59" satisfied condition "Succeeded or Failed"
Jul 12 07:56:22.220: INFO: Trying to get logs from node 10.32.0.100 pod pod-c65b0b6a-61eb-4667-ab8c-566640d67a59 container test-container: <nil>
STEP: delete the pod
Jul 12 07:56:22.285: INFO: Waiting for pod pod-c65b0b6a-61eb-4667-ab8c-566640d67a59 to disappear
Jul 12 07:56:22.287: INFO: Pod pod-c65b0b6a-61eb-4667-ab8c-566640d67a59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:22.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7531" for this suite.

• [SLOW TEST:11.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":196,"skipped":3258,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:22.320: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:39.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3142" for this suite.

• [SLOW TEST:17.307 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":303,"completed":197,"skipped":3261,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:39.627: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 12 07:56:39.806: INFO: Waiting up to 5m0s for pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05" in namespace "downward-api-5723" to be "Succeeded or Failed"
Jul 12 07:56:39.809: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503519ms
Jul 12 07:56:41.814: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008164684s
Jul 12 07:56:43.817: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01116877s
Jul 12 07:56:45.825: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019049732s
Jul 12 07:56:47.828: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022055876s
Jul 12 07:56:49.840: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033354788s
Jul 12 07:56:51.843: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036594797s
Jul 12 07:56:53.846: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.039503881s
STEP: Saw pod success
Jul 12 07:56:53.846: INFO: Pod "downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05" satisfied condition "Succeeded or Failed"
Jul 12 07:56:53.848: INFO: Trying to get logs from node 10.32.0.100 pod downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05 container dapi-container: <nil>
STEP: delete the pod
Jul 12 07:56:53.958: INFO: Waiting for pod downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05 to disappear
Jul 12 07:56:53.960: INFO: Pod downward-api-3e8ceef1-4434-4323-b702-2cd17f54fe05 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:53.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5723" for this suite.

• [SLOW TEST:14.360 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":303,"completed":198,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:53.989: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 12 07:56:54.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2351 /api/v1/namespaces/watch-2351/configmaps/e2e-watch-test-watch-closed d45df34d-b2de-440b-aecd-bd93b0bf5b8d 2848269 0 2021-07-12 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 07:56:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:56:54.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2351 /api/v1/namespaces/watch-2351/configmaps/e2e-watch-test-watch-closed d45df34d-b2de-440b-aecd-bd93b0bf5b8d 2848270 0 2021-07-12 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 12 07:56:54.189: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2351 /api/v1/namespaces/watch-2351/configmaps/e2e-watch-test-watch-closed d45df34d-b2de-440b-aecd-bd93b0bf5b8d 2848271 0 2021-07-12 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 07:56:54.189: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2351 /api/v1/namespaces/watch-2351/configmaps/e2e-watch-test-watch-closed d45df34d-b2de-440b-aecd-bd93b0bf5b8d 2848272 0 2021-07-12 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:56:54.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2351" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":303,"completed":199,"skipped":3304,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:56:54.206: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 07:57:07.413: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:57:07.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6566" for this suite.

• [SLOW TEST:13.484 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":200,"skipped":3321,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:57:07.690: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 12 07:57:07.959: INFO: Waiting up to 5m0s for pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b" in namespace "emptydir-7498" to be "Succeeded or Failed"
Jul 12 07:57:07.995: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.99546ms
Jul 12 07:57:10.636: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.677668315s
Jul 12 07:57:12.644: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.685331374s
Jul 12 07:57:14.661: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.702113569s
Jul 12 07:57:16.671: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.712239298s
Jul 12 07:57:18.747: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.788026978s
Jul 12 07:57:20.751: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.792357266s
STEP: Saw pod success
Jul 12 07:57:20.751: INFO: Pod "pod-80beef83-b40a-438b-aa8e-86cf9402563b" satisfied condition "Succeeded or Failed"
Jul 12 07:57:20.754: INFO: Trying to get logs from node 10.32.0.100 pod pod-80beef83-b40a-438b-aa8e-86cf9402563b container test-container: <nil>
STEP: delete the pod
Jul 12 07:57:21.350: INFO: Waiting for pod pod-80beef83-b40a-438b-aa8e-86cf9402563b to disappear
Jul 12 07:57:21.375: INFO: Pod pod-80beef83-b40a-438b-aa8e-86cf9402563b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:57:21.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7498" for this suite.

• [SLOW TEST:13.710 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":201,"skipped":3327,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:57:21.400: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 07:57:22.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}}, CollisionCount:(*int32)(nil)}
Jul 12 07:57:24.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:57:26.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:57:28.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:57:30.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 07:57:32.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673442, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 07:57:35.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:57:35.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5143" for this suite.
STEP: Destroying namespace "webhook-5143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.927 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":303,"completed":202,"skipped":3328,"failed":0}
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:57:36.327: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 07:57:37.017: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 07:57:37.432: INFO: waiting for watch events with expected annotations
Jul 12 07:57:37.432: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:57:38.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7529" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":303,"completed":203,"skipped":3332,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:57:38.485: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 12 07:57:38.770: INFO: Waiting up to 5m0s for pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e" in namespace "emptydir-6288" to be "Succeeded or Failed"
Jul 12 07:57:38.778: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.476279ms
Jul 12 07:57:40.781: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011256366s
Jul 12 07:57:42.784: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014702199s
Jul 12 07:57:44.787: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017229863s
Jul 12 07:57:46.790: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019950471s
Jul 12 07:57:48.800: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030001447s
Jul 12 07:57:50.802: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.032547829s
STEP: Saw pod success
Jul 12 07:57:50.802: INFO: Pod "pod-bd6324de-9992-42ac-b2ec-fc1c5911607e" satisfied condition "Succeeded or Failed"
Jul 12 07:57:50.804: INFO: Trying to get logs from node 10.32.0.100 pod pod-bd6324de-9992-42ac-b2ec-fc1c5911607e container test-container: <nil>
STEP: delete the pod
Jul 12 07:57:51.228: INFO: Waiting for pod pod-bd6324de-9992-42ac-b2ec-fc1c5911607e to disappear
Jul 12 07:57:51.230: INFO: Pod pod-bd6324de-9992-42ac-b2ec-fc1c5911607e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:57:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6288" for this suite.

• [SLOW TEST:12.803 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":204,"skipped":3336,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:57:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 07:57:51.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a" in namespace "projected-650" to be "Succeeded or Failed"
Jul 12 07:57:51.465: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706427ms
Jul 12 07:57:53.468: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00467747s
Jul 12 07:57:55.471: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007082751s
Jul 12 07:57:57.474: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010077001s
Jul 12 07:57:59.480: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016582205s
Jul 12 07:58:01.483: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019612141s
Jul 12 07:58:03.820: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.356599927s
STEP: Saw pod success
Jul 12 07:58:03.820: INFO: Pod "downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a" satisfied condition "Succeeded or Failed"
Jul 12 07:58:03.823: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a container client-container: <nil>
STEP: delete the pod
Jul 12 07:58:03.996: INFO: Waiting for pod downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a to disappear
Jul 12 07:58:03.998: INFO: Pod downwardapi-volume-f5f480f0-747c-4c45-9f63-62ea3806224a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:58:03.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-650" for this suite.

• [SLOW TEST:12.800 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":303,"completed":205,"skipped":3346,"failed":0}
SSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:58:04.088: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 07:58:05.512: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 12 07:58:05.517: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 07:58:05.562: INFO: waiting for watch events with expected annotations
Jul 12 07:58:05.562: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:58:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4206" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":303,"completed":206,"skipped":3353,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:58:05.821: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 07:58:05.960: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 07:58:05.966: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 07:58:05.969: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.100 before test
Jul 12 07:58:05.979: INFO: csi-rbdplugin-8rsp7 from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:05.980: INFO: calico-node-mrt5l from kube-system started at 2021-07-12 01:23:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:58:05.980: INFO: coredns-7699c68bdc-nrw9s from kube-system started at 2021-07-12 06:02:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:58:05.980: INFO: cube-logstash-sztvc from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-core-8448fcd4dd-hsvz2 from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container core ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-exporter-5d8b88579-7bpfn from kube-system started at 2021-07-12 06:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-jobservice-5c7d4566ff-lp4hp from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-nginx-86f8877965-s48pm from kube-system started at 2021-07-12 06:03:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-portal-76856b56c7-p6zqk from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:58:05.980: INFO: harbor-app-harbor-registry-6bc47dd67f-rcrrl from kube-system started at 2021-07-12 06:02:49 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:58:05.980: INFO: stolon-app-keeper-0 from kube-system started at 2021-07-12 06:02:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:05.980: INFO: stolon-app-proxy-59967b544f-8cssc from kube-system started at 2021-07-12 06:02:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:05.980: INFO: stolon-app-sentinel-5fc7bf8cf8-sfjnr from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:05.980: INFO: alertmanager-main-2 from monitoring started at 2021-07-12 06:02:32 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:58:05.980: INFO: cubenode-exporter-p2bg8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:58:05.980: INFO: node-exporter-nvbt2 from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:58:05.980: INFO: sonobuoy from sonobuoy started at 2021-07-12 06:23:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 07:58:05.980: INFO: sonobuoy-e2e-job-c08e481580ac4ebf from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container e2e ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:58:05.980: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:05.980: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:58:05.980: INFO: 	Container systemd-logs ready: false, restart count 21
Jul 12 07:58:05.980: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.102 before test
Jul 12 07:58:05.999: INFO: captain-controller-manager-56dd98c4dd-cbspp from captain-system started at 2021-07-09 02:45:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container manager ready: true, restart count 6
Jul 12 07:58:05.999: INFO: csi-rbdplugin-46rrc from ceph-csi started at 2021-07-09 02:46:27 +0000 UTC (3 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:05.999: INFO: csi-rbdplugin-provisioner-74d4496bf7-22rlt from ceph-csi started at 2021-07-12 06:00:59 +0000 UTC (6 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:05.999: INFO: csi-rbdplugin-provisioner-74d4496bf7-xzzbm from ceph-csi started at 2021-07-09 02:45:32 +0000 UTC (6 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:58:05.999: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 07:58:05.999: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:58:05.999: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:05.999: INFO: alpine-595b7c4b74-xvrn8 from default started at 2021-07-10 02:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container alpine ready: true, restart count 0
Jul 12 07:58:05.999: INFO: dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde from dns-1249 started at 2021-07-09 07:18:28 +0000 UTC (3 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container jessie-querier ready: true, restart count 381
Jul 12 07:58:05.999: INFO: 	Container querier ready: true, restart count 398
Jul 12 07:58:05.999: INFO: 	Container webserver ready: true, restart count 0
Jul 12 07:58:05.999: INFO: pod-047c84e7-4461-47fd-ad72-3e49a5479adb from emptydir-6449 started at 2021-07-12 02:54:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container test-container ready: false, restart count 0
Jul 12 07:58:05.999: INFO: ipsection-controllers-6d8496c6bd-vd86c from ipsection-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container ipsection-controllers ready: true, restart count 0
Jul 12 07:58:05.999: INFO: calico-kube-controllers-668c8b44f6-vfp6k from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:58:05.999: INFO: calico-node-x7txv from kube-system started at 2021-07-12 01:23:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:58:05.999: INFO: coredns-7699c68bdc-bzzjr from kube-system started at 2021-07-10 06:56:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-kong-b86d5bf9c-pkln4 from kube-system started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-logging-58f8c874b8-964j2 from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-logstash-xbhqg from kube-system started at 2021-07-09 02:46:27 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-networking-68749499f7-7vqt4 from kube-system started at 2021-07-12 06:01:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-node-65789b66c-tglb7 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-openapi-v1-658dd7f87b-nhzqv from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-ops-webapp-85c8594b7b-krvgz from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-storage-ceph-access-5f4db88cdd-hh7xq from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-storage-ceph-access ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-storage-f6786ff45-6q6b2 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-ticket-7955cf677f-scx9z from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:58:05.999: INFO: cube-webapp-57c694b676-pscrq from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:58:05.999: INFO: gpushare-schd-extender-9b5766d8c-xx5n2 from kube-system started at 2021-07-12 06:00:52 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container gpushare-schd-extender ready: true, restart count 0
Jul 12 07:58:05.999: INFO: harbor-app-harbor-core-8448fcd4dd-cvk8h from kube-system started at 2021-07-09 02:46:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container core ready: true, restart count 0
Jul 12 07:58:05.999: INFO: harbor-app-harbor-exporter-5d8b88579-jsq2l from kube-system started at 2021-07-09 02:46:40 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:58:05.999: INFO: harbor-app-harbor-jobservice-5c7d4566ff-6brx2 from kube-system started at 2021-07-09 02:46:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:58:05.999: INFO: harbor-app-harbor-nginx-86f8877965-8qcgn from kube-system started at 2021-07-09 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:05.999: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:58:05.999: INFO: harbor-app-harbor-portal-76856b56c7-b2p7w from kube-system started at 2021-07-09 02:46:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:58:06.000: INFO: harbor-app-harbor-registry-6bc47dd67f-dwjmv from kube-system started at 2021-07-09 02:46:39 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:58:06.000: INFO: kubernetes-dashboard-749844f89d-xflsb from kube-system started at 2021-07-12 06:00:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 12 07:58:06.000: INFO: local-path-provisioner-67d77c895b-ptflh from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jul 12 07:58:06.000: INFO: stolon-app-keeper-1 from kube-system started at 2021-07-09 02:46:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.000: INFO: stolon-app-proxy-59967b544f-t57n4 from kube-system started at 2021-07-09 02:46:19 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.000: INFO: stolon-app-sentinel-5fc7bf8cf8-5fkrt from kube-system started at 2021-07-09 02:46:06 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.000: INFO: alertmanager-main-1 from monitoring started at 2021-07-09 02:46:01 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:58:06.000: INFO: cubenode-exporter-dnhsc from monitoring started at 2021-07-09 02:46:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:58:06.000: INFO: mysqld-exporter-1-5d6b99c586-bsghg from monitoring started at 2021-07-12 06:01:01 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:58:06.000: INFO: mysqld-exporter-2-5f45bbf595-stzdj from monitoring started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 07:58:06.000: INFO: node-exporter-dgd55 from monitoring started at 2021-07-08 11:02:46 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:58:06.000: INFO: prometheus-k8s-0 from monitoring started at 2021-07-09 02:46:08 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container prometheus ready: true, restart count 1
Jul 12 07:58:06.000: INFO: prometheus-operator-6786cb4fc5-t8456 from monitoring started at 2021-07-09 02:45:33 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 12 07:58:06.000: INFO: redis-exporter-1-5877697f69-86xfb from monitoring started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:58:06.000: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:58:06.000: INFO: 	Container systemd-logs ready: false, restart count 21
Jul 12 07:58:06.000: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.3 before test
Jul 12 07:58:06.020: INFO: csi-rbdplugin-provisioner-74d4496bf7-ch8g6 from ceph-csi started at 2021-07-08 11:06:48 +0000 UTC (6 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container csi-attacher ready: true, restart count 4
Jul 12 07:58:06.020: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 07:58:06.020: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:06.020: INFO: 	Container csi-resizer ready: true, restart count 6
Jul 12 07:58:06.020: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 07:58:06.020: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:06.020: INFO: csi-rbdplugin-zcqsm from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 07:58:06.020: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 07:58:06.020: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 07:58:06.020: INFO: calico-kube-controllers-668c8b44f6-7vr9t from kube-system started at 2021-07-08 09:44:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:58:06.020: INFO: calico-kube-controllers-668c8b44f6-vcfkc from kube-system started at 2021-07-08 09:44:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 07:58:06.020: INFO: calico-node-zmmxk from kube-system started at 2021-07-12 01:24:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 07:58:06.020: INFO: coredns-7699c68bdc-klbx8 from kube-system started at 2021-07-10 06:57:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container coredns ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-appstore-0 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-appstore ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-ha-rabbitmq-0 from kube-system started at 2021-07-08 11:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container rabbitmq ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-kong-b86d5bf9c-4vtvh from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-logging-58f8c874b8-mjz4v from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-logstash-wfm47 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-networking-68749499f7-rt2dc from kube-system started at 2021-07-08 11:02:00 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-node-65789b66c-6tmls from kube-system started at 2021-07-09 02:45:30 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-openapi-v1-658dd7f87b-psc7x from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-ops-webapp-85c8594b7b-b544h from kube-system started at 2021-07-09 02:45:31 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-storage-f6786ff45-7649l from kube-system started at 2021-07-08 11:06:26 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-ticket-7955cf677f-jvn8v from kube-system started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cube-webapp-57c694b676-9qpnc from kube-system started at 2021-07-08 11:02:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-core-8448fcd4dd-h5fdq from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container core ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-exporter-5d8b88579-9wf79 from kube-system started at 2021-07-08 10:36:46 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container exporter ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-jobservice-5c7d4566ff-fsx5c from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-nginx-86f8877965-4rcpv from kube-system started at 2021-07-08 10:36:45 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container nginx ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-portal-76856b56c7-8nczp from kube-system started at 2021-07-08 10:36:44 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container portal ready: true, restart count 0
Jul 12 07:58:06.020: INFO: harbor-app-harbor-registry-6bc47dd67f-7kjrn from kube-system started at 2021-07-08 10:36:43 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container registry ready: true, restart count 0
Jul 12 07:58:06.020: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 07:58:06.020: INFO: kube-mini-chartmuseum-55d66cd548-2b8x2 from kube-system started at 2021-07-08 09:45:51 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container kube-mini-chartmuseum ready: true, restart count 0
Jul 12 07:58:06.020: INFO: logdir-admission-webhook-deployment-5b8587f876-dlgl5 from kube-system started at 2021-07-08 11:06:20 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container logdir-admission-webhook ready: true, restart count 0
Jul 12 07:58:06.020: INFO: metrics-server-86d56f4667-s8nqj from kube-system started at 2021-07-08 11:08:32 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 07:58:06.020: INFO: stolon-app-keeper-2 from kube-system started at 2021-07-08 10:34:49 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.020: INFO: stolon-app-proxy-59967b544f-ww9rx from kube-system started at 2021-07-08 10:33:36 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.020: INFO: stolon-app-sentinel-5fc7bf8cf8-grbwv from kube-system started at 2021-07-08 10:33:33 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container stolon ready: true, restart count 0
Jul 12 07:58:06.020: INFO: alertmanager-main-0 from monitoring started at 2021-07-08 11:02:21 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.020: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 07:58:06.020: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 07:58:06.020: INFO: cubenode-exporter-dvvm8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 07:58:06.021: INFO: grafana-5cd649d575-q2cdw from monitoring started at 2021-07-08 11:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container grafana ready: true, restart count 0
Jul 12 07:58:06.021: INFO: influxdb-exporter-1-7599cc4587-8f6tn from monitoring started at 2021-07-08 11:08:17 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container influxdb-exporter ready: true, restart count 0
Jul 12 07:58:06.021: INFO: kube-state-metrics-5d5d4d46c-g5zl8 from monitoring started at 2021-07-08 11:02:42 +0000 UTC (3 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 12 07:58:06.021: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 12 07:58:06.021: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 12 07:58:06.021: INFO: node-exporter-d42br from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 07:58:06.021: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 07:58:06.021: INFO: prometheus-adapter-84fbb7d77b-4qbgb from monitoring started at 2021-07-08 11:02:56 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 12 07:58:06.021: INFO: redis-exporter-2-6d778f7cf8-6kvb5 from monitoring started at 2021-07-08 11:08:25 +0000 UTC (1 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 07:58:06.021: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 07:58:06.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 07:58:06.021: INFO: 	Container systemd-logs ready: false, restart count 21
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 10.32.0.100
STEP: verifying the node has the label node 10.32.0.102
STEP: verifying the node has the label node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod captain-controller-manager-56dd98c4dd-cbspp requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-46rrc requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-8rsp7 requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-provisioner-74d4496bf7-22rlt requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-provisioner-74d4496bf7-ch8g6 requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-provisioner-74d4496bf7-xzzbm requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod csi-rbdplugin-zcqsm requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod alpine-595b7c4b74-xvrn8 requesting resource cpu=100m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod ipsection-controllers-6d8496c6bd-vd86c requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod calico-kube-controllers-668c8b44f6-7vr9t requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod calico-kube-controllers-668c8b44f6-vcfkc requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod calico-kube-controllers-668c8b44f6-vfp6k requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod calico-node-mrt5l requesting resource cpu=250m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod calico-node-x7txv requesting resource cpu=250m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod calico-node-zmmxk requesting resource cpu=250m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod coredns-7699c68bdc-bzzjr requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod coredns-7699c68bdc-klbx8 requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod coredns-7699c68bdc-nrw9s requesting resource cpu=1000m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod cube-appstore-0 requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-ha-rabbitmq-0 requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-kong-b86d5bf9c-4vtvh requesting resource cpu=2000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-kong-b86d5bf9c-pkln4 requesting resource cpu=2000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-logging-58f8c874b8-964j2 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-logging-58f8c874b8-mjz4v requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-logstash-sztvc requesting resource cpu=1000m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod cube-logstash-wfm47 requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-logstash-xbhqg requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-networking-68749499f7-7vqt4 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-networking-68749499f7-rt2dc requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-node-65789b66c-6tmls requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-node-65789b66c-tglb7 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-openapi-v1-658dd7f87b-nhzqv requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-openapi-v1-658dd7f87b-psc7x requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-ops-webapp-85c8594b7b-b544h requesting resource cpu=2000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-ops-webapp-85c8594b7b-krvgz requesting resource cpu=2000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-storage-ceph-access-5f4db88cdd-hh7xq requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-storage-f6786ff45-6q6b2 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-storage-f6786ff45-7649l requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-ticket-7955cf677f-jvn8v requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-ticket-7955cf677f-scx9z requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cube-webapp-57c694b676-9qpnc requesting resource cpu=2000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cube-webapp-57c694b676-pscrq requesting resource cpu=2000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod gpushare-schd-extender-9b5766d8c-xx5n2 requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-core-8448fcd4dd-cvk8h requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-core-8448fcd4dd-h5fdq requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-core-8448fcd4dd-hsvz2 requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-exporter-5d8b88579-7bpfn requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-exporter-5d8b88579-9wf79 requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-exporter-5d8b88579-jsq2l requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-jobservice-5c7d4566ff-6brx2 requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-jobservice-5c7d4566ff-fsx5c requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-jobservice-5c7d4566ff-lp4hp requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-nginx-86f8877965-4rcpv requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-nginx-86f8877965-8qcgn requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-nginx-86f8877965-s48pm requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-portal-76856b56c7-8nczp requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-portal-76856b56c7-b2p7w requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-portal-76856b56c7-p6zqk requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-registry-6bc47dd67f-7kjrn requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-registry-6bc47dd67f-dwjmv requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod harbor-app-harbor-registry-6bc47dd67f-rcrrl requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod kube-mini-chartmuseum-55d66cd548-2b8x2 requesting resource cpu=2000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod kubernetes-dashboard-749844f89d-xflsb requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod local-path-provisioner-67d77c895b-ptflh requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod logdir-admission-webhook-deployment-5b8587f876-dlgl5 requesting resource cpu=500m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod metrics-server-86d56f4667-s8nqj requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod stolon-app-keeper-0 requesting resource cpu=1000m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod stolon-app-keeper-1 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod stolon-app-keeper-2 requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod stolon-app-proxy-59967b544f-8cssc requesting resource cpu=1000m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod stolon-app-proxy-59967b544f-t57n4 requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod stolon-app-proxy-59967b544f-ww9rx requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod stolon-app-sentinel-5fc7bf8cf8-5fkrt requesting resource cpu=1000m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod stolon-app-sentinel-5fc7bf8cf8-grbwv requesting resource cpu=1000m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod stolon-app-sentinel-5fc7bf8cf8-sfjnr requesting resource cpu=1000m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod cubenode-exporter-dnhsc requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod cubenode-exporter-dvvm8 requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod cubenode-exporter-p2bg8 requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod grafana-5cd649d575-q2cdw requesting resource cpu=100m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod influxdb-exporter-1-7599cc4587-8f6tn requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod kube-state-metrics-5d5d4d46c-g5zl8 requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod mysqld-exporter-1-5d6b99c586-bsghg requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod mysqld-exporter-2-5f45bbf595-stzdj requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod node-exporter-d42br requesting resource cpu=112m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod node-exporter-dgd55 requesting resource cpu=112m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod node-exporter-nvbt2 requesting resource cpu=112m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod prometheus-adapter-84fbb7d77b-4qbgb requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod prometheus-k8s-0 requesting resource cpu=2100m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod prometheus-operator-6786cb4fc5-t8456 requesting resource cpu=100m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod redis-exporter-1-5877697f69-86xfb requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod redis-exporter-2-6d778f7cf8-6kvb5 requesting resource cpu=0m on Node 10.32.0.3
Jul 12 07:58:06.233: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod sonobuoy-e2e-job-c08e481580ac4ebf requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh requesting resource cpu=0m on Node 10.32.0.102
Jul 12 07:58:06.233: INFO: Pod sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn requesting resource cpu=0m on Node 10.32.0.100
Jul 12 07:58:06.233: INFO: Pod sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w requesting resource cpu=0m on Node 10.32.0.3
STEP: Starting Pods to consume most of the cluster CPU.
Jul 12 07:58:06.233: INFO: Creating a pod which consumes cpu=35376m on Node 10.32.0.100
Jul 12 07:58:06.247: INFO: Creating a pod which consumes cpu=23966m on Node 10.32.0.102
Jul 12 07:58:06.312: INFO: Creating a pod which consumes cpu=23756m on Node 10.32.0.3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49.1690fc218c2bdda1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7754/filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49 to 10.32.0.102]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49.1690fc2198a7d4ed], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-7754.svc.cluster.local svc.cluster.local cluster.local kube-system.svc.cluster.local kube-system.pod.cluster.local pod.cluster.local]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49.1690fc23fb2cb029], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49.1690fc24a6e71263], Reason = [Created], Message = [Created container filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49.1690fc24c5bca6e6], Reason = [Started], Message = [Started container filler-pod-0e7e9284-40af-4bd4-b897-ab5fb24e7b49]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d.1690fc2182ca6b64], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7754/filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d to 10.32.0.100]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d.1690fc21aafd2774], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-7754.svc.cluster.local svc.cluster.local cluster.local kube-system.svc.cluster.local kube-system.pod.cluster.local pod.cluster.local]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d.1690fc237c8f3968], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d.1690fc23b0a0eb54], Reason = [Created], Message = [Created container filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d.1690fc23c68626ff], Reason = [Started], Message = [Started container filler-pod-4e4dc404-0302-46f3-ae95-3737115bbb9d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876.1690fc218ca66b71], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7754/filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876 to 10.32.0.3]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876.1690fc219f35d541], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-7754.svc.cluster.local svc.cluster.local cluster.local kube-system.svc.cluster.local kube-system.pod.cluster.local pod.cluster.local]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876.1690fc23220a24ca], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876.1690fc232466147e], Reason = [Created], Message = [Created container filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876.1690fc232e039b49], Reason = [Started], Message = [Started container filler-pod-e4203d71-d0c0-420d-8af3-98cdbffe2876]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1690fc24e7350fbc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1690fc24ec4a17fc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.32.0.100
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.32.0.102
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.32.0.3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 07:58:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7754" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.338 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":303,"completed":207,"skipped":3353,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 07:58:22.159: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-938463f4-0462-4557-9ee7-0c2be09ca4c4 in namespace container-probe-945
Jul 12 07:58:34.379: INFO: Started pod busybox-938463f4-0462-4557-9ee7-0c2be09ca4c4 in namespace container-probe-945
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 07:58:34.381: INFO: Initial restart count of pod busybox-938463f4-0462-4557-9ee7-0c2be09ca4c4 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:02:36.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-945" for this suite.

• [SLOW TEST:254.762 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":208,"skipped":3356,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:02:36.922: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:02:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2401" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":303,"completed":209,"skipped":3374,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:02:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:02:37.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001" in namespace "projected-5118" to be "Succeeded or Failed"
Jul 12 08:02:37.610: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256104ms
Jul 12 08:02:39.614: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011452669s
Jul 12 08:02:41.617: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014786467s
Jul 12 08:02:43.621: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018308117s
Jul 12 08:02:45.625: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021844623s
Jul 12 08:02:47.627: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024551905s
Jul 12 08:02:50.113: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Pending", Reason="", readiness=false. Elapsed: 12.510138967s
Jul 12 08:02:52.122: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.519066333s
STEP: Saw pod success
Jul 12 08:02:52.122: INFO: Pod "downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001" satisfied condition "Succeeded or Failed"
Jul 12 08:02:52.129: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001 container client-container: <nil>
STEP: delete the pod
Jul 12 08:02:52.304: INFO: Waiting for pod downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001 to disappear
Jul 12 08:02:52.307: INFO: Pod downwardapi-volume-e17a87b5-4725-466f-9c02-ae7dbcb37001 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:02:52.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5118" for this suite.

• [SLOW TEST:14.891 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":210,"skipped":3378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:02:52.335: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:02:52.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-5271'
Jul 12 08:02:53.716: INFO: stderr: ""
Jul 12 08:02:53.716: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 12 08:02:53.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-5271'
Jul 12 08:02:54.165: INFO: stderr: ""
Jul 12 08:02:54.165: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 08:02:55.169: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:02:55.169: INFO: Found 0 / 1
Jul 12 08:02:56.169: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:02:56.169: INFO: Found 0 / 1
Jul 12 08:02:57.174: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:02:57.174: INFO: Found 0 / 1
Jul 12 08:02:58.173: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:02:58.173: INFO: Found 0 / 1
Jul 12 08:02:59.173: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:02:59.173: INFO: Found 0 / 1
Jul 12 08:03:00.188: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:00.188: INFO: Found 0 / 1
Jul 12 08:03:01.214: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:01.214: INFO: Found 0 / 1
Jul 12 08:03:02.179: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:02.179: INFO: Found 0 / 1
Jul 12 08:03:03.172: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:03.172: INFO: Found 0 / 1
Jul 12 08:03:04.168: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:04.168: INFO: Found 1 / 1
Jul 12 08:03:04.168: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 12 08:03:04.170: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:03:04.170: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 08:03:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 describe pod agnhost-primary-g6xlv --namespace=kubectl-5271'
Jul 12 08:03:04.265: INFO: stderr: ""
Jul 12 08:03:04.265: INFO: stdout: "Name:         agnhost-primary-g6xlv\nNamespace:    kubectl-5271\nPriority:     0\nNode:         10.32.0.100/10.32.0.100\nStart Time:   Mon, 12 Jul 2021 08:02:53 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  calicoIP: 192.168.205.208/26\n              cni.projectcalico.org/podIP: 192.168.205.208/32\n              cni.projectcalico.org/podIPs: 192.168.205.208/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"192.168.205.208\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\nStatus:       Running\nIP:           192.168.205.208\nIPs:\n  IP:           192.168.205.208\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://32b99552586dbd4e814954e5d07480d783c763407b36b71001ef9c836de06b7d\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 12 Jul 2021 08:03:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-llzpf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-llzpf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-llzpf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason            Age               From               Message\n  ----     ------            ----              ----               -------\n  Normal   Scheduled         10s               default-scheduler  Successfully assigned kubectl-5271/agnhost-primary-g6xlv to 10.32.0.100\n  Normal   Pulled            3s                kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Warning  DNSConfigForming  1s (x3 over 10s)  kubelet            Search Line limits were exceeded, some search paths have been omitted, the applied search line is: kubectl-5271.svc.cluster.local svc.cluster.local cluster.local kube-system.svc.cluster.local kube-system.pod.cluster.local pod.cluster.local\n  Normal   Created           1s                kubelet            Created container agnhost-primary\n  Normal   Started           1s                kubelet            Started container agnhost-primary\n"
Jul 12 08:03:04.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 describe rc agnhost-primary --namespace=kubectl-5271'
Jul 12 08:03:04.397: INFO: stderr: ""
Jul 12 08:03:04.397: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5271\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  11s   replication-controller  Created pod: agnhost-primary-g6xlv\n"
Jul 12 08:03:04.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 describe service agnhost-primary --namespace=kubectl-5271'
Jul 12 08:03:04.483: INFO: stderr: ""
Jul 12 08:03:04.483: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5271\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.254.222.240\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.205.208:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 12 08:03:04.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 describe node 10.32.0.100'
Jul 12 08:03:04.670: INFO: stderr: ""
Jul 12 08:03:04.670: INFO: stdout: "Name:               10.32.0.100\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    dhc.dahuatech.com/business=\n                    dhc.dahuatech.com/database=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.32.0.100\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    route-reflector=true\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"rbd.csi.ceph.com\":\"10.32.0.100\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.32.0.100/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.205.129\n                    projectcalico.org/RouteReflectorClusterID: 244.0.0.2\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 08 Jul 2021 09:42:19 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.32.0.100\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 12 Jul 2021 08:02:56 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 12 Jul 2021 01:23:49 +0000   Mon, 12 Jul 2021 01:23:49 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 12 Jul 2021 07:59:39 +0000   Mon, 12 Jul 2021 06:01:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 12 Jul 2021 07:59:39 +0000   Mon, 12 Jul 2021 06:01:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 12 Jul 2021 07:59:39 +0000   Mon, 12 Jul 2021 06:01:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 12 Jul 2021 07:59:39 +0000   Mon, 12 Jul 2021 06:01:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.32.0.100\n  Hostname:    10.32.0.100\nCapacity:\n  cpu:                  56\n  ephemeral-storage:    2135952388Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               131638184Ki\n  pods:                 110\nAllocatable:\n  cpu:                  56\n  ephemeral-storage:    2135952388Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               129541032Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 6ba89416b13f4a74a24a0f0d038b7897\n  System UUID:                4C4C4544-005A-4210-804E-B1C04F4C5032\n  Boot ID:                    c71a2b56-1254-4c54-a218-d8c185c25ab3\n  Kernel Version:             4.14.224\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.5\n  Kubelet Version:            v1.19.4-dirty\n  Kube-Proxy Version:         v1.19.4-dirty\nPodCIDR:                      192.168.2.0/24\nPodCIDRs:                     192.168.2.0/24\nNon-terminated Pods:          (20 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ceph-csi                    csi-rbdplugin-8rsp7                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 calico-node-mrt5l                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         6h39m\n  kube-system                 coredns-7699c68bdc-nrw9s                                   1 (1%)        0 (0%)      1Gi (0%)         2Gi (1%)       122m\n  kube-system                 cube-logstash-sztvc                                        1 (1%)        2 (3%)      2Gi (1%)         4Gi (3%)       3d20h\n  kube-system                 harbor-app-harbor-core-8448fcd4dd-hsvz2                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 harbor-app-harbor-exporter-5d8b88579-7bpfn                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 harbor-app-harbor-jobservice-5c7d4566ff-lp4hp              0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 harbor-app-harbor-nginx-86f8877965-s48pm                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 harbor-app-harbor-portal-76856b56c7-p6zqk                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 harbor-app-harbor-registry-6bc47dd67f-rcrrl                0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                 stolon-app-keeper-0                                        1 (1%)        4 (7%)      2Gi (1%)         8Gi (6%)       120m\n  kube-system                 stolon-app-proxy-59967b544f-8cssc                          1 (1%)        2 (3%)      2Gi (1%)         4Gi (3%)       122m\n  kube-system                 stolon-app-sentinel-5fc7bf8cf8-sfjnr                       1 (1%)        2 (3%)      2Gi (1%)         4Gi (3%)       122m\n  kubectl-5271                agnhost-primary-g6xlv                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11s\n  monitoring                  alertmanager-main-2                                        100m (0%)     100m (0%)   250Mi (0%)       50Mi (0%)      120m\n  monitoring                  cubenode-exporter-p2bg8                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  monitoring                  node-exporter-nvbt2                                        112m (0%)     270m (0%)   200Mi (0%)       220Mi (0%)     3d21h\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\n  sonobuoy                    sonobuoy-e2e-job-c08e481580ac4ebf                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests     Limits\n  --------             --------     ------\n  cpu                  5462m (9%)   10370m (18%)\n  memory               9666Mi (7%)  22798Mi (18%)\n  ephemeral-storage    0 (0%)       0 (0%)\n  hugepages-1Gi        0 (0%)       0 (0%)\n  hugepages-2Mi        0 (0%)       0 (0%)\n  example.com/fakecpu  0            0\nEvents:\n  Type     Reason                    Age                  From        Message\n  ----     ------                    ----                 ----        -------\n  Normal   Starting                  5h7m                 kube-proxy  Starting kube-proxy.\n  Normal   Starting                  5h7m                 kube-proxy  Starting kube-proxy.\n  Normal   Starting                  121m                 kubelet     Starting kubelet.\n  Warning  CheckLimitsForResolvConf  121m                 kubelet     Resolv.conf file '/etc/resolv.conf' contains search line consisting of more than 3 domains!\n  Normal   NodeAllocatableEnforced   121m                 kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory   121m (x2 over 121m)  kubelet     Node 10.32.0.100 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     121m (x2 over 121m)  kubelet     Node 10.32.0.100 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      121m (x2 over 121m)  kubelet     Node 10.32.0.100 status is now: NodeHasSufficientPID\n  Normal   NodeReady                 121m                 kubelet     Node 10.32.0.100 status is now: NodeReady\n"
Jul 12 08:03:04.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 describe namespace kubectl-5271'
Jul 12 08:03:04.812: INFO: stderr: ""
Jul 12 08:03:04.812: INFO: stdout: "Name:         kubectl-5271\nLabels:       e2e-framework=kubectl\n              e2e-run=b811cf48-c30b-4b3b-a060-cb9b76a73b8a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:03:04.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5271" for this suite.

• [SLOW TEST:12.570 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":303,"completed":211,"skipped":3402,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:03:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:03:05.074: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 12 08:03:05.114: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 12 08:03:10.131: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 08:03:18.150: INFO: Creating deployment "test-rolling-update-deployment"
Jul 12 08:03:18.164: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 12 08:03:18.258: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 12 08:03:20.292: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 12 08:03:20.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:03:22.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:03:24.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:03:26.331: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673798, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:03:28.326: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 12 08:03:28.525: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4987 /apis/apps/v1/namespaces/deployment-4987/deployments/test-rolling-update-deployment eb601439-e13e-438f-ac98-967b0e275535 2851954 1 2021-07-12 08:03:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-12 08:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 08:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d2d048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-12 08:03:18 +0000 UTC,LastTransitionTime:2021-07-12 08:03:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-07-12 08:03:28 +0000 UTC,LastTransitionTime:2021-07-12 08:03:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 12 08:03:28.530: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-4987 /apis/apps/v1/namespaces/deployment-4987/replicasets/test-rolling-update-deployment-c4cb8d6d9 e615d0cc-4a34-49ad-8db0-b788f8766f92 2851942 1 2021-07-12 08:03:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment eb601439-e13e-438f-ac98-967b0e275535 0xc002d2d5b0 0xc002d2d5b1}] []  [{kube-controller-manager Update apps/v1 2021-07-12 08:03:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb601439-e13e-438f-ac98-967b0e275535\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d2d628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:03:28.530: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 12 08:03:28.530: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4987 /apis/apps/v1/namespaces/deployment-4987/replicasets/test-rolling-update-controller 74950610-8783-437e-9048-11e6c407fa82 2851953 2 2021-07-12 08:03:05 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment eb601439-e13e-438f-ac98-967b0e275535 0xc002d2d4a7 0xc002d2d4a8}] []  [{e2e.test Update apps/v1 2021-07-12 08:03:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 08:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb601439-e13e-438f-ac98-967b0e275535\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d2d548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:03:28.533: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-v889n" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-v889n test-rolling-update-deployment-c4cb8d6d9- deployment-4987 /api/v1/namespaces/deployment-4987/pods/test-rolling-update-deployment-c4cb8d6d9-v889n f1e69fef-8431-4818-a1c4-a86a757f9b68 2851940 0 2021-07-12 08:03:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[calicoIP:192.168.205.221/26 cni.projectcalico.org/podIP:192.168.205.221/32 cni.projectcalico.org/podIPs:192.168.205.221/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.205.221"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 e615d0cc-4a34-49ad-8db0-b788f8766f92 0xc00740cbcf 0xc00740cbe0}] []  [{kube-controller-manager Update v1 2021-07-12 08:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e615d0cc-4a34-49ad-8db0-b788f8766f92\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:03:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:03:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:03:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.205.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjrj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjrj7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjrj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:03:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.205.221,StartTime:2021-07-12 08:03:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:03:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://608dfc5236be55a0413aa98294352fd919a7553cd8206daf2a9933a368cb4dff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.205.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:03:28.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4987" for this suite.

• [SLOW TEST:23.743 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":212,"skipped":3413,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:03:28.649: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:03:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2845" for this suite.

• [SLOW TEST:11.451 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":303,"completed":213,"skipped":3417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:03:40.100: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-zn7w
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 08:03:40.359: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zn7w" in namespace "subpath-7922" to be "Succeeded or Failed"
Jul 12 08:03:40.361: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007748ms
Jul 12 08:03:42.364: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005264894s
Jul 12 08:03:44.367: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00814231s
Jul 12 08:03:46.383: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024433043s
Jul 12 08:03:48.386: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027064676s
Jul 12 08:03:50.428: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Pending", Reason="", readiness=false. Elapsed: 10.069019765s
Jul 12 08:03:52.442: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 12.0829881s
Jul 12 08:03:54.869: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 14.510443102s
Jul 12 08:03:56.874: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 16.514714027s
Jul 12 08:03:58.877: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 18.518565008s
Jul 12 08:04:00.881: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 20.521580678s
Jul 12 08:04:02.884: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 22.525538296s
Jul 12 08:04:04.889: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 24.530027816s
Jul 12 08:04:06.892: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 26.533553372s
Jul 12 08:04:08.917: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Running", Reason="", readiness=true. Elapsed: 28.557736991s
Jul 12 08:04:10.958: INFO: Pod "pod-subpath-test-downwardapi-zn7w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.598850954s
STEP: Saw pod success
Jul 12 08:04:10.958: INFO: Pod "pod-subpath-test-downwardapi-zn7w" satisfied condition "Succeeded or Failed"
Jul 12 08:04:11.271: INFO: Trying to get logs from node 10.32.0.100 pod pod-subpath-test-downwardapi-zn7w container test-container-subpath-downwardapi-zn7w: <nil>
STEP: delete the pod
Jul 12 08:04:11.502: INFO: Waiting for pod pod-subpath-test-downwardapi-zn7w to disappear
Jul 12 08:04:11.504: INFO: Pod pod-subpath-test-downwardapi-zn7w no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zn7w
Jul 12 08:04:11.504: INFO: Deleting pod "pod-subpath-test-downwardapi-zn7w" in namespace "subpath-7922"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:04:11.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7922" for this suite.

• [SLOW TEST:31.446 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":303,"completed":214,"skipped":3440,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:04:11.546: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:04:11.794: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04" in namespace "security-context-test-5493" to be "Succeeded or Failed"
Jul 12 08:04:11.837: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 42.880018ms
Jul 12 08:04:13.840: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045930924s
Jul 12 08:04:15.862: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068228795s
Jul 12 08:04:17.918: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124020405s
Jul 12 08:04:19.921: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 8.126779025s
Jul 12 08:04:21.924: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Pending", Reason="", readiness=false. Elapsed: 10.129768462s
Jul 12 08:04:23.927: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.133225672s
Jul 12 08:04:23.927: INFO: Pod "alpine-nnp-false-c3cec7f0-3237-492f-9ec0-bc4c64564f04" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:04:23.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5493" for this suite.

• [SLOW TEST:12.467 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":215,"skipped":3449,"failed":0}
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:04:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:04:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:04:36.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5381" for this suite.

• [SLOW TEST:12.768 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":303,"completed":216,"skipped":3449,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:04:36.782: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:04:53.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7746" for this suite.

• [SLOW TEST:16.757 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":303,"completed":217,"skipped":3462,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:04:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:04:53.714: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 12 08:04:58.717: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 08:05:04.753: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 12 08:05:06.755: INFO: Creating deployment "test-rollover-deployment"
Jul 12 08:05:06.874: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 12 08:05:08.964: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 12 08:05:08.971: INFO: Ensure that both replica sets have 1 created replica
Jul 12 08:05:08.994: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 12 08:05:09.128: INFO: Updating deployment test-rollover-deployment
Jul 12 08:05:09.128: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 12 08:05:11.195: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 12 08:05:11.200: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 12 08:05:11.205: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:11.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673909, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:13.243: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:13.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673909, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:15.220: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:15.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673909, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:17.227: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:17.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673909, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:19.248: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:19.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673909, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:21.212: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:21.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:23.213: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:23.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:25.212: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:25.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:27.212: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:27.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:29.213: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 08:05:29.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673906, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:31.265: INFO: 
Jul 12 08:05:31.265: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 12 08:05:31.280: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4414 /apis/apps/v1/namespaces/deployment-4414/deployments/test-rollover-deployment f064d917-128d-48c2-98ce-c964ecd636ac 2853221 2 2021-07-12 08:05:06 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 08:05:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 08:05:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ad67318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-12 08:05:06 +0000 UTC,LastTransitionTime:2021-07-12 08:05:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-07-12 08:05:31 +0000 UTC,LastTransitionTime:2021-07-12 08:05:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 12 08:05:31.283: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-4414 /apis/apps/v1/namespaces/deployment-4414/replicasets/test-rollover-deployment-5797c7764 fa283c54-1b7d-498f-9259-469dc7303850 2853211 2 2021-07-12 08:05:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f064d917-128d-48c2-98ce-c964ecd636ac 0xc0072c5c80 0xc0072c5c81}] []  [{kube-controller-manager Update apps/v1 2021-07-12 08:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f064d917-128d-48c2-98ce-c964ecd636ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072c5cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:05:31.283: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 12 08:05:31.283: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4414 /apis/apps/v1/namespaces/deployment-4414/replicasets/test-rollover-controller 85bc3dac-1e16-4bab-bd34-f9919d66f83e 2853219 2 2021-07-12 08:04:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f064d917-128d-48c2-98ce-c964ecd636ac 0xc0072c5b77 0xc0072c5b78}] []  [{e2e.test Update apps/v1 2021-07-12 08:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 08:05:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f064d917-128d-48c2-98ce-c964ecd636ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0072c5c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:05:31.283: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4414 /apis/apps/v1/namespaces/deployment-4414/replicasets/test-rollover-deployment-78bc8b888c f8f52e97-101d-40bb-92cf-395edbfa5280 2853011 2 2021-07-12 08:05:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f064d917-128d-48c2-98ce-c964ecd636ac 0xc0072c5d67 0xc0072c5d68}] []  [{kube-controller-manager Update apps/v1 2021-07-12 08:05:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f064d917-128d-48c2-98ce-c964ecd636ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072c5df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:05:31.286: INFO: Pod "test-rollover-deployment-5797c7764-twdft" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-twdft test-rollover-deployment-5797c7764- deployment-4414 /api/v1/namespaces/deployment-4414/pods/test-rollover-deployment-5797c7764-twdft 7ea64163-25dc-42a7-8b92-345b899f7020 2853126 0 2021-07-12 08:05:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[calicoIP:192.168.205.197/26 cni.projectcalico.org/podIP:192.168.205.197/32 cni.projectcalico.org/podIPs:192.168.205.197/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.205.197"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 fa283c54-1b7d-498f-9259-469dc7303850 0xc003534a3f 0xc003534a50}] []  [{kube-controller-manager Update v1 2021-07-12 08:05:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa283c54-1b7d-498f-9259-469dc7303850\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:05:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:05:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:05:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.205.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nvhmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nvhmq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nvhmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:05:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:05:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.205.197,StartTime:2021-07-12 08:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:05:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://1833c4fbcef4878a056deaa7a2fed0e85d3a10ce46e0d68108d1d3d7896dab9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.205.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:05:31.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4414" for this suite.

• [SLOW TEST:37.757 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":303,"completed":218,"skipped":3477,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:05:31.297: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-d4ca210f-477d-410d-bc83-801ff40aabe2
STEP: Creating a pod to test consume secrets
Jul 12 08:05:31.728: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f" in namespace "projected-1149" to be "Succeeded or Failed"
Jul 12 08:05:31.735: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.943939ms
Jul 12 08:05:33.738: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034578s
Jul 12 08:05:35.741: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013234572s
Jul 12 08:05:37.762: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033441572s
Jul 12 08:05:39.778: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050036943s
Jul 12 08:05:41.816: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.087954601s
Jul 12 08:05:43.820: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.091588951s
STEP: Saw pod success
Jul 12 08:05:43.820: INFO: Pod "pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f" satisfied condition "Succeeded or Failed"
Jul 12 08:05:44.101: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 08:05:44.241: INFO: Waiting for pod pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f to disappear
Jul 12 08:05:44.243: INFO: Pod pod-projected-secrets-db2d3f01-2d0a-4a4e-9dfb-e269b2714b4f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:05:44.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1149" for this suite.

• [SLOW TEST:13.094 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":219,"skipped":3477,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:05:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 08:05:46.550: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 08:05:48.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:50.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:52.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:54.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:05:56.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761673946, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 08:05:59.593: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:05:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9339-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:06:07.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-442" for this suite.
STEP: Destroying namespace "webhook-442-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:23.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":303,"completed":220,"skipped":3480,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:06:07.687: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-89902b37-9dd9-4088-ab1b-ad257aeb902f
STEP: Creating a pod to test consume secrets
Jul 12 08:06:08.012: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab" in namespace "projected-6780" to be "Succeeded or Failed"
Jul 12 08:06:08.015: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.602085ms
Jul 12 08:06:10.343: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330988487s
Jul 12 08:06:13.047: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.034861059s
Jul 12 08:06:15.052: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.040347508s
Jul 12 08:06:17.056: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Pending", Reason="", readiness=false. Elapsed: 9.043972154s
Jul 12 08:06:19.212: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.200586581s
STEP: Saw pod success
Jul 12 08:06:19.212: INFO: Pod "pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab" satisfied condition "Succeeded or Failed"
Jul 12 08:06:19.245: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 08:06:19.482: INFO: Waiting for pod pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab to disappear
Jul 12 08:06:19.485: INFO: Pod pod-projected-secrets-7e0d944e-bd2a-424b-98a9-0643dfc9baab no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:06:19.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6780" for this suite.

• [SLOW TEST:11.837 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":221,"skipped":3491,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:06:19.524: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:06:19.788: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71" in namespace "downward-api-4411" to be "Succeeded or Failed"
Jul 12 08:06:19.790: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103971ms
Jul 12 08:06:21.908: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120651067s
Jul 12 08:06:23.911: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123768961s
Jul 12 08:06:25.966: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.17849251s
Jul 12 08:06:27.997: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.209151304s
Jul 12 08:06:30.000: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Pending", Reason="", readiness=false. Elapsed: 10.212328309s
Jul 12 08:06:32.071: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.283751582s
STEP: Saw pod success
Jul 12 08:06:32.071: INFO: Pod "downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71" satisfied condition "Succeeded or Failed"
Jul 12 08:06:32.097: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71 container client-container: <nil>
STEP: delete the pod
Jul 12 08:06:32.264: INFO: Waiting for pod downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71 to disappear
Jul 12 08:06:32.271: INFO: Pod downwardapi-volume-0fd24041-4cb0-4584-9170-e79b0309de71 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:06:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4411" for this suite.

• [SLOW TEST:12.993 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":222,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:06:32.517: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 12 08:06:43.952: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:06:45.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9284" for this suite.

• [SLOW TEST:12.626 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":303,"completed":223,"skipped":3508,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:06:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jul 12 08:06:45.428: INFO: Waiting up to 5m0s for pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc" in namespace "var-expansion-8988" to be "Succeeded or Failed"
Jul 12 08:06:45.431: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587665ms
Jul 12 08:06:47.469: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040804964s
Jul 12 08:06:49.472: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043294298s
Jul 12 08:06:51.647: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.218414718s
Jul 12 08:06:53.649: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221104527s
Jul 12 08:06:55.652: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.223768483s
Jul 12 08:06:57.671: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.243039394s
STEP: Saw pod success
Jul 12 08:06:57.671: INFO: Pod "var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc" satisfied condition "Succeeded or Failed"
Jul 12 08:06:57.674: INFO: Trying to get logs from node 10.32.0.100 pod var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc container dapi-container: <nil>
STEP: delete the pod
Jul 12 08:06:57.828: INFO: Waiting for pod var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc to disappear
Jul 12 08:06:57.831: INFO: Pod var-expansion-06b63de0-54a5-44b0-a972-242b15c4b6cc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:06:57.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8988" for this suite.

• [SLOW TEST:12.766 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":303,"completed":224,"skipped":3516,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:06:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 08:06:59.113: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 08:07:01.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:07:03.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:07:05.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:07:07.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:07:09.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674019, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 08:07:12.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
Jul 12 08:07:12.311: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:07:12.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5769" for this suite.
STEP: Destroying namespace "webhook-5769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":303,"completed":225,"skipped":3537,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:07:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 12 08:07:26.004: INFO: Successfully updated pod "annotationupdate6bd48478-8940-4ddf-bba3-47a9d56e17c7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:07:28.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6366" for this suite.

• [SLOW TEST:15.587 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":226,"skipped":3541,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:07:28.728: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-77a80fcb-daf5-43c7-868b-f8f66d467abb in namespace container-probe-2974
Jul 12 08:07:40.983: INFO: Started pod busybox-77a80fcb-daf5-43c7-868b-f8f66d467abb in namespace container-probe-2974
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 08:07:40.985: INFO: Initial restart count of pod busybox-77a80fcb-daf5-43c7-868b-f8f66d467abb is 0
Jul 12 08:08:36.096: INFO: Restart count of pod container-probe-2974/busybox-77a80fcb-daf5-43c7-868b-f8f66d467abb is now 1 (55.11076692s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:08:36.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2974" for this suite.

• [SLOW TEST:67.434 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":227,"skipped":3546,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:08:36.163: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-faa2f050-e6c4-432b-a9bb-57bcc908f3c8
STEP: Creating secret with name secret-projected-all-test-volume-1f426ae2-f02c-48e1-ae24-2ba628d242ac
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 12 08:08:36.452: INFO: Waiting up to 5m0s for pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a" in namespace "projected-6350" to be "Succeeded or Failed"
Jul 12 08:08:36.454: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260917ms
Jul 12 08:08:38.457: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005171135s
Jul 12 08:08:40.523: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071497451s
Jul 12 08:08:42.527: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074802059s
Jul 12 08:08:44.532: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079669099s
Jul 12 08:08:46.536: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.08376776s
Jul 12 08:08:48.579: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.126941847s
STEP: Saw pod success
Jul 12 08:08:48.579: INFO: Pod "projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a" satisfied condition "Succeeded or Failed"
Jul 12 08:08:48.581: INFO: Trying to get logs from node 10.32.0.100 pod projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 12 08:08:49.041: INFO: Waiting for pod projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a to disappear
Jul 12 08:08:49.121: INFO: Pod projected-volume-c2913ccc-ff8a-4623-8dc7-ed135a1da51a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:08:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6350" for this suite.

• [SLOW TEST:12.976 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":303,"completed":228,"skipped":3554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:08:49.139: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5578
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5578
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5578
Jul 12 08:08:49.583: INFO: Found 0 stateful pods, waiting for 1
Jul 12 08:08:59.587: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 12 08:08:59.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 08:09:02.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 08:09:02.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 08:09:02.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 08:09:02.831: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 12 08:09:12.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 08:09:12.835: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 08:09:12.860: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999472s
Jul 12 08:09:13.864: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997383887s
Jul 12 08:09:14.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993785674s
Jul 12 08:09:15.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990024543s
Jul 12 08:09:16.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983488694s
Jul 12 08:09:17.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979508118s
Jul 12 08:09:18.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973939719s
Jul 12 08:09:19.890: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970423487s
Jul 12 08:09:20.894: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967208319s
Jul 12 08:09:21.898: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.101537ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5578
Jul 12 08:09:22.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 08:09:23.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 08:09:23.180: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 08:09:23.180: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 08:09:23.183: INFO: Found 1 stateful pods, waiting for 3
Jul 12 08:09:33.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:33.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:33.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 08:09:43.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:43.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:43.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 12 08:09:53.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:53.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 08:09:53.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 12 08:09:53.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 08:09:53.454: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 08:09:53.454: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 08:09:53.454: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 08:09:53.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 08:09:53.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 08:09:53.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 08:09:53.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 08:09:53.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 08:09:53.927: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 08:09:53.927: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 08:09:53.927: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 08:09:53.927: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 08:09:53.941: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 12 08:10:03.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 08:10:03.982: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 08:10:03.982: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 08:10:04.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999473s
Jul 12 08:10:05.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976338738s
Jul 12 08:10:06.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971736462s
Jul 12 08:10:07.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967445325s
Jul 12 08:10:08.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963037528s
Jul 12 08:10:09.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.517699791s
Jul 12 08:10:10.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.51118191s
Jul 12 08:10:11.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.507295269s
Jul 12 08:10:12.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.502818975s
Jul 12 08:10:13.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 494.346868ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5578
Jul 12 08:10:14.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 08:10:14.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 08:10:14.782: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 08:10:14.782: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 08:10:14.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 08:10:15.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 08:10:15.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 08:10:15.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 08:10:15.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=statefulset-5578 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 08:10:15.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 08:10:15.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 08:10:15.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 08:10:15.272: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 08:10:45.283: INFO: Deleting all statefulset in ns statefulset-5578
Jul 12 08:10:45.286: INFO: Scaling statefulset ss to 0
Jul 12 08:10:45.295: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 08:10:45.297: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:10:45.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5578" for this suite.

• [SLOW TEST:116.196 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":303,"completed":229,"skipped":3577,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:10:45.335: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 12 08:10:45.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856468 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:10:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:10:45.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856468 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:10:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 12 08:10:55.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856583 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:10:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:10:55.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856583 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:10:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 12 08:11:05.703: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856663 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:11:05.703: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856663 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 12 08:11:16.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856740 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:11:16.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-a 64de20a7-6db1-413f-8bae-b6af070f88f6 2856740 0 2021-07-12 08:10:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 12 08:11:26.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-b cb640ebd-eb6a-4acd-8e52-fdef7eda8402 2856822 0 2021-07-12 08:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:11:26.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-b cb640ebd-eb6a-4acd-8e52-fdef7eda8402 2856822 0 2021-07-12 08:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 12 08:11:36.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-b cb640ebd-eb6a-4acd-8e52-fdef7eda8402 2856904 0 2021-07-12 08:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:11:36.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5899 /api/v1/namespaces/watch-5899/configmaps/e2e-watch-test-configmap-b cb640ebd-eb6a-4acd-8e52-fdef7eda8402 2856904 0 2021-07-12 08:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 08:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:11:46.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5899" for this suite.

• [SLOW TEST:61.207 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":303,"completed":230,"skipped":3577,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:11:46.542: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3654
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 08:11:46.701: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 08:11:46.877: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:48.927: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:50.927: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:52.881: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:54.879: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:56.945: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:11:58.883: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 08:12:00.896: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:02.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:04.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:06.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:08.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:10.890: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:12.879: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:14.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 08:12:16.880: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 08:12:16.885: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 08:12:16.889: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 08:12:28.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.206.10:8080/dial?request=hostname&protocol=udp&host=192.168.206.9&port=8081&tries=1'] Namespace:pod-network-test-3654 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:12:28.921: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:12:29.104: INFO: Waiting for responses: map[]
Jul 12 08:12:29.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.206.10:8080/dial?request=hostname&protocol=udp&host=192.168.168.124&port=8081&tries=1'] Namespace:pod-network-test-3654 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:12:29.106: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:12:29.272: INFO: Waiting for responses: map[]
Jul 12 08:12:29.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.206.10:8080/dial?request=hostname&protocol=udp&host=192.168.173.182&port=8081&tries=1'] Namespace:pod-network-test-3654 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:12:29.317: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:12:29.481: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:12:29.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3654" for this suite.

• [SLOW TEST:42.966 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":303,"completed":231,"skipped":3578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:12:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 12 08:12:29.683: INFO: Waiting up to 5m0s for pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d" in namespace "emptydir-2407" to be "Succeeded or Failed"
Jul 12 08:12:29.704: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.616462ms
Jul 12 08:12:31.728: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045549721s
Jul 12 08:12:33.754: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071653485s
Jul 12 08:12:35.769: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086038723s
Jul 12 08:12:37.811: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.128883673s
Jul 12 08:12:39.849: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.166114555s
Jul 12 08:12:41.852: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.169184651s
STEP: Saw pod success
Jul 12 08:12:41.852: INFO: Pod "pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d" satisfied condition "Succeeded or Failed"
Jul 12 08:12:41.854: INFO: Trying to get logs from node 10.32.0.100 pod pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d container test-container: <nil>
STEP: delete the pod
Jul 12 08:12:41.998: INFO: Waiting for pod pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d to disappear
Jul 12 08:12:42.000: INFO: Pod pod-da37e220-ed5b-4ef5-b507-7d31a1bd149d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:12:42.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2407" for this suite.

• [SLOW TEST:12.782 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":232,"skipped":3604,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:12:42.290: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 08:12:43.836: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 08:12:45.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:12:47.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:12:49.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:12:51.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:12:53.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674363, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 08:12:56.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:12:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8513" for this suite.
STEP: Destroying namespace "webhook-8513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.218 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":303,"completed":233,"skipped":3615,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:12:58.509: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:12:59.019: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a6792eb4-f38b-4152-be1d-6e77db075f06", Controller:(*bool)(0xc0062ff87a), BlockOwnerDeletion:(*bool)(0xc0062ff87b)}}
Jul 12 08:12:59.042: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"35b68a2a-15d2-4a8e-9570-431823a32bf9", Controller:(*bool)(0xc006b13c42), BlockOwnerDeletion:(*bool)(0xc006b13c43)}}
Jul 12 08:13:00.085: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b30bae82-713c-469e-be8b-23097aead21d", Controller:(*bool)(0xc0062ffa92), BlockOwnerDeletion:(*bool)(0xc0062ffa93)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:05.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9274" for this suite.

• [SLOW TEST:7.095 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":303,"completed":234,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:05.605: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jul 12 08:13:06.454: INFO: created pod pod-service-account-defaultsa
Jul 12 08:13:06.454: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 12 08:13:06.477: INFO: created pod pod-service-account-mountsa
Jul 12 08:13:06.477: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 12 08:13:06.518: INFO: created pod pod-service-account-nomountsa
Jul 12 08:13:06.518: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 12 08:13:06.577: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 12 08:13:06.577: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 12 08:13:06.614: INFO: created pod pod-service-account-mountsa-mountspec
Jul 12 08:13:06.614: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 12 08:13:06.627: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 12 08:13:06.627: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 12 08:13:07.207: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 12 08:13:07.207: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 12 08:13:07.227: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 12 08:13:07.227: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 12 08:13:07.260: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 12 08:13:07.260: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:07.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6790" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":303,"completed":235,"skipped":3671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:07.907: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:12.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3899" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":303,"completed":236,"skipped":3782,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:12.544: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:13:13.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7" in namespace "downward-api-8620" to be "Succeeded or Failed"
Jul 12 08:13:13.464: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187729ms
Jul 12 08:13:15.467: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005685687s
Jul 12 08:13:17.471: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00894806s
Jul 12 08:13:19.545: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083012457s
Jul 12 08:13:21.548: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085863889s
Jul 12 08:13:23.552: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089929031s
Jul 12 08:13:25.556: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09427198s
Jul 12 08:13:27.610: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.147922526s
STEP: Saw pod success
Jul 12 08:13:27.610: INFO: Pod "downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7" satisfied condition "Succeeded or Failed"
Jul 12 08:13:27.696: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7 container client-container: <nil>
STEP: delete the pod
Jul 12 08:13:27.855: INFO: Waiting for pod downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7 to disappear
Jul 12 08:13:27.864: INFO: Pod downwardapi-volume-85071293-05de-4ea6-b561-243a4318f0d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:27.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8620" for this suite.

• [SLOW TEST:15.368 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":237,"skipped":3795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:27.912: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:40.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7067" for this suite.

• [SLOW TEST:12.795 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":303,"completed":238,"skipped":3819,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:40.708: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jul 12 08:13:40.777: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-439874929 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:13:40.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1782" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":303,"completed":239,"skipped":3819,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:13:41.471: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 08:13:41.667: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 08:13:41.679: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 08:13:41.683: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.100 before test
Jul 12 08:13:41.694: INFO: csi-rbdplugin-8rsp7 from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.694: INFO: calico-node-mrt5l from kube-system started at 2021-07-12 01:23:30 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 08:13:41.694: INFO: coredns-7699c68bdc-nrw9s from kube-system started at 2021-07-12 06:02:49 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container coredns ready: true, restart count 0
Jul 12 08:13:41.694: INFO: cube-logstash-sztvc from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-core-8448fcd4dd-hsvz2 from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container core ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-exporter-5d8b88579-7bpfn from kube-system started at 2021-07-12 06:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container exporter ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-jobservice-5c7d4566ff-lp4hp from kube-system started at 2021-07-12 06:02:50 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-nginx-86f8877965-s48pm from kube-system started at 2021-07-12 06:03:01 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container nginx ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-portal-76856b56c7-p6zqk from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container portal ready: true, restart count 0
Jul 12 08:13:41.694: INFO: harbor-app-harbor-registry-6bc47dd67f-rcrrl from kube-system started at 2021-07-12 06:02:49 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container registry ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 08:13:41.694: INFO: stolon-app-keeper-0 from kube-system started at 2021-07-12 06:02:21 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.694: INFO: stolon-app-proxy-59967b544f-8cssc from kube-system started at 2021-07-12 06:02:40 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.694: INFO: stolon-app-sentinel-5fc7bf8cf8-sfjnr from kube-system started at 2021-07-12 06:02:27 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.694: INFO: alertmanager-main-2 from monitoring started at 2021-07-12 06:02:32 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 08:13:41.694: INFO: cubenode-exporter-p2bg8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 08:13:41.694: INFO: node-exporter-nvbt2 from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 08:13:41.694: INFO: sonobuoy from sonobuoy started at 2021-07-12 06:23:52 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 08:13:41.694: INFO: sonobuoy-e2e-job-c08e481580ac4ebf from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container e2e ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 08:13:41.694: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-k88pn from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 08:13:41.694: INFO: 	Container systemd-logs ready: false, restart count 23
Jul 12 08:13:41.694: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.102 before test
Jul 12 08:13:41.745: INFO: captain-controller-manager-56dd98c4dd-cbspp from captain-system started at 2021-07-09 02:45:32 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container manager ready: true, restart count 6
Jul 12 08:13:41.745: INFO: csi-rbdplugin-46rrc from ceph-csi started at 2021-07-09 02:46:27 +0000 UTC (3 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.745: INFO: csi-rbdplugin-provisioner-74d4496bf7-22rlt from ceph-csi started at 2021-07-12 06:00:59 +0000 UTC (6 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.745: INFO: csi-rbdplugin-provisioner-74d4496bf7-xzzbm from ceph-csi started at 2021-07-09 02:45:32 +0000 UTC (6 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 08:13:41.745: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 08:13:41.745: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.745: INFO: alpine-595b7c4b74-xvrn8 from default started at 2021-07-10 02:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container alpine ready: true, restart count 0
Jul 12 08:13:41.745: INFO: dns-test-994f7f1c-62a2-4bf1-9e8a-c411247e1bde from dns-1249 started at 2021-07-09 07:18:28 +0000 UTC (3 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container jessie-querier ready: true, restart count 382
Jul 12 08:13:41.745: INFO: 	Container querier ready: true, restart count 399
Jul 12 08:13:41.745: INFO: 	Container webserver ready: true, restart count 0
Jul 12 08:13:41.745: INFO: pod-047c84e7-4461-47fd-ad72-3e49a5479adb from emptydir-6449 started at 2021-07-12 02:54:00 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container test-container ready: false, restart count 0
Jul 12 08:13:41.745: INFO: ipsection-controllers-6d8496c6bd-vd86c from ipsection-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container ipsection-controllers ready: true, restart count 0
Jul 12 08:13:41.745: INFO: calico-kube-controllers-668c8b44f6-vfp6k from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 08:13:41.745: INFO: calico-node-x7txv from kube-system started at 2021-07-12 01:23:01 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 08:13:41.745: INFO: coredns-7699c68bdc-bzzjr from kube-system started at 2021-07-10 06:56:57 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container coredns ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-kong-b86d5bf9c-pkln4 from kube-system started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-logging-58f8c874b8-964j2 from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-logstash-xbhqg from kube-system started at 2021-07-09 02:46:27 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-networking-68749499f7-7vqt4 from kube-system started at 2021-07-12 06:01:00 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-node-65789b66c-tglb7 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-openapi-v1-658dd7f87b-nhzqv from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-ops-webapp-85c8594b7b-krvgz from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-storage-ceph-access-5f4db88cdd-hh7xq from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-storage-ceph-access ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-storage-f6786ff45-6q6b2 from kube-system started at 2021-07-12 06:01:02 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-ticket-7955cf677f-scx9z from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cube-webapp-57c694b676-pscrq from kube-system started at 2021-07-12 06:00:59 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 08:13:41.745: INFO: gpushare-schd-extender-9b5766d8c-xx5n2 from kube-system started at 2021-07-12 06:00:52 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container gpushare-schd-extender ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-core-8448fcd4dd-cvk8h from kube-system started at 2021-07-09 02:46:56 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container core ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-exporter-5d8b88579-jsq2l from kube-system started at 2021-07-09 02:46:40 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-jobservice-5c7d4566ff-6brx2 from kube-system started at 2021-07-09 02:46:36 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-nginx-86f8877965-8qcgn from kube-system started at 2021-07-09 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container nginx ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-portal-76856b56c7-b2p7w from kube-system started at 2021-07-09 02:46:30 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container portal ready: true, restart count 0
Jul 12 08:13:41.745: INFO: harbor-app-harbor-registry-6bc47dd67f-dwjmv from kube-system started at 2021-07-09 02:46:39 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container registry ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 08:13:41.745: INFO: kubernetes-dashboard-749844f89d-xflsb from kube-system started at 2021-07-12 06:00:56 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 12 08:13:41.745: INFO: local-path-provisioner-67d77c895b-ptflh from kube-system started at 2021-07-12 06:01:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jul 12 08:13:41.745: INFO: stolon-app-keeper-1 from kube-system started at 2021-07-09 02:46:17 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.745: INFO: stolon-app-proxy-59967b544f-t57n4 from kube-system started at 2021-07-09 02:46:19 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.745: INFO: stolon-app-sentinel-5fc7bf8cf8-5fkrt from kube-system started at 2021-07-09 02:46:06 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.745: INFO: alertmanager-main-1 from monitoring started at 2021-07-09 02:46:01 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 08:13:41.745: INFO: cubenode-exporter-dnhsc from monitoring started at 2021-07-09 02:46:21 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: mysqld-exporter-1-5d6b99c586-bsghg from monitoring started at 2021-07-12 06:01:01 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: mysqld-exporter-2-5f45bbf595-stzdj from monitoring started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container mysqld-exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: node-exporter-dgd55 from monitoring started at 2021-07-08 11:02:46 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: prometheus-k8s-0 from monitoring started at 2021-07-09 02:46:08 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container prometheus ready: true, restart count 1
Jul 12 08:13:41.745: INFO: prometheus-operator-6786cb4fc5-t8456 from monitoring started at 2021-07-09 02:45:33 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 12 08:13:41.745: INFO: redis-exporter-1-5877697f69-86xfb from monitoring started at 2021-07-09 02:45:33 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 08:13:41.745: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-2k6mh from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 08:13:41.745: INFO: 	Container systemd-logs ready: false, restart count 24
Jul 12 08:13:41.745: INFO: 
Logging pods the apiserver thinks is on node 10.32.0.3 before test
Jul 12 08:13:41.767: INFO: csi-rbdplugin-provisioner-74d4496bf7-ch8g6 from ceph-csi started at 2021-07-08 11:06:48 +0000 UTC (6 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container csi-attacher ready: true, restart count 4
Jul 12 08:13:41.767: INFO: 	Container csi-provisioner ready: true, restart count 2
Jul 12 08:13:41.767: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container csi-resizer ready: true, restart count 6
Jul 12 08:13:41.767: INFO: 	Container csi-snapshotter ready: true, restart count 3
Jul 12 08:13:41.767: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.767: INFO: csi-rbdplugin-zcqsm from ceph-csi started at 2021-07-08 11:06:49 +0000 UTC (3 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container liveness-prometheus ready: true, restart count 0
Jul 12 08:13:41.767: INFO: calico-kube-controllers-668c8b44f6-7vr9t from kube-system started at 2021-07-08 09:44:33 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 08:13:41.767: INFO: calico-kube-controllers-668c8b44f6-vcfkc from kube-system started at 2021-07-08 09:44:32 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 12 08:13:41.767: INFO: calico-node-zmmxk from kube-system started at 2021-07-12 01:24:00 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container calico-node ready: true, restart count 0
Jul 12 08:13:41.767: INFO: coredns-7699c68bdc-klbx8 from kube-system started at 2021-07-10 06:57:49 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container coredns ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-appstore-0 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-appstore ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-ha-rabbitmq-0 from kube-system started at 2021-07-08 11:00:57 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container rabbitmq ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-kong-b86d5bf9c-4vtvh from kube-system started at 2021-07-12 06:00:54 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-kong ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-logging-58f8c874b8-mjz4v from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-log ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-logstash-wfm47 from kube-system started at 2021-07-08 11:06:24 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-logstash ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-networking-68749499f7-rt2dc from kube-system started at 2021-07-08 11:02:00 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-networking ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-node-65789b66c-6tmls from kube-system started at 2021-07-09 02:45:30 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-node ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-openapi-v1-658dd7f87b-psc7x from kube-system started at 2021-07-08 11:06:23 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-openapi-v1 ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-ops-webapp-85c8594b7b-b544h from kube-system started at 2021-07-09 02:45:31 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-ops-webapp ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-storage-f6786ff45-7649l from kube-system started at 2021-07-08 11:06:26 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-storage ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-ticket-7955cf677f-jvn8v from kube-system started at 2021-07-09 02:45:21 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-ticket ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cube-webapp-57c694b676-9qpnc from kube-system started at 2021-07-08 11:02:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cube-webapp ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-core-8448fcd4dd-h5fdq from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container core ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-exporter-5d8b88579-9wf79 from kube-system started at 2021-07-08 10:36:46 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container exporter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-jobservice-5c7d4566ff-fsx5c from kube-system started at 2021-07-08 10:36:43 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container jobservice ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-nginx-86f8877965-4rcpv from kube-system started at 2021-07-08 10:36:45 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container nginx ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-portal-76856b56c7-8nczp from kube-system started at 2021-07-08 10:36:44 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container portal ready: true, restart count 0
Jul 12 08:13:41.767: INFO: harbor-app-harbor-registry-6bc47dd67f-7kjrn from kube-system started at 2021-07-08 10:36:43 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container registry ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container registryctl ready: true, restart count 0
Jul 12 08:13:41.767: INFO: kube-mini-chartmuseum-55d66cd548-2b8x2 from kube-system started at 2021-07-08 09:45:51 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container kube-mini-chartmuseum ready: true, restart count 0
Jul 12 08:13:41.767: INFO: logdir-admission-webhook-deployment-5b8587f876-dlgl5 from kube-system started at 2021-07-08 11:06:20 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container logdir-admission-webhook ready: true, restart count 0
Jul 12 08:13:41.767: INFO: metrics-server-86d56f4667-s8nqj from kube-system started at 2021-07-08 11:08:32 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 08:13:41.767: INFO: stolon-app-keeper-2 from kube-system started at 2021-07-08 10:34:49 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.767: INFO: stolon-app-proxy-59967b544f-ww9rx from kube-system started at 2021-07-08 10:33:36 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.767: INFO: stolon-app-sentinel-5fc7bf8cf8-grbwv from kube-system started at 2021-07-08 10:33:33 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container stolon ready: true, restart count 0
Jul 12 08:13:41.767: INFO: alertmanager-main-0 from monitoring started at 2021-07-08 11:02:21 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container alertmanager ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container config-reloader ready: true, restart count 0
Jul 12 08:13:41.767: INFO: cubenode-exporter-dvvm8 from monitoring started at 2021-07-08 11:03:04 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container cubenode-exporter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: grafana-5cd649d575-q2cdw from monitoring started at 2021-07-08 11:02:41 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container grafana ready: true, restart count 0
Jul 12 08:13:41.767: INFO: influxdb-exporter-1-7599cc4587-8f6tn from monitoring started at 2021-07-08 11:08:17 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container influxdb-exporter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: kube-state-metrics-5d5d4d46c-g5zl8 from monitoring started at 2021-07-08 11:02:42 +0000 UTC (3 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 12 08:13:41.767: INFO: node-exporter-d42br from monitoring started at 2021-07-08 11:02:45 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container node-exporter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: prometheus-adapter-84fbb7d77b-4qbgb from monitoring started at 2021-07-08 11:02:56 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: redis-exporter-2-6d778f7cf8-6kvb5 from monitoring started at 2021-07-08 11:08:25 +0000 UTC (1 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container redis-exporter ready: true, restart count 0
Jul 12 08:13:41.767: INFO: sonobuoy-systemd-logs-daemon-set-8f319023755848a8-wd48w from sonobuoy started at 2021-07-12 06:24:02 +0000 UTC (2 container statuses recorded)
Jul 12 08:13:41.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 08:13:41.767: INFO: 	Container systemd-logs ready: false, restart count 23
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e4df9b06-57d2-4da5-9465-1d2dca873b54 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e4df9b06-57d2-4da5-9465-1d2dca873b54 off the node 10.32.0.100
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e4df9b06-57d2-4da5-9465-1d2dca873b54
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:19:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5012" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:325.667 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":303,"completed":240,"skipped":3864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:19:07.139: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 12 08:19:07.408: INFO: Waiting up to 5m0s for pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c" in namespace "emptydir-8905" to be "Succeeded or Failed"
Jul 12 08:19:07.411: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83632ms
Jul 12 08:19:09.438: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029941379s
Jul 12 08:19:11.456: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048172253s
Jul 12 08:19:13.876: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.468582099s
Jul 12 08:19:15.879: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.471594321s
Jul 12 08:19:18.219: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.81152523s
Jul 12 08:19:20.222: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.814223707s
STEP: Saw pod success
Jul 12 08:19:20.222: INFO: Pod "pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c" satisfied condition "Succeeded or Failed"
Jul 12 08:19:20.224: INFO: Trying to get logs from node 10.32.0.100 pod pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c container test-container: <nil>
STEP: delete the pod
Jul 12 08:19:20.439: INFO: Waiting for pod pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c to disappear
Jul 12 08:19:20.442: INFO: Pod pod-23bbd9a9-51d7-4647-bfee-a7bb02a3904c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:19:20.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8905" for this suite.

• [SLOW TEST:13.403 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":241,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:19:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 12 08:19:20.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-5148'
Jul 12 08:19:23.914: INFO: stderr: ""
Jul 12 08:19:23.914: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 08:19:23.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5148'
Jul 12 08:19:24.014: INFO: stderr: ""
Jul 12 08:19:24.014: INFO: stdout: "update-demo-nautilus-4kzn8 update-demo-nautilus-tbndb "
Jul 12 08:19:24.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4kzn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:24.308: INFO: stderr: ""
Jul 12 08:19:24.308: INFO: stdout: ""
Jul 12 08:19:24.308: INFO: update-demo-nautilus-4kzn8 is created but not running
Jul 12 08:19:29.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5148'
Jul 12 08:19:29.405: INFO: stderr: ""
Jul 12 08:19:29.405: INFO: stdout: "update-demo-nautilus-4kzn8 update-demo-nautilus-tbndb "
Jul 12 08:19:29.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4kzn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:29.487: INFO: stderr: ""
Jul 12 08:19:29.487: INFO: stdout: ""
Jul 12 08:19:29.487: INFO: update-demo-nautilus-4kzn8 is created but not running
Jul 12 08:19:34.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5148'
Jul 12 08:19:34.584: INFO: stderr: ""
Jul 12 08:19:34.584: INFO: stdout: "update-demo-nautilus-4kzn8 update-demo-nautilus-tbndb "
Jul 12 08:19:34.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4kzn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:34.680: INFO: stderr: ""
Jul 12 08:19:34.680: INFO: stdout: "true"
Jul 12 08:19:34.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-4kzn8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:34.759: INFO: stderr: ""
Jul 12 08:19:34.759: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 08:19:34.759: INFO: validating pod update-demo-nautilus-4kzn8
Jul 12 08:19:34.764: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 08:19:34.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 08:19:34.764: INFO: update-demo-nautilus-4kzn8 is verified up and running
Jul 12 08:19:34.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-tbndb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:34.853: INFO: stderr: ""
Jul 12 08:19:34.853: INFO: stdout: "true"
Jul 12 08:19:34.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods update-demo-nautilus-tbndb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5148'
Jul 12 08:19:34.948: INFO: stderr: ""
Jul 12 08:19:34.948: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 12 08:19:34.948: INFO: validating pod update-demo-nautilus-tbndb
Jul 12 08:19:34.952: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 08:19:34.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 08:19:34.952: INFO: update-demo-nautilus-tbndb is verified up and running
STEP: using delete to clean up resources
Jul 12 08:19:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 delete --grace-period=0 --force -f - --namespace=kubectl-5148'
Jul 12 08:19:35.061: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 08:19:35.061: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 12 08:19:35.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5148'
Jul 12 08:19:35.232: INFO: stderr: "No resources found in kubectl-5148 namespace.\n"
Jul 12 08:19:35.232: INFO: stdout: ""
Jul 12 08:19:35.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 get pods -l name=update-demo --namespace=kubectl-5148 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 08:19:35.317: INFO: stderr: ""
Jul 12 08:19:35.317: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:19:35.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5148" for this suite.

• [SLOW TEST:14.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":303,"completed":242,"skipped":3944,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:19:35.350: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 08:19:36.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:19:38.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:19:40.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:19:42.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:19:44.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:19:46.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761674776, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 08:19:49.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:19:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1513-crds.webhook.example.com via the AdmissionRegistration API
Jul 12 08:19:55.563: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:19:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8773" for this suite.
STEP: Destroying namespace "webhook-8773-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:21.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":303,"completed":243,"skipped":3965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:19:56.920: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:19:57.914: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 08:20:07.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-8346 create -f -'
Jul 12 08:20:11.176: INFO: stderr: ""
Jul 12 08:20:11.176: INFO: stdout: "e2e-test-crd-publish-openapi-9954-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 12 08:20:11.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-8346 delete e2e-test-crd-publish-openapi-9954-crds test-cr'
Jul 12 08:20:11.310: INFO: stderr: ""
Jul 12 08:20:11.310: INFO: stdout: "e2e-test-crd-publish-openapi-9954-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 12 08:20:11.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-8346 apply -f -'
Jul 12 08:20:11.655: INFO: stderr: ""
Jul 12 08:20:11.655: INFO: stdout: "e2e-test-crd-publish-openapi-9954-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 12 08:20:11.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 --namespace=crd-publish-openapi-8346 delete e2e-test-crd-publish-openapi-9954-crds test-cr'
Jul 12 08:20:11.782: INFO: stderr: ""
Jul 12 08:20:11.782: INFO: stdout: "e2e-test-crd-publish-openapi-9954-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 12 08:20:11.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 explain e2e-test-crd-publish-openapi-9954-crds'
Jul 12 08:20:12.167: INFO: stderr: ""
Jul 12 08:20:12.167: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9954-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:20:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8346" for this suite.

• [SLOW TEST:19.837 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":303,"completed":244,"skipped":4042,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:20:16.757: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 12 08:20:17.062: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 08:21:17.145: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 12 08:21:17.209: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 12 08:21:17.350: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul 12 08:21:17.513: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:22:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7285" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:113.982 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":303,"completed":245,"skipped":4059,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:22:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:22:10.944: INFO: Create a RollingUpdate DaemonSet
Jul 12 08:22:10.972: INFO: Check that daemon pods launch on every node of the cluster
Jul 12 08:22:11.045: INFO: Number of nodes with available pods: 0
Jul 12 08:22:11.045: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:12.924: INFO: Number of nodes with available pods: 0
Jul 12 08:22:12.924: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:13.059: INFO: Number of nodes with available pods: 0
Jul 12 08:22:13.059: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:14.053: INFO: Number of nodes with available pods: 0
Jul 12 08:22:14.053: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:15.053: INFO: Number of nodes with available pods: 0
Jul 12 08:22:15.053: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:16.052: INFO: Number of nodes with available pods: 0
Jul 12 08:22:16.052: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:17.088: INFO: Number of nodes with available pods: 0
Jul 12 08:22:17.088: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:18.149: INFO: Number of nodes with available pods: 0
Jul 12 08:22:18.149: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:19.053: INFO: Number of nodes with available pods: 0
Jul 12 08:22:19.053: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:20.409: INFO: Number of nodes with available pods: 0
Jul 12 08:22:20.409: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:21.057: INFO: Number of nodes with available pods: 1
Jul 12 08:22:21.057: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:22.438: INFO: Number of nodes with available pods: 1
Jul 12 08:22:22.438: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:22:23.450: INFO: Number of nodes with available pods: 2
Jul 12 08:22:23.450: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:22:24.117: INFO: Number of nodes with available pods: 3
Jul 12 08:22:24.117: INFO: Number of running nodes: 3, number of available pods: 3
Jul 12 08:22:24.117: INFO: Update the DaemonSet to trigger a rollout
Jul 12 08:22:24.140: INFO: Updating DaemonSet daemon-set
Jul 12 08:22:40.154: INFO: Roll back the DaemonSet before rollout is complete
Jul 12 08:22:41.992: INFO: Updating DaemonSet daemon-set
Jul 12 08:22:41.992: INFO: Make sure DaemonSet rollback is complete
Jul 12 08:22:42.676: INFO: Wrong image for pod: daemon-set-4wkcq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 12 08:22:42.676: INFO: Pod daemon-set-4wkcq is not available
Jul 12 08:22:43.705: INFO: Wrong image for pod: daemon-set-4wkcq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 12 08:22:43.705: INFO: Pod daemon-set-4wkcq is not available
Jul 12 08:22:44.741: INFO: Wrong image for pod: daemon-set-4wkcq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 12 08:22:44.741: INFO: Pod daemon-set-4wkcq is not available
Jul 12 08:22:45.706: INFO: Wrong image for pod: daemon-set-4wkcq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 12 08:22:45.706: INFO: Pod daemon-set-4wkcq is not available
Jul 12 08:22:47.005: INFO: Wrong image for pod: daemon-set-4wkcq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 12 08:22:47.005: INFO: Pod daemon-set-4wkcq is not available
Jul 12 08:22:47.706: INFO: Pod daemon-set-wdshr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5701, will wait for the garbage collector to delete the pods
Jul 12 08:22:47.867: INFO: Deleting DaemonSet.extensions daemon-set took: 20.052287ms
Jul 12 08:22:47.968: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.760113ms
Jul 12 08:22:58.671: INFO: Number of nodes with available pods: 0
Jul 12 08:22:58.671: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 08:22:58.680: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5701/daemonsets","resourceVersion":"2863335"},"items":null}

Jul 12 08:22:58.686: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5701/pods","resourceVersion":"2863335"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:22:58.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5701" for this suite.

• [SLOW TEST:48.029 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":303,"completed":246,"skipped":4062,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:22:58.769: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-def090d4-fee6-4712-930c-b694c56efbae
STEP: Creating a pod to test consume secrets
Jul 12 08:22:58.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea" in namespace "projected-3901" to be "Succeeded or Failed"
Jul 12 08:22:58.992: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950793ms
Jul 12 08:23:01.082: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091943515s
Jul 12 08:23:03.085: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095101576s
Jul 12 08:23:05.088: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098034315s
Jul 12 08:23:07.125: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.135380382s
Jul 12 08:23:09.181: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.191692335s
STEP: Saw pod success
Jul 12 08:23:09.182: INFO: Pod "pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea" satisfied condition "Succeeded or Failed"
Jul 12 08:23:09.238: INFO: Trying to get logs from node 10.32.0.100 pod pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 08:23:09.830: INFO: Waiting for pod pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea to disappear
Jul 12 08:23:09.832: INFO: Pod pod-projected-secrets-f3ed28fe-60ef-48d4-a34b-c2d970ace8ea no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:23:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3901" for this suite.

• [SLOW TEST:11.090 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":247,"skipped":4070,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:23:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:23:10.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57" in namespace "projected-7853" to be "Succeeded or Failed"
Jul 12 08:23:10.158: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 1.903326ms
Jul 12 08:23:12.235: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079065005s
Jul 12 08:23:14.239: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08254342s
Jul 12 08:23:16.242: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08535502s
Jul 12 08:23:18.245: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08919295s
Jul 12 08:23:20.264: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107830261s
Jul 12 08:23:22.295: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.139278418s
STEP: Saw pod success
Jul 12 08:23:22.296: INFO: Pod "downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57" satisfied condition "Succeeded or Failed"
Jul 12 08:23:22.307: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57 container client-container: <nil>
STEP: delete the pod
Jul 12 08:23:22.367: INFO: Waiting for pod downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57 to disappear
Jul 12 08:23:22.422: INFO: Pod downwardapi-volume-436f68b4-587c-469e-b380-54a9dd34de57 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:23:22.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7853" for this suite.

• [SLOW TEST:12.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":248,"skipped":4076,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:23:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:23:23.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-714" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":249,"skipped":4077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:23:23.177: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:23:23.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5" in namespace "downward-api-2614" to be "Succeeded or Failed"
Jul 12 08:23:23.778: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 80.087233ms
Jul 12 08:23:25.781: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083070352s
Jul 12 08:23:27.785: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086587369s
Jul 12 08:23:29.791: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092792179s
Jul 12 08:23:31.808: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109961481s
Jul 12 08:23:33.812: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113584987s
Jul 12 08:23:35.815: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.116748805s
STEP: Saw pod success
Jul 12 08:23:35.815: INFO: Pod "downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5" satisfied condition "Succeeded or Failed"
Jul 12 08:23:35.818: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5 container client-container: <nil>
STEP: delete the pod
Jul 12 08:23:35.942: INFO: Waiting for pod downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5 to disappear
Jul 12 08:23:35.944: INFO: Pod downwardapi-volume-b3156d09-5c3e-43bc-aafb-61e5a22140a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:23:35.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2614" for this suite.

• [SLOW TEST:12.782 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":250,"skipped":4132,"failed":0}
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:23:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:23:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Creating first CR 
Jul 12 08:23:42.743: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:23:41Z]] name:name1 resourceVersion:2863847 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:72dc4071-8fee-4604-a0df-0ebaa461809c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 12 08:23:52.754: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:23:52Z]] name:name2 resourceVersion:2863933 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21d4351a-9deb-4a42-8619-138f20e98867] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 12 08:24:02.784: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:24:02Z]] name:name1 resourceVersion:2864014 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:72dc4071-8fee-4604-a0df-0ebaa461809c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 12 08:24:12.797: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:24:12Z]] name:name2 resourceVersion:2864094 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21d4351a-9deb-4a42-8619-138f20e98867] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 12 08:24:22.859: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:24:02Z]] name:name1 resourceVersion:2864174 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:72dc4071-8fee-4604-a0df-0ebaa461809c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 12 08:24:32.892: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T08:23:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T08:24:12Z]] name:name2 resourceVersion:2864251 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21d4351a-9deb-4a42-8619-138f20e98867] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:24:43.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5058" for this suite.

• [SLOW TEST:67.574 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":303,"completed":251,"skipped":4132,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:24:43.533: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 12 08:24:43.730: INFO: namespace kubectl-8628
Jul 12 08:24:43.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 create -f - --namespace=kubectl-8628'
Jul 12 08:24:44.531: INFO: stderr: ""
Jul 12 08:24:44.531: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 08:24:45.535: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:45.535: INFO: Found 0 / 1
Jul 12 08:24:46.535: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:46.535: INFO: Found 0 / 1
Jul 12 08:24:47.534: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:47.534: INFO: Found 0 / 1
Jul 12 08:24:48.545: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:48.545: INFO: Found 0 / 1
Jul 12 08:24:49.908: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:49.908: INFO: Found 0 / 1
Jul 12 08:24:50.550: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:50.550: INFO: Found 0 / 1
Jul 12 08:24:52.202: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:52.202: INFO: Found 0 / 1
Jul 12 08:24:52.859: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:52.859: INFO: Found 0 / 1
Jul 12 08:24:53.534: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:53.534: INFO: Found 0 / 1
Jul 12 08:24:54.610: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:54.610: INFO: Found 1 / 1
Jul 12 08:24:54.610: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 12 08:24:54.634: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 08:24:54.634: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 08:24:54.634: INFO: wait on agnhost-primary startup in kubectl-8628 
Jul 12 08:24:54.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 logs agnhost-primary-5dh2n agnhost-primary --namespace=kubectl-8628'
Jul 12 08:24:54.741: INFO: stderr: ""
Jul 12 08:24:54.741: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 12 08:24:54.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8628'
Jul 12 08:24:54.890: INFO: stderr: ""
Jul 12 08:24:54.890: INFO: stdout: "service/rm2 exposed\n"
Jul 12 08:24:54.893: INFO: Service rm2 in namespace kubectl-8628 found.
STEP: exposing service
Jul 12 08:24:56.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8628'
Jul 12 08:24:57.136: INFO: stderr: ""
Jul 12 08:24:57.136: INFO: stdout: "service/rm3 exposed\n"
Jul 12 08:24:57.160: INFO: Service rm3 in namespace kubectl-8628 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:24:59.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8628" for this suite.

• [SLOW TEST:15.660 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":303,"completed":252,"skipped":4150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:24:59.194: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:24:59.921: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 12 08:24:59.922: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 12 08:24:59.922: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.922: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 12 08:24:59.922: INFO: Checking APIGroup: extensions
Jul 12 08:24:59.922: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 12 08:24:59.923: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 12 08:24:59.923: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 12 08:24:59.923: INFO: Checking APIGroup: apps
Jul 12 08:24:59.923: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 12 08:24:59.923: INFO: Versions found [{apps/v1 v1}]
Jul 12 08:24:59.923: INFO: apps/v1 matches apps/v1
Jul 12 08:24:59.923: INFO: Checking APIGroup: events.k8s.io
Jul 12 08:24:59.924: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 12 08:24:59.924: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.924: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 12 08:24:59.924: INFO: Checking APIGroup: authentication.k8s.io
Jul 12 08:24:59.925: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 12 08:24:59.925: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.925: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 12 08:24:59.926: INFO: Checking APIGroup: authorization.k8s.io
Jul 12 08:24:59.927: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 12 08:24:59.927: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.927: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 12 08:24:59.927: INFO: Checking APIGroup: autoscaling
Jul 12 08:24:59.928: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 12 08:24:59.928: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 12 08:24:59.928: INFO: autoscaling/v1 matches autoscaling/v1
Jul 12 08:24:59.928: INFO: Checking APIGroup: batch
Jul 12 08:24:59.929: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 12 08:24:59.929: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 12 08:24:59.929: INFO: batch/v1 matches batch/v1
Jul 12 08:24:59.929: INFO: Checking APIGroup: certificates.k8s.io
Jul 12 08:24:59.930: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 12 08:24:59.930: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.930: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 12 08:24:59.930: INFO: Checking APIGroup: networking.k8s.io
Jul 12 08:24:59.930: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 12 08:24:59.930: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.930: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 12 08:24:59.930: INFO: Checking APIGroup: policy
Jul 12 08:24:59.931: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jul 12 08:24:59.931: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jul 12 08:24:59.931: INFO: policy/v1beta1 matches policy/v1beta1
Jul 12 08:24:59.931: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 12 08:24:59.932: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 12 08:24:59.932: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.932: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 12 08:24:59.932: INFO: Checking APIGroup: storage.k8s.io
Jul 12 08:24:59.933: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 12 08:24:59.933: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.933: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 12 08:24:59.933: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 12 08:24:59.934: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 12 08:24:59.934: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.934: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 12 08:24:59.934: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 12 08:24:59.934: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 12 08:24:59.935: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.935: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 12 08:24:59.935: INFO: Checking APIGroup: scheduling.k8s.io
Jul 12 08:24:59.935: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 12 08:24:59.935: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.935: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 12 08:24:59.935: INFO: Checking APIGroup: coordination.k8s.io
Jul 12 08:24:59.936: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 12 08:24:59.936: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.936: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 12 08:24:59.936: INFO: Checking APIGroup: node.k8s.io
Jul 12 08:24:59.937: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jul 12 08:24:59.937: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.937: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jul 12 08:24:59.937: INFO: Checking APIGroup: discovery.k8s.io
Jul 12 08:24:59.938: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jul 12 08:24:59.938: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.938: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jul 12 08:24:59.938: INFO: Checking APIGroup: core.dahuatech.com
Jul 12 08:24:59.938: INFO: PreferredVersion.GroupVersion: core.dahuatech.com/v1
Jul 12 08:24:59.938: INFO: Versions found [{core.dahuatech.com/v1 v1}]
Jul 12 08:24:59.938: INFO: core.dahuatech.com/v1 matches core.dahuatech.com/v1
Jul 12 08:24:59.938: INFO: Checking APIGroup: crd.projectcalico.org
Jul 12 08:24:59.939: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul 12 08:24:59.939: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul 12 08:24:59.939: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul 12 08:24:59.939: INFO: Checking APIGroup: ipsection.ipsections
Jul 12 08:24:59.940: INFO: PreferredVersion.GroupVersion: ipsection.ipsections/v1
Jul 12 08:24:59.940: INFO: Versions found [{ipsection.ipsections/v1 v1}]
Jul 12 08:24:59.940: INFO: ipsection.ipsections/v1 matches ipsection.ipsections/v1
Jul 12 08:24:59.940: INFO: Checking APIGroup: localstorage.dahuatech.com
Jul 12 08:24:59.941: INFO: PreferredVersion.GroupVersion: localstorage.dahuatech.com/v1
Jul 12 08:24:59.941: INFO: Versions found [{localstorage.dahuatech.com/v1 v1}]
Jul 12 08:24:59.941: INFO: localstorage.dahuatech.com/v1 matches localstorage.dahuatech.com/v1
Jul 12 08:24:59.941: INFO: Checking APIGroup: monitoring.coreos.com
Jul 12 08:24:59.941: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jul 12 08:24:59.941: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jul 12 08:24:59.941: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jul 12 08:24:59.941: INFO: Checking APIGroup: app.alauda.io
Jul 12 08:24:59.942: INFO: PreferredVersion.GroupVersion: app.alauda.io/v1beta1
Jul 12 08:24:59.942: INFO: Versions found [{app.alauda.io/v1beta1 v1beta1} {app.alauda.io/v1alpha1 v1alpha1}]
Jul 12 08:24:59.942: INFO: app.alauda.io/v1beta1 matches app.alauda.io/v1beta1
Jul 12 08:24:59.942: INFO: Checking APIGroup: dahuatech.com
Jul 12 08:24:59.942: INFO: PreferredVersion.GroupVersion: dahuatech.com/v1alpha1
Jul 12 08:24:59.942: INFO: Versions found [{dahuatech.com/v1alpha1 v1alpha1}]
Jul 12 08:24:59.942: INFO: dahuatech.com/v1alpha1 matches dahuatech.com/v1alpha1
Jul 12 08:24:59.942: INFO: Checking APIGroup: metrics.k8s.io
Jul 12 08:24:59.943: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 12 08:24:59.943: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 12 08:24:59.943: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:24:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4903" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":303,"completed":253,"skipped":4183,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:24:59.977: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:25:00.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd" in namespace "projected-1793" to be "Succeeded or Failed"
Jul 12 08:25:00.254: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.221092ms
Jul 12 08:25:02.257: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005456769s
Jul 12 08:25:04.274: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021695431s
Jul 12 08:25:06.277: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025160887s
Jul 12 08:25:08.280: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028047976s
Jul 12 08:25:10.284: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031816007s
Jul 12 08:25:12.288: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.035984905s
STEP: Saw pod success
Jul 12 08:25:12.288: INFO: Pod "downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd" satisfied condition "Succeeded or Failed"
Jul 12 08:25:12.291: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd container client-container: <nil>
STEP: delete the pod
Jul 12 08:25:12.391: INFO: Waiting for pod downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd to disappear
Jul 12 08:25:12.393: INFO: Pod downwardapi-volume-563f204f-52ba-441a-a527-011e2a2cc2dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:25:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1793" for this suite.

• [SLOW TEST:12.439 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":254,"skipped":4184,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:25:12.416: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2f50f095-3a0d-4fa2-b569-aa174b2f4ed5
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2f50f095-3a0d-4fa2-b569-aa174b2f4ed5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:25:24.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8678" for this suite.

• [SLOW TEST:12.340 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":255,"skipped":4194,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:25:24.756: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-059605b2-e878-4254-944a-5955a88bd6f0
STEP: Creating a pod to test consume configMaps
Jul 12 08:25:24.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a" in namespace "configmap-9979" to be "Succeeded or Failed"
Jul 12 08:25:24.967: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922545ms
Jul 12 08:25:26.970: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006124206s
Jul 12 08:25:28.973: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008983568s
Jul 12 08:25:30.976: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01174602s
Jul 12 08:25:32.999: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03525239s
Jul 12 08:25:35.003: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038673744s
Jul 12 08:25:37.216: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.252019818s
STEP: Saw pod success
Jul 12 08:25:37.216: INFO: Pod "pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a" satisfied condition "Succeeded or Failed"
Jul 12 08:25:37.219: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 08:25:37.298: INFO: Waiting for pod pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a to disappear
Jul 12 08:25:37.300: INFO: Pod pod-configmaps-51352349-195d-4c14-8483-5f43e1d6722a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:25:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9979" for this suite.

• [SLOW TEST:13.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":256,"skipped":4203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:25:37.935: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:25:53.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1750" for this suite.

• [SLOW TEST:15.466 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":303,"completed":257,"skipped":4229,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:25:53.402: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-946d8d1e-d0ce-4cf6-976c-5ed9b79b3ee0
STEP: Creating a pod to test consume secrets
Jul 12 08:25:53.668: INFO: Waiting up to 5m0s for pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8" in namespace "secrets-5078" to be "Succeeded or Failed"
Jul 12 08:25:53.670: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943852ms
Jul 12 08:25:55.679: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01037931s
Jul 12 08:25:57.683: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014866317s
Jul 12 08:25:59.687: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018442156s
Jul 12 08:26:01.690: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021475117s
Jul 12 08:26:03.693: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024679418s
Jul 12 08:26:05.710: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.041167508s
STEP: Saw pod success
Jul 12 08:26:05.710: INFO: Pod "pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8" satisfied condition "Succeeded or Failed"
Jul 12 08:26:05.712: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 08:26:05.878: INFO: Waiting for pod pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8 to disappear
Jul 12 08:26:05.880: INFO: Pod pod-secrets-36518d68-5d66-481a-8734-e2d6b8350ab8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:26:05.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5078" for this suite.

• [SLOW TEST:12.504 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":258,"skipped":4230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:26:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jul 12 08:26:06.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 cluster-info'
Jul 12 08:26:06.165: INFO: stderr: ""
Jul 12 08:26:06.166: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:26:06.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5687" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":303,"completed":259,"skipped":4254,"failed":0}

------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:26:06.179: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-3efffdf5-2ba3-4ae8-ad37-9d4fcec1c6f0
STEP: Creating secret with name s-test-opt-upd-6a70d659-3fe5-48d4-bfc1-73ed9a4168f9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3efffdf5-2ba3-4ae8-ad37-9d4fcec1c6f0
STEP: Updating secret s-test-opt-upd-6a70d659-3fe5-48d4-bfc1-73ed9a4168f9
STEP: Creating secret with name s-test-opt-create-0a0fbbd7-b9e1-471d-b75e-1fb91924be4f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:27:49.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-892" for this suite.

• [SLOW TEST:102.972 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":260,"skipped":4254,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:27:49.152: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1774
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1774
I0712 08:27:49.524123      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1774, replica count: 2
I0712 08:27:52.574431      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 08:27:55.574683      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 08:27:58.574893      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 08:28:01.575: INFO: Creating new exec pod
I0712 08:28:01.575092      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 08:28:15.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1774 execpodbnfrz -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 12 08:28:16.323: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 12 08:28:16.323: INFO: stdout: ""
Jul 12 08:28:16.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1774 execpodbnfrz -- /bin/sh -x -c nc -zv -t -w 2 10.254.180.41 80'
Jul 12 08:28:16.684: INFO: stderr: "+ nc -zv -t -w 2 10.254.180.41 80\nConnection to 10.254.180.41 80 port [tcp/http] succeeded!\n"
Jul 12 08:28:16.684: INFO: stdout: ""
Jul 12 08:28:16.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1774 execpodbnfrz -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.3 30732'
Jul 12 08:28:16.916: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.3 30732\nConnection to 10.32.0.3 30732 port [tcp/30732] succeeded!\n"
Jul 12 08:28:16.916: INFO: stdout: ""
Jul 12 08:28:16.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-439874929 exec --namespace=services-1774 execpodbnfrz -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.100 30732'
Jul 12 08:28:17.162: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.100 30732\nConnection to 10.32.0.100 30732 port [tcp/30732] succeeded!\n"
Jul 12 08:28:17.162: INFO: stdout: ""
Jul 12 08:28:17.162: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:28:17.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1774" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.198 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":303,"completed":261,"skipped":4262,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:28:17.349: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-7692ccd4-207d-4d4c-a968-fd8207e07f74
STEP: Creating configMap with name cm-test-opt-upd-875326bd-480f-49cd-8b2a-297e042e5cf0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7692ccd4-207d-4d4c-a968-fd8207e07f74
STEP: Updating configmap cm-test-opt-upd-875326bd-480f-49cd-8b2a-297e042e5cf0
STEP: Creating configMap with name cm-test-opt-create-579a3401-d3a7-4db0-80fe-0f0450c54f5b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:29:45.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7804" for this suite.

• [SLOW TEST:88.235 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":262,"skipped":4273,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:29:45.585: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:29:45.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6965" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":303,"completed":263,"skipped":4282,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:29:45.818: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:29:45.903: INFO: Creating deployment "webserver-deployment"
Jul 12 08:29:45.909: INFO: Waiting for observed generation 1
Jul 12 08:29:49.834: INFO: Waiting for all required pods to come up
Jul 12 08:29:50.063: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 12 08:30:08.069: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 12 08:30:08.073: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 12 08:30:08.105: INFO: Updating deployment webserver-deployment
Jul 12 08:30:08.106: INFO: Waiting for observed generation 2
Jul 12 08:30:10.133: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 12 08:30:12.217: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 12 08:30:12.219: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 12 08:30:12.226: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 12 08:30:12.226: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 12 08:30:12.228: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 12 08:30:12.233: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 12 08:30:12.233: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 12 08:30:12.263: INFO: Updating deployment webserver-deployment
Jul 12 08:30:12.263: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 12 08:30:12.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 12 08:30:14.281: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 12 08:30:15.692: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7622 /apis/apps/v1/namespaces/deployment-7622/deployments/webserver-deployment 8563465f-437f-40ad-bfa5-ad9114953952 2867705 3 2021-07-12 08:29:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 08:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 08:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003881668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-12 08:30:10 +0000 UTC,LastTransitionTime:2021-07-12 08:29:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-12 08:30:12 +0000 UTC,LastTransitionTime:2021-07-12 08:30:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 12 08:30:16.122: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7622 /apis/apps/v1/namespaces/deployment-7622/replicasets/webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 2867700 3 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8563465f-437f-40ad-bfa5-ad9114953952 0xc001e311e7 0xc001e311e8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 08:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8563465f-437f-40ad-bfa5-ad9114953952\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e31318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:30:16.122: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 12 08:30:16.122: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-7622 /apis/apps/v1/namespaces/deployment-7622/replicasets/webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 2867781 3 2021-07-12 08:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8563465f-437f-40ad-bfa5-ad9114953952 0xc001e313f7 0xc001e313f8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 08:30:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8563465f-437f-40ad-bfa5-ad9114953952\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e31478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 12 08:30:16.944: INFO: Pod "webserver-deployment-795d758f88-6pbpj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6pbpj webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-6pbpj b982a878-8af4-43ae-a1b4-146f85d8d112 2867629 0 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc003881ae7 0xc003881ae8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.102,PodIP:,StartTime:2021-07-12 08:30:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.944: INFO: Pod "webserver-deployment-795d758f88-7dp86" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7dp86 webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-7dp86 c2327331-898b-47cd-a31c-2b941cfc37fa 2867752 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc003881ca7 0xc003881ca8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.944: INFO: Pod "webserver-deployment-795d758f88-8m5qw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8m5qw webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-8m5qw 8fcd70d9-d15c-4cc6-80c8-a4861a7e3f79 2867759 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc003881de7 0xc003881de8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.945: INFO: Pod "webserver-deployment-795d758f88-bwgj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bwgj7 webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-bwgj7 6b609d05-a1fa-4a50-aa7c-86370048e233 2867634 0 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc003881f27 0xc003881f28}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:,StartTime:2021-07-12 08:30:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.945: INFO: Pod "webserver-deployment-795d758f88-bxqbd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bxqbd webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-bxqbd 0aae108a-08ad-4995-86e9-5a84c55f36c4 2867775 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aea387 0xc002aea388}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.945: INFO: Pod "webserver-deployment-795d758f88-mk8dz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mk8dz webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-mk8dz 4cdf7cff-5beb-4b06-b236-34cb1c224685 2867783 0 2021-07-12 08:30:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aea837 0xc002aea838}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.946: INFO: Pod "webserver-deployment-795d758f88-n99tq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n99tq webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-n99tq 8fd61e92-7f0c-4449-927e-f7a6d68d7c86 2867684 0 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeab00 0xc002aeab01}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:,StartTime:2021-07-12 08:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.946: INFO: Pod "webserver-deployment-795d758f88-nl8w7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nl8w7 webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-nl8w7 551f9fa7-6354-47ce-9298-53b9a1bbd982 2867786 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeaca7 0xc002aeaca8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.947: INFO: Pod "webserver-deployment-795d758f88-pnp8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pnp8z webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-pnp8z ead5aa44-9c66-4cc2-ab34-7132c287d2f3 2867679 0 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeae67 0xc002aeae68}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:,StartTime:2021-07-12 08:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.947: INFO: Pod "webserver-deployment-795d758f88-s9jf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s9jf8 webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-s9jf8 f0ab2f27-ebcc-4448-8d92-d9bc9677e5b1 2867787 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeb017 0xc002aeb018}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:,StartTime:2021-07-12 08:30:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.948: INFO: Pod "webserver-deployment-795d758f88-wcq62" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wcq62 webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-wcq62 4bc0313a-91db-4f99-ba7e-30ae766ad4ee 2867639 0 2021-07-12 08:30:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeb1c7 0xc002aeb1c8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:,StartTime:2021-07-12 08:30:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.948: INFO: Pod "webserver-deployment-795d758f88-zdqxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zdqxt webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-zdqxt a91140c5-6538-49ff-9dc5-d245c1235916 2867715 0 2021-07-12 08:30:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeb377 0xc002aeb378}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.948: INFO: Pod "webserver-deployment-795d758f88-zzmgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zzmgj webserver-deployment-795d758f88- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-795d758f88-zzmgj 78d55c1f-0f5a-40b6-8dc3-daed1d023995 2867730 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 314d3e1c-7650-4502-98c9-84b6ef8b8085 0xc002aeb4b7 0xc002aeb4b8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314d3e1c-7650-4502-98c9-84b6ef8b8085\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.949: INFO: Pod "webserver-deployment-dd94f59b7-4dgmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4dgmq webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-4dgmq 68592d14-f67f-426f-bfb7-e84b64c8b4a4 2867779 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aeb5f7 0xc002aeb5f8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.949: INFO: Pod "webserver-deployment-dd94f59b7-4x5rs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4x5rs webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-4x5rs d241c631-ae02-445b-96c7-302e83a0f2e0 2867748 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aeb727 0xc002aeb728}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.102,PodIP:,StartTime:2021-07-12 08:30:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.950: INFO: Pod "webserver-deployment-dd94f59b7-56jwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-56jwd webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-56jwd 8aad70ab-ab44-4b6b-aaf6-8c562600ec1b 2867727 0 2021-07-12 08:30:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aeb8b7 0xc002aeb8b8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.102,PodIP:,StartTime:2021-07-12 08:30:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.950: INFO: Pod "webserver-deployment-dd94f59b7-75b7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-75b7v webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-75b7v 14d46f2f-001d-437b-9458-edb0a493877e 2867769 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aeba47 0xc002aeba48}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.950: INFO: Pod "webserver-deployment-dd94f59b7-7mczl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7mczl webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-7mczl 2de8ca49-de63-477f-81a2-eee63beb00a8 2867760 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aebb77 0xc002aebb78}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.951: INFO: Pod "webserver-deployment-dd94f59b7-8cznc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8cznc webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-8cznc 167fae9d-c424-4093-8cad-f505ad05e572 2867402 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.173.188/26 cni.projectcalico.org/podIP:192.168.173.188/32 cni.projectcalico.org/podIPs:192.168.173.188/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.173.188"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aebc9f 0xc002aebcc0}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.173.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:192.168.173.188,StartTime:2021-07-12 08:29:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:29:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://a0abb15aa7ac7096ae3d4f4edaef6fe3903bdf469db13b011484ab785bea7fab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.173.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.951: INFO: Pod "webserver-deployment-dd94f59b7-98t9v" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-98t9v webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-98t9v 884d1f67-181e-4f9b-a935-95dc0a7459ca 2867436 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.206.36/26 cni.projectcalico.org/podIP:192.168.206.36/32 cni.projectcalico.org/podIPs:192.168.206.36/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.206.36"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc002aebe8f 0xc002aebea0}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:29:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.206.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.206.36,StartTime:2021-07-12 08:29:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:29:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://18867cf74170e87b8783e268f390dd8f01264eebd2d29126af39ed9cfb3b1102,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.206.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.952: INFO: Pod "webserver-deployment-dd94f59b7-9kqgk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9kqgk webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-9kqgk 1b04fbaf-5a0c-462e-90b1-66a2af155262 2867762 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886077 0xc008886078}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:,StartTime:2021-07-12 08:30:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.952: INFO: Pod "webserver-deployment-dd94f59b7-hmqqn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hmqqn webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-hmqqn 6b8a2da1-9eab-4bda-9bf1-38fac387ebeb 2867743 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886217 0xc008886218}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.952: INFO: Pod "webserver-deployment-dd94f59b7-jq4sq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jq4sq webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-jq4sq 65ec08cb-77cc-4392-a14c-316b471968bf 2867746 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886347 0xc008886348}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:,StartTime:2021-07-12 08:30:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.953: INFO: Pod "webserver-deployment-dd94f59b7-kjq4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kjq4f webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-kjq4f bd436499-3e04-4bb1-a661-df49c3ff3d13 2867770 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc0088864d7 0xc0088864d8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.953: INFO: Pod "webserver-deployment-dd94f59b7-njszw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-njszw webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-njszw a5f366c3-6212-44ff-af2e-a9e7c26d223a 2867515 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.206.37/26 cni.projectcalico.org/podIP:192.168.206.37/32 cni.projectcalico.org/podIPs:192.168.206.37/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.206.37"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc0088865ff 0xc008886610}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:30:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.206.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.206.37,StartTime:2021-07-12 08:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:30:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://412ef7eb96ae0b7ca6eb51f1e2656cf59dac27925c86dbd946e5bd09a359bd7b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.206.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.953: INFO: Pod "webserver-deployment-dd94f59b7-q4srh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q4srh webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-q4srh f3dad782-0fca-4d2a-9de2-2de7083a816b 2867757 0 2021-07-12 08:30:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc0088867e7 0xc0088867e8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.953: INFO: Pod "webserver-deployment-dd94f59b7-q9qbs" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q9qbs webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-q9qbs 6d694bb6-0b38-48aa-8a6e-538b54ae67b6 2867388 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.173.187/26 cni.projectcalico.org/podIP:192.168.173.187/32 cni.projectcalico.org/podIPs:192.168.173.187/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.173.187"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc00888690f 0xc008886920}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.173.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:192.168.173.187,StartTime:2021-07-12 08:29:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:29:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://cb455c7ca0dc87642b8fdf3cf5e0f507ba938be2945436d87dbec8ab8cd63463,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.173.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-r4jj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r4jj8 webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-r4jj8 59220d53-c180-41bb-b4bb-64364a82c20f 2867732 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886af7 0xc008886af8}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-sb2x9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sb2x9 webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-sb2x9 f709cd65-f24d-42fc-bf61-eecbd5813458 2867532 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.206.38/26 cni.projectcalico.org/podIP:192.168.206.38/32 cni.projectcalico.org/podIPs:192.168.206.38/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.206.38"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886c1f 0xc008886c30}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:30:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.206.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.206.38,StartTime:2021-07-12 08:29:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:30:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://1bc6d7b7170e519c5b23fb1746d559bca7c490930fe1a58deea5bf290da90001,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.206.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-shvvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-shvvp webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-shvvp 0de334e8-affd-40e9-a68a-ffc72bf0a0ce 2867719 0 2021-07-12 08:30:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886e07 0xc008886e08}] []  [{kube-controller-manager Update v1 2021-07-12 08:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-vfr8d" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vfr8d webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-vfr8d 36209dc7-bf46-4244-815b-586eca805bac 2867586 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.168.64/26 cni.projectcalico.org/podIP:192.168.168.64/32 cni.projectcalico.org/podIPs:192.168.168.64/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.168.64"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc008886f2f 0xc008886f40}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:30:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.168.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.102,PodIP:192.168.168.64,StartTime:2021-07-12 08:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:30:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://3a6ec6a0e01f75e3625bb518a794221227b173bade416180347c26bf4c3e8740,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.168.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-wlbcs" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wlbcs webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-wlbcs 2e21e2c6-ab0a-4881-9667-f850f8497b04 2867548 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.206.39/26 cni.projectcalico.org/podIP:192.168.206.39/32 cni.projectcalico.org/podIPs:192.168.206.39/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.206.39"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc00888710f 0xc008887120}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:30:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.206.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.206.39,StartTime:2021-07-12 08:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:30:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://6d5479f0ba313d4715eabe8be8ea9d73cdecf9d3c4ed4935ded9eb92cddc44a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.206.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 08:30:16.954: INFO: Pod "webserver-deployment-dd94f59b7-z7p8m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z7p8m webserver-deployment-dd94f59b7- deployment-7622 /api/v1/namespaces/deployment-7622/pods/webserver-deployment-dd94f59b7-z7p8m a9c7dfb9-aee4-4d52-a078-410551437173 2867416 0 2021-07-12 08:29:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[calicoIP:192.168.173.189/26 cni.projectcalico.org/podIP:192.168.173.189/32 cni.projectcalico.org/podIPs:192.168.173.189/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.173.189"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 8641b3d8-c6a9-47fb-88f5-10604d0f584c 0xc0088872ef 0xc008887300}] []  [{kube-controller-manager Update v1 2021-07-12 08:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641b3d8-c6a9-47fb-88f5-10604d0f584c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:29:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.173.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g2sqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g2sqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g2sqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.3,PodIP:192.168.173.189,StartTime:2021-07-12 08:29:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:29:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://278893e8e0121e74c5018aa7459f44d28fbe594a38c4a14cd8e0ee6a8d69d090,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.173.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:30:16.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7622" for this suite.

• [SLOW TEST:34.874 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":303,"completed":264,"skipped":4296,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:30:20.693: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-17c2961f-71a4-415a-8420-f956cc207ee2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:30:23.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-647" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":303,"completed":265,"skipped":4309,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:30:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 12 08:30:26.070: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:30:53.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1589" for this suite.

• [SLOW TEST:29.958 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":303,"completed":266,"skipped":4311,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:30:53.817: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6134
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6134
STEP: Creating statefulset with conflicting port in namespace statefulset-6134
STEP: Waiting until pod test-pod will start running in namespace statefulset-6134
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6134
Jul 12 08:31:14.114: INFO: Observed stateful pod in namespace: statefulset-6134, name: ss-0, uid: c4ac3549-8dd1-4218-8f6d-fbb6719ca09a, status phase: Failed. Waiting for statefulset controller to delete.
Jul 12 08:31:14.828: INFO: Observed stateful pod in namespace: statefulset-6134, name: ss-0, uid: c4ac3549-8dd1-4218-8f6d-fbb6719ca09a, status phase: Failed. Waiting for statefulset controller to delete.
Jul 12 08:31:15.901: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6134
STEP: Removing pod with conflicting port in namespace statefulset-6134
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6134 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 12 08:31:32.686: INFO: Deleting all statefulset in ns statefulset-6134
Jul 12 08:31:32.725: INFO: Scaling statefulset ss to 0
Jul 12 08:31:52.786: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 08:31:52.811: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:31:52.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6134" for this suite.

• [SLOW TEST:59.593 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":303,"completed":267,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:31:53.410: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 12 08:31:53.703: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8604 /api/v1/namespaces/watch-8604/configmaps/e2e-watch-test-resource-version 820f1375-0c0f-4cba-ba90-234c6ec76f46 2869369 0 2021-07-12 08:31:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-12 08:31:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 08:31:53.709: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8604 /api/v1/namespaces/watch-8604/configmaps/e2e-watch-test-resource-version 820f1375-0c0f-4cba-ba90-234c6ec76f46 2869370 0 2021-07-12 08:31:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-12 08:31:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:31:53.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8604" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":303,"completed":268,"skipped":4339,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:31:53.718: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 08:32:05.276: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:32:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6164" for this suite.

• [SLOW TEST:11.836 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":269,"skipped":4354,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:32:05.554: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 12 08:32:08.026: INFO: Pod name wrapped-volume-race-544f787d-75bf-41bb-99e8-c237e187e501: Found 0 pods out of 5
Jul 12 08:32:13.037: INFO: Pod name wrapped-volume-race-544f787d-75bf-41bb-99e8-c237e187e501: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-544f787d-75bf-41bb-99e8-c237e187e501 in namespace emptydir-wrapper-1715, will wait for the garbage collector to delete the pods
Jul 12 08:32:31.283: INFO: Deleting ReplicationController wrapped-volume-race-544f787d-75bf-41bb-99e8-c237e187e501 took: 44.54425ms
Jul 12 08:32:31.484: INFO: Terminating ReplicationController wrapped-volume-race-544f787d-75bf-41bb-99e8-c237e187e501 pods took: 200.240988ms
STEP: Creating RC which spawns configmap-volume pods
Jul 12 08:32:39.737: INFO: Pod name wrapped-volume-race-18427f18-facf-4ea4-9fae-82a95526919b: Found 0 pods out of 5
Jul 12 08:32:44.743: INFO: Pod name wrapped-volume-race-18427f18-facf-4ea4-9fae-82a95526919b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-18427f18-facf-4ea4-9fae-82a95526919b in namespace emptydir-wrapper-1715, will wait for the garbage collector to delete the pods
Jul 12 08:33:06.837: INFO: Deleting ReplicationController wrapped-volume-race-18427f18-facf-4ea4-9fae-82a95526919b took: 19.163577ms
Jul 12 08:33:07.137: INFO: Terminating ReplicationController wrapped-volume-race-18427f18-facf-4ea4-9fae-82a95526919b pods took: 300.205587ms
STEP: Creating RC which spawns configmap-volume pods
Jul 12 08:33:19.368: INFO: Pod name wrapped-volume-race-4b543d26-4bbf-4182-8838-093ce344612e: Found 0 pods out of 5
Jul 12 08:33:24.374: INFO: Pod name wrapped-volume-race-4b543d26-4bbf-4182-8838-093ce344612e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4b543d26-4bbf-4182-8838-093ce344612e in namespace emptydir-wrapper-1715, will wait for the garbage collector to delete the pods
Jul 12 08:33:48.956: INFO: Deleting ReplicationController wrapped-volume-race-4b543d26-4bbf-4182-8838-093ce344612e took: 107.046324ms
Jul 12 08:33:49.156: INFO: Terminating ReplicationController wrapped-volume-race-4b543d26-4bbf-4182-8838-093ce344612e pods took: 200.202873ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:34:10.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1715" for this suite.

• [SLOW TEST:124.684 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":303,"completed":270,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:34:10.239: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 12 08:34:10.460: INFO: Waiting up to 5m0s for pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7" in namespace "downward-api-7034" to be "Succeeded or Failed"
Jul 12 08:34:10.462: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105076ms
Jul 12 08:34:12.465: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004861839s
Jul 12 08:34:14.467: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007701681s
Jul 12 08:34:16.470: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010323633s
Jul 12 08:34:18.473: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012981803s
Jul 12 08:34:20.493: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033471858s
Jul 12 08:34:22.499: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.039173234s
STEP: Saw pod success
Jul 12 08:34:22.499: INFO: Pod "downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7" satisfied condition "Succeeded or Failed"
Jul 12 08:34:22.529: INFO: Trying to get logs from node 10.32.0.100 pod downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7 container dapi-container: <nil>
STEP: delete the pod
Jul 12 08:34:22.686: INFO: Waiting for pod downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7 to disappear
Jul 12 08:34:22.688: INFO: Pod downward-api-07dbd663-bcf4-46bf-90e0-ddd4e30314e7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:34:22.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7034" for this suite.

• [SLOW TEST:12.504 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":303,"completed":271,"skipped":4400,"failed":0}
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:34:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:34:23.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":303,"completed":272,"skipped":4400,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:34:24.561: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 12 08:34:24.769: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:34:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:34:55.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3615" for this suite.

• [SLOW TEST:30.870 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":303,"completed":273,"skipped":4405,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:34:55.432: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 12 08:35:17.747: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:17.750: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:19.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:19.767: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:21.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:21.754: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:23.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:23.753: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:25.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:25.767: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:27.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:27.764: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 08:35:29.750: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 08:35:29.754: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:35:29.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1521" for this suite.

• [SLOW TEST:34.333 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":303,"completed":274,"skipped":4411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:35:29.765: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a3fe87d1-56b3-4715-af5e-f2317a657acf
STEP: Creating a pod to test consume configMaps
Jul 12 08:35:30.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77" in namespace "configmap-6944" to be "Succeeded or Failed"
Jul 12 08:35:30.011: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006399ms
Jul 12 08:35:32.072: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063563317s
Jul 12 08:35:34.076: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066832769s
Jul 12 08:35:36.473: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.464217274s
Jul 12 08:35:38.543: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.534406497s
Jul 12 08:35:40.547: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 10.537736411s
Jul 12 08:35:42.550: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.541246114s
STEP: Saw pod success
Jul 12 08:35:42.550: INFO: Pod "pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77" satisfied condition "Succeeded or Failed"
Jul 12 08:35:42.553: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 08:35:42.625: INFO: Waiting for pod pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77 to disappear
Jul 12 08:35:42.629: INFO: Pod pod-configmaps-c0345e48-d088-4e71-888f-4ad6f951ec77 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:35:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6944" for this suite.

• [SLOW TEST:12.907 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":275,"skipped":4464,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:35:42.672: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:37:43.052: INFO: Deleting pod "var-expansion-9fff40cc-f4eb-43ba-9f4b-4a8357228274" in namespace "var-expansion-79"
Jul 12 08:37:43.105: INFO: Wait up to 5m0s for pod "var-expansion-9fff40cc-f4eb-43ba-9f4b-4a8357228274" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:37:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-79" for this suite.

• [SLOW TEST:124.467 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":303,"completed":276,"skipped":4469,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:37:47.140: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 08:37:47.431: INFO: Number of nodes with available pods: 0
Jul 12 08:37:47.431: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:48.439: INFO: Number of nodes with available pods: 0
Jul 12 08:37:48.439: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:50.107: INFO: Number of nodes with available pods: 0
Jul 12 08:37:50.107: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:50.438: INFO: Number of nodes with available pods: 0
Jul 12 08:37:50.438: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:51.494: INFO: Number of nodes with available pods: 0
Jul 12 08:37:51.494: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:52.440: INFO: Number of nodes with available pods: 0
Jul 12 08:37:52.440: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:53.461: INFO: Number of nodes with available pods: 0
Jul 12 08:37:53.461: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:54.442: INFO: Number of nodes with available pods: 0
Jul 12 08:37:54.442: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:55.481: INFO: Number of nodes with available pods: 0
Jul 12 08:37:55.481: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:56.446: INFO: Number of nodes with available pods: 1
Jul 12 08:37:56.446: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:37:57.438: INFO: Number of nodes with available pods: 2
Jul 12 08:37:57.438: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:37:58.639: INFO: Number of nodes with available pods: 2
Jul 12 08:37:58.639: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:37:59.779: INFO: Number of nodes with available pods: 2
Jul 12 08:37:59.779: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:38:00.439: INFO: Number of nodes with available pods: 2
Jul 12 08:38:00.439: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:38:01.446: INFO: Number of nodes with available pods: 2
Jul 12 08:38:01.446: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:38:02.445: INFO: Number of nodes with available pods: 3
Jul 12 08:38:02.445: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 12 08:38:02.520: INFO: Number of nodes with available pods: 3
Jul 12 08:38:02.520: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6316, will wait for the garbage collector to delete the pods
Jul 12 08:38:05.983: INFO: Deleting DaemonSet.extensions daemon-set took: 890.897813ms
Jul 12 08:38:06.383: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.147476ms
Jul 12 08:38:19.518: INFO: Number of nodes with available pods: 0
Jul 12 08:38:19.518: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 08:38:19.521: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6316/daemonsets","resourceVersion":"2873922"},"items":null}

Jul 12 08:38:19.524: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6316/pods","resourceVersion":"2873922"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:38:19.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6316" for this suite.

• [SLOW TEST:32.413 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":303,"completed":277,"skipped":4480,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:38:19.553: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:39:15.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4567" for this suite.

• [SLOW TEST:55.641 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":303,"completed":278,"skipped":4494,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:39:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:39:15.473: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78" in namespace "security-context-test-2602" to be "Succeeded or Failed"
Jul 12 08:39:15.572: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Pending", Reason="", readiness=false. Elapsed: 99.100909ms
Jul 12 08:39:17.576: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102696207s
Jul 12 08:39:19.579: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105747982s
Jul 12 08:39:21.586: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113298139s
Jul 12 08:39:23.590: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116627179s
Jul 12 08:39:25.592: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119199485s
Jul 12 08:39:25.592: INFO: Pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78" satisfied condition "Succeeded or Failed"
Jul 12 08:39:25.622: INFO: Got logs for pod "busybox-privileged-false-cadc30c0-c363-4c5a-a7b1-01aa08f07b78": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:39:25.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2602" for this suite.

• [SLOW TEST:10.463 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":279,"skipped":4504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:39:25.657: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 12 08:39:35.974: INFO: &Pod{ObjectMeta:{send-events-f38ad11f-2a0c-48cd-8712-00f59604a2f0  events-8212 /api/v1/namespaces/events-8212/pods/send-events-f38ad11f-2a0c-48cd-8712-00f59604a2f0 54190041-64b4-47c1-a6ba-8f583b0a089e 2874703 0 2021-07-12 08:39:25 +0000 UTC <nil> <nil> map[name:foo time:838629568] map[calicoIP:192.168.206.29/26 cni.projectcalico.org/podIP:192.168.206.29/32 cni.projectcalico.org/podIPs:192.168.206.29/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.206.29"
    ],
    "default": true,
    "dns": {}
}]] [] []  [{e2e.test Update v1 2021-07-12 08:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-12 08:39:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:calicoIP":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-07-12 08:39:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-07-12 08:39:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.206.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m49kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m49kw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m49kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.32.0.100,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:39:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 08:39:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.32.0.100,PodIP:192.168.206.29,StartTime:2021-07-12 08:39:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 08:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://e622a49cb90397cc4c20ce0e5be4b4f92bcd83ee42949f3a6c790cda5c7c00fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.206.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 12 08:39:37.978: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 12 08:39:39.982: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:39:40.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8212" for this suite.

• [SLOW TEST:14.372 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":303,"completed":280,"skipped":4536,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:39:40.030: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-7568
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7568
STEP: Deleting pre-stop pod
Jul 12 08:40:09.296: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:40:09.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7568" for this suite.

• [SLOW TEST:29.353 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":303,"completed":281,"skipped":4551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:40:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3603139b-0d16-4f61-a02b-677de317a4d6
STEP: Creating a pod to test consume configMaps
Jul 12 08:40:10.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d" in namespace "configmap-5963" to be "Succeeded or Failed"
Jul 12 08:40:10.710: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.367006ms
Jul 12 08:40:13.159: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455560585s
Jul 12 08:40:15.162: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458954855s
Jul 12 08:40:17.166: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.463196655s
Jul 12 08:40:19.170: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466837857s
Jul 12 08:40:21.173: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.469814852s
Jul 12 08:40:23.177: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.473434986s
Jul 12 08:40:25.180: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.477007787s
STEP: Saw pod success
Jul 12 08:40:25.180: INFO: Pod "pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d" satisfied condition "Succeeded or Failed"
Jul 12 08:40:25.206: INFO: Trying to get logs from node 10.32.0.100 pod pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 08:40:25.329: INFO: Waiting for pod pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d to disappear
Jul 12 08:40:25.331: INFO: Pod pod-configmaps-bf78dce5-5a25-4202-aea1-7a052cc5ad3d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:40:25.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5963" for this suite.

• [SLOW TEST:15.963 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":282,"skipped":4596,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:40:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:40:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3907" for this suite.

• [SLOW TEST:12.294 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":303,"completed":283,"skipped":4596,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:40:37.640: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 12 08:40:37.876: INFO: Waiting up to 5m0s for pod "pod-474f3ab9-28f5-4da0-b391-66a055861393" in namespace "emptydir-4658" to be "Succeeded or Failed"
Jul 12 08:40:37.878: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070354ms
Jul 12 08:40:39.883: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007124754s
Jul 12 08:40:41.908: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032248144s
Jul 12 08:40:43.917: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041044214s
Jul 12 08:40:45.923: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047253586s
Jul 12 08:40:47.926: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049724316s
Jul 12 08:40:49.960: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.083563854s
STEP: Saw pod success
Jul 12 08:40:49.960: INFO: Pod "pod-474f3ab9-28f5-4da0-b391-66a055861393" satisfied condition "Succeeded or Failed"
Jul 12 08:40:49.962: INFO: Trying to get logs from node 10.32.0.100 pod pod-474f3ab9-28f5-4da0-b391-66a055861393 container test-container: <nil>
STEP: delete the pod
Jul 12 08:40:50.602: INFO: Waiting for pod pod-474f3ab9-28f5-4da0-b391-66a055861393 to disappear
Jul 12 08:40:51.180: INFO: Pod pod-474f3ab9-28f5-4da0-b391-66a055861393 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:40:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4658" for this suite.

• [SLOW TEST:13.556 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":284,"skipped":4596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:40:51.197: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 12 08:40:51.443: INFO: Waiting up to 5m0s for pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd" in namespace "emptydir-8512" to be "Succeeded or Failed"
Jul 12 08:40:51.446: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366937ms
Jul 12 08:40:53.448: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004928126s
Jul 12 08:40:55.451: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008168671s
Jul 12 08:40:57.491: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047644598s
Jul 12 08:40:59.504: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060498096s
Jul 12 08:41:01.509: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.065912542s
STEP: Saw pod success
Jul 12 08:41:01.509: INFO: Pod "pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd" satisfied condition "Succeeded or Failed"
Jul 12 08:41:01.548: INFO: Trying to get logs from node 10.32.0.100 pod pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd container test-container: <nil>
STEP: delete the pod
Jul 12 08:41:01.606: INFO: Waiting for pod pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd to disappear
Jul 12 08:41:01.608: INFO: Pod pod-e0529d31-c97f-44c7-aeb7-d76f811ee8fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:41:01.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8512" for this suite.

• [SLOW TEST:10.427 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":285,"skipped":4646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:41:01.625: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:41:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5368" for this suite.
STEP: Destroying namespace "nsdeletetest-5102" for this suite.
Jul 12 08:41:42.547: INFO: Namespace nsdeletetest-5102 was already deleted
STEP: Destroying namespace "nsdeletetest-4050" for this suite.

• [SLOW TEST:40.937 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":303,"completed":286,"skipped":4706,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:41:42.562: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:41:59.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6132" for this suite.

• [SLOW TEST:16.534 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":303,"completed":287,"skipped":4744,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:41:59.097: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 12 08:41:59.217: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:42:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4008" for this suite.

• [SLOW TEST:15.480 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":303,"completed":288,"skipped":4748,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:42:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 12 08:42:15.324: INFO: Waiting up to 5m0s for pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0" in namespace "emptydir-9930" to be "Succeeded or Failed"
Jul 12 08:42:15.331: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.637086ms
Jul 12 08:42:17.335: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011824492s
Jul 12 08:42:19.340: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016134864s
Jul 12 08:42:21.342: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018611467s
Jul 12 08:42:23.646: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.322778666s
Jul 12 08:42:25.680: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.356027839s
STEP: Saw pod success
Jul 12 08:42:25.680: INFO: Pod "pod-51cceffd-25f0-4258-aac7-9ea64e6733e0" satisfied condition "Succeeded or Failed"
Jul 12 08:42:25.682: INFO: Trying to get logs from node 10.32.0.100 pod pod-51cceffd-25f0-4258-aac7-9ea64e6733e0 container test-container: <nil>
STEP: delete the pod
Jul 12 08:42:25.868: INFO: Waiting for pod pod-51cceffd-25f0-4258-aac7-9ea64e6733e0 to disappear
Jul 12 08:42:25.942: INFO: Pod pod-51cceffd-25f0-4258-aac7-9ea64e6733e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:42:25.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9930" for this suite.

• [SLOW TEST:11.380 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":289,"skipped":4749,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:42:25.958: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-klbm
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 08:42:26.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-klbm" in namespace "subpath-4122" to be "Succeeded or Failed"
Jul 12 08:42:26.196: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.153499ms
Jul 12 08:42:28.198: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006617947s
Jul 12 08:42:30.201: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009667138s
Jul 12 08:42:32.234: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042541552s
Jul 12 08:42:34.239: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047147949s
Jul 12 08:42:36.417: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.225236689s
Jul 12 08:42:38.506: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.31460898s
Jul 12 08:42:40.509: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 14.317602444s
Jul 12 08:42:42.513: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 16.320986927s
Jul 12 08:42:44.516: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 18.324056879s
Jul 12 08:42:46.520: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 20.328044019s
Jul 12 08:42:48.522: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 22.330777206s
Jul 12 08:42:50.526: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 24.334304368s
Jul 12 08:42:52.535: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 26.343131251s
Jul 12 08:42:56.054: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 29.862774896s
Jul 12 08:42:58.062: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 31.870817047s
Jul 12 08:43:00.066: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Running", Reason="", readiness=true. Elapsed: 33.874064348s
Jul 12 08:43:02.070: INFO: Pod "pod-subpath-test-configmap-klbm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 35.878588641s
STEP: Saw pod success
Jul 12 08:43:02.070: INFO: Pod "pod-subpath-test-configmap-klbm" satisfied condition "Succeeded or Failed"
Jul 12 08:43:02.119: INFO: Trying to get logs from node 10.32.0.100 pod pod-subpath-test-configmap-klbm container test-container-subpath-configmap-klbm: <nil>
STEP: delete the pod
Jul 12 08:43:02.320: INFO: Waiting for pod pod-subpath-test-configmap-klbm to disappear
Jul 12 08:43:02.337: INFO: Pod pod-subpath-test-configmap-klbm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-klbm
Jul 12 08:43:02.337: INFO: Deleting pod "pod-subpath-test-configmap-klbm" in namespace "subpath-4122"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:43:02.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4122" for this suite.

• [SLOW TEST:36.395 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":303,"completed":290,"skipped":4762,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:43:02.353: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-782 to expose endpoints map[]
Jul 12 08:43:02.827: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 12 08:43:03.915: INFO: successfully validated that service endpoint-test2 in namespace services-782 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-782 to expose endpoints map[pod1:[80]]
Jul 12 08:43:07.997: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Jul 12 08:43:12.998: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Jul 12 08:43:14.004: INFO: successfully validated that service endpoint-test2 in namespace services-782 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-782 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 12 08:43:18.066: INFO: Unexpected endpoints: found map[7e69ea61-8f0a-43a3-bde1-487d4b3ab453:[80]], expected map[pod1:[80] pod2:[80]], will retry
Jul 12 08:43:22.675: INFO: successfully validated that service endpoint-test2 in namespace services-782 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-782 to expose endpoints map[pod2:[80]]
Jul 12 08:43:22.765: INFO: successfully validated that service endpoint-test2 in namespace services-782 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-782 to expose endpoints map[]
Jul 12 08:43:23.796: INFO: successfully validated that service endpoint-test2 in namespace services-782 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:43:24.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-782" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.728 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":303,"completed":291,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:43:24.081: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 12 08:43:24.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c" in namespace "projected-2526" to be "Succeeded or Failed"
Jul 12 08:43:24.380: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522452ms
Jul 12 08:43:26.382: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005005034s
Jul 12 08:43:28.386: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008349144s
Jul 12 08:43:30.389: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011221503s
Jul 12 08:43:32.392: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014914617s
Jul 12 08:43:34.396: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.018521317s
STEP: Saw pod success
Jul 12 08:43:34.396: INFO: Pod "downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c" satisfied condition "Succeeded or Failed"
Jul 12 08:43:34.398: INFO: Trying to get logs from node 10.32.0.100 pod downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c container client-container: <nil>
STEP: delete the pod
Jul 12 08:43:34.562: INFO: Waiting for pod downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c to disappear
Jul 12 08:43:34.565: INFO: Pod downwardapi-volume-927951f4-eb46-4991-827b-ab07f7df159c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:43:34.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2526" for this suite.

• [SLOW TEST:10.493 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":292,"skipped":4797,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:43:34.574: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 12 08:43:34.760: INFO: PodSpec: initContainers in spec.initContainers
Jul 12 08:44:37.254: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-31e3aa97-5ad1-4b87-a13e-b7996ca5239e", GenerateName:"", Namespace:"init-container-4798", SelfLink:"/api/v1/namespaces/init-container-4798/pods/pod-init-31e3aa97-5ad1-4b87-a13e-b7996ca5239e", UID:"1f299134-2271-49c1-948b-b2569d1e1d05", ResourceVersion:"2877714", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63761676214, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"759997141"}, Annotations:map[string]string{"calicoIP":"192.168.206.55/26", "cni.projectcalico.org/podIP":"192.168.206.55/32", "cni.projectcalico.org/podIPs":"192.168.206.55/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.206.55\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00346d7e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00346d800)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00346d840), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00346d880)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00346d9e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00346da80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00346dd60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00346dda0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xzjhl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004f61300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xzjhl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xzjhl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xzjhl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc008fa2658), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.32.0.100", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001852700), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008fa26f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008fa2710)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc008fa2718), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc008fa271c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004667dc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676215, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676215, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676215, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676214, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.32.0.100", PodIP:"192.168.206.55", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.206.55"}}, StartTime:(*v1.Time)(0xc00346ddc0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0018527e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001852850)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://9d7a9f672343d70a4cd3c2af198c651132fbc3421e8c1d96958061b7c9152d61", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00346de40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00346de00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc008fa279f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:44:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4798" for this suite.

• [SLOW TEST:62.694 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":303,"completed":293,"skipped":4807,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:44:37.269: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:44:37.568: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 08:44:37.645: INFO: Number of nodes with available pods: 0
Jul 12 08:44:37.645: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:40.256: INFO: Number of nodes with available pods: 0
Jul 12 08:44:40.256: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:41.293: INFO: Number of nodes with available pods: 0
Jul 12 08:44:41.293: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:41.652: INFO: Number of nodes with available pods: 0
Jul 12 08:44:41.652: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:42.853: INFO: Number of nodes with available pods: 0
Jul 12 08:44:42.853: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:43.651: INFO: Number of nodes with available pods: 0
Jul 12 08:44:43.651: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:44.671: INFO: Number of nodes with available pods: 0
Jul 12 08:44:44.671: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:45.651: INFO: Number of nodes with available pods: 0
Jul 12 08:44:45.651: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:47.891: INFO: Number of nodes with available pods: 1
Jul 12 08:44:47.891: INFO: Node 10.32.0.100 is running more than one daemon pod
Jul 12 08:44:48.651: INFO: Number of nodes with available pods: 2
Jul 12 08:44:48.651: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:44:49.652: INFO: Number of nodes with available pods: 2
Jul 12 08:44:49.652: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:44:52.423: INFO: Number of nodes with available pods: 2
Jul 12 08:44:52.423: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:44:52.651: INFO: Number of nodes with available pods: 2
Jul 12 08:44:52.651: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:44:53.664: INFO: Number of nodes with available pods: 2
Jul 12 08:44:53.664: INFO: Node 10.32.0.102 is running more than one daemon pod
Jul 12 08:44:54.653: INFO: Number of nodes with available pods: 3
Jul 12 08:44:54.653: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 12 08:44:54.705: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:54.705: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:54.705: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:55.739: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:55.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:55.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:56.751: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:56.751: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:56.751: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:57.791: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:57.791: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:57.791: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:58.773: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:58.773: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:44:58.773: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.090: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.090: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.090: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.738: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.738: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:00.738: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:01.739: INFO: Wrong image for pod: daemon-set-7d6bk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:01.739: INFO: Pod daemon-set-7d6bk is not available
Jul 12 08:45:01.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:01.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:02.828: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:02.828: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:02.828: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:03.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:03.739: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:03.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:05.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:05.739: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:05.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:06.849: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:06.849: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:06.849: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:07.872: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:07.872: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:07.872: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:08.740: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:08.740: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:08.740: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:09.742: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:09.742: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:09.742: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:10.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:10.739: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:10.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:11.748: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:11.748: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:11.748: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:12.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:12.739: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:12.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:13.741: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:13.741: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:13.741: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:15.310: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:15.310: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:15.310: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:15.756: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:15.756: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:15.756: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:16.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:16.739: INFO: Pod daemon-set-msmps is not available
Jul 12 08:45:16.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:17.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:17.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:18.755: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:18.755: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:19.745: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:19.745: INFO: Pod daemon-set-cxs22 is not available
Jul 12 08:45:19.745: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:20.739: INFO: Wrong image for pod: daemon-set-cxs22. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:20.739: INFO: Pod daemon-set-cxs22 is not available
Jul 12 08:45:20.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:21.962: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:21.962: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:22.739: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:22.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:23.739: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:23.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:24.739: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:24.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:25.757: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:25.757: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:26.740: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:26.740: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:27.781: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:27.781: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:28.739: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:28.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:29.743: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:29.743: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:30.739: INFO: Pod daemon-set-rfpcj is not available
Jul 12 08:45:30.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:31.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:32.757: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:33.739: INFO: Wrong image for pod: daemon-set-vk2rc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 12 08:45:33.739: INFO: Pod daemon-set-vk2rc is not available
Jul 12 08:45:34.807: INFO: Pod daemon-set-t6q57 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 12 08:45:34.818: INFO: Number of nodes with available pods: 2
Jul 12 08:45:34.818: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:35.853: INFO: Number of nodes with available pods: 2
Jul 12 08:45:35.853: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:36.830: INFO: Number of nodes with available pods: 2
Jul 12 08:45:36.830: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:37.827: INFO: Number of nodes with available pods: 2
Jul 12 08:45:37.827: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:38.825: INFO: Number of nodes with available pods: 2
Jul 12 08:45:38.825: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:39.826: INFO: Number of nodes with available pods: 2
Jul 12 08:45:39.826: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:40.827: INFO: Number of nodes with available pods: 2
Jul 12 08:45:40.827: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:41.865: INFO: Number of nodes with available pods: 2
Jul 12 08:45:41.865: INFO: Node 10.32.0.3 is running more than one daemon pod
Jul 12 08:45:42.847: INFO: Number of nodes with available pods: 3
Jul 12 08:45:42.847: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7199, will wait for the garbage collector to delete the pods
Jul 12 08:45:42.989: INFO: Deleting DaemonSet.extensions daemon-set took: 41.592679ms
Jul 12 08:45:43.189: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.157747ms
Jul 12 08:45:59.492: INFO: Number of nodes with available pods: 0
Jul 12 08:45:59.492: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 08:45:59.495: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7199/daemonsets","resourceVersion":"2878551"},"items":null}

Jul 12 08:45:59.497: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7199/pods","resourceVersion":"2878551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:45:59.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7199" for this suite.

• [SLOW TEST:82.264 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":303,"completed":294,"skipped":4812,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:45:59.533: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jul 12 08:46:11.871: INFO: Pod pod-hostip-e3cbb78e-8d86-400a-91eb-db6fd121f44e has hostIP: 10.32.0.100
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:46:11.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8756" for this suite.

• [SLOW TEST:12.356 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":303,"completed":295,"skipped":4824,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:46:11.889: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 12 08:46:11.984: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:46:28.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7058" for this suite.

• [SLOW TEST:17.127 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":303,"completed":296,"skipped":4833,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:46:29.016: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 12 08:46:39.795: INFO: Successfully updated pod "pod-update-31e35a59-0cdb-4e84-a573-76c9dbb59867"
STEP: verifying the updated pod is in kubernetes
Jul 12 08:46:39.799: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:46:39.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7539" for this suite.

• [SLOW TEST:10.793 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":303,"completed":297,"skipped":4835,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:46:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 12 08:46:39.980: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:46:56.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-418" for this suite.

• [SLOW TEST:16.817 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":303,"completed":298,"skipped":4856,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:46:56.628: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-90dfcb6a-3966-4d02-8081-db2912d18fbb
STEP: Creating a pod to test consume secrets
Jul 12 08:46:57.463: INFO: Waiting up to 5m0s for pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5" in namespace "secrets-2467" to be "Succeeded or Failed"
Jul 12 08:46:57.466: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.444031ms
Jul 12 08:46:59.468: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005371263s
Jul 12 08:47:01.517: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054042054s
Jul 12 08:47:03.520: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057064849s
Jul 12 08:47:05.589: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125679306s
Jul 12 08:47:07.592: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.128665216s
Jul 12 08:47:09.596: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.132605334s
STEP: Saw pod success
Jul 12 08:47:09.596: INFO: Pod "pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5" satisfied condition "Succeeded or Failed"
Jul 12 08:47:09.599: INFO: Trying to get logs from node 10.32.0.100 pod pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 08:47:09.730: INFO: Waiting for pod pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5 to disappear
Jul 12 08:47:09.732: INFO: Pod pod-secrets-7ebdf25a-9edc-4197-8203-72991293a3a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:47:09.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2467" for this suite.

• [SLOW TEST:13.118 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":299,"skipped":4863,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:47:09.746: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 12 08:47:09.924: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 12 08:47:11.481: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:47:12.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9677" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":303,"completed":300,"skipped":4871,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:47:12.822: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 12 08:47:33.170: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:33.170: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:33.402: INFO: Exec stderr: ""
Jul 12 08:47:33.402: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:33.402: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:33.589: INFO: Exec stderr: ""
Jul 12 08:47:33.589: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:33.589: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:33.731: INFO: Exec stderr: ""
Jul 12 08:47:33.731: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:33.915: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 12 08:47:33.915: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:33.916: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.042: INFO: Exec stderr: ""
Jul 12 08:47:34.042: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:34.042: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.239: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 12 08:47:34.239: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.407: INFO: Exec stderr: ""
Jul 12 08:47:34.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:34.407: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.590: INFO: Exec stderr: ""
Jul 12 08:47:34.590: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:34.590: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.730: INFO: Exec stderr: ""
Jul 12 08:47:34.730: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5413 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 12 08:47:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
Jul 12 08:47:34.925: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:47:34.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5413" for this suite.

• [SLOW TEST:22.119 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":301,"skipped":4885,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:47:34.941: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 08:47:35.661: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 08:47:37.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:47:39.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:47:41.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:47:43.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 08:47:45.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761676455, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 08:47:48.786: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:47:49.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2743" for this suite.
STEP: Destroying namespace "webhook-2743-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":303,"completed":302,"skipped":4900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 12 08:47:49.962: INFO: >>> kubeConfig: /tmp/kubeconfig-439874929
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 12 08:47:57.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2809" for this suite.

• [SLOW TEST:7.416 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":303,"completed":303,"skipped":4930,"failed":0}
SJul 12 08:47:57.378: INFO: Running AfterSuite actions on all nodes
Jul 12 08:47:57.378: INFO: Running AfterSuite actions on node 1
Jul 12 08:47:57.378: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":303,"completed":303,"skipped":4931,"failed":0}

Ran 303 of 5234 Specs in 8621.954 seconds
SUCCESS! -- 303 Passed | 0 Failed | 0 Pending | 4931 Skipped
PASS

Ginkgo ran 1 suite in 2h23m43.338797858s
Test Suite Passed
