Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1607952090 - Will randomize all specs
Will run 5234 specs

Running in parallel across 7 nodes

Dec 14 13:21:33.340: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
Dec 14 13:21:33.346: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 14 13:21:33.383: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 14 13:21:33.457: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 13:21:33.457: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Dec 14 13:21:33.457: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 13:21:33.481: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'magnum-auto-healer' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'magnum-nginx-ingress-controller' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Dec 14 13:21:33.481: INFO: e2e test version: v1.19.4
Dec 14 13:21:33.484: INFO: kube-apiserver version: v1.19.4
Dec 14 13:21:33.484: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
Dec 14 13:21:33.491: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSS
------------------------------
Dec 14 13:21:33.489: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:21:33.524: INFO: Cluster IP family: ipv4

SS
------------------------------
Dec 14 13:21:33.494: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
Dec 14 13:21:33.527: INFO: Cluster IP family: ipv4

SS
------------------------------
Dec 14 13:21:33.496: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
Dec 14 13:21:33.529: INFO: Cluster IP family: ipv4

S
------------------------------
Dec 14 13:21:33.504: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
Dec 14 13:21:33.534: INFO: Cluster IP family: ipv4

SSSSSS
------------------------------
Dec 14 13:21:33.505: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
Dec 14 13:21:33.544: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
Dec 14 13:21:33.537: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
Dec 14 13:21:33.575: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.576: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename resourcequota
Dec 14 13:21:33.640: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.649: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:33.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8807" for this suite.

•SS
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":-1,"completed":1,"skipped":7,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.549: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename containers
Dec 14 13:21:33.628: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.649: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:41.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6541" for this suite.


• [SLOW TEST:8.269 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":20,"failed":0}

SS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.574: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename kubectl
Dec 14 13:21:33.628: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.650: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 13:21:33.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8948'
Dec 14 13:21:34.596: INFO: stderr: ""
Dec 14 13:21:34.596: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Dec 14 13:21:34.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 delete pods e2e-test-httpd-pod --namespace=kubectl-8948'
Dec 14 13:21:42.084: INFO: stderr: ""
Dec 14 13:21:42.084: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:42.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8948" for this suite.


• [SLOW TEST:8.531 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":-1,"completed":1,"skipped":9,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:42.157: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Dec 14 13:21:42.206: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-214741859 proxy --unix-socket=/tmp/kubectl-proxy-unix585364975/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:42.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6132" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":-1,"completed":2,"skipped":40,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:21:43.517: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:21:43.539: INFO: waiting for watch events with expected annotations
Dec 14 13:21:43.539: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:43.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4337" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":-1,"completed":3,"skipped":49,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.575: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename pods
Dec 14 13:21:33.630: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.649: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:21:33.653: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-618" for this suite.


• [SLOW TEST:10.178 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":10,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-78d1895a-014b-41e6-9c31-d51ad08fad2a
STEP: Creating a pod to test consume configMaps
Dec 14 13:21:33.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9" in namespace "configmap-6515" to be "Succeeded or Failed"
Dec 14 13:21:33.845: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956921ms
Dec 14 13:21:35.850: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008382569s
Dec 14 13:21:37.854: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012178787s
Dec 14 13:21:39.857: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015760244s
Dec 14 13:21:41.866: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024390976s
Dec 14 13:21:43.871: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029643956s
STEP: Saw pod success
Dec 14 13:21:43.871: INFO: Pod "pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9" satisfied condition "Succeeded or Failed"
Dec 14 13:21:43.874: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:21:43.894: INFO: Waiting for pod pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9 to disappear
Dec 14 13:21:43.897: INFO: Pod pod-configmaps-a8d2ed08-b2fb-42ef-a189-0cf8da98efe9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:43.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6515" for this suite.


• [SLOW TEST:10.166 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":19,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.544: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename webhook
Dec 14 13:21:33.628: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.649: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:21:34.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:21:36.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:21:38.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:21:40.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:21:42.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548894, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:21:45.692: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:45.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4429" for this suite.
STEP: Destroying namespace "webhook-4429-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:12.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":-1,"completed":1,"skipped":1,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:43.639: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-77ff16a2-029d-4b4d-8673-363e58a85afe
STEP: Creating a pod to test consume secrets
Dec 14 13:21:43.700: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089" in namespace "projected-2402" to be "Succeeded or Failed"
Dec 14 13:21:43.718: INFO: Pod "pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089": Phase="Pending", Reason="", readiness=false. Elapsed: 17.590795ms
Dec 14 13:21:45.725: INFO: Pod "pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024766364s
Dec 14 13:21:47.747: INFO: Pod "pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046203275s
STEP: Saw pod success
Dec 14 13:21:47.747: INFO: Pod "pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089" satisfied condition "Succeeded or Failed"
Dec 14 13:21:47.752: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:21:47.776: INFO: Waiting for pod pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089 to disappear
Dec 14 13:21:47.780: INFO: Pod pod-projected-secrets-f2df621e-9a5e-454d-a92c-e4ef82809089 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:47.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2402" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":59,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:43.806: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 14 13:21:43.850: INFO: Waiting up to 5m0s for pod "downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f" in namespace "downward-api-5193" to be "Succeeded or Failed"
Dec 14 13:21:43.853: INFO: Pod "downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.794814ms
Dec 14 13:21:45.857: INFO: Pod "downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007228318s
Dec 14 13:21:47.867: INFO: Pod "downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017002231s
STEP: Saw pod success
Dec 14 13:21:47.867: INFO: Pod "downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f" satisfied condition "Succeeded or Failed"
Dec 14 13:21:47.869: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:21:47.892: INFO: Waiting for pod downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f to disappear
Dec 14 13:21:47.899: INFO: Pod downward-api-8e7255a4-6831-4e09-8b10-657e84fcf05f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5193" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":46,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:41.826: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 13:21:48.407: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d8808b57-c0be-4ee8-a8fe-f81f9708f759"
Dec 14 13:21:48.407: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d8808b57-c0be-4ee8-a8fe-f81f9708f759" in namespace "pods-5458" to be "terminated due to deadline exceeded"
Dec 14 13:21:48.412: INFO: Pod "pod-update-activedeadlineseconds-d8808b57-c0be-4ee8-a8fe-f81f9708f759": Phase="Running", Reason="", readiness=true. Elapsed: 4.52232ms
Dec 14 13:21:50.416: INFO: Pod "pod-update-activedeadlineseconds-d8808b57-c0be-4ee8-a8fe-f81f9708f759": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009103999s
Dec 14 13:21:50.417: INFO: Pod "pod-update-activedeadlineseconds-d8808b57-c0be-4ee8-a8fe-f81f9708f759" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:50.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5458" for this suite.


• [SLOW TEST:8.600 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":22,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:21:50.889: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:50.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5314" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":77,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.726: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename kubectl
Dec 14 13:21:33.799: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.813: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Dec 14 13:21:33.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-8734 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 13:21:34.711: INFO: stderr: ""
Dec 14 13:21:34.711: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Dec 14 13:21:34.711: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 13:21:34.711: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8734" to be "running and ready, or succeeded"
Dec 14 13:21:34.727: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.53033ms
Dec 14 13:21:36.731: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02059866s
Dec 14 13:21:38.740: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02941547s
Dec 14 13:21:40.744: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033324726s
Dec 14 13:21:42.748: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.036972558s
Dec 14 13:21:42.748: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 13:21:42.748: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 14 13:21:42.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734'
Dec 14 13:21:43.043: INFO: stderr: ""
Dec 14 13:21:43.043: INFO: stdout: "I1214 13:21:40.279386       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/9xp 357\nI1214 13:21:40.479482       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/9n6q 580\nI1214 13:21:40.679484       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/s7nj 307\nI1214 13:21:40.879497       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/qwf 404\nI1214 13:21:41.079491       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/l466 339\nI1214 13:21:41.279475       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hjh 439\nI1214 13:21:41.479503       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/j7l 235\nI1214 13:21:41.679482       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/ndr 336\nI1214 13:21:41.879447       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/ws4p 395\nI1214 13:21:42.079473       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/r52 407\nI1214 13:21:42.279563       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/lm6 445\nI1214 13:21:42.479541       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/5xr 549\nI1214 13:21:42.679573       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/fkfc 572\nI1214 13:21:42.879511       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/blc 550\n"
STEP: limiting log lines
Dec 14 13:21:43.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734 --tail=1'
Dec 14 13:21:43.252: INFO: stderr: ""
Dec 14 13:21:43.252: INFO: stdout: "I1214 13:21:43.079397       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/ttc 520\n"
Dec 14 13:21:43.253: INFO: got output "I1214 13:21:43.079397       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/ttc 520\n"
STEP: limiting log bytes
Dec 14 13:21:43.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734 --limit-bytes=1'
Dec 14 13:21:43.421: INFO: stderr: ""
Dec 14 13:21:43.421: INFO: stdout: "I"
Dec 14 13:21:43.421: INFO: got output "I"
STEP: exposing timestamps
Dec 14 13:21:43.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734 --tail=1 --timestamps'
Dec 14 13:21:43.590: INFO: stderr: ""
Dec 14 13:21:43.590: INFO: stdout: "2020-12-14T13:21:43.479830519Z I1214 13:21:43.479493       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zj9 557\n"
Dec 14 13:21:43.590: INFO: got output "2020-12-14T13:21:43.479830519Z I1214 13:21:43.479493       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zj9 557\n"
STEP: restricting to a time range
Dec 14 13:21:46.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734 --since=1s'
Dec 14 13:21:46.275: INFO: stderr: ""
Dec 14 13:21:46.275: INFO: stdout: "I1214 13:21:45.279468       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/xs2 259\nI1214 13:21:45.479549       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/x5pg 359\nI1214 13:21:45.679626       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/kj7 377\nI1214 13:21:45.879523       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/462b 353\nI1214 13:21:46.079403       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/zt4 303\n"
Dec 14 13:21:46.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 logs logs-generator logs-generator --namespace=kubectl-8734 --since=24h'
Dec 14 13:21:46.473: INFO: stderr: ""
Dec 14 13:21:46.473: INFO: stdout: "I1214 13:21:40.279386       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/9xp 357\nI1214 13:21:40.479482       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/9n6q 580\nI1214 13:21:40.679484       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/s7nj 307\nI1214 13:21:40.879497       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/qwf 404\nI1214 13:21:41.079491       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/l466 339\nI1214 13:21:41.279475       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hjh 439\nI1214 13:21:41.479503       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/j7l 235\nI1214 13:21:41.679482       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/ndr 336\nI1214 13:21:41.879447       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/ws4p 395\nI1214 13:21:42.079473       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/r52 407\nI1214 13:21:42.279563       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/lm6 445\nI1214 13:21:42.479541       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/5xr 549\nI1214 13:21:42.679573       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/fkfc 572\nI1214 13:21:42.879511       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/blc 550\nI1214 13:21:43.079397       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/ttc 520\nI1214 13:21:43.279504       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/mxhx 236\nI1214 13:21:43.479493       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zj9 557\nI1214 13:21:43.679525       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/6wj 388\nI1214 13:21:43.879505       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/kpt 238\nI1214 13:21:44.080131       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/2q6p 537\nI1214 13:21:44.282980       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/7nl 517\nI1214 13:21:44.479408       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/tlhk 312\nI1214 13:21:44.679468       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/xkt 271\nI1214 13:21:44.879498       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/pw2 301\nI1214 13:21:45.079494       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/wmt 278\nI1214 13:21:45.279468       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/xs2 259\nI1214 13:21:45.479549       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/x5pg 359\nI1214 13:21:45.679626       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/kj7 377\nI1214 13:21:45.879523       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/462b 353\nI1214 13:21:46.079403       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/zt4 303\nI1214 13:21:46.279851       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/rmzz 223\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Dec 14 13:21:46.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 delete pod logs-generator --namespace=kubectl-8734'
Dec 14 13:21:52.082: INFO: stderr: ""
Dec 14 13:21:52.083: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:52.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8734" for this suite.


• [SLOW TEST:18.372 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":-1,"completed":1,"skipped":76,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:50.438: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 14 13:21:50.482: INFO: Waiting up to 5m0s for pod "pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb" in namespace "emptydir-8726" to be "Succeeded or Failed"
Dec 14 13:21:50.503: INFO: Pod "pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.207889ms
Dec 14 13:21:52.508: INFO: Pod "pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025939834s
STEP: Saw pod success
Dec 14 13:21:52.509: INFO: Pod "pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb" satisfied condition "Succeeded or Failed"
Dec 14 13:21:52.511: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb container test-container: <nil>
STEP: delete the pod
Dec 14 13:21:52.535: INFO: Waiting for pod pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb to disappear
Dec 14 13:21:52.546: INFO: Pod pod-2e1f19e7-b33b-4c0d-a546-6e167635a7eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:21:52.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8726" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":26,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:52.586: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9890" for this suite.


• [SLOW TEST:11.083 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":-1,"completed":4,"skipped":41,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:50.980: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:21:51.020: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 13:21:51.035: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 13:21:56.039: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 13:22:00.050: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 13:22:00.057: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 13:22:00.066: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 13:22:02.075: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 13:22:02.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548920, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548920, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548920, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:22:04.081: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 14 13:22:04.091: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2679 /apis/apps/v1/namespaces/deployment-2679/deployments/test-rolling-update-deployment 74b6a280-c0b8-4e94-ad81-2615688b16a5 6797 1 2020-12-14 13:22:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-12-14 13:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a667a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-14 13:22:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-12-14 13:22:02 +0000 UTC,LastTransitionTime:2020-12-14 13:22:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 13:22:04.094: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-2679 /apis/apps/v1/namespaces/deployment-2679/replicasets/test-rolling-update-deployment-c4cb8d6d9 915efb54-8fa8-4f10-bc89-03473dc0169f 6785 1 2020-12-14 13:22:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 74b6a280-c0b8-4e94-ad81-2615688b16a5 0xc003a66cf0 0xc003a66cf1}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74b6a280-c0b8-4e94-ad81-2615688b16a5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a66d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:22:04.095: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 13:22:04.095: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2679 /apis/apps/v1/namespaces/deployment-2679/replicasets/test-rolling-update-controller 64968a1b-026a-42b5-a081-12f3dd48c526 6795 2 2020-12-14 13:21:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 74b6a280-c0b8-4e94-ad81-2615688b16a5 0xc003a66bef 0xc003a66c00}] []  [{e2e.test Update apps/v1 2020-12-14 13:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74b6a280-c0b8-4e94-ad81-2615688b16a5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a66c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:22:04.098: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-7xrn6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-7xrn6 test-rolling-update-deployment-c4cb8d6d9- deployment-2679 /api/v1/namespaces/deployment-2679/pods/test-rolling-update-deployment-c4cb8d6d9-7xrn6 ece68233-1eff-40b7-abd7-ca67171da369 6783 0 2020-12-14 13:22:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:10.100.227.147/32 cni.projectcalico.org/podIPs:10.100.227.147/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 915efb54-8fa8-4f10-bc89-03473dc0169f 0xc003a67230 0xc003a67231}] []  [{kube-controller-manager Update v1 2020-12-14 13:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915efb54-8fa8-4f10-bc89-03473dc0169f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:22:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:22:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cn9w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cn9w2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cn9w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.147,StartTime:2020-12-14 13:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:22:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://614e38a0f5b734b3b58cb68ffdbbdac416f9e6e147a337d5344b3818307f1ee8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:04.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2679" for this suite.


• [SLOW TEST:13.127 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":6,"skipped":100,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:04.125: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-8d10d7a3-6862-4dea-b60b-580867799b52
STEP: Creating a pod to test consume configMaps
Dec 14 13:22:04.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2" in namespace "projected-1369" to be "Succeeded or Failed"
Dec 14 13:22:04.184: INFO: Pod "pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.708641ms
Dec 14 13:22:06.193: INFO: Pod "pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01733494s
Dec 14 13:22:08.205: INFO: Pod "pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029352978s
STEP: Saw pod success
Dec 14 13:22:08.205: INFO: Pod "pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2" satisfied condition "Succeeded or Failed"
Dec 14 13:22:08.211: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:22:08.238: INFO: Waiting for pod pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2 to disappear
Dec 14 13:22:08.247: INFO: Pod pod-projected-configmaps-0c88ed48-6504-4f44-8fb1-c2bf4fb477c2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:08.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1369" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":107,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:45.940: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:21:56.035: INFO: DNS probes using dns-test-000a6e14-e0ea-46a0-83d0-72482b62d3ae succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:22:00.124: INFO: File jessie_udp@dns-test-service-3.dns-3400.svc.cluster.local from pod  dns-3400/dns-test-e8128165-80c8-4f5c-840d-7b79ef71ac48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 13:22:00.124: INFO: Lookups using dns-3400/dns-test-e8128165-80c8-4f5c-840d-7b79ef71ac48 failed for: [jessie_udp@dns-test-service-3.dns-3400.svc.cluster.local]

Dec 14 13:22:05.136: INFO: DNS probes using dns-test-e8128165-80c8-4f5c-840d-7b79ef71ac48 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3400.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3400.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:22:09.233: INFO: DNS probes using dns-test-4e6cb882-1813-463e-b186-ab9bb817965b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3400" for this suite.


• [SLOW TEST:23.383 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":-1,"completed":2,"skipped":3,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:03.749: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 14 13:22:06.328: INFO: Successfully updated pod "labelsupdate48a92bd2-c072-4bbf-9c33-fddcc6f7206f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:10.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9706" for this suite.


• [SLOW TEST:6.626 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":82,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:47.934: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-fw2v
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:21:47.991: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fw2v" in namespace "subpath-2587" to be "Succeeded or Failed"
Dec 14 13:21:47.994: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321125ms
Dec 14 13:21:50.007: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016068335s
Dec 14 13:21:52.012: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 4.021169402s
Dec 14 13:21:54.016: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 6.025373021s
Dec 14 13:21:56.020: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 8.028820577s
Dec 14 13:21:58.022: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 10.031490694s
Dec 14 13:22:00.026: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 12.034835578s
Dec 14 13:22:02.033: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 14.041576224s
Dec 14 13:22:04.037: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 16.045505005s
Dec 14 13:22:06.042: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 18.051368371s
Dec 14 13:22:08.047: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 20.055917331s
Dec 14 13:22:10.060: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Running", Reason="", readiness=true. Elapsed: 22.069348477s
Dec 14 13:22:12.065: INFO: Pod "pod-subpath-test-downwardapi-fw2v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.073809733s
STEP: Saw pod success
Dec 14 13:22:12.065: INFO: Pod "pod-subpath-test-downwardapi-fw2v" satisfied condition "Succeeded or Failed"
Dec 14 13:22:12.068: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-subpath-test-downwardapi-fw2v container test-container-subpath-downwardapi-fw2v: <nil>
STEP: delete the pod
Dec 14 13:22:12.098: INFO: Waiting for pod pod-subpath-test-downwardapi-fw2v to disappear
Dec 14 13:22:12.101: INFO: Pod pod-subpath-test-downwardapi-fw2v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fw2v
Dec 14 13:22:12.101: INFO: Deleting pod "pod-subpath-test-downwardapi-fw2v" in namespace "subpath-2587"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:12.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2587" for this suite.


• [SLOW TEST:24.180 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":-1,"completed":3,"skipped":60,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:09.397: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:22:09.455: INFO: Creating deployment "test-recreate-deployment"
Dec 14 13:22:09.461: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 13:22:09.482: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 14 13:22:11.489: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 13:22:11.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548929, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548929, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548929, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548929, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:22:13.501: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 13:22:13.511: INFO: Updating deployment test-recreate-deployment
Dec 14 13:22:13.511: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 14 13:22:13.639: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8573 /apis/apps/v1/namespaces/deployment-8573/deployments/test-recreate-deployment 2bd35520-aaf0-4355-ab58-3a6d1652b31d 7214 2 2020-12-14 13:22:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b549b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-14 13:22:13 +0000 UTC,LastTransitionTime:2020-12-14 13:22:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-12-14 13:22:13 +0000 UTC,LastTransitionTime:2020-12-14 13:22:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 13:22:13.647: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8573 /apis/apps/v1/namespaces/deployment-8573/replicasets/test-recreate-deployment-f79dd4667 aef3a8e6-17c5-4673-abaf-9dfef454b22d 7212 1 2020-12-14 13:22:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2bd35520-aaf0-4355-ab58-3a6d1652b31d 0xc0035ae650 0xc0035ae651}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bd35520-aaf0-4355-ab58-3a6d1652b31d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ae7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:22:13.647: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 13:22:13.647: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-8573 /apis/apps/v1/namespaces/deployment-8573/replicasets/test-recreate-deployment-c96cf48f c9db4de4-0cae-48d9-aff5-6fd1e96e219d 7202 2 2020-12-14 13:22:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2bd35520-aaf0-4355-ab58-3a6d1652b31d 0xc0035ae19f 0xc0035ae1b0}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bd35520-aaf0-4355-ab58-3a6d1652b31d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ae5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:22:13.656: INFO: Pod "test-recreate-deployment-f79dd4667-nwnqv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-nwnqv test-recreate-deployment-f79dd4667- deployment-8573 /api/v1/namespaces/deployment-8573/pods/test-recreate-deployment-f79dd4667-nwnqv 1c953cbe-4e85-44c1-8e7a-e878fb7d31be 7215 0 2020-12-14 13:22:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 aef3a8e6-17c5-4673-abaf-9dfef454b22d 0xc0035aef00 0xc0035aef01}] []  [{kube-controller-manager Update v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aef3a8e6-17c5-4673-abaf-9dfef454b22d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mtsmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mtsmv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mtsmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:13.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8573" for this suite.

•
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":3,"skipped":25,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:08.301: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1214 13:22:18.458634      32 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:22:18.458735      32 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:22:18.458763      32 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:22:18.458: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 13:22:18.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bnz8" in namespace "gc-9492"
Dec 14 13:22:18.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kzxb" in namespace "gc-9492"
Dec 14 13:22:18.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lv9v" in namespace "gc-9492"
Dec 14 13:22:18.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s5kh" in namespace "gc-9492"
Dec 14 13:22:18.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjbvb" in namespace "gc-9492"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:18.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9492" for this suite.


• [SLOW TEST:10.266 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":-1,"completed":8,"skipped":113,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:52.126: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:22.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5015" for this suite.


• [SLOW TEST:30.397 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":82,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:18.583: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 13:22:18.661: INFO: Waiting up to 5m0s for pod "pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee" in namespace "emptydir-3783" to be "Succeeded or Failed"
Dec 14 13:22:18.673: INFO: Pod "pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.320866ms
Dec 14 13:22:20.677: INFO: Pod "pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016169887s
Dec 14 13:22:22.684: INFO: Pod "pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02323556s
STEP: Saw pod success
Dec 14 13:22:22.684: INFO: Pod "pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee" satisfied condition "Succeeded or Failed"
Dec 14 13:22:22.689: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee container test-container: <nil>
STEP: delete the pod
Dec 14 13:22:22.723: INFO: Waiting for pod pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee to disappear
Dec 14 13:22:22.727: INFO: Pod pod-6b23b083-d85d-4f63-bc08-4afa0dfb27ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:22.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3783" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":9,"skipped":115,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:10.425: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:22:10.473: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 13:22:15.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 --namespace=crd-publish-openapi-8783 create -f -'
Dec 14 13:22:17.594: INFO: stderr: ""
Dec 14 13:22:17.595: INFO: stdout: "e2e-test-crd-publish-openapi-432-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 13:22:17.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 --namespace=crd-publish-openapi-8783 delete e2e-test-crd-publish-openapi-432-crds test-cr'
Dec 14 13:22:17.861: INFO: stderr: ""
Dec 14 13:22:17.861: INFO: stdout: "e2e-test-crd-publish-openapi-432-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 13:22:17.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 --namespace=crd-publish-openapi-8783 apply -f -'
Dec 14 13:22:18.360: INFO: stderr: ""
Dec 14 13:22:18.360: INFO: stdout: "e2e-test-crd-publish-openapi-432-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 13:22:18.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 --namespace=crd-publish-openapi-8783 delete e2e-test-crd-publish-openapi-432-crds test-cr'
Dec 14 13:22:18.693: INFO: stderr: ""
Dec 14 13:22:18.693: INFO: stdout: "e2e-test-crd-publish-openapi-432-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 14 13:22:18.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 explain e2e-test-crd-publish-openapi-432-crds'
Dec 14 13:22:19.250: INFO: stderr: ""
Dec 14 13:22:19.251: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-432-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:24.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8783" for this suite.


• [SLOW TEST:14.293 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":-1,"completed":6,"skipped":101,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:24.764: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:22:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:25.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-303" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":-1,"completed":7,"skipped":125,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:22.745: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-v225f in namespace proxy-7685
I1214 13:22:22.814461      32 runners.go:190] Created replication controller with name: proxy-service-v225f, namespace: proxy-7685, replica count: 1
I1214 13:22:23.864922      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:22:24.865306      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:22:25.865634      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:22:26.867028      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:27.867266      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:28.867522      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:29.867809      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:30.868091      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:31.868707      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:32.868931      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:22:33.869387      32 runners.go:190] proxy-service-v225f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:22:33.874: INFO: setup took 11.081637472s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 14 13:22:33.888: INFO: (0) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 13.99035ms)
Dec 14 13:22:33.888: INFO: (0) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 13.718291ms)
Dec 14 13:22:33.888: INFO: (0) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 12.681257ms)
Dec 14 13:22:33.888: INFO: (0) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 14.398732ms)
Dec 14 13:22:33.889: INFO: (0) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 14.391324ms)
Dec 14 13:22:33.889: INFO: (0) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 14.360497ms)
Dec 14 13:22:33.889: INFO: (0) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 13.896203ms)
Dec 14 13:22:33.892: INFO: (0) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 17.55985ms)
Dec 14 13:22:33.892: INFO: (0) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 18.243396ms)
Dec 14 13:22:33.893: INFO: (0) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 18.528745ms)
Dec 14 13:22:33.893: INFO: (0) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 18.220211ms)
Dec 14 13:22:33.893: INFO: (0) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 17.930523ms)
Dec 14 13:22:33.893: INFO: (0) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 18.878223ms)
Dec 14 13:22:33.894: INFO: (0) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 19.459836ms)
Dec 14 13:22:33.895: INFO: (0) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 20.033852ms)
Dec 14 13:22:33.895: INFO: (0) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 20.394377ms)
Dec 14 13:22:33.898: INFO: (1) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 2.993493ms)
Dec 14 13:22:33.898: INFO: (1) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 3.372527ms)
Dec 14 13:22:33.901: INFO: (1) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.436227ms)
Dec 14 13:22:33.901: INFO: (1) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 5.062791ms)
Dec 14 13:22:33.901: INFO: (1) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 6.508273ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.452781ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 6.049339ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.076804ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.963612ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.499018ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.280975ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 4.694428ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.02778ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 4.913956ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.395076ms)
Dec 14 13:22:33.902: INFO: (1) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 5.784929ms)
Dec 14 13:22:33.910: INFO: (2) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 8.430218ms)
Dec 14 13:22:33.910: INFO: (2) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 8.377904ms)
Dec 14 13:22:33.911: INFO: (2) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 9.105158ms)
Dec 14 13:22:33.911: INFO: (2) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 9.469889ms)
Dec 14 13:22:33.911: INFO: (2) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 9.425245ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 9.544398ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 9.766763ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 9.659351ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 9.800977ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 9.71528ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 9.989886ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 10.331894ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 10.029868ms)
Dec 14 13:22:33.912: INFO: (2) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 10.336558ms)
Dec 14 13:22:33.913: INFO: (2) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 10.606931ms)
Dec 14 13:22:33.913: INFO: (2) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 10.897639ms)
Dec 14 13:22:33.917: INFO: (3) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.564472ms)
Dec 14 13:22:33.918: INFO: (3) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 5.357999ms)
Dec 14 13:22:33.918: INFO: (3) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.43366ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 5.879424ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.942659ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 6.136812ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 6.113885ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.088835ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 6.145049ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 6.191296ms)
Dec 14 13:22:33.919: INFO: (3) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.13826ms)
Dec 14 13:22:33.920: INFO: (3) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 7.092978ms)
Dec 14 13:22:33.920: INFO: (3) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 7.198162ms)
Dec 14 13:22:33.920: INFO: (3) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 7.474417ms)
Dec 14 13:22:33.921: INFO: (3) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 7.95064ms)
Dec 14 13:22:33.921: INFO: (3) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 8.566009ms)
Dec 14 13:22:33.924: INFO: (4) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 2.799024ms)
Dec 14 13:22:33.925: INFO: (4) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 3.168513ms)
Dec 14 13:22:33.925: INFO: (4) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.196452ms)
Dec 14 13:22:33.925: INFO: (4) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.75162ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 4.058886ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 4.002998ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.103057ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 4.150595ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.215759ms)
Dec 14 13:22:33.926: INFO: (4) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 4.447699ms)
Dec 14 13:22:33.927: INFO: (4) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 5.01848ms)
Dec 14 13:22:33.927: INFO: (4) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.104852ms)
Dec 14 13:22:33.927: INFO: (4) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.293336ms)
Dec 14 13:22:33.927: INFO: (4) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.126748ms)
Dec 14 13:22:33.928: INFO: (4) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.765519ms)
Dec 14 13:22:33.928: INFO: (4) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.37122ms)
Dec 14 13:22:33.952: INFO: (5) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 23.504068ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 24.283542ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 23.884699ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 24.599387ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 24.195317ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 23.997072ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 24.328397ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 25.094113ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 24.080808ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 24.297541ms)
Dec 14 13:22:33.953: INFO: (5) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 25.022209ms)
Dec 14 13:22:33.954: INFO: (5) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 24.655077ms)
Dec 14 13:22:33.954: INFO: (5) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 25.558617ms)
Dec 14 13:22:33.954: INFO: (5) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 25.437597ms)
Dec 14 13:22:33.955: INFO: (5) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 25.46079ms)
Dec 14 13:22:33.955: INFO: (5) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 25.686869ms)
Dec 14 13:22:33.958: INFO: (6) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 3.447545ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.473701ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 3.906291ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 4.06014ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.545499ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 3.848872ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.071749ms)
Dec 14 13:22:33.959: INFO: (6) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 3.723325ms)
Dec 14 13:22:33.960: INFO: (6) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 4.616407ms)
Dec 14 13:22:33.960: INFO: (6) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 4.746569ms)
Dec 14 13:22:33.960: INFO: (6) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 4.783177ms)
Dec 14 13:22:33.960: INFO: (6) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.347288ms)
Dec 14 13:22:33.960: INFO: (6) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 5.042881ms)
Dec 14 13:22:33.962: INFO: (6) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.974969ms)
Dec 14 13:22:33.962: INFO: (6) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 6.878834ms)
Dec 14 13:22:33.962: INFO: (6) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.946564ms)
Dec 14 13:22:33.966: INFO: (7) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 3.85463ms)
Dec 14 13:22:33.966: INFO: (7) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 3.439023ms)
Dec 14 13:22:33.967: INFO: (7) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 3.972269ms)
Dec 14 13:22:33.967: INFO: (7) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 4.865127ms)
Dec 14 13:22:33.967: INFO: (7) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.398064ms)
Dec 14 13:22:33.968: INFO: (7) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.227984ms)
Dec 14 13:22:33.968: INFO: (7) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 4.333309ms)
Dec 14 13:22:33.968: INFO: (7) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.303851ms)
Dec 14 13:22:33.968: INFO: (7) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 4.292462ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 4.942083ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.286533ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.438311ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.195338ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 5.41151ms)
Dec 14 13:22:33.969: INFO: (7) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.791851ms)
Dec 14 13:22:33.971: INFO: (7) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 6.976613ms)
Dec 14 13:22:33.975: INFO: (8) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.587368ms)
Dec 14 13:22:33.975: INFO: (8) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.847628ms)
Dec 14 13:22:33.975: INFO: (8) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 4.459155ms)
Dec 14 13:22:33.975: INFO: (8) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 4.217858ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 5.438762ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 6.061704ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.570195ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.621998ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.625671ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.640404ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.680632ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 5.722142ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.815203ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 5.882204ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.93552ms)
Dec 14 13:22:33.977: INFO: (8) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.647489ms)
Dec 14 13:22:33.980: INFO: (9) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 2.675186ms)
Dec 14 13:22:33.981: INFO: (9) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.49829ms)
Dec 14 13:22:33.982: INFO: (9) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 4.179206ms)
Dec 14 13:22:33.982: INFO: (9) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 4.323471ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 5.367982ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 5.479997ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.895492ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.95158ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.900778ms)
Dec 14 13:22:33.983: INFO: (9) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.982982ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 6.089092ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.059276ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 6.337367ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.58618ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 6.454221ms)
Dec 14 13:22:33.984: INFO: (9) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 6.862321ms)
Dec 14 13:22:33.987: INFO: (10) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 2.572467ms)
Dec 14 13:22:33.988: INFO: (10) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.39714ms)
Dec 14 13:22:33.988: INFO: (10) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 3.562683ms)
Dec 14 13:22:33.989: INFO: (10) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.906859ms)
Dec 14 13:22:33.989: INFO: (10) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 4.001877ms)
Dec 14 13:22:33.989: INFO: (10) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.267608ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.296153ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 5.514464ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.430017ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.554531ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.593731ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.7022ms)
Dec 14 13:22:33.990: INFO: (10) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.795669ms)
Dec 14 13:22:33.991: INFO: (10) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 6.550534ms)
Dec 14 13:22:33.991: INFO: (10) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 6.609534ms)
Dec 14 13:22:33.992: INFO: (10) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 7.294298ms)
Dec 14 13:22:33.995: INFO: (11) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.29465ms)
Dec 14 13:22:34.002: INFO: (11) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 9.577835ms)
Dec 14 13:22:34.005: INFO: (11) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 12.665971ms)
Dec 14 13:22:34.006: INFO: (11) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 14.117728ms)
Dec 14 13:22:34.007: INFO: (11) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 14.827669ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 15.417881ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 15.744738ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 15.925765ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 15.859414ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 15.950918ms)
Dec 14 13:22:34.008: INFO: (11) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 16.314314ms)
Dec 14 13:22:34.009: INFO: (11) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 16.667437ms)
Dec 14 13:22:34.009: INFO: (11) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 16.55947ms)
Dec 14 13:22:34.009: INFO: (11) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 16.600035ms)
Dec 14 13:22:34.009: INFO: (11) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 16.906356ms)
Dec 14 13:22:34.009: INFO: (11) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 17.111424ms)
Dec 14 13:22:34.012: INFO: (12) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 3.1085ms)
Dec 14 13:22:34.013: INFO: (12) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 3.263857ms)
Dec 14 13:22:34.013: INFO: (12) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 3.389823ms)
Dec 14 13:22:34.013: INFO: (12) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.611281ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.382426ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.242939ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 4.500493ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 4.410296ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 4.816031ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 4.831627ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 4.920301ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 4.826714ms)
Dec 14 13:22:34.014: INFO: (12) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.027103ms)
Dec 14 13:22:34.015: INFO: (12) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.123097ms)
Dec 14 13:22:34.015: INFO: (12) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.842098ms)
Dec 14 13:22:34.016: INFO: (12) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.250751ms)
Dec 14 13:22:34.019: INFO: (13) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.440518ms)
Dec 14 13:22:34.020: INFO: (13) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 3.967043ms)
Dec 14 13:22:34.021: INFO: (13) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.889419ms)
Dec 14 13:22:34.021: INFO: (13) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 4.913652ms)
Dec 14 13:22:34.021: INFO: (13) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.934704ms)
Dec 14 13:22:34.021: INFO: (13) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.797624ms)
Dec 14 13:22:34.021: INFO: (13) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.989647ms)
Dec 14 13:22:34.022: INFO: (13) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 5.728551ms)
Dec 14 13:22:34.022: INFO: (13) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.874255ms)
Dec 14 13:22:34.022: INFO: (13) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 5.767718ms)
Dec 14 13:22:34.022: INFO: (13) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 5.929023ms)
Dec 14 13:22:34.023: INFO: (13) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 6.628402ms)
Dec 14 13:22:34.023: INFO: (13) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.703308ms)
Dec 14 13:22:34.023: INFO: (13) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 7.011837ms)
Dec 14 13:22:34.023: INFO: (13) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 6.887252ms)
Dec 14 13:22:34.023: INFO: (13) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 6.88727ms)
Dec 14 13:22:34.025: INFO: (14) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 2.593024ms)
Dec 14 13:22:34.026: INFO: (14) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.524993ms)
Dec 14 13:22:34.027: INFO: (14) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.653006ms)
Dec 14 13:22:34.027: INFO: (14) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 3.925209ms)
Dec 14 13:22:34.027: INFO: (14) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 4.056073ms)
Dec 14 13:22:34.027: INFO: (14) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 4.344357ms)
Dec 14 13:22:34.028: INFO: (14) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.262254ms)
Dec 14 13:22:34.028: INFO: (14) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.464032ms)
Dec 14 13:22:34.029: INFO: (14) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 5.578491ms)
Dec 14 13:22:34.029: INFO: (14) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 6.280443ms)
Dec 14 13:22:34.030: INFO: (14) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 7.092275ms)
Dec 14 13:22:34.030: INFO: (14) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 7.01129ms)
Dec 14 13:22:34.030: INFO: (14) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 6.967575ms)
Dec 14 13:22:34.031: INFO: (14) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 7.523429ms)
Dec 14 13:22:34.031: INFO: (14) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 7.527241ms)
Dec 14 13:22:34.032: INFO: (14) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 8.829096ms)
Dec 14 13:22:34.036: INFO: (15) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.544366ms)
Dec 14 13:22:34.036: INFO: (15) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 3.553082ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 4.634155ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 4.616761ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 4.808203ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 4.783807ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 5.180937ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 5.135773ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.072659ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.993577ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 5.080787ms)
Dec 14 13:22:34.037: INFO: (15) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.34899ms)
Dec 14 13:22:34.038: INFO: (15) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 5.314774ms)
Dec 14 13:22:34.038: INFO: (15) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.547578ms)
Dec 14 13:22:34.038: INFO: (15) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.766478ms)
Dec 14 13:22:34.038: INFO: (15) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.604687ms)
Dec 14 13:22:34.041: INFO: (16) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 3.179101ms)
Dec 14 13:22:34.041: INFO: (16) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 3.407897ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 11.406475ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 11.692341ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 11.54306ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 11.690389ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 11.580878ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 11.565389ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 11.716844ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 11.633345ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 11.754154ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 11.821239ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 11.885616ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 11.990171ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 11.973905ms)
Dec 14 13:22:34.050: INFO: (16) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 11.912883ms)
Dec 14 13:22:34.053: INFO: (17) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 3.236844ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 3.411168ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 3.626411ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 3.341138ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.270285ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.503519ms)
Dec 14 13:22:34.054: INFO: (17) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 3.681583ms)
Dec 14 13:22:34.055: INFO: (17) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 4.399484ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 4.513688ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.331119ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 4.439429ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.716458ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.129819ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 5.306441ms)
Dec 14 13:22:34.056: INFO: (17) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 4.420387ms)
Dec 14 13:22:34.058: INFO: (17) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.529962ms)
Dec 14 13:22:34.061: INFO: (18) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 2.75295ms)
Dec 14 13:22:34.062: INFO: (18) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.754734ms)
Dec 14 13:22:34.062: INFO: (18) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 4.090769ms)
Dec 14 13:22:34.063: INFO: (18) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 4.220903ms)
Dec 14 13:22:34.063: INFO: (18) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 4.037471ms)
Dec 14 13:22:34.063: INFO: (18) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 4.197884ms)
Dec 14 13:22:34.063: INFO: (18) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 4.508928ms)
Dec 14 13:22:34.064: INFO: (18) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 5.00335ms)
Dec 14 13:22:34.064: INFO: (18) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 4.498171ms)
Dec 14 13:22:34.064: INFO: (18) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 5.797317ms)
Dec 14 13:22:34.064: INFO: (18) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 4.466525ms)
Dec 14 13:22:34.064: INFO: (18) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.220607ms)
Dec 14 13:22:34.066: INFO: (18) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 6.167625ms)
Dec 14 13:22:34.066: INFO: (18) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 6.473927ms)
Dec 14 13:22:34.066: INFO: (18) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 5.989952ms)
Dec 14 13:22:34.066: INFO: (18) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 6.561074ms)
Dec 14 13:22:34.069: INFO: (19) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:1080/proxy/rewriteme">test<... (200; 3.072705ms)
Dec 14 13:22:34.070: INFO: (19) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:162/proxy/: bar (200; 3.60629ms)
Dec 14 13:22:34.071: INFO: (19) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:460/proxy/: tls baz (200; 3.896615ms)
Dec 14 13:22:34.071: INFO: (19) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:462/proxy/: tls qux (200; 3.847313ms)
Dec 14 13:22:34.071: INFO: (19) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:162/proxy/: bar (200; 4.06302ms)
Dec 14 13:22:34.072: INFO: (19) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:1080/proxy/rewriteme">... (200; 5.046926ms)
Dec 14 13:22:34.073: INFO: (19) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname1/proxy/: foo (200; 5.344022ms)
Dec 14 13:22:34.073: INFO: (19) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.818883ms)
Dec 14 13:22:34.073: INFO: (19) /api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/proxy-service-v225f-jxr4h/proxy/rewriteme">test</a> (200; 5.625364ms)
Dec 14 13:22:34.073: INFO: (19) /api/v1/namespaces/proxy-7685/pods/http:proxy-service-v225f-jxr4h:160/proxy/: foo (200; 5.637526ms)
Dec 14 13:22:34.073: INFO: (19) /api/v1/namespaces/proxy-7685/services/proxy-service-v225f:portname2/proxy/: bar (200; 5.913078ms)
Dec 14 13:22:34.074: INFO: (19) /api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/: <a href="/api/v1/namespaces/proxy-7685/pods/https:proxy-service-v225f-jxr4h:443/proxy/tlsrewritem... (200; 7.184997ms)
Dec 14 13:22:34.074: INFO: (19) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname1/proxy/: foo (200; 6.37466ms)
Dec 14 13:22:34.074: INFO: (19) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname1/proxy/: tls baz (200; 7.176257ms)
Dec 14 13:22:34.074: INFO: (19) /api/v1/namespaces/proxy-7685/services/https:proxy-service-v225f:tlsportname2/proxy/: tls qux (200; 5.927934ms)
Dec 14 13:22:34.074: INFO: (19) /api/v1/namespaces/proxy-7685/services/http:proxy-service-v225f:portname2/proxy/: bar (200; 6.102611ms)
STEP: deleting ReplicationController proxy-service-v225f in namespace proxy-7685, will wait for the garbage collector to delete the pods
Dec 14 13:22:34.135: INFO: Deleting ReplicationController proxy-service-v225f took: 6.698039ms
Dec 14 13:22:35.135: INFO: Terminating ReplicationController proxy-service-v225f pods took: 1.000244377s
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:37.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7685" for this suite.


• [SLOW TEST:14.800 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":-1,"completed":10,"skipped":118,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:13.728: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7494
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7494
STEP: creating replication controller externalsvc in namespace services-7494
I1214 13:22:13.809859      29 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7494, replica count: 2
I1214 13:22:16.860806      29 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 14 13:22:16.893: INFO: Creating new exec pod
Dec 14 13:22:20.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-7494 execpodglk5r -- /bin/sh -x -c nslookup clusterip-service.services-7494.svc.cluster.local'
Dec 14 13:22:21.428: INFO: stderr: "+ nslookup clusterip-service.services-7494.svc.cluster.local\n"
Dec 14 13:22:21.428: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-7494.svc.cluster.local\tcanonical name = externalsvc.services-7494.svc.cluster.local.\nName:\texternalsvc.services-7494.svc.cluster.local\nAddress: 10.254.134.81\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7494, will wait for the garbage collector to delete the pods
Dec 14 13:22:21.494: INFO: Deleting ReplicationController externalsvc took: 9.642303ms
Dec 14 13:22:21.600: INFO: Terminating ReplicationController externalsvc pods took: 105.720006ms
Dec 14 13:22:38.719: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:38.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:25.026 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":-1,"completed":4,"skipped":38,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5819.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5819.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5819.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5819.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5819.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5819.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:22:36.142: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9: the server could not find the requested resource (get pods dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9)
Dec 14 13:22:36.145: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9: the server could not find the requested resource (get pods dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9)
Dec 14 13:22:36.154: INFO: Unable to read jessie_udp@PodARecord from pod dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9: the server could not find the requested resource (get pods dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9)
Dec 14 13:22:36.157: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9: the server could not find the requested resource (get pods dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9)
Dec 14 13:22:36.157: INFO: Lookups using dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:22:41.190: INFO: DNS probes using dns-5819/dns-test-d783e1e5-ee3d-43d7-9aa1-1bf26924e7c9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:41.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5819" for this suite.


• [SLOW TEST:15.169 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":-1,"completed":8,"skipped":162,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:38.774: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:22:38.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3" in namespace "downward-api-4457" to be "Succeeded or Failed"
Dec 14 13:22:38.858: INFO: Pod "downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.66561ms
Dec 14 13:22:40.862: INFO: Pod "downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011519702s
Dec 14 13:22:42.866: INFO: Pod "downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016116948s
STEP: Saw pod success
Dec 14 13:22:42.866: INFO: Pod "downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3" satisfied condition "Succeeded or Failed"
Dec 14 13:22:42.869: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3 container client-container: <nil>
STEP: delete the pod
Dec 14 13:22:42.896: INFO: Waiting for pod downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3 to disappear
Dec 14 13:22:42.899: INFO: Pod downwardapi-volume-00295658-eb10-4e4f-96ca-009ba36d18b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:42.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4457" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":47,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:41.315: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-53a67f32-df2b-481d-a33c-562b02c61b3e
STEP: Creating a pod to test consume secrets
Dec 14 13:22:41.396: INFO: Waiting up to 5m0s for pod "pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488" in namespace "secrets-5574" to be "Succeeded or Failed"
Dec 14 13:22:41.400: INFO: Pod "pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08218ms
Dec 14 13:22:43.404: INFO: Pod "pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007443636s
Dec 14 13:22:45.408: INFO: Pod "pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011112878s
STEP: Saw pod success
Dec 14 13:22:45.408: INFO: Pod "pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488" satisfied condition "Succeeded or Failed"
Dec 14 13:22:45.410: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:22:45.438: INFO: Waiting for pod pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488 to disappear
Dec 14 13:22:45.442: INFO: Pod pod-secrets-121544be-f95e-4abc-844f-1bdd334d0488 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:45.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5574" for this suite.
STEP: Destroying namespace "secret-namespace-4335" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":-1,"completed":9,"skipped":214,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:45.476: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7109223f-1dc8-4c21-bec7-7e5aec227c8e
STEP: Creating a pod to test consume configMaps
Dec 14 13:22:45.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018" in namespace "configmap-8153" to be "Succeeded or Failed"
Dec 14 13:22:45.534: INFO: Pod "pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230928ms
Dec 14 13:22:47.538: INFO: Pod "pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009408859s
Dec 14 13:22:49.542: INFO: Pod "pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014158378s
STEP: Saw pod success
Dec 14 13:22:49.543: INFO: Pod "pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018" satisfied condition "Succeeded or Failed"
Dec 14 13:22:49.546: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:22:49.569: INFO: Waiting for pod pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018 to disappear
Dec 14 13:22:49.572: INFO: Pod pod-configmaps-2a69e55d-0f97-470e-aa0c-d40db744c018 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:49.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8153" for this suite.

•
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:42.919: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:22:43.449: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:22:45.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548963, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548963, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548963, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548963, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:22:48.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:22:48.491: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3609-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:49.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7379" for this suite.
STEP: Destroying namespace "webhook-7379-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.872 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":-1,"completed":6,"skipped":50,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 14 13:22:49.920: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:56.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4702" for this suite.


• [SLOW TEST:6.332 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":-1,"completed":7,"skipped":85,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:56.230: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:56.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1002" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":-1,"completed":8,"skipped":91,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:37.585: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 14 13:22:45.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:45.693: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:47.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:47.698: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:49.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:49.713: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:51.694: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:51.701: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:53.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:53.732: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:55.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:55.697: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:57.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:57.697: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:22:59.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:22:59.697: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:22:59.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3201" for this suite.


• [SLOW TEST:22.136 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":139,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:56.324: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4949
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4949
I1214 13:22:56.392628      29 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4949, replica count: 2
I1214 13:22:59.449350      29 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:22:59.449: INFO: Creating new exec pod
Dec 14 13:23:04.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 14 13:23:04.824: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:23:04.824: INFO: stdout: ""
Dec 14 13:23:04.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 10.254.121.178 80'
Dec 14 13:23:05.265: INFO: stderr: "+ nc -zv -t -w 2 10.254.121.178 80\nConnection to 10.254.121.178 80 port [tcp/http] succeeded!\n"
Dec 14 13:23:05.265: INFO: stdout: ""
Dec 14 13:23:05.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.200 31438'
Dec 14 13:23:05.637: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.200 31438\nConnection to 10.0.0.200 31438 port [tcp/31438] succeeded!\n"
Dec 14 13:23:05.637: INFO: stdout: ""
Dec 14 13:23:05.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.55 31438'
Dec 14 13:23:05.981: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.55 31438\nConnection to 10.0.0.55 31438 port [tcp/31438] succeeded!\n"
Dec 14 13:23:05.981: INFO: stdout: ""
Dec 14 13:23:05.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.93 31438'
Dec 14 13:23:06.347: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.93 31438\nConnection to 38.108.68.93 31438 port [tcp/31438] succeeded!\n"
Dec 14 13:23:06.347: INFO: stdout: ""
Dec 14 13:23:06.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-4949 execpod74gwn -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.247 31438'
Dec 14 13:23:06.711: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.247 31438\nConnection to 38.108.68.247 31438 port [tcp/31438] succeeded!\n"
Dec 14 13:23:06.711: INFO: stdout: ""
Dec 14 13:23:06.711: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:06.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4949" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:10.430 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":-1,"completed":9,"skipped":93,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:22.567: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 14 13:22:22.601: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 13:23:14.117: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-94776f80-a9d0-4b1b-b28e-4ea032376ed6", GenerateName:"", Namespace:"init-container-4609", SelfLink:"/api/v1/namespaces/init-container-4609/pods/pod-init-94776f80-a9d0-4b1b-b28e-4ea032376ed6", UID:"7ed3c8dc-b632-4cf9-88d4-8133e74208da", ResourceVersion:"8678", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63743548942, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"601842340"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.100.227.160/32", "cni.projectcalico.org/podIPs":"10.100.227.160/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a896a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a896c0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a896e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a89700)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a89720), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a89740)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-46ws5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0021c6900), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-46ws5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-46ws5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-46ws5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028f5358), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance-v1-19-pwlsgdsa6hrh-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0035aaa10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028f53d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028f53f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0028f53f8), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548942, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548942, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548942, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548942, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.55", PodIP:"10.100.227.160", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.227.160"}}, StartTime:(*v1.Time)(0xc002a89760), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035aaaf0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035aabd0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://16ab3aace44b9a79248ae90a08d369d63bb2e6594e3b3e19c6ace841569b6c7c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a897a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a89780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0028f546f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:14.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4609" for this suite.


• [SLOW TEST:51.565 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":-1,"completed":3,"skipped":100,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:14.184: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-f994fd9d-c7a5-4738-80de-f5d90f3dab70
STEP: Creating a pod to test consume configMaps
Dec 14 13:23:14.261: INFO: Waiting up to 5m0s for pod "pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98" in namespace "configmap-2099" to be "Succeeded or Failed"
Dec 14 13:23:14.268: INFO: Pod "pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.592483ms
Dec 14 13:23:16.272: INFO: Pod "pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010314516s
STEP: Saw pod success
Dec 14 13:23:16.272: INFO: Pod "pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98" satisfied condition "Succeeded or Failed"
Dec 14 13:23:16.274: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:23:16.296: INFO: Waiting for pod pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98 to disappear
Dec 14 13:23:16.298: INFO: Pod pod-configmaps-ceaa9380-b572-4368-8cda-fa4902a0eb98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:16.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2099" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":116,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:06.759: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:23:07.936: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:23:09.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548987, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548987, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548987, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743548987, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:23:12.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 14 13:23:17.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 attach --namespace=webhook-2807 to-be-attached-pod -i -c=container1'
Dec 14 13:23:17.224: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:17.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2807" for this suite.
STEP: Destroying namespace "webhook-2807-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:10.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":-1,"completed":10,"skipped":94,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:16.322: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:23:19.374: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:19.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8524" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":123,"failed":0}

SSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":10,"skipped":221,"failed":0}
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:49.586: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Dec 14 13:22:49.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 create -f - --namespace=kubectl-3756'
Dec 14 13:22:50.286: INFO: stderr: ""
Dec 14 13:22:50.286: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 13:22:50.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:22:50.487: INFO: stderr: ""
Dec 14 13:22:50.487: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-v8gnt "
Dec 14 13:22:50.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:22:50.686: INFO: stderr: ""
Dec 14 13:22:50.686: INFO: stdout: ""
Dec 14 13:22:50.686: INFO: update-demo-nautilus-7kmfn is created but not running
Dec 14 13:22:55.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:22:55.896: INFO: stderr: ""
Dec 14 13:22:55.896: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-v8gnt "
Dec 14 13:22:55.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:22:56.053: INFO: stderr: ""
Dec 14 13:22:56.053: INFO: stdout: "true"
Dec 14 13:22:56.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:22:56.220: INFO: stderr: ""
Dec 14 13:22:56.220: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:22:56.220: INFO: validating pod update-demo-nautilus-7kmfn
Dec 14 13:22:56.226: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:22:56.226: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:22:56.226: INFO: update-demo-nautilus-7kmfn is verified up and running
Dec 14 13:22:56.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-v8gnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:22:56.413: INFO: stderr: ""
Dec 14 13:22:56.413: INFO: stdout: "true"
Dec 14 13:22:56.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-v8gnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:22:56.597: INFO: stderr: ""
Dec 14 13:22:56.597: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:22:56.598: INFO: validating pod update-demo-nautilus-v8gnt
Dec 14 13:22:56.605: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:22:56.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:22:56.606: INFO: update-demo-nautilus-v8gnt is verified up and running
STEP: scaling down the replication controller
Dec 14 13:22:56.612: INFO: scanned /root for discovery docs: <nil>
Dec 14 13:22:56.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3756'
Dec 14 13:22:56.840: INFO: stderr: ""
Dec 14 13:22:56.840: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 13:22:56.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:22:57.021: INFO: stderr: ""
Dec 14 13:22:57.021: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-v8gnt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 14 13:23:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:23:02.204: INFO: stderr: ""
Dec 14 13:23:02.204: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-v8gnt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 14 13:23:07.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:23:07.428: INFO: stderr: ""
Dec 14 13:23:07.428: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-v8gnt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 14 13:23:12.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:23:12.608: INFO: stderr: ""
Dec 14 13:23:12.608: INFO: stdout: "update-demo-nautilus-7kmfn "
Dec 14 13:23:12.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:12.771: INFO: stderr: ""
Dec 14 13:23:12.771: INFO: stdout: "true"
Dec 14 13:23:12.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:12.934: INFO: stderr: ""
Dec 14 13:23:12.934: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:23:12.934: INFO: validating pod update-demo-nautilus-7kmfn
Dec 14 13:23:12.942: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:23:12.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:23:12.942: INFO: update-demo-nautilus-7kmfn is verified up and running
STEP: scaling up the replication controller
Dec 14 13:23:12.949: INFO: scanned /root for discovery docs: <nil>
Dec 14 13:23:12.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3756'
Dec 14 13:23:13.176: INFO: stderr: ""
Dec 14 13:23:13.176: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 13:23:13.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:23:13.374: INFO: stderr: ""
Dec 14 13:23:13.374: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-8jrc9 "
Dec 14 13:23:13.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:13.545: INFO: stderr: ""
Dec 14 13:23:13.545: INFO: stdout: "true"
Dec 14 13:23:13.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:13.728: INFO: stderr: ""
Dec 14 13:23:13.728: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:23:13.728: INFO: validating pod update-demo-nautilus-7kmfn
Dec 14 13:23:13.732: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:23:13.732: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:23:13.732: INFO: update-demo-nautilus-7kmfn is verified up and running
Dec 14 13:23:13.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-8jrc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:13.898: INFO: stderr: ""
Dec 14 13:23:13.898: INFO: stdout: ""
Dec 14 13:23:13.898: INFO: update-demo-nautilus-8jrc9 is created but not running
Dec 14 13:23:18.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3756'
Dec 14 13:23:19.095: INFO: stderr: ""
Dec 14 13:23:19.095: INFO: stdout: "update-demo-nautilus-7kmfn update-demo-nautilus-8jrc9 "
Dec 14 13:23:19.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:19.273: INFO: stderr: ""
Dec 14 13:23:19.273: INFO: stdout: "true"
Dec 14 13:23:19.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-7kmfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:19.462: INFO: stderr: ""
Dec 14 13:23:19.462: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:23:19.462: INFO: validating pod update-demo-nautilus-7kmfn
Dec 14 13:23:19.467: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:23:19.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:23:19.467: INFO: update-demo-nautilus-7kmfn is verified up and running
Dec 14 13:23:19.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-8jrc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:19.630: INFO: stderr: ""
Dec 14 13:23:19.630: INFO: stdout: "true"
Dec 14 13:23:19.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods update-demo-nautilus-8jrc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3756'
Dec 14 13:23:19.815: INFO: stderr: ""
Dec 14 13:23:19.815: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:23:19.815: INFO: validating pod update-demo-nautilus-8jrc9
Dec 14 13:23:19.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:23:19.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:23:19.826: INFO: update-demo-nautilus-8jrc9 is verified up and running
STEP: using delete to clean up resources
Dec 14 13:23:19.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 delete --grace-period=0 --force -f - --namespace=kubectl-3756'
Dec 14 13:23:19.968: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:23:19.968: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 13:23:19.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3756'
Dec 14 13:23:20.219: INFO: stderr: "No resources found in kubectl-3756 namespace.\n"
Dec 14 13:23:20.220: INFO: stdout: ""
Dec 14 13:23:20.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 get pods -l name=update-demo --namespace=kubectl-3756 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 13:23:20.408: INFO: stderr: ""
Dec 14 13:23:20.408: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:20.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3756" for this suite.


• [SLOW TEST:30.832 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":-1,"completed":11,"skipped":221,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:19.421: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-ca344863-31ed-4ee9-9187-d7df99ebb803
STEP: Creating a pod to test consume secrets
Dec 14 13:23:19.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390" in namespace "projected-3280" to be "Succeeded or Failed"
Dec 14 13:23:19.477: INFO: Pod "pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390": Phase="Pending", Reason="", readiness=false. Elapsed: 8.768834ms
Dec 14 13:23:21.482: INFO: Pod "pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013439865s
Dec 14 13:23:23.485: INFO: Pod "pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016729936s
STEP: Saw pod success
Dec 14 13:23:23.485: INFO: Pod "pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390" satisfied condition "Succeeded or Failed"
Dec 14 13:23:23.487: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:23:23.510: INFO: Waiting for pod pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390 to disappear
Dec 14 13:23:23.514: INFO: Pod pod-projected-secrets-a5685c30-c15a-4192-b541-f921fe7b8390 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:23.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3280" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":134,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:12.131: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-2971e7f6-fc44-4e05-9fb9-20ee3f4a1557
STEP: Creating configMap with name cm-test-opt-upd-f3a8142f-95ee-42bf-9270-cd4be56ac389
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2971e7f6-fc44-4e05-9fb9-20ee3f4a1557
STEP: Updating configmap cm-test-opt-upd-f3a8142f-95ee-42bf-9270-cd4be56ac389
STEP: Creating configMap with name cm-test-opt-create-7e7949ca-a3a6-4adc-96d0-7a187410279e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:26.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9975" for this suite.


• [SLOW TEST:74.630 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":65,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:23.580: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 14 13:23:23.622: INFO: Created pod &Pod{ObjectMeta:{dns-2891  dns-2891 /api/v1/namespaces/dns-2891/pods/dns-2891 64971d46-57a2-49cb-a921-56bc147d60d7 8937 0 2020-12-14 13:23:23 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-12-14 13:23:23 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-56dk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-56dk6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-56dk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:23:23.635: INFO: The status of Pod dns-2891 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:23:25.638: INFO: The status of Pod dns-2891 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:23:27.638: INFO: The status of Pod dns-2891 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 14 13:23:27.639: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2891 PodName:dns-2891 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:23:27.639: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Verifying customized DNS server is configured on pod...
Dec 14 13:23:27.888: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2891 PodName:dns-2891 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:23:27.888: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
Dec 14 13:23:28.087: INFO: Deleting pod dns-2891...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:28.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2891" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":-1,"completed":7,"skipped":155,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:17.349: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-6383
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6383
STEP: Deleting pre-stop pod
Dec 14 13:23:30.483: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:30.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6383" for this suite.


• [SLOW TEST:13.170 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":-1,"completed":11,"skipped":96,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:30.619: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:30.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6220" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":-1,"completed":12,"skipped":142,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Dec 14 13:23:30.827: INFO: created test-podtemplate-1
Dec 14 13:23:30.832: INFO: created test-podtemplate-2
Dec 14 13:23:30.837: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 14 13:23:30.842: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 14 13:23:30.870: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:30.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5195" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":-1,"completed":13,"skipped":156,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:30.939: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-801eaba5-08bd-4910-b484-579652219c58
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:30.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2059" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":-1,"completed":14,"skipped":175,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:20.433: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 14 13:23:20.471: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:32.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-655" for this suite.


• [SLOW TEST:11.652 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":226,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:32.099: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:32.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8163" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":-1,"completed":13,"skipped":231,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-941413eb-f3eb-43d6-b934-db0764642dcb
STEP: Creating a pod to test consume secrets
Dec 14 13:23:28.209: INFO: Waiting up to 5m0s for pod "pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7" in namespace "secrets-6539" to be "Succeeded or Failed"
Dec 14 13:23:28.220: INFO: Pod "pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.759319ms
Dec 14 13:23:30.224: INFO: Pod "pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014946793s
Dec 14 13:23:32.230: INFO: Pod "pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02035609s
STEP: Saw pod success
Dec 14 13:23:32.230: INFO: Pod "pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7" satisfied condition "Succeeded or Failed"
Dec 14 13:23:32.233: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:23:32.273: INFO: Waiting for pod pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7 to disappear
Dec 14 13:23:32.277: INFO: Pod pod-secrets-e35ff1b8-f761-4bab-a362-78260e464cd7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6539" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":163,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:26.775: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:23:27.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:23:29.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549007, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549007, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549007, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549007, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:23:32.462: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:32.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4682" for this suite.
STEP: Destroying namespace "webhook-4682-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:5.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":-1,"completed":5,"skipped":70,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:30.999: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 14 13:23:31.034: INFO: Waiting up to 5m0s for pod "pod-beb5902a-1f70-455b-8d43-03b7d23778e1" in namespace "emptydir-3069" to be "Succeeded or Failed"
Dec 14 13:23:31.043: INFO: Pod "pod-beb5902a-1f70-455b-8d43-03b7d23778e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139848ms
Dec 14 13:23:33.047: INFO: Pod "pod-beb5902a-1f70-455b-8d43-03b7d23778e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01375479s
Dec 14 13:23:35.052: INFO: Pod "pod-beb5902a-1f70-455b-8d43-03b7d23778e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018138192s
STEP: Saw pod success
Dec 14 13:23:35.052: INFO: Pod "pod-beb5902a-1f70-455b-8d43-03b7d23778e1" satisfied condition "Succeeded or Failed"
Dec 14 13:23:35.055: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-beb5902a-1f70-455b-8d43-03b7d23778e1 container test-container: <nil>
STEP: delete the pod
Dec 14 13:23:35.077: INFO: Waiting for pod pod-beb5902a-1f70-455b-8d43-03b7d23778e1 to disappear
Dec 14 13:23:35.081: INFO: Pod pod-beb5902a-1f70-455b-8d43-03b7d23778e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:35.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3069" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":177,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:35.144: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:35.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4875" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":-1,"completed":16,"skipped":199,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:32.194: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-2e0826c1-498e-4adc-a32d-23c4ef4d295f
STEP: Creating a pod to test consume configMaps
Dec 14 13:23:32.249: INFO: Waiting up to 5m0s for pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb" in namespace "configmap-5426" to be "Succeeded or Failed"
Dec 14 13:23:32.255: INFO: Pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.692148ms
Dec 14 13:23:34.260: INFO: Pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010899534s
Dec 14 13:23:36.265: INFO: Pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01610489s
Dec 14 13:23:38.270: INFO: Pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020911759s
STEP: Saw pod success
Dec 14 13:23:38.270: INFO: Pod "pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb" satisfied condition "Succeeded or Failed"
Dec 14 13:23:38.273: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:23:38.294: INFO: Waiting for pod pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb to disappear
Dec 14 13:23:38.297: INFO: Pod pod-configmaps-1072faab-9b77-4926-8844-48e3743588cb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:38.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5426" for this suite.


• [SLOW TEST:6.118 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":240,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:32.327: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 14 13:23:37.404: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:38.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6043" for this suite.


• [SLOW TEST:6.114 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":-1,"completed":9,"skipped":181,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:32.569: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-847.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-847.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-847.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-847.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-847.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-847.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:23:36.680: INFO: Unable to read wheezy_udp@PodARecord from pod dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be: the server could not find the requested resource (get pods dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be)
Dec 14 13:23:36.683: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be: the server could not find the requested resource (get pods dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be)
Dec 14 13:23:36.707: INFO: Unable to read jessie_udp@PodARecord from pod dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be: the server could not find the requested resource (get pods dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be)
Dec 14 13:23:36.713: INFO: Unable to read jessie_tcp@PodARecord from pod dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be: the server could not find the requested resource (get pods dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be)
Dec 14 13:23:36.713: INFO: Lookups using dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:23:41.740: INFO: DNS probes using dns-847/dns-test-4d43d77c-03b8-42ef-a34f-2fe48ed323be succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:41.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-847" for this suite.


• [SLOW TEST:9.226 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:38.381: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 13:23:38.418: INFO: Waiting up to 5m0s for pod "pod-f01356eb-c07d-422d-ad64-1bf77c1b51df" in namespace "emptydir-6477" to be "Succeeded or Failed"
Dec 14 13:23:38.429: INFO: Pod "pod-f01356eb-c07d-422d-ad64-1bf77c1b51df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.625934ms
Dec 14 13:23:40.432: INFO: Pod "pod-f01356eb-c07d-422d-ad64-1bf77c1b51df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013821291s
Dec 14 13:23:42.436: INFO: Pod "pod-f01356eb-c07d-422d-ad64-1bf77c1b51df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017868492s
STEP: Saw pod success
Dec 14 13:23:42.436: INFO: Pod "pod-f01356eb-c07d-422d-ad64-1bf77c1b51df" satisfied condition "Succeeded or Failed"
Dec 14 13:23:42.439: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-f01356eb-c07d-422d-ad64-1bf77c1b51df container test-container: <nil>
STEP: delete the pod
Dec 14 13:23:42.464: INFO: Waiting for pod pod-f01356eb-c07d-422d-ad64-1bf77c1b51df to disappear
Dec 14 13:23:42.468: INFO: Pod pod-f01356eb-c07d-422d-ad64-1bf77c1b51df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6477" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":-1,"completed":6,"skipped":78,"failed":0}
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 14 13:23:41.848: INFO: Waiting up to 5m0s for pod "downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829" in namespace "downward-api-9940" to be "Succeeded or Failed"
Dec 14 13:23:41.855: INFO: Pod "downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829": Phase="Pending", Reason="", readiness=false. Elapsed: 7.819652ms
Dec 14 13:23:43.859: INFO: Pod "downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011153869s
Dec 14 13:23:45.862: INFO: Pod "downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014921697s
STEP: Saw pod success
Dec 14 13:23:45.863: INFO: Pod "downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829" satisfied condition "Succeeded or Failed"
Dec 14 13:23:45.865: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:23:45.886: INFO: Waiting for pod downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829 to disappear
Dec 14 13:23:45.891: INFO: Pod downward-api-b4b59998-59dc-4db5-b2fb-19914d1ae829 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:45.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9940" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":78,"failed":0}

SS
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":271,"failed":0}
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 14 13:23:42.543: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 13:23:47.547: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:48.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5104" for this suite.


• [SLOW TEST:6.093 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":-1,"completed":16,"skipped":271,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:22:59.741: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:23:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3882" for this suite.


• [SLOW TEST:60.059 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":148,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:59.845: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:23:59.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2" in namespace "downward-api-8159" to be "Succeeded or Failed"
Dec 14 13:23:59.903: INFO: Pod "downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.431169ms
Dec 14 13:24:01.908: INFO: Pod "downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013019223s
Dec 14 13:24:03.913: INFO: Pod "downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017463635s
STEP: Saw pod success
Dec 14 13:24:03.913: INFO: Pod "downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2" satisfied condition "Succeeded or Failed"
Dec 14 13:24:03.916: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2 container client-container: <nil>
STEP: delete the pod
Dec 14 13:24:03.944: INFO: Waiting for pod downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2 to disappear
Dec 14 13:24:03.953: INFO: Pod downwardapi-volume-567b872f-0962-4394-b2a7-f0c91780c3e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:03.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8159" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":13,"skipped":167,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Dec 14 13:24:04.013: INFO: Waiting up to 5m0s for pod "var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34" in namespace "var-expansion-2595" to be "Succeeded or Failed"
Dec 14 13:24:04.017: INFO: Pod "var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269558ms
Dec 14 13:24:06.025: INFO: Pod "var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012275611s
Dec 14 13:24:08.030: INFO: Pod "var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017356517s
STEP: Saw pod success
Dec 14 13:24:08.031: INFO: Pod "var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34" satisfied condition "Succeeded or Failed"
Dec 14 13:24:08.033: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:24:08.052: INFO: Waiting for pod var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34 to disappear
Dec 14 13:24:08.057: INFO: Pod var-expansion-82314bbf-5510-4ccc-ab1b-c77528fcec34 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:08.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2595" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":168,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:43.951: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 14 13:23:44.519: INFO: Successfully updated pod "var-expansion-b1032bab-03e5-47ca-afa3-358e9264296a"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 14 13:23:46.537: INFO: Deleting pod "var-expansion-b1032bab-03e5-47ca-afa3-358e9264296a" in namespace "var-expansion-5135"
Dec 14 13:23:46.542: INFO: Wait up to 5m0s for pod "var-expansion-b1032bab-03e5-47ca-afa3-358e9264296a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5135" for this suite.


• [SLOW TEST:166.614 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":-1,"completed":3,"skipped":46,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:30.637: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:24:30.686: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae" in namespace "projected-3832" to be "Succeeded or Failed"
Dec 14 13:24:30.690: INFO: Pod "downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606521ms
Dec 14 13:24:32.693: INFO: Pod "downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006967601s
Dec 14 13:24:34.698: INFO: Pod "downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011864268s
STEP: Saw pod success
Dec 14 13:24:34.698: INFO: Pod "downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae" satisfied condition "Succeeded or Failed"
Dec 14 13:24:34.704: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae container client-container: <nil>
STEP: delete the pod
Dec 14 13:24:34.740: INFO: Waiting for pod downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae to disappear
Dec 14 13:24:34.748: INFO: Pod downwardapi-volume-37d50b7c-cfb9-4ef4-b441-b9471ba6b9ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:34.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3832" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":76,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6372/configmap-test-303b51a7-3143-4ba4-b247-dd4bbdc641af
STEP: Creating a pod to test consume configMaps
Dec 14 13:24:34.877: INFO: Waiting up to 5m0s for pod "pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6" in namespace "configmap-6372" to be "Succeeded or Failed"
Dec 14 13:24:34.887: INFO: Pod "pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.407361ms
Dec 14 13:24:36.891: INFO: Pod "pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014398394s
STEP: Saw pod success
Dec 14 13:24:36.891: INFO: Pod "pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6" satisfied condition "Succeeded or Failed"
Dec 14 13:24:36.894: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6 container env-test: <nil>
STEP: delete the pod
Dec 14 13:24:36.918: INFO: Waiting for pod pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6 to disappear
Dec 14 13:24:36.928: INFO: Pod pod-configmaps-e71259d2-f612-4ae8-a3b4-9e5dfd6914e6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:36.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6372" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":100,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:48.602: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-a9832d34-2785-4f2c-bd77-9dd466831c86 in namespace container-probe-2563
Dec 14 13:23:52.687: INFO: Started pod busybox-a9832d34-2785-4f2c-bd77-9dd466831c86 in namespace container-probe-2563
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:23:52.690: INFO: Initial restart count of pod busybox-a9832d34-2785-4f2c-bd77-9dd466831c86 is 0
Dec 14 13:24:40.825: INFO: Restart count of pod container-probe-2563/busybox-a9832d34-2785-4f2c-bd77-9dd466831c86 is now 1 (48.134852245s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:40.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2563" for this suite.


• [SLOW TEST:52.251 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":275,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:35.244: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-4ce4b488-b02c-4285-bbf1-22ba472cd4a8
STEP: Creating configMap with name cm-test-opt-upd-8f6e8513-b194-4d92-ab4d-27545a96fe2b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4ce4b488-b02c-4285-bbf1-22ba472cd4a8
STEP: Updating configmap cm-test-opt-upd-8f6e8513-b194-4d92-ab4d-27545a96fe2b
STEP: Creating configMap with name cm-test-opt-create-abbc4bb9-93a5-4686-bfb3-357d3e7a12f6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:47.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7847" for this suite.


• [SLOW TEST:72.510 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":209,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:40.866: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-2780
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2780 to expose endpoints map[]
Dec 14 13:24:40.937: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 14 13:24:41.953: INFO: successfully validated that service multi-endpoint-test in namespace services-2780 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2780
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2780 to expose endpoints map[pod1:[100]]
Dec 14 13:24:44.993: INFO: successfully validated that service multi-endpoint-test in namespace services-2780 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2780
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2780 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 14 13:24:48.032: INFO: successfully validated that service multi-endpoint-test in namespace services-2780 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2780
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2780 to expose endpoints map[pod2:[101]]
Dec 14 13:24:48.058: INFO: successfully validated that service multi-endpoint-test in namespace services-2780 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2780
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2780 to expose endpoints map[]
Dec 14 13:24:49.087: INFO: successfully validated that service multi-endpoint-test in namespace services-2780 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2780" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:8.273 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":-1,"completed":18,"skipped":281,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:47.793: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:51.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9263" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":223,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:08.169: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 14 13:24:08.208: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 14 13:24:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
Dec 14 13:24:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:52.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3186" for this suite.


• [SLOW TEST:44.690 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":-1,"completed":15,"skipped":212,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:45.908: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-30873f74-230d-4dea-8104-d75c37b1f7ed
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-30873f74-230d-4dea-8104-d75c37b1f7ed
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:24:58.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-767" for this suite.


• [SLOW TEST:72.505 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":80,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:36.955: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:24:36.997: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 13:24:42.003: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 13:24:42.004: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 13:24:44.010: INFO: Creating deployment "test-rollover-deployment"
Dec 14 13:24:44.022: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 13:24:46.031: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 13:24:46.038: INFO: Ensure that both replica sets have 1 created replica
Dec 14 13:24:46.044: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 13:24:46.055: INFO: Updating deployment test-rollover-deployment
Dec 14 13:24:46.055: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 13:24:48.068: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 13:24:48.089: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 13:24:48.095: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:48.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549086, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:24:50.103: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:50.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549088, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:24:52.100: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:52.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549088, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:24:54.107: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:54.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549088, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:24:56.102: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:56.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549088, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:24:58.109: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 13:24:58.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549088, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:25:00.104: INFO: 
Dec 14 13:25:00.104: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 14 13:25:00.119: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3618 /apis/apps/v1/namespaces/deployment-3618/deployments/test-rollover-deployment d68fe957-4cd3-40d7-b88e-906c88db00cc 10546 2 2020-12-14 13:24:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-14 13:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:24:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000804cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-14 13:24:44 +0000 UTC,LastTransitionTime:2020-12-14 13:24:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-12-14 13:24:58 +0000 UTC,LastTransitionTime:2020-12-14 13:24:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 13:25:00.123: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-3618 /apis/apps/v1/namespaces/deployment-3618/replicasets/test-rollover-deployment-5797c7764 b9a94055-ffd1-413c-9127-50198526cbfc 10530 2 2020-12-14 13:24:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d68fe957-4cd3-40d7-b88e-906c88db00cc 0xc002e944a0 0xc002e944a1}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:24:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68fe957-4cd3-40d7-b88e-906c88db00cc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e94518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:25:00.123: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 13:25:00.123: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3618 /apis/apps/v1/namespaces/deployment-3618/replicasets/test-rollover-controller e53b0380-b764-4854-855d-bc04228e025b 10545 2 2020-12-14 13:24:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d68fe957-4cd3-40d7-b88e-906c88db00cc 0xc002e94397 0xc002e94398}] []  [{e2e.test Update apps/v1 2020-12-14 13:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:24:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68fe957-4cd3-40d7-b88e-906c88db00cc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002e94438 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:25:00.124: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3618 /apis/apps/v1/namespaces/deployment-3618/replicasets/test-rollover-deployment-78bc8b888c 557a18b5-ab65-4a09-a161-989a01770f88 10307 2 2020-12-14 13:24:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d68fe957-4cd3-40d7-b88e-906c88db00cc 0xc002e94787 0xc002e94788}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68fe957-4cd3-40d7-b88e-906c88db00cc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e94818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:25:00.130: INFO: Pod "test-rollover-deployment-5797c7764-gnbrl" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-gnbrl test-rollover-deployment-5797c7764- deployment-3618 /api/v1/namespaces/deployment-3618/pods/test-rollover-deployment-5797c7764-gnbrl 77d63df7-28bd-4640-af0c-b489d557315a 10362 0 2020-12-14 13:24:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:10.100.227.190/32 cni.projectcalico.org/podIPs:10.100.227.190/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 b9a94055-ffd1-413c-9127-50198526cbfc 0xc000805310 0xc000805311}] []  [{kube-controller-manager Update v1 2020-12-14 13:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9a94055-ffd1-413c-9127-50198526cbfc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:24:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kvmhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kvmhv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kvmhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.190,StartTime:2020-12-14 13:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:24:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://d3cadd64bacf63e73fd7555156192c9c5dce5d0b258212e37cd6418cce037284,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3618" for this suite.


• [SLOW TEST:23.187 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":-1,"completed":6,"skipped":109,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8
Dec 14 13:24:52.985: INFO: Pod name my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8: Found 0 pods out of 1
Dec 14 13:24:57.989: INFO: Pod name my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8: Found 1 pods out of 1
Dec 14 13:24:57.989: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8" are running
Dec 14 13:24:57.994: INFO: Pod "my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8-zhhw8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:24:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:24:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:24:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:24:52 +0000 UTC Reason: Message:}])
Dec 14 13:24:57.995: INFO: Trying to dial the pod
Dec 14 13:25:03.011: INFO: Controller my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8: Got expected result from replica 1 [my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8-zhhw8]: "my-hostname-basic-00d2b89b-b623-4a92-833e-880c37c34bc8-zhhw8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:03.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2415" for this suite.


• [SLOW TEST:10.136 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":16,"skipped":229,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:00.165: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:25:00.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120" in namespace "projected-2701" to be "Succeeded or Failed"
Dec 14 13:25:00.242: INFO: Pod "downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016472ms
Dec 14 13:25:02.245: INFO: Pod "downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013539832s
Dec 14 13:25:04.260: INFO: Pod "downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028169909s
STEP: Saw pod success
Dec 14 13:25:04.260: INFO: Pod "downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120" satisfied condition "Succeeded or Failed"
Dec 14 13:25:04.263: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120 container client-container: <nil>
STEP: delete the pod
Dec 14 13:25:04.296: INFO: Waiting for pod downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120 to disappear
Dec 14 13:25:04.302: INFO: Pod downwardapi-volume-7dc5f933-ba97-4469-8966-a216f8a18120 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:04.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2701" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":7,"skipped":119,"failed":0}

S
------------------------------
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:58.450: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 14 13:24:58.526: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 14 13:24:58.545: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 13:24:58.545: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 14 13:24:58.565: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 13:24:58.565: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 14 13:24:58.588: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 14 13:24:58.588: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 14 13:25:05.636: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:05.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8379" for this suite.


• [SLOW TEST:7.213 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":-1,"completed":9,"skipped":96,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:03.089: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-97c11006-80f6-4714-988e-9c548d36b1a6
STEP: Creating a pod to test consume configMaps
Dec 14 13:25:03.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552" in namespace "configmap-1036" to be "Succeeded or Failed"
Dec 14 13:25:03.149: INFO: Pod "pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552": Phase="Pending", Reason="", readiness=false. Elapsed: 8.63178ms
Dec 14 13:25:05.154: INFO: Pod "pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013694332s
Dec 14 13:25:07.158: INFO: Pod "pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018326425s
STEP: Saw pod success
Dec 14 13:25:07.159: INFO: Pod "pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552" satisfied condition "Succeeded or Failed"
Dec 14 13:25:07.161: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:25:07.189: INFO: Waiting for pod pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552 to disappear
Dec 14 13:25:07.192: INFO: Pod pod-configmaps-80cd4929-b37e-4b8f-af7c-1ecfb6251552 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:07.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1036" for this suite.

•
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Dec 14 13:25:07.796: INFO: Pod pod-hostip-dabc8851-42b6-4640-b62b-8dfc3d146d82 has hostIP: 10.0.0.55
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-150" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":-1,"completed":10,"skipped":133,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:04.333: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-2e689dba-3359-44aa-987f-4eae1fc94363
STEP: Creating a pod to test consume secrets
Dec 14 13:25:04.379: INFO: Waiting up to 5m0s for pod "pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d" in namespace "secrets-274" to be "Succeeded or Failed"
Dec 14 13:25:04.382: INFO: Pod "pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267888ms
Dec 14 13:25:06.386: INFO: Pod "pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006998497s
Dec 14 13:25:08.390: INFO: Pod "pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010734134s
STEP: Saw pod success
Dec 14 13:25:08.390: INFO: Pod "pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d" satisfied condition "Succeeded or Failed"
Dec 14 13:25:08.392: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:25:08.412: INFO: Waiting for pod pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d to disappear
Dec 14 13:25:08.415: INFO: Pod pod-secrets-19172b25-3a0b-4643-a876-fd6c075e252d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:08.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-274" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":120,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:08.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6484" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

•
------------------------------
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":-1,"completed":9,"skipped":131,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:07.916: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 13:25:07.966: INFO: Waiting up to 5m0s for pod "pod-2a392c82-8261-49b1-a063-3db1e4176efa" in namespace "emptydir-4489" to be "Succeeded or Failed"
Dec 14 13:25:07.973: INFO: Pod "pod-2a392c82-8261-49b1-a063-3db1e4176efa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.610709ms
Dec 14 13:25:09.977: INFO: Pod "pod-2a392c82-8261-49b1-a063-3db1e4176efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010956203s
Dec 14 13:25:11.982: INFO: Pod "pod-2a392c82-8261-49b1-a063-3db1e4176efa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015669631s
STEP: Saw pod success
Dec 14 13:25:11.982: INFO: Pod "pod-2a392c82-8261-49b1-a063-3db1e4176efa" satisfied condition "Succeeded or Failed"
Dec 14 13:25:11.984: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-2a392c82-8261-49b1-a063-3db1e4176efa container test-container: <nil>
STEP: delete the pod
Dec 14 13:25:12.005: INFO: Waiting for pod pod-2a392c82-8261-49b1-a063-3db1e4176efa to disappear
Dec 14 13:25:12.009: INFO: Pod pod-2a392c82-8261-49b1-a063-3db1e4176efa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4489" for this suite.

•
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:51.937: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:24:52.033: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:24:54.044: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:24:56.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:24:58.037: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:00.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:02.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:04.048: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:06.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:08.044: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:10.037: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:12.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = false)
Dec 14 13:25:14.038: INFO: The status of Pod test-webserver-7fd2eb5d-ddd0-4104-b245-24dacca71a2a is Running (Ready = true)
Dec 14 13:25:14.041: INFO: Container started at 2020-12-14 13:24:53 +0000 UTC, pod became ready at 2020-12-14 13:25:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:14.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2783" for this suite.


• [SLOW TEST:22.114 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":253,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:14.105: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:14.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-760" for this suite.

•
------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":20,"skipped":272,"failed":0}

SSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":196,"failed":0}
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:12.026: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:25:12.081: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec" in namespace "projected-7124" to be "Succeeded or Failed"
Dec 14 13:25:12.090: INFO: Pod "downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.073809ms
Dec 14 13:25:14.095: INFO: Pod "downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013674969s
Dec 14 13:25:16.097: INFO: Pod "downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016621172s
STEP: Saw pod success
Dec 14 13:25:16.098: INFO: Pod "downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec" satisfied condition "Succeeded or Failed"
Dec 14 13:25:16.100: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec container client-container: <nil>
STEP: delete the pod
Dec 14 13:25:16.125: INFO: Waiting for pod downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec to disappear
Dec 14 13:25:16.128: INFO: Pod downwardapi-volume-5835101b-0245-4f57-afbd-6eb3822588ec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:16.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7124" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":196,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:24:49.152: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 14 13:24:49.195: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
Dec 14 13:24:56.679: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:18.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2160" for this suite.


• [SLOW TEST:29.134 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":-1,"completed":19,"skipped":285,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-f9b88e26-b4cb-4398-9eab-a883ba081b77
STEP: Creating a pod to test consume configMaps
Dec 14 13:25:18.426: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994" in namespace "projected-477" to be "Succeeded or Failed"
Dec 14 13:25:18.438: INFO: Pod "pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994": Phase="Pending", Reason="", readiness=false. Elapsed: 12.424ms
Dec 14 13:25:20.442: INFO: Pod "pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015881195s
Dec 14 13:25:22.445: INFO: Pod "pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01903868s
STEP: Saw pod success
Dec 14 13:25:22.445: INFO: Pod "pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994" satisfied condition "Succeeded or Failed"
Dec 14 13:25:22.447: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:25:22.468: INFO: Waiting for pod pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994 to disappear
Dec 14 13:25:22.472: INFO: Pod pod-projected-configmaps-d51323af-69bf-470f-95a1-ddd707d68994 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:22.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-477" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":336,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":261,"failed":0}
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:07.208: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:23.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4503" for this suite.


• [SLOW TEST:16.154 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":-1,"completed":18,"skipped":261,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:23.386: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-9529fe06-05e5-414e-bdd0-9ad6e309aba0
STEP: Creating secret with name s-test-opt-upd-7664d317-f96f-4d78-87ef-d6e6a691c8b0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9529fe06-05e5-414e-bdd0-9ad6e309aba0
STEP: Updating secret s-test-opt-upd-7664d317-f96f-4d78-87ef-d6e6a691c8b0
STEP: Creating secret with name s-test-opt-create-80cabdf0-48fc-4ca2-8c5c-4a233af1eb11
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:29.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5655" for this suite.


• [SLOW TEST:6.163 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":273,"failed":0}

SSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-227ccece-dc0f-44e9-9fb1-5888b352d22c in namespace container-probe-2355
Dec 14 13:25:18.296: INFO: Started pod liveness-227ccece-dc0f-44e9-9fb1-5888b352d22c in namespace container-probe-2355
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:25:18.298: INFO: Initial restart count of pod liveness-227ccece-dc0f-44e9-9fb1-5888b352d22c is 0
Dec 14 13:25:38.343: INFO: Restart count of pod container-probe-2355/liveness-227ccece-dc0f-44e9-9fb1-5888b352d22c is now 1 (20.04454562s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:38.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2355" for this suite.


• [SLOW TEST:24.134 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":279,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:38.480: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:25:38.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 version'
Dec 14 13:25:38.706: INFO: stderr: ""
Dec 14 13:25:38.706: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:38.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3974" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":-1,"completed":22,"skipped":315,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:21:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename container-probe
Dec 14 13:21:33.733: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 14 13:21:33.747: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-3501c625-921f-4519-bca6-2cd299a55791 in namespace container-probe-2840
Dec 14 13:21:41.789: INFO: Started pod test-webserver-3501c625-921f-4519-bca6-2cd299a55791 in namespace container-probe-2840
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:21:41.792: INFO: Initial restart count of pod test-webserver-3501c625-921f-4519-bca6-2cd299a55791 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:42.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2840" for this suite.


• [SLOW TEST:248.784 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":47,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:16.183: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6586
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:25:16.233: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:25:16.267: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:25:18.271: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:25:20.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:22.271: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:24.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:26.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:28.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:30.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:32.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:34.297: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:25:36.273: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:25:36.281: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 13:25:38.284: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:25:42.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.196.174:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6586 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:25:42.335: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:25:42.531: INFO: Found all expected endpoints: [netserver-0]
Dec 14 13:25:42.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.227.146:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6586 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:25:42.536: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:25:42.734: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:42.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6586" for this suite.


• [SLOW TEST:26.561 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":13,"skipped":217,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Dec 14 13:25:42.791: INFO: created test-event-1
Dec 14 13:25:42.794: INFO: created test-event-2
Dec 14 13:25:42.797: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 14 13:25:42.802: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 14 13:25:42.817: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:42.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5365" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":-1,"completed":14,"skipped":220,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3a5ce5d0-33dd-40ec-8a0c-79e0056761c6
STEP: Creating a pod to test consume secrets
Dec 14 13:25:42.980: INFO: Waiting up to 5m0s for pod "pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1" in namespace "secrets-5814" to be "Succeeded or Failed"
Dec 14 13:25:42.985: INFO: Pod "pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501759ms
Dec 14 13:25:44.989: INFO: Pod "pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008353903s
STEP: Saw pod success
Dec 14 13:25:44.989: INFO: Pod "pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1" satisfied condition "Succeeded or Failed"
Dec 14 13:25:44.992: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:25:45.013: INFO: Waiting for pod pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1 to disappear
Dec 14 13:25:45.015: INFO: Pod pod-secrets-db0a6969-5323-462f-9869-04f381b6c1a1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:45.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5814" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":257,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:38.753: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1563 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1563;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1563 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1563;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1563.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1563.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1563.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1563.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1563.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1563.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1563.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1563.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1563.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.66.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.66.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.66.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.66.98_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1563 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1563;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1563 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1563;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1563.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1563.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1563.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1563.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1563.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1563.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1563.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1563.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1563.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1563.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.66.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.66.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.66.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.66.98_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:25:42.853: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.856: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.860: INFO: Unable to read wheezy_udp@dns-test-service.dns-1563 from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.863: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1563 from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.866: INFO: Unable to read wheezy_udp@dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.869: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.872: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.875: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.886: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.889: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.899: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.902: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.914: INFO: Unable to read jessie_udp@dns-test-service.dns-1563 from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-1563 from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.923: INFO: Unable to read jessie_udp@dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.928: INFO: Unable to read jessie_tcp@dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.933: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.936: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1563.svc from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.954: INFO: Unable to read jessie_udp@PodARecord from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.959: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b: the server could not find the requested resource (get pods dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b)
Dec 14 13:25:42.967: INFO: Lookups using dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1563 wheezy_tcp@dns-test-service.dns-1563 wheezy_udp@dns-test-service.dns-1563.svc wheezy_tcp@dns-test-service.dns-1563.svc wheezy_udp@_http._tcp.dns-test-service.dns-1563.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1563.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1563 jessie_tcp@dns-test-service.dns-1563 jessie_udp@dns-test-service.dns-1563.svc jessie_tcp@dns-test-service.dns-1563.svc jessie_udp@_http._tcp.dns-test-service.dns-1563.svc jessie_tcp@_http._tcp.dns-test-service.dns-1563.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:25:48.149: INFO: DNS probes using dns-1563/dns-test-5115deda-c2d6-4ad9-8e4b-9f1307625a8b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:48.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1563" for this suite.


• [SLOW TEST:9.506 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":-1,"completed":23,"skipped":325,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:45.112: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a98cb23e-fa92-454c-8d92-d4746cdcf706
STEP: Creating a pod to test consume configMaps
Dec 14 13:25:45.161: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a" in namespace "projected-9388" to be "Succeeded or Failed"
Dec 14 13:25:45.170: INFO: Pod "pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.449367ms
Dec 14 13:25:47.176: INFO: Pod "pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852214s
Dec 14 13:25:49.180: INFO: Pod "pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018872895s
STEP: Saw pod success
Dec 14 13:25:49.180: INFO: Pod "pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a" satisfied condition "Succeeded or Failed"
Dec 14 13:25:49.182: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:25:49.204: INFO: Waiting for pod pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a to disappear
Dec 14 13:25:49.210: INFO: Pod pod-projected-configmaps-92d1a828-01eb-418a-a2e1-4743ff290d1a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:49.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9388" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":16,"skipped":292,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:42.518: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2076.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2076.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2076.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2076.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 43.39.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.39.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.39.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.39.43_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2076.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2076.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2076.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2076.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2076.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2076.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 43.39.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.39.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.39.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.39.43_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:25:46.604: INFO: Unable to read wheezy_udp@dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.608: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.611: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.614: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.624: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.627: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.636: INFO: Unable to read jessie_udp@dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.642: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.644: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.661: INFO: Unable to read jessie_udp@PodARecord from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.665: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723: the server could not find the requested resource (get pods dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723)
Dec 14 13:25:46.672: INFO: Lookups using dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723 failed for: [wheezy_udp@dns-test-service.dns-2076.svc.cluster.local wheezy_tcp@dns-test-service.dns-2076.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-2076.svc.cluster.local jessie_tcp@dns-test-service.dns-2076.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2076.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:25:51.745: INFO: DNS probes using dns-2076/dns-test-c7a9a031-cf26-40d0-b5b2-4df271ff0723 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:51.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2076" for this suite.


• [SLOW TEST:9.348 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":-1,"completed":2,"skipped":52,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:48.272: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Dec 14 13:25:48.348: INFO: Waiting up to 5m0s for pod "client-containers-89b29f74-df82-4613-9005-e3636190459c" in namespace "containers-4507" to be "Succeeded or Failed"
Dec 14 13:25:48.356: INFO: Pod "client-containers-89b29f74-df82-4613-9005-e3636190459c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.43713ms
Dec 14 13:25:50.360: INFO: Pod "client-containers-89b29f74-df82-4613-9005-e3636190459c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011970024s
Dec 14 13:25:52.372: INFO: Pod "client-containers-89b29f74-df82-4613-9005-e3636190459c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023695288s
STEP: Saw pod success
Dec 14 13:25:52.372: INFO: Pod "client-containers-89b29f74-df82-4613-9005-e3636190459c" satisfied condition "Succeeded or Failed"
Dec 14 13:25:52.375: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod client-containers-89b29f74-df82-4613-9005-e3636190459c container test-container: <nil>
STEP: delete the pod
Dec 14 13:25:52.394: INFO: Waiting for pod client-containers-89b29f74-df82-4613-9005-e3636190459c to disappear
Dec 14 13:25:52.398: INFO: Pod client-containers-89b29f74-df82-4613-9005-e3636190459c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:52.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4507" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":327,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:49.316: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:25:49.358: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ba0b1a98-ab5d-4cc3-9a3f-17a5538ad757" in namespace "security-context-test-8062" to be "Succeeded or Failed"
Dec 14 13:25:49.367: INFO: Pod "alpine-nnp-false-ba0b1a98-ab5d-4cc3-9a3f-17a5538ad757": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91736ms
Dec 14 13:25:51.370: INFO: Pod "alpine-nnp-false-ba0b1a98-ab5d-4cc3-9a3f-17a5538ad757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012279002s
Dec 14 13:25:53.377: INFO: Pod "alpine-nnp-false-ba0b1a98-ab5d-4cc3-9a3f-17a5538ad757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01899943s
Dec 14 13:25:53.377: INFO: Pod "alpine-nnp-false-ba0b1a98-ab5d-4cc3-9a3f-17a5538ad757" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:53.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8062" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":341,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:52.441: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:25:52.470: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 14 13:25:53.508: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:53.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1655" for this suite.

•
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":-1,"completed":25,"skipped":338,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:53.463: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:25:53.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404" in namespace "projected-9628" to be "Succeeded or Failed"
Dec 14 13:25:53.561: INFO: Pod "downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063392ms
Dec 14 13:25:55.568: INFO: Pod "downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010539954s
Dec 14 13:25:57.577: INFO: Pod "downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020039721s
STEP: Saw pod success
Dec 14 13:25:57.577: INFO: Pod "downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404" satisfied condition "Succeeded or Failed"
Dec 14 13:25:57.581: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404 container client-container: <nil>
STEP: delete the pod
Dec 14 13:25:57.608: INFO: Waiting for pod downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404 to disappear
Dec 14 13:25:57.614: INFO: Pod downwardapi-volume-adf6a420-0461-4990-ba64-4bd3833c3404 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:57.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9628" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":359,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 14 13:25:53.651: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7098" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":-1,"completed":26,"skipped":368,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:57.648: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:25:57.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3382" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

•SS
------------------------------
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":-1,"completed":19,"skipped":363,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:52.006: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 14 13:25:56.078: INFO: &Pod{ObjectMeta:{send-events-37ae0886-8bf3-4cce-9aeb-ee8ed1a2f65e  events-5117 /api/v1/namespaces/events-5117/pods/send-events-37ae0886-8bf3-4cce-9aeb-ee8ed1a2f65e a826cd9a-22b0-4acf-8764-95d1227d7f21 11877 0 2020-12-14 13:25:52 +0000 UTC <nil> <nil> map[name:foo time:44786886] map[cni.projectcalico.org/podIP:10.100.227.157/32 cni.projectcalico.org/podIPs:10.100.227.157/32] [] []  [{e2e.test Update v1 2020-12-14 13:25:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:25:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:25:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4qlhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4qlhg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4qlhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:25:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:25:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.157,StartTime:2020-12-14 13:25:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:25:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://da8658b403f715fa9201c70886bb3618078468b6a6b3a606aa0cb51bb91f2517,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 14 13:25:58.090: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 14 13:26:00.103: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:00.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5117" for this suite.


• [SLOW TEST:8.116 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":-1,"completed":3,"skipped":104,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:57.789: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:25:57.874: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0" in namespace "security-context-test-985" to be "Succeeded or Failed"
Dec 14 13:25:57.878: INFO: Pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818677ms
Dec 14 13:25:59.881: INFO: Pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007251136s
Dec 14 13:26:01.885: INFO: Pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011147934s
Dec 14 13:26:01.885: INFO: Pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0" satisfied condition "Succeeded or Failed"
Dec 14 13:26:01.895: INFO: Got logs for pod "busybox-privileged-false-11c8d6fa-35fa-4f1d-b6e5-f3218afd65e0": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-985" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":27,"skipped":380,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:57.965: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:25:58.540: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 14 13:26:00.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549158, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549158, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549158, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549158, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:26:03.568: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:26:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:04.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7664" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:6.828 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":-1,"completed":20,"skipped":451,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:04.908: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1214 13:26:05.551935      33 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:26:05.552164      33 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:26:05.552191      33 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:26:05.552: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:05.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9909" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":-1,"completed":21,"skipped":497,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:00.146: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:26:01.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:26:03.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549161, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549161, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549161, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549161, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:26:06.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:06.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1769" for this suite.
STEP: Destroying namespace "webhook-1769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.032 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":-1,"completed":4,"skipped":114,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:05.623: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1214 13:26:06.367535      33 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:26:06.367597      33 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:26:06.367618      33 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:26:06.367: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:06.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9681" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":-1,"completed":22,"skipped":527,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:01.947: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 14 13:26:01.988: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:08.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8079" for this suite.


• [SLOW TEST:6.238 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":-1,"completed":28,"skipped":400,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:08.349: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:26:08.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899" in namespace "downward-api-99" to be "Succeeded or Failed"
Dec 14 13:26:08.436: INFO: Pod "downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017789ms
Dec 14 13:26:10.453: INFO: Pod "downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027044719s
Dec 14 13:26:12.460: INFO: Pod "downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034193082s
STEP: Saw pod success
Dec 14 13:26:12.461: INFO: Pod "downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899" satisfied condition "Succeeded or Failed"
Dec 14 13:26:12.464: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899 container client-container: <nil>
STEP: delete the pod
Dec 14 13:26:12.501: INFO: Waiting for pod downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899 to disappear
Dec 14 13:26:12.513: INFO: Pod downwardapi-volume-5f0e5317-e504-41df-9f1f-13973ad58899 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:12.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-99" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":441,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:26:06.237: INFO: Creating deployment "webserver-deployment"
Dec 14 13:26:06.241: INFO: Waiting for observed generation 1
Dec 14 13:26:08.249: INFO: Waiting for all required pods to come up
Dec 14 13:26:08.256: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 14 13:26:12.274: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 13:26:12.294: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 13:26:12.305: INFO: Updating deployment webserver-deployment
Dec 14 13:26:12.305: INFO: Waiting for observed generation 2
Dec 14 13:26:14.312: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 13:26:14.318: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 13:26:14.321: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:26:14.330: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 13:26:14.330: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 13:26:14.335: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:26:14.340: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 13:26:14.340: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 13:26:14.348: INFO: Updating deployment webserver-deployment
Dec 14 13:26:14.348: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:26:14.373: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 13:26:14.378: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 14 13:26:16.420: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5019 /apis/apps/v1/namespaces/deployment-5019/deployments/webserver-deployment 7fae1981-2a9a-4795-be1f-a5520d8fa4d6 12865 3 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003030678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-14 13:26:14 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-12-14 13:26:14 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 13:26:16.426: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5019 /apis/apps/v1/namespaces/deployment-5019/replicasets/webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 12860 3 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7fae1981-2a9a-4795-be1f-a5520d8fa4d6 0xc003030b07 0xc003030b08}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7fae1981-2a9a-4795-be1f-a5520d8fa4d6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003030b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:26:16.426: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 13:26:16.426: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5019 /apis/apps/v1/namespaces/deployment-5019/replicasets/webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 12864 3 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7fae1981-2a9a-4795-be1f-a5520d8fa4d6 0xc003030be7 0xc003030be8}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7fae1981-2a9a-4795-be1f-a5520d8fa4d6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003030c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:26:16.445: INFO: Pod "webserver-deployment-795d758f88-6fgzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6fgzk webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-6fgzk 9a07c154-627a-4215-b9f7-2d0eeaaebeac 12830 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031167 0xc003031168}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.447: INFO: Pod "webserver-deployment-795d758f88-7d5mc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7d5mc webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-7d5mc edfc2d9a-1bfb-485a-8004-9e60c63018b0 12908 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.227.160/32 cni.projectcalico.org/podIPs:10.100.227.160/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031280 0xc003031281}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.447: INFO: Pod "webserver-deployment-795d758f88-8mzxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8mzxj webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-8mzxj ffc07620-da91-4f64-885f-623bfa155f2b 12756 0 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.196.188/32 cni.projectcalico.org/podIPs:10.100.196.188/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031420 0xc003031421}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.447: INFO: Pod "webserver-deployment-795d758f88-g269p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g269p webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-g269p f081c391-b65d-4147-8d84-efb31ed50178 12844 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc0030315c0 0xc0030315c1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.449: INFO: Pod "webserver-deployment-795d758f88-gjhd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gjhd6 webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-gjhd6 7b4855f3-f0b3-4deb-b1e2-2a2ab379ce3d 12755 0 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.227.175/32 cni.projectcalico.org/podIPs:10.100.227.175/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc0030316e0 0xc0030316e1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.450: INFO: Pod "webserver-deployment-795d758f88-h6q8g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h6q8g webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-h6q8g 6f2b27f6-42c3-48c2-b19f-261e783b7d1d 12766 0 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.227.176/32 cni.projectcalico.org/podIPs:10.100.227.176/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031880 0xc003031881}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.450: INFO: Pod "webserver-deployment-795d758f88-jhspv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jhspv webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-jhspv 9466e9eb-16a5-4c03-a8ca-f87a42bd64f9 12772 0 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.227.172/32 cni.projectcalico.org/podIPs:10.100.227.172/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031a20 0xc003031a21}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.452: INFO: Pod "webserver-deployment-795d758f88-jmccq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jmccq webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-jmccq 5b455999-cdb2-4cee-9aec-635a6d0985cb 12832 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031bc0 0xc003031bc1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.453: INFO: Pod "webserver-deployment-795d758f88-lltmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lltmn webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-lltmn 0fefd9ff-ba1d-434e-8f96-0670bba2bdc0 12833 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031ce0 0xc003031ce1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.454: INFO: Pod "webserver-deployment-795d758f88-ntdw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ntdw9 webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-ntdw9 5ad7f571-1c3b-4e22-b01a-e7fce25f439b 12930 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031e00 0xc003031e01}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.455: INFO: Pod "webserver-deployment-795d758f88-srv6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-srv6c webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-srv6c 5268947b-1db8-4fac-ae41-bb0f7862e717 12879 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003031f80 0xc003031f81}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.456: INFO: Pod "webserver-deployment-795d758f88-v45fw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v45fw webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-v45fw 1eb487c9-db1f-418e-805e-6cef148ff196 12848 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003102100 0xc003102101}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.458: INFO: Pod "webserver-deployment-795d758f88-zrmtv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zrmtv webserver-deployment-795d758f88- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-795d758f88-zrmtv 53d0565e-f3ac-4d03-a9d4-72b83aadd5fa 12770 0 2020-12-14 13:26:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.196.189/32 cni.projectcalico.org/podIPs:10.100.196.189/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3aa96118-37d8-4189-ae5b-bb9608e1e9e5 0xc003102280 0xc003102281}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa96118-37d8-4189-ae5b-bb9608e1e9e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-14 13:26:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.458: INFO: Pod "webserver-deployment-dd94f59b7-47thf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-47thf webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-47thf e0ae16a5-90fb-4162-8f02-cba138deef4c 12899 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.174/32 cni.projectcalico.org/podIPs:10.100.227.174/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102420 0xc003102421}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.459: INFO: Pod "webserver-deployment-dd94f59b7-5msdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5msdf webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-5msdf a87c8d6b-5f96-467a-bbc9-699528a1126f 12927 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.173/32 cni.projectcalico.org/podIPs:10.100.227.173/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031025a0 0xc0031025a1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.460: INFO: Pod "webserver-deployment-dd94f59b7-5nwhc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5nwhc webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-5nwhc 90e4bf92-efcf-4ee0-96c4-bd882f82569c 12526 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.182/32 cni.projectcalico.org/podIPs:10.100.196.182/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102720 0xc003102721}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.182,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a20bac406e7cfb6b9281a3b87ce00277950100103141907eee2c2c7ba70f2142,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.462: INFO: Pod "webserver-deployment-dd94f59b7-5pg79" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5pg79 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-5pg79 9d220a2f-b66f-4fa4-8b0c-b2017af9d2aa 12841 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031028c0 0xc0031028c1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.463: INFO: Pod "webserver-deployment-dd94f59b7-6kcsh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6kcsh webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-6kcsh 2d5392bd-8f61-4994-b41e-674eefbf7e2a 12506 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.183/32 cni.projectcalico.org/podIPs:10.100.196.183/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031029d0 0xc0031029d1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.183,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a144ee4ce558ad16e135dca6194f4760821a2fd1f182debfb9e6ecd052b0c6e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.463: INFO: Pod "webserver-deployment-dd94f59b7-8n8mt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8n8mt webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-8n8mt 9d329ebc-ce1e-4616-ad5f-53287d3e3010 12850 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102b70 0xc003102b71}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.464: INFO: Pod "webserver-deployment-dd94f59b7-8wb8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8wb8w webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-8wb8w d76e7b86-b83f-493f-ae25-c56fbec3c54b 12871 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102c80 0xc003102c81}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.464: INFO: Pod "webserver-deployment-dd94f59b7-cd4tl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cd4tl webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-cd4tl c5ab769d-84ee-4794-a313-32107c264769 12514 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.186/32 cni.projectcalico.org/podIPs:10.100.196.186/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102de0 0xc003102de1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.186,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a154dd190033cdb7df7b93b46cea6e53eaf5f479768dd9329aec64a1dd3822af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.465: INFO: Pod "webserver-deployment-dd94f59b7-d652g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d652g webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-d652g bc773b29-b4e8-4934-a213-d67f2a19bac3 12509 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.185/32 cni.projectcalico.org/podIPs:10.100.196.185/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003102f80 0xc003102f81}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.185,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cba8f9406c667a9d7556829a737a5a123149d08292509e87dc7e230ff9bc71bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.466: INFO: Pod "webserver-deployment-dd94f59b7-dnpcp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dnpcp webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-dnpcp f404d328-0184-4fe0-992f-0d116552e816 12500 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.184/32 cni.projectcalico.org/podIPs:10.100.196.184/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103120 0xc003103121}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.184,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9de28117b0ec592111893422deb95fa000cc77270fc9d7676d102a11d92edc11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.467: INFO: Pod "webserver-deployment-dd94f59b7-kss2s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kss2s webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-kss2s ccfdca0b-aad5-4e8f-8ccc-5bfe5cb748b6 12890 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031032c0 0xc0031032c1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.474: INFO: Pod "webserver-deployment-dd94f59b7-mk26l" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mk26l webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-mk26l 380c4198-928e-41dd-83c7-8b0ca5aa58f4 12919 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.178/32 cni.projectcalico.org/podIPs:10.100.227.178/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103420 0xc003103421}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.475: INFO: Pod "webserver-deployment-dd94f59b7-phpz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-phpz2 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-phpz2 cd71c543-f60c-40e8-9931-d843965f7567 12924 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.196.190/32 cni.projectcalico.org/podIPs:10.100.196.190/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031035a0 0xc0031035a1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.475: INFO: Pod "webserver-deployment-dd94f59b7-qbft2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qbft2 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-qbft2 95695b2e-2eb8-4541-8a5d-cf7013dd1922 12592 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.166/32 cni.projectcalico.org/podIPs:10.100.227.166/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc0031036c0 0xc0031036c1}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.166,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0419542a90d3540fa7df992325c2948c8b1cd4716c588a5a502a4cc8ea4c83c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.477: INFO: Pod "webserver-deployment-dd94f59b7-r2qr2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r2qr2 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-r2qr2 585e34f0-7ba4-4d99-ae36-a69860488efa 12501 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.164/32 cni.projectcalico.org/podIPs:10.100.227.164/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103860 0xc003103861}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.164,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://512672f808badf6959cb21049f4da0ba4941143e989d301eae2983c0d863c9d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.477: INFO: Pod "webserver-deployment-dd94f59b7-rkn9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rkn9d webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-rkn9d 123f9462-5548-4234-bccf-0d0cd15f4ea2 12902 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103a00 0xc003103a01}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-14 13:26:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:,StartTime:2020-12-14 13:26:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.481: INFO: Pod "webserver-deployment-dd94f59b7-t57g2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t57g2 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-t57g2 03a92e91-8cc0-43d4-a4c1-b4776782879b 12843 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103b60 0xc003103b61}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.481: INFO: Pod "webserver-deployment-dd94f59b7-tnsr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tnsr9 webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-tnsr9 56746498-5a5b-4b70-b7b1-e5c50a2b6d9f 12821 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103c70 0xc003103c71}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.481: INFO: Pod "webserver-deployment-dd94f59b7-vcn7g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vcn7g webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-vcn7g 941fc884-3474-4744-8212-af62001e0f57 12567 0 2020-12-14 13:26:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.227.168/32 cni.projectcalico.org/podIPs:10.100.227.168/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103d80 0xc003103d81}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:26:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:26:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.227.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.55,PodIP:10.100.227.168,StartTime:2020-12-14 13:26:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:26:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://586e7ffc731aa0f569702de8d2151150dc9a13a01ff97d346998ad38752f331b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.227.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:26:16.482: INFO: Pod "webserver-deployment-dd94f59b7-wqccs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wqccs webserver-deployment-dd94f59b7- deployment-5019 /api/v1/namespaces/deployment-5019/pods/webserver-deployment-dd94f59b7-wqccs 578312b9-b1e0-4fdd-b7d3-dd0afd1e9d8d 12849 0 2020-12-14 13:26:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 787639b8-f3d7-4cba-9b8a-a4ff40fbb235 0xc003103f20 0xc003103f21}] []  [{kube-controller-manager Update v1 2020-12-14 13:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"787639b8-f3d7-4cba-9b8a-a4ff40fbb235\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rv6tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rv6tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rv6tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:26:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:16.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5019" for this suite.


• [SLOW TEST:10.295 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":-1,"completed":5,"skipped":125,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:16.576: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5546/configmap-test-45de0e0a-b784-44ef-8223-a933e70dfd68
STEP: Creating a pod to test consume configMaps
Dec 14 13:26:16.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0" in namespace "configmap-5546" to be "Succeeded or Failed"
Dec 14 13:26:16.659: INFO: Pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.688319ms
Dec 14 13:26:18.664: INFO: Pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012694614s
Dec 14 13:26:20.668: INFO: Pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016392658s
Dec 14 13:26:22.682: INFO: Pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030437706s
STEP: Saw pod success
Dec 14 13:26:22.682: INFO: Pod "pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0" satisfied condition "Succeeded or Failed"
Dec 14 13:26:22.690: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0 container env-test: <nil>
STEP: delete the pod
Dec 14 13:26:22.727: INFO: Waiting for pod pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0 to disappear
Dec 14 13:26:22.736: INFO: Pod pod-configmaps-973713b1-4052-4f01-ba8b-0b4595d03be0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:22.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5546" for this suite.


• [SLOW TEST:6.175 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":132,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:29.558: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5256
STEP: creating service affinity-clusterip-transition in namespace services-5256
STEP: creating replication controller affinity-clusterip-transition in namespace services-5256
I1214 13:25:29.614438      32 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5256, replica count: 3
I1214 13:25:32.665256      32 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:25:32.672: INFO: Creating new exec pod
Dec 14 13:25:37.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-5256 execpod-affinitypgjmk -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Dec 14 13:25:38.122: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 14 13:25:38.122: INFO: stdout: ""
Dec 14 13:25:38.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-5256 execpod-affinitypgjmk -- /bin/sh -x -c nc -zv -t -w 2 10.254.160.36 80'
Dec 14 13:25:38.523: INFO: stderr: "+ nc -zv -t -w 2 10.254.160.36 80\nConnection to 10.254.160.36 80 port [tcp/http] succeeded!\n"
Dec 14 13:25:38.523: INFO: stdout: ""
Dec 14 13:25:38.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-5256 execpod-affinitypgjmk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.160.36:80/ ; done'
Dec 14 13:25:39.147: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n"
Dec 14 13:25:39.147: INFO: stdout: "\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-s65ch"
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:25:39.147: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-5256 execpod-affinitypgjmk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.160.36:80/ ; done'
Dec 14 13:25:39.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n"
Dec 14 13:25:39.638: INFO: stdout: "\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-s65ch\naffinity-clusterip-transition-4sgnc\naffinity-clusterip-transition-4sgnc"
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-s65ch
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:25:39.638: INFO: Received response from host: affinity-clusterip-transition-4sgnc
Dec 14 13:26:09.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-5256 execpod-affinitypgjmk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.160.36:80/ ; done'
Dec 14 13:26:10.180: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.160.36:80/\n"
Dec 14 13:26:10.181: INFO: stdout: "\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc\naffinity-clusterip-transition-9mpjc"
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Received response from host: affinity-clusterip-transition-9mpjc
Dec 14 13:26:10.181: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5256, will wait for the garbage collector to delete the pods
Dec 14 13:26:10.259: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.486935ms
Dec 14 13:26:10.360: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.518992ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:23.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5256" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:53.764 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":20,"skipped":276,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:23.339: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:26:23.387: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4" in namespace "downward-api-4394" to be "Succeeded or Failed"
Dec 14 13:26:23.392: INFO: Pod "downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42055ms
Dec 14 13:26:25.407: INFO: Pod "downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019333964s
Dec 14 13:26:27.411: INFO: Pod "downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023147489s
STEP: Saw pod success
Dec 14 13:26:27.411: INFO: Pod "downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4" satisfied condition "Succeeded or Failed"
Dec 14 13:26:27.414: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4 container client-container: <nil>
STEP: delete the pod
Dec 14 13:26:27.441: INFO: Waiting for pod downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4 to disappear
Dec 14 13:26:27.447: INFO: Pod downwardapi-volume-08fcf386-a7b0-4cdf-8693-f69781aa86d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:27.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4394" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":281,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:12.741: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:26:12.792: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 13:26:19.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 --namespace=crd-publish-openapi-1383 create -f -'
Dec 14 13:26:20.539: INFO: stderr: ""
Dec 14 13:26:20.539: INFO: stdout: "e2e-test-crd-publish-openapi-1389-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 13:26:20.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 --namespace=crd-publish-openapi-1383 delete e2e-test-crd-publish-openapi-1389-crds test-cr'
Dec 14 13:26:20.734: INFO: stderr: ""
Dec 14 13:26:20.734: INFO: stdout: "e2e-test-crd-publish-openapi-1389-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 13:26:20.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 --namespace=crd-publish-openapi-1383 apply -f -'
Dec 14 13:26:21.168: INFO: stderr: ""
Dec 14 13:26:21.168: INFO: stdout: "e2e-test-crd-publish-openapi-1389-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 13:26:21.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 --namespace=crd-publish-openapi-1383 delete e2e-test-crd-publish-openapi-1389-crds test-cr'
Dec 14 13:26:21.380: INFO: stderr: ""
Dec 14 13:26:21.381: INFO: stdout: "e2e-test-crd-publish-openapi-1389-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 13:26:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 explain e2e-test-crd-publish-openapi-1389-crds'
Dec 14 13:26:21.920: INFO: stderr: ""
Dec 14 13:26:21.920: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1389-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:27.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1383" for this suite.


• [SLOW TEST:14.740 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
S
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":-1,"completed":30,"skipped":515,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:27.494: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:26:28.357: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 14 13:26:30.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549188, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549188, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549188, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549188, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:26:33.385: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:26:33.390: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:34.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4198" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:7.127 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":-1,"completed":31,"skipped":518,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:34.681: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:26:34.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3" in namespace "downward-api-1505" to be "Succeeded or Failed"
Dec 14 13:26:34.743: INFO: Pod "downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.406328ms
Dec 14 13:26:36.750: INFO: Pod "downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011003721s
Dec 14 13:26:38.763: INFO: Pod "downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024535345s
STEP: Saw pod success
Dec 14 13:26:38.763: INFO: Pod "downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3" satisfied condition "Succeeded or Failed"
Dec 14 13:26:38.767: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3 container client-container: <nil>
STEP: delete the pod
Dec 14 13:26:38.797: INFO: Waiting for pod downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3 to disappear
Dec 14 13:26:38.803: INFO: Pod downwardapi-volume-3ba533d5-e763-4918-8630-98da319d17a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1505" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":529,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:22.773: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:26:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 14 13:26:30.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 create -f -'
Dec 14 13:26:32.098: INFO: stderr: ""
Dec 14 13:26:32.098: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 13:26:32.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 delete e2e-test-crd-publish-openapi-4291-crds test-foo'
Dec 14 13:26:32.295: INFO: stderr: ""
Dec 14 13:26:32.295: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 13:26:32.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 apply -f -'
Dec 14 13:26:32.733: INFO: stderr: ""
Dec 14 13:26:32.733: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 13:26:32.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 delete e2e-test-crd-publish-openapi-4291-crds test-foo'
Dec 14 13:26:32.920: INFO: stderr: ""
Dec 14 13:26:32.920: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 14 13:26:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 create -f -'
Dec 14 13:26:33.278: INFO: rc: 1
Dec 14 13:26:33.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 apply -f -'
Dec 14 13:26:33.605: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 14 13:26:33.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 create -f -'
Dec 14 13:26:34.008: INFO: rc: 1
Dec 14 13:26:34.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-5355 apply -f -'
Dec 14 13:26:34.410: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 14 13:26:34.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-4291-crds'
Dec 14 13:26:34.892: INFO: stderr: ""
Dec 14 13:26:34.892: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 14 13:26:34.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-4291-crds.metadata'
Dec 14 13:26:35.426: INFO: stderr: ""
Dec 14 13:26:35.426: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 13:26:35.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-4291-crds.spec'
Dec 14 13:26:35.909: INFO: stderr: ""
Dec 14 13:26:35.909: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 13:26:35.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-4291-crds.spec.bars'
Dec 14 13:26:36.315: INFO: stderr: ""
Dec 14 13:26:36.315: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 14 13:26:36.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-4291-crds.spec.bars2'
Dec 14 13:26:36.838: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5355" for this suite.


• [SLOW TEST:19.550 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":-1,"completed":7,"skipped":136,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:38.848: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 14 13:26:41.410: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8984 pod-service-account-4a5f6b01-927c-403b-821b-8fa2a293832c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 14 13:26:41.785: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8984 pod-service-account-4a5f6b01-927c-403b-821b-8fa2a293832c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 14 13:26:42.125: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8984 pod-service-account-4a5f6b01-927c-403b-821b-8fa2a293832c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:42.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8984" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":-1,"completed":33,"skipped":534,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:42.832: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 14 13:26:42.900: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:42.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9600" for this suite.

•
------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":-1,"completed":34,"skipped":647,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:42.333: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:26:42.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40" in namespace "projected-2749" to be "Succeeded or Failed"
Dec 14 13:26:42.377: INFO: Pod "downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801264ms
Dec 14 13:26:44.381: INFO: Pod "downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012120544s
Dec 14 13:26:46.384: INFO: Pod "downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015173948s
STEP: Saw pod success
Dec 14 13:26:46.384: INFO: Pod "downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40" satisfied condition "Succeeded or Failed"
Dec 14 13:26:46.387: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40 container client-container: <nil>
STEP: delete the pod
Dec 14 13:26:46.410: INFO: Waiting for pod downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40 to disappear
Dec 14 13:26:46.414: INFO: Pod downwardapi-volume-eec55b6c-067b-4256-82b4-c014511c6a40 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:46.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2749" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":139,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:42.964: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-5341/secret-test-f89675c7-ae45-4dc1-a441-39eea803abea
STEP: Creating a pod to test consume secrets
Dec 14 13:26:43.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb" in namespace "secrets-5341" to be "Succeeded or Failed"
Dec 14 13:26:43.014: INFO: Pod "pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.821987ms
Dec 14 13:26:45.018: INFO: Pod "pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994739s
Dec 14 13:26:47.021: INFO: Pod "pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937394s
STEP: Saw pod success
Dec 14 13:26:47.021: INFO: Pod "pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb" satisfied condition "Succeeded or Failed"
Dec 14 13:26:47.023: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb container env-test: <nil>
STEP: delete the pod
Dec 14 13:26:47.043: INFO: Waiting for pod pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb to disappear
Dec 14 13:26:47.046: INFO: Pod pod-configmaps-d1e05a2a-4d18-4ff3-8610-5ca2a9b07deb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:47.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5341" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":35,"skipped":652,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 13:26:47.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4506'
Dec 14 13:26:47.359: INFO: stderr: ""
Dec 14 13:26:47.359: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 14 13:26:47.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get pod e2e-test-httpd-pod -o json --namespace=kubectl-4506'
Dec 14 13:26:47.563: INFO: stderr: ""
Dec 14 13:26:47.563: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-12-14T13:26:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-14T13:26:47Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-14T13:26:47Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4506\",\n        \"resourceVersion\": \"13928\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4506/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8a266699-b159-477d-ace7-26dfc19bf3e2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-f7vt2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance-v1-19-pwlsgdsa6hrh-node-1\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-f7vt2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-f7vt2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:26:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:26:47Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:26:47Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:26:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.55\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-14T13:26:47Z\"\n    }\n}\n"
Dec 14 13:26:47.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 replace -f - --dry-run server --namespace=kubectl-4506'
Dec 14 13:26:48.148: INFO: stderr: "W1214 13:26:47.666158    1001 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Dec 14 13:26:48.148: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Dec 14 13:26:48.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 delete pods e2e-test-httpd-pod --namespace=kubectl-4506'
Dec 14 13:26:58.664: INFO: stderr: ""
Dec 14 13:26:58.664: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:58.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4506" for this suite.


• [SLOW TEST:11.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":-1,"completed":36,"skipped":663,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:22.546: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-5584bb8f-e19b-433a-b63f-cb40f6a78f46
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5584bb8f-e19b-433a-b63f-cb40f6a78f46
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:59.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9309" for this suite.


• [SLOW TEST:96.681 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":370,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 14 13:26:59.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2528 /api/v1/namespaces/watch-2528/configmaps/e2e-watch-test-resource-version d8676b77-3ecf-48a0-a9f6-26f34c08f416 14087 0 2020-12-14 13:26:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-14 13:26:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:26:59.323: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2528 /api/v1/namespaces/watch-2528/configmaps/e2e-watch-test-resource-version d8676b77-3ecf-48a0-a9f6-26f34c08f416 14088 0 2020-12-14 13:26:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-14 13:26:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:59.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2528" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":-1,"completed":22,"skipped":379,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:59.358: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-3332f6cd-f76d-4a53-934d-fedbbaf1d941
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:26:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-765" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":-1,"completed":23,"skipped":387,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 13:26:59.522: INFO: Waiting up to 5m0s for pod "pod-a3ce7e19-56de-4890-aab5-0f67f421178f" in namespace "emptydir-6552" to be "Succeeded or Failed"
Dec 14 13:26:59.527: INFO: Pod "pod-a3ce7e19-56de-4890-aab5-0f67f421178f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663015ms
Dec 14 13:27:01.532: INFO: Pod "pod-a3ce7e19-56de-4890-aab5-0f67f421178f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009608578s
Dec 14 13:27:03.536: INFO: Pod "pod-a3ce7e19-56de-4890-aab5-0f67f421178f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013702169s
STEP: Saw pod success
Dec 14 13:27:03.536: INFO: Pod "pod-a3ce7e19-56de-4890-aab5-0f67f421178f" satisfied condition "Succeeded or Failed"
Dec 14 13:27:03.539: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-a3ce7e19-56de-4890-aab5-0f67f421178f container test-container: <nil>
STEP: delete the pod
Dec 14 13:27:03.566: INFO: Waiting for pod pod-a3ce7e19-56de-4890-aab5-0f67f421178f to disappear
Dec 14 13:27:03.570: INFO: Pod pod-a3ce7e19-56de-4890-aab5-0f67f421178f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:03.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6552" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":414,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:03.626: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Dec 14 13:27:03.668: INFO: Waiting up to 5m0s for pod "var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735" in namespace "var-expansion-2879" to be "Succeeded or Failed"
Dec 14 13:27:03.671: INFO: Pod "var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259035ms
Dec 14 13:27:05.674: INFO: Pod "var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006635568s
Dec 14 13:27:07.679: INFO: Pod "var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010981889s
STEP: Saw pod success
Dec 14 13:27:07.679: INFO: Pod "var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735" satisfied condition "Succeeded or Failed"
Dec 14 13:27:07.681: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:27:07.703: INFO: Waiting for pod var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735 to disappear
Dec 14 13:27:07.706: INFO: Pod var-expansion-37ddad2b-eddd-4e36-a322-e27807ec8735 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:07.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2879" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":-1,"completed":25,"skipped":437,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:07.731: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:27:07.764: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8187" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":-1,"completed":26,"skipped":443,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-q486
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:26:46.576: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-q486" in namespace "subpath-4807" to be "Succeeded or Failed"
Dec 14 13:26:46.587: INFO: Pod "pod-subpath-test-secret-q486": Phase="Pending", Reason="", readiness=false. Elapsed: 10.753885ms
Dec 14 13:26:48.590: INFO: Pod "pod-subpath-test-secret-q486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01418945s
Dec 14 13:26:50.594: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 4.017941571s
Dec 14 13:26:52.598: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 6.021871085s
Dec 14 13:26:54.602: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 8.026502509s
Dec 14 13:26:56.606: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 10.029938305s
Dec 14 13:26:58.609: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 12.033210302s
Dec 14 13:27:00.619: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 14.042922501s
Dec 14 13:27:02.625: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 16.048778394s
Dec 14 13:27:04.629: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 18.05312085s
Dec 14 13:27:06.634: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 20.057842098s
Dec 14 13:27:08.640: INFO: Pod "pod-subpath-test-secret-q486": Phase="Running", Reason="", readiness=true. Elapsed: 22.064650484s
Dec 14 13:27:10.646: INFO: Pod "pod-subpath-test-secret-q486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.070638796s
STEP: Saw pod success
Dec 14 13:27:10.647: INFO: Pod "pod-subpath-test-secret-q486" satisfied condition "Succeeded or Failed"
Dec 14 13:27:10.650: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-subpath-test-secret-q486 container test-container-subpath-secret-q486: <nil>
STEP: delete the pod
Dec 14 13:27:10.677: INFO: Waiting for pod pod-subpath-test-secret-q486 to disappear
Dec 14 13:27:10.680: INFO: Pod pod-subpath-test-secret-q486 no longer exists
STEP: Deleting pod pod-subpath-test-secret-q486
Dec 14 13:27:10.680: INFO: Deleting pod "pod-subpath-test-secret-q486" in namespace "subpath-4807"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:10.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4807" for this suite.


• [SLOW TEST:24.164 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":-1,"completed":9,"skipped":185,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:08.354: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:27:08.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:27:10.928: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549228, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549228, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549228, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549228, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:27:13.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:14.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4260" for this suite.
STEP: Destroying namespace "webhook-4260-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:5.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":-1,"completed":27,"skipped":458,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:14.280: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:27:14.788: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:27:16.806: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549234, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549234, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549234, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549234, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:27:19.823: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:27:19.831: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4861-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:20.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3554" for this suite.
STEP: Destroying namespace "webhook-3554-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.744 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":-1,"completed":28,"skipped":471,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:21.105: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Dec 14 13:27:21.668: INFO: created pod pod-service-account-defaultsa
Dec 14 13:27:21.668: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 13:27:21.677: INFO: created pod pod-service-account-mountsa
Dec 14 13:27:21.677: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 13:27:21.686: INFO: created pod pod-service-account-nomountsa
Dec 14 13:27:21.686: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 13:27:21.696: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 13:27:21.697: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 13:27:21.705: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 13:27:21.705: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 13:27:21.711: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 13:27:21.711: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 13:27:21.720: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 13:27:21.720: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 13:27:21.728: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 13:27:21.728: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 13:27:21.735: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 13:27:21.735: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4978" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":-1,"completed":29,"skipped":489,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:58.693: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4301
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:26:58.722: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:26:58.760: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:27:00.765: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:27:02.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:04.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:06.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:08.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:10.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:12.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:14.764: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:27:14.774: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 13:27:16.785: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:27:20.822: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.196.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4301 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:27:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
Dec 14 13:27:22.034: INFO: Found all expected endpoints: [netserver-0]
Dec 14 13:27:22.038: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.227.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4301 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:27:22.038: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
Dec 14 13:27:23.255: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:23.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4301" for this suite.


• [SLOW TEST:24.579 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:21.775: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 13:27:21.819: INFO: Waiting up to 5m0s for pod "pod-c6314c48-b65f-443c-be6c-fc86e0ad928e" in namespace "emptydir-1431" to be "Succeeded or Failed"
Dec 14 13:27:21.823: INFO: Pod "pod-c6314c48-b65f-443c-be6c-fc86e0ad928e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708857ms
Dec 14 13:27:23.835: INFO: Pod "pod-c6314c48-b65f-443c-be6c-fc86e0ad928e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015767007s
Dec 14 13:27:25.840: INFO: Pod "pod-c6314c48-b65f-443c-be6c-fc86e0ad928e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021012567s
STEP: Saw pod success
Dec 14 13:27:25.841: INFO: Pod "pod-c6314c48-b65f-443c-be6c-fc86e0ad928e" satisfied condition "Succeeded or Failed"
Dec 14 13:27:25.843: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-c6314c48-b65f-443c-be6c-fc86e0ad928e container test-container: <nil>
STEP: delete the pod
Dec 14 13:27:25.867: INFO: Waiting for pod pod-c6314c48-b65f-443c-be6c-fc86e0ad928e to disappear
Dec 14 13:27:25.882: INFO: Pod pod-c6314c48-b65f-443c-be6c-fc86e0ad928e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:25.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1431" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":30,"skipped":491,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:25.994: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:27:26.090: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:27:26.120: INFO: waiting for watch events with expected annotations
Dec 14 13:27:26.120: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:26.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3163" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":-1,"completed":31,"skipped":534,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":37,"skipped":666,"failed":0}
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:23.285: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 13:27:23.327: INFO: Waiting up to 5m0s for pod "pod-51af9dbd-911b-4183-ae2d-7703502b67f7" in namespace "emptydir-2276" to be "Succeeded or Failed"
Dec 14 13:27:23.335: INFO: Pod "pod-51af9dbd-911b-4183-ae2d-7703502b67f7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.842908ms
Dec 14 13:27:25.348: INFO: Pod "pod-51af9dbd-911b-4183-ae2d-7703502b67f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021314304s
Dec 14 13:27:27.353: INFO: Pod "pod-51af9dbd-911b-4183-ae2d-7703502b67f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02660977s
STEP: Saw pod success
Dec 14 13:27:27.353: INFO: Pod "pod-51af9dbd-911b-4183-ae2d-7703502b67f7" satisfied condition "Succeeded or Failed"
Dec 14 13:27:27.367: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-51af9dbd-911b-4183-ae2d-7703502b67f7 container test-container: <nil>
STEP: delete the pod
Dec 14 13:27:27.395: INFO: Waiting for pod pod-51af9dbd-911b-4183-ae2d-7703502b67f7 to disappear
Dec 14 13:27:27.399: INFO: Pod pod-51af9dbd-911b-4183-ae2d-7703502b67f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:27.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2276" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":666,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:10.835: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 14 13:27:12.079: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:27:12.124: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:27:14.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549232, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549232, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549232, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549232, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:27:17.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8821" for this suite.
STEP: Destroying namespace "webhook-8821-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:17.622 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":-1,"completed":10,"skipped":255,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:26.231: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:27:26.306: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4942703b-44a5-4708-93e5-6779561992a7" in namespace "security-context-test-4370" to be "Succeeded or Failed"
Dec 14 13:27:26.319: INFO: Pod "busybox-readonly-false-4942703b-44a5-4708-93e5-6779561992a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.266179ms
Dec 14 13:27:28.326: INFO: Pod "busybox-readonly-false-4942703b-44a5-4708-93e5-6779561992a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020144363s
Dec 14 13:27:30.338: INFO: Pod "busybox-readonly-false-4942703b-44a5-4708-93e5-6779561992a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032187943s
Dec 14 13:27:30.338: INFO: Pod "busybox-readonly-false-4942703b-44a5-4708-93e5-6779561992a7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:30.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4370" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":568,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:28.515: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 13:27:28.561: INFO: Waiting up to 5m0s for pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07" in namespace "emptydir-2557" to be "Succeeded or Failed"
Dec 14 13:27:28.568: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.447167ms
Dec 14 13:27:30.575: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01376568s
Dec 14 13:27:32.579: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01738569s
Dec 14 13:27:34.582: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020682872s
Dec 14 13:27:36.586: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025062168s
STEP: Saw pod success
Dec 14 13:27:36.586: INFO: Pod "pod-b4dc7e55-4e04-4821-b765-7770e4603e07" satisfied condition "Succeeded or Failed"
Dec 14 13:27:36.589: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-b4dc7e55-4e04-4821-b765-7770e4603e07 container test-container: <nil>
STEP: delete the pod
Dec 14 13:27:36.608: INFO: Waiting for pod pod-b4dc7e55-4e04-4821-b765-7770e4603e07 to disappear
Dec 14 13:27:36.614: INFO: Pod pod-b4dc7e55-4e04-4821-b765-7770e4603e07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:36.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2557" for this suite.


• [SLOW TEST:8.110 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":273,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:36.692: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 13:27:41.279: INFO: Successfully updated pod "pod-update-2bc3f822-d6a2-4538-8b2d-4fe9f3da5ac2"
STEP: verifying the updated pod is in kubernetes
Dec 14 13:27:41.288: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:41.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5053" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":304,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:23:38.462: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4257a8b3-c7d4-4cc2-a800-19c4e28bf342 in namespace container-probe-2495
Dec 14 13:23:42.514: INFO: Started pod liveness-4257a8b3-c7d4-4cc2-a800-19c4e28bf342 in namespace container-probe-2495
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:23:42.516: INFO: Initial restart count of pod liveness-4257a8b3-c7d4-4cc2-a800-19c4e28bf342 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:43.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2495" for this suite.


• [SLOW TEST:244.724 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":-1,"completed":10,"skipped":188,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:41.313: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Dec 14 13:27:41.345: INFO: namespace kubectl-749
Dec 14 13:27:41.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-749'
Dec 14 13:27:41.808: INFO: stderr: ""
Dec 14 13:27:41.808: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 13:27:42.813: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:27:42.813: INFO: Found 0 / 1
Dec 14 13:27:43.813: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:27:43.813: INFO: Found 0 / 1
Dec 14 13:27:44.813: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:27:44.813: INFO: Found 1 / 1
Dec 14 13:27:44.813: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 13:27:44.818: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:27:44.818: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 13:27:44.818: INFO: wait on agnhost-primary startup in kubectl-749 
Dec 14 13:27:44.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 logs agnhost-primary-9pk62 agnhost-primary --namespace=kubectl-749'
Dec 14 13:27:45.016: INFO: stderr: ""
Dec 14 13:27:45.016: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 14 13:27:45.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-749'
Dec 14 13:27:45.249: INFO: stderr: ""
Dec 14 13:27:45.249: INFO: stdout: "service/rm2 exposed\n"
Dec 14 13:27:45.253: INFO: Service rm2 in namespace kubectl-749 found.
STEP: exposing service
Dec 14 13:27:47.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-749'
Dec 14 13:27:47.540: INFO: stderr: ""
Dec 14 13:27:47.540: INFO: stdout: "service/rm3 exposed\n"
Dec 14 13:27:47.547: INFO: Service rm3 in namespace kubectl-749 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:49.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-749" for this suite.


• [SLOW TEST:8.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":-1,"completed":13,"skipped":310,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:49.606: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 13:27:49.683: INFO: Waiting up to 5m0s for pod "pod-343877fa-7927-4823-a2f3-091571faf4fa" in namespace "emptydir-3510" to be "Succeeded or Failed"
Dec 14 13:27:49.690: INFO: Pod "pod-343877fa-7927-4823-a2f3-091571faf4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.848762ms
Dec 14 13:27:51.694: INFO: Pod "pod-343877fa-7927-4823-a2f3-091571faf4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596226s
Dec 14 13:27:53.698: INFO: Pod "pod-343877fa-7927-4823-a2f3-091571faf4fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014464362s
STEP: Saw pod success
Dec 14 13:27:53.698: INFO: Pod "pod-343877fa-7927-4823-a2f3-091571faf4fa" satisfied condition "Succeeded or Failed"
Dec 14 13:27:53.701: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-343877fa-7927-4823-a2f3-091571faf4fa container test-container: <nil>
STEP: delete the pod
Dec 14 13:27:53.727: INFO: Waiting for pod pod-343877fa-7927-4823-a2f3-091571faf4fa to disappear
Dec 14 13:27:53.731: INFO: Pod pod-343877fa-7927-4823-a2f3-091571faf4fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:53.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3510" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":316,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:53.822: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Dec 14 13:27:53.857: INFO: Waiting up to 5m0s for pod "var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2" in namespace "var-expansion-2924" to be "Succeeded or Failed"
Dec 14 13:27:53.867: INFO: Pod "var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947142ms
Dec 14 13:27:55.872: INFO: Pod "var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015260729s
Dec 14 13:27:57.876: INFO: Pod "var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019252955s
STEP: Saw pod success
Dec 14 13:27:57.876: INFO: Pod "var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2" satisfied condition "Succeeded or Failed"
Dec 14 13:27:57.879: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:27:57.910: INFO: Waiting for pod var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2 to disappear
Dec 14 13:27:57.913: INFO: Pod var-expansion-07a6f86d-c977-44b6-aac0-7d60ed3412b2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:27:57.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2924" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":-1,"completed":15,"skipped":349,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-629
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:27:43.289: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:27:43.329: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:27:45.334: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:27:47.335: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:49.334: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:51.334: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:53.334: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:55.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:57.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:27:59.333: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:27:59.340: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:28:03.370: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.196.169:8080/dial?request=hostname&protocol=http&host=10.100.196.164&port=8080&tries=1'] Namespace:pod-network-test-629 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:28:03.370: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
Dec 14 13:28:03.597: INFO: Waiting for responses: map[]
Dec 14 13:28:03.602: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.196.169:8080/dial?request=hostname&protocol=http&host=10.100.227.148&port=8080&tries=1'] Namespace:pod-network-test-629 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:28:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
Dec 14 13:28:03.812: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:03.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-629" for this suite.


• [SLOW TEST:20.583 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":199,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:03.855: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:28:03.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092" in namespace "projected-2165" to be "Succeeded or Failed"
Dec 14 13:28:03.908: INFO: Pod "downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092": Phase="Pending", Reason="", readiness=false. Elapsed: 5.542423ms
Dec 14 13:28:05.915: INFO: Pod "downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012627336s
Dec 14 13:28:07.922: INFO: Pod "downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020099837s
STEP: Saw pod success
Dec 14 13:28:07.922: INFO: Pod "downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092" satisfied condition "Succeeded or Failed"
Dec 14 13:28:07.925: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092 container client-container: <nil>
STEP: delete the pod
Dec 14 13:28:07.950: INFO: Waiting for pod downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092 to disappear
Dec 14 13:28:07.955: INFO: Pod downwardapi-volume-d2f8c859-cfb6-4f90-bf2e-29476ac08092 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:07.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2165" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":212,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:08.106: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 14 13:28:08.817: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:28:08.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:28:10.851: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549288, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549288, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549288, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549288, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:28:13.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 14 13:28:14.950: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:14.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1216" for this suite.
STEP: Destroying namespace "webhook-1216-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.939 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":-1,"completed":13,"skipped":258,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:57.932: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:27:59.930: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:28:01.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549279, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549279, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549279, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549279, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:28:04.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:17.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4826" for this suite.
STEP: Destroying namespace "webhook-4826-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:19.225 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":-1,"completed":16,"skipped":351,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:17.203: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Dec 14 13:28:17.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-517469967 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:17.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1742" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":-1,"completed":17,"skipped":372,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:17.490: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:17.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2774" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":18,"skipped":407,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:30.539: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2152
Dec 14 13:27:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 13:27:37.080: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 13:27:37.080: INFO: stdout: "iptables"
Dec 14 13:27:37.080: INFO: proxyMode: iptables
Dec 14 13:27:37.087: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:27:37.091: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:27:39.091: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:27:39.095: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:27:41.091: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:27:41.094: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2152
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2152
I1214 13:27:41.124891      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2152, replica count: 3
I1214 13:27:44.176032      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:27:44.190: INFO: Creating new exec pod
Dec 14 13:27:49.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Dec 14 13:27:49.632: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 13:27:49.633: INFO: stdout: ""
Dec 14 13:27:49.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 10.254.53.127 80'
Dec 14 13:27:50.011: INFO: stderr: "+ nc -zv -t -w 2 10.254.53.127 80\nConnection to 10.254.53.127 80 port [tcp/http] succeeded!\n"
Dec 14 13:27:50.011: INFO: stdout: ""
Dec 14 13:27:50.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.200 32651'
Dec 14 13:27:50.418: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.200 32651\nConnection to 10.0.0.200 32651 port [tcp/32651] succeeded!\n"
Dec 14 13:27:50.418: INFO: stdout: ""
Dec 14 13:27:50.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.55 32651'
Dec 14 13:27:50.817: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.55 32651\nConnection to 10.0.0.55 32651 port [tcp/32651] succeeded!\n"
Dec 14 13:27:50.818: INFO: stdout: ""
Dec 14 13:27:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.93 32651'
Dec 14 13:27:51.204: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.93 32651\nConnection to 38.108.68.93 32651 port [tcp/32651] succeeded!\n"
Dec 14 13:27:51.204: INFO: stdout: ""
Dec 14 13:27:51.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.247 32651'
Dec 14 13:27:51.664: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.247 32651\nConnection to 38.108.68.247 32651 port [tcp/32651] succeeded!\n"
Dec 14 13:27:51.664: INFO: stdout: ""
Dec 14 13:27:51.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:32651/ ; done'
Dec 14 13:27:52.255: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n"
Dec 14 13:27:52.255: INFO: stdout: "\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8\naffinity-nodeport-timeout-55sm8"
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.255: INFO: Received response from host: affinity-nodeport-timeout-55sm8
Dec 14 13:27:52.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.200:32651/'
Dec 14 13:27:52.656: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n"
Dec 14 13:27:52.656: INFO: stdout: "affinity-nodeport-timeout-55sm8"
Dec 14 13:28:07.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-2152 execpod-affinityd8km5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.200:32651/'
Dec 14 13:28:08.121: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.200:32651/\n"
Dec 14 13:28:08.121: INFO: stdout: "affinity-nodeport-timeout-vnm54"
Dec 14 13:28:08.121: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2152, will wait for the garbage collector to delete the pods
Dec 14 13:28:08.211: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.968694ms
Dec 14 13:28:09.212: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.00087558s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:22.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2152" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:51.628 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":33,"skipped":650,"failed":0}

SSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:28:22.227: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:26.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2450" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":-1,"completed":34,"skipped":653,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:26.518: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:26.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1211" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":-1,"completed":35,"skipped":654,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:17.614: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:28:17.644: INFO: Creating ReplicaSet my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db
Dec 14 13:28:17.663: INFO: Pod name my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db: Found 0 pods out of 1
Dec 14 13:28:22.667: INFO: Pod name my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db: Found 1 pods out of 1
Dec 14 13:28:22.667: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db" is running
Dec 14 13:28:22.671: INFO: Pod "my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db-56bbj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:28:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:28:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:28:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-14 13:28:17 +0000 UTC Reason: Message:}])
Dec 14 13:28:22.673: INFO: Trying to dial the pod
Dec 14 13:28:27.684: INFO: Controller my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db: Got expected result from replica 1 [my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db-56bbj]: "my-hostname-basic-4b5a7e54-972c-4f24-9bcf-8faf9f91c5db-56bbj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:27.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6683" for this suite.


• [SLOW TEST:10.081 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":19,"skipped":411,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:27.581: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:28:27.647: INFO: Deleting pod "var-expansion-a6e7be60-6825-4c8f-9c10-35c701954c25" in namespace "var-expansion-9390"
Dec 14 13:28:27.653: INFO: Wait up to 5m0s for pod "var-expansion-a6e7be60-6825-4c8f-9c10-35c701954c25" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9390" for this suite.


• [SLOW TEST:122.096 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":-1,"completed":22,"skipped":321,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:26.636: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-6b512b53-76a9-4098-b2db-06d8ec061bb5
STEP: Creating a pod to test consume secrets
Dec 14 13:28:26.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a" in namespace "projected-6440" to be "Succeeded or Failed"
Dec 14 13:28:26.700: INFO: Pod "pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116992ms
Dec 14 13:28:28.705: INFO: Pod "pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010599097s
Dec 14 13:28:30.709: INFO: Pod "pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015143864s
STEP: Saw pod success
Dec 14 13:28:30.710: INFO: Pod "pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a" satisfied condition "Succeeded or Failed"
Dec 14 13:28:30.719: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:28:30.762: INFO: Waiting for pod pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a to disappear
Dec 14 13:28:30.766: INFO: Pod pod-projected-secrets-6ad37d86-8976-4b2d-b9c9-1fe2d05c099a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:30.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6440" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":679,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:27.705: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 13:28:27.753: INFO: Waiting up to 5m0s for pod "pod-f48722bd-e747-48ca-8df9-097f6e4cee0e" in namespace "emptydir-6854" to be "Succeeded or Failed"
Dec 14 13:28:27.759: INFO: Pod "pod-f48722bd-e747-48ca-8df9-097f6e4cee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87382ms
Dec 14 13:28:29.770: INFO: Pod "pod-f48722bd-e747-48ca-8df9-097f6e4cee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016453568s
Dec 14 13:28:31.809: INFO: Pod "pod-f48722bd-e747-48ca-8df9-097f6e4cee0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05596852s
STEP: Saw pod success
Dec 14 13:28:31.809: INFO: Pod "pod-f48722bd-e747-48ca-8df9-097f6e4cee0e" satisfied condition "Succeeded or Failed"
Dec 14 13:28:31.812: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-f48722bd-e747-48ca-8df9-097f6e4cee0e container test-container: <nil>
STEP: delete the pod
Dec 14 13:28:31.852: INFO: Waiting for pod pod-f48722bd-e747-48ca-8df9-097f6e4cee0e to disappear
Dec 14 13:28:31.855: INFO: Pod pod-f48722bd-e747-48ca-8df9-097f6e4cee0e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:31.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6854" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":412,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:30.879: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-711" for this suite.


• [SLOW TEST:7.082 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":-1,"completed":37,"skipped":722,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:31.989: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1214 13:28:42.084028      57 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:28:42.084113      57 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:28:42.084140      57 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:28:42.084: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:42.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1384" for this suite.


• [SLOW TEST:10.106 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":-1,"completed":21,"skipped":452,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Dec 14 13:28:42.159: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 14 13:28:42.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:42.807: INFO: stderr: ""
Dec 14 13:28:42.807: INFO: stdout: "service/agnhost-replica created\n"
Dec 14 13:28:42.808: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 14 13:28:42.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:43.354: INFO: stderr: ""
Dec 14 13:28:43.355: INFO: stdout: "service/agnhost-primary created\n"
Dec 14 13:28:43.356: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 13:28:43.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:43.823: INFO: stderr: ""
Dec 14 13:28:43.823: INFO: stdout: "service/frontend created\n"
Dec 14 13:28:43.824: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 13:28:43.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:44.288: INFO: stderr: ""
Dec 14 13:28:44.289: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 13:28:44.289: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 13:28:44.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:44.756: INFO: stderr: ""
Dec 14 13:28:44.757: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 14 13:28:44.757: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 13:28:44.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-3387'
Dec 14 13:28:45.287: INFO: stderr: ""
Dec 14 13:28:45.287: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 14 13:28:45.287: INFO: Waiting for all frontend pods to be Running.
Dec 14 13:28:50.338: INFO: Waiting for frontend to serve content.
Dec 14 13:28:50.352: INFO: Trying to add a new entry to the guestbook.
Dec 14 13:28:50.372: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 14 13:28:50.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:50.596: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:50.596: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:28:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:50.843: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:50.843: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:28:50.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:51.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:51.059: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:28:51.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:51.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:51.259: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:28:51.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:51.455: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:51.455: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:28:51.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 delete --grace-period=0 --force -f - --namespace=kubectl-3387'
Dec 14 13:28:51.645: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:51.645: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:51.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3387" for this suite.


• [SLOW TEST:9.544 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":-1,"completed":22,"skipped":463,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:51.761: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 14 13:28:51.804: INFO: Waiting up to 5m0s for pod "downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5" in namespace "downward-api-8297" to be "Succeeded or Failed"
Dec 14 13:28:51.823: INFO: Pod "downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.892573ms
Dec 14 13:28:53.827: INFO: Pod "downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022809429s
Dec 14 13:28:55.832: INFO: Pod "downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027296374s
STEP: Saw pod success
Dec 14 13:28:55.832: INFO: Pod "downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5" satisfied condition "Succeeded or Failed"
Dec 14 13:28:55.834: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:28:55.857: INFO: Waiting for pod downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5 to disappear
Dec 14 13:28:55.862: INFO: Pod downward-api-d3e93610-7103-4e7c-ade0-cf30d83601a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:55.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8297" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":485,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-06a36296-3b9e-4823-9f02-2a77719ab1a4
STEP: Creating a pod to test consume configMaps
Dec 14 13:28:55.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242" in namespace "configmap-9296" to be "Succeeded or Failed"
Dec 14 13:28:55.952: INFO: Pod "pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926803ms
Dec 14 13:28:57.955: INFO: Pod "pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008330226s
Dec 14 13:28:59.961: INFO: Pod "pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013808131s
STEP: Saw pod success
Dec 14 13:28:59.961: INFO: Pod "pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242" satisfied condition "Succeeded or Failed"
Dec 14 13:28:59.964: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:28:59.989: INFO: Waiting for pod pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242 to disappear
Dec 14 13:28:59.993: INFO: Pod pod-configmaps-755149d0-a2dd-422b-890b-dde9f0cf7242 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:28:59.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9296" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":498,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:00.069: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-77cfcc0d-e670-4062-b1d4-4cd672744355
STEP: Creating a pod to test consume secrets
Dec 14 13:29:00.112: INFO: Waiting up to 5m0s for pod "pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99" in namespace "secrets-3474" to be "Succeeded or Failed"
Dec 14 13:29:00.117: INFO: Pod "pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99": Phase="Pending", Reason="", readiness=false. Elapsed: 5.255408ms
Dec 14 13:29:02.122: INFO: Pod "pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99": Phase="Running", Reason="", readiness=true. Elapsed: 2.010750203s
Dec 14 13:29:04.128: INFO: Pod "pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016264949s
STEP: Saw pod success
Dec 14 13:29:04.128: INFO: Pod "pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99" satisfied condition "Succeeded or Failed"
Dec 14 13:29:04.131: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:29:04.156: INFO: Waiting for pod pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99 to disappear
Dec 14 13:29:04.160: INFO: Pod pod-secrets-b4eb2282-ebf2-4939-8e95-8ab3b8079f99 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:04.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3474" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":25,"skipped":522,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:26:06.420: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1541
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Dec 14 13:26:06.486: INFO: Found 0 stateful pods, waiting for 3
Dec 14 13:26:16.499: INFO: Found 2 stateful pods, waiting for 3
Dec 14 13:26:26.504: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:26:26.504: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:26:26.504: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:26:26.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 exec --namespace=statefulset-1541 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:26:27.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:26:27.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:26:27.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 14 13:26:37.058: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 14 13:26:47.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 exec --namespace=statefulset-1541 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:26:47.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:26:47.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:26:47.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:26:57.649: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:26:57.649: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:26:57.649: INFO: Waiting for Pod statefulset-1541/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:26:57.649: INFO: Waiting for Pod statefulset-1541/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:27:07.659: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:27:07.659: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:27:07.659: INFO: Waiting for Pod statefulset-1541/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:27:17.658: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:27:17.658: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:27:27.662: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:27:27.662: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 14 13:27:37.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 exec --namespace=statefulset-1541 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:27:38.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:27:38.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:27:38.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:27:48.168: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 14 13:27:58.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 exec --namespace=statefulset-1541 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:27:58.615: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:27:58.615: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:27:58.615: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:28:08.669: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:28:08.670: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 14 13:28:08.670: INFO: Waiting for Pod statefulset-1541/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 14 13:28:08.670: INFO: Waiting for Pod statefulset-1541/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 14 13:28:18.681: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
Dec 14 13:28:18.682: INFO: Waiting for Pod statefulset-1541/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 14 13:28:18.682: INFO: Waiting for Pod statefulset-1541/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 14 13:28:28.695: INFO: Waiting for StatefulSet statefulset-1541/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:28:38.680: INFO: Deleting all statefulset in ns statefulset-1541
Dec 14 13:28:38.685: INFO: Scaling statefulset ss2 to 0
Dec 14 13:29:08.706: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:29:08.711: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:08.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1541" for this suite.


• [SLOW TEST:182.320 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":-1,"completed":23,"skipped":537,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:29.702: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1214 13:29:09.808387      32 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:29:09.808487      32 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:29:09.808532      32 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:29:09.808: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 13:29:09.808: INFO: Deleting pod "simpletest.rc-7ddhv" in namespace "gc-5396"
Dec 14 13:29:09.826: INFO: Deleting pod "simpletest.rc-9rsrv" in namespace "gc-5396"
Dec 14 13:29:09.841: INFO: Deleting pod "simpletest.rc-c4zm4" in namespace "gc-5396"
Dec 14 13:29:09.858: INFO: Deleting pod "simpletest.rc-d9zlm" in namespace "gc-5396"
Dec 14 13:29:09.878: INFO: Deleting pod "simpletest.rc-hwfmw" in namespace "gc-5396"
Dec 14 13:29:09.902: INFO: Deleting pod "simpletest.rc-j526q" in namespace "gc-5396"
Dec 14 13:29:09.922: INFO: Deleting pod "simpletest.rc-nfct5" in namespace "gc-5396"
Dec 14 13:29:09.941: INFO: Deleting pod "simpletest.rc-nsh8j" in namespace "gc-5396"
Dec 14 13:29:09.960: INFO: Deleting pod "simpletest.rc-qgnfw" in namespace "gc-5396"
Dec 14 13:29:09.975: INFO: Deleting pod "simpletest.rc-xghfz" in namespace "gc-5396"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5396" for this suite.


• [SLOW TEST:40.330 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":-1,"completed":23,"skipped":328,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:25:08.557: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-f1921f42-3f0b-4e6f-8568-9032632817b3 in namespace container-probe-8047
Dec 14 13:25:12.613: INFO: Started pod busybox-f1921f42-3f0b-4e6f-8568-9032632817b3 in namespace container-probe-8047
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:25:12.616: INFO: Initial restart count of pod busybox-f1921f42-3f0b-4e6f-8568-9032632817b3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:13.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8047" for this suite.


• [SLOW TEST:244.762 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":10,"skipped":150,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-3959d713-293b-453f-a0e0-d3fc0fcaf435
STEP: Creating a pod to test consume configMaps
Dec 14 13:29:10.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12" in namespace "projected-8847" to be "Succeeded or Failed"
Dec 14 13:29:10.243: INFO: Pod "pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.954928ms
Dec 14 13:29:12.256: INFO: Pod "pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021190906s
Dec 14 13:29:14.263: INFO: Pod "pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028581812s
STEP: Saw pod success
Dec 14 13:29:14.263: INFO: Pod "pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12" satisfied condition "Succeeded or Failed"
Dec 14 13:29:14.278: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:29:14.312: INFO: Waiting for pod pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12 to disappear
Dec 14 13:29:14.319: INFO: Pod pod-projected-configmaps-3c7bb282-3a28-4ab2-bb17-f034b260df12 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:14.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8847" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":362,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:13.380: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:13.424: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:14.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9728" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":-1,"completed":11,"skipped":175,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:04.182: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:04.213: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 13:29:09.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-3729 create -f -'
Dec 14 13:29:11.300: INFO: stderr: ""
Dec 14 13:29:11.300: INFO: stdout: "e2e-test-crd-publish-openapi-2155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 13:29:11.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-3729 delete e2e-test-crd-publish-openapi-2155-crds test-cr'
Dec 14 13:29:11.488: INFO: stderr: ""
Dec 14 13:29:11.488: INFO: stdout: "e2e-test-crd-publish-openapi-2155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 13:29:11.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-3729 apply -f -'
Dec 14 13:29:11.900: INFO: stderr: ""
Dec 14 13:29:11.900: INFO: stdout: "e2e-test-crd-publish-openapi-2155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 13:29:11.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 --namespace=crd-publish-openapi-3729 delete e2e-test-crd-publish-openapi-2155-crds test-cr'
Dec 14 13:29:12.195: INFO: stderr: ""
Dec 14 13:29:12.195: INFO: stdout: "e2e-test-crd-publish-openapi-2155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 13:29:12.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 explain e2e-test-crd-publish-openapi-2155-crds'
Dec 14 13:29:12.735: INFO: stderr: ""
Dec 14 13:29:12.735: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2155-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:18.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3729" for this suite.


• [SLOW TEST:14.198 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":-1,"completed":26,"skipped":526,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:18.418: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Dec 14 13:29:18.458: INFO: Major version: 1
STEP: Confirm minor version
Dec 14 13:29:18.458: INFO: cleanMinorVersion: 19
Dec 14 13:29:18.458: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:18.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9402" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":-1,"completed":27,"skipped":540,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:14.518: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-03696222-a73b-4af2-9231-4033c35f6f34
STEP: Creating a pod to test consume configMaps
Dec 14 13:29:14.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f" in namespace "configmap-867" to be "Succeeded or Failed"
Dec 14 13:29:14.603: INFO: Pod "pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.222562ms
Dec 14 13:29:16.611: INFO: Pod "pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016867695s
Dec 14 13:29:18.615: INFO: Pod "pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02074025s
STEP: Saw pod success
Dec 14 13:29:18.615: INFO: Pod "pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f" satisfied condition "Succeeded or Failed"
Dec 14 13:29:18.622: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:29:18.672: INFO: Waiting for pod pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f to disappear
Dec 14 13:29:18.683: INFO: Pod pod-configmaps-5986dc68-dc6b-4b4f-a1a7-68ad1888455f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:18.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-867" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":192,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:08.750: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:08.783: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3171
I1214 13:29:08.815540      33 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3171, replica count: 1
I1214 13:29:09.867486      33 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:29:10.868473      33 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:29:11.869237      33 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:29:11.983: INFO: Created: latency-svc-4v7ch
Dec 14 13:29:11.996: INFO: Got endpoints: latency-svc-4v7ch [26.403722ms]
Dec 14 13:29:12.017: INFO: Created: latency-svc-sz6tf
Dec 14 13:29:12.031: INFO: Created: latency-svc-ngc5g
Dec 14 13:29:12.031: INFO: Got endpoints: latency-svc-sz6tf [33.77845ms]
Dec 14 13:29:12.039: INFO: Got endpoints: latency-svc-ngc5g [43.263778ms]
Dec 14 13:29:12.042: INFO: Created: latency-svc-r5vc9
Dec 14 13:29:12.054: INFO: Created: latency-svc-5rv77
Dec 14 13:29:12.054: INFO: Got endpoints: latency-svc-r5vc9 [56.470398ms]
Dec 14 13:29:12.060: INFO: Got endpoints: latency-svc-5rv77 [62.512392ms]
Dec 14 13:29:12.066: INFO: Created: latency-svc-cgqgw
Dec 14 13:29:12.101: INFO: Created: latency-svc-qchg5
Dec 14 13:29:12.101: INFO: Got endpoints: latency-svc-cgqgw [104.627207ms]
Dec 14 13:29:12.108: INFO: Got endpoints: latency-svc-qchg5 [111.647166ms]
Dec 14 13:29:12.116: INFO: Created: latency-svc-tkssq
Dec 14 13:29:12.124: INFO: Created: latency-svc-glhdf
Dec 14 13:29:12.125: INFO: Got endpoints: latency-svc-tkssq [127.402344ms]
Dec 14 13:29:12.134: INFO: Got endpoints: latency-svc-glhdf [137.338006ms]
Dec 14 13:29:12.135: INFO: Created: latency-svc-phtgs
Dec 14 13:29:12.141: INFO: Got endpoints: latency-svc-phtgs [143.498585ms]
Dec 14 13:29:12.148: INFO: Created: latency-svc-c8fz6
Dec 14 13:29:12.156: INFO: Got endpoints: latency-svc-c8fz6 [159.604813ms]
Dec 14 13:29:12.171: INFO: Created: latency-svc-tzrfc
Dec 14 13:29:12.175: INFO: Got endpoints: latency-svc-tzrfc [178.059321ms]
Dec 14 13:29:12.189: INFO: Created: latency-svc-tttp8
Dec 14 13:29:12.193: INFO: Got endpoints: latency-svc-tttp8 [195.638792ms]
Dec 14 13:29:12.211: INFO: Created: latency-svc-lpqcf
Dec 14 13:29:12.217: INFO: Created: latency-svc-jcch4
Dec 14 13:29:12.220: INFO: Got endpoints: latency-svc-lpqcf [224.356097ms]
Dec 14 13:29:12.224: INFO: Got endpoints: latency-svc-jcch4 [226.835859ms]
Dec 14 13:29:12.229: INFO: Created: latency-svc-g26tn
Dec 14 13:29:12.241: INFO: Created: latency-svc-j9l2t
Dec 14 13:29:12.242: INFO: Got endpoints: latency-svc-g26tn [245.235006ms]
Dec 14 13:29:12.246: INFO: Got endpoints: latency-svc-j9l2t [25.91353ms]
Dec 14 13:29:12.252: INFO: Created: latency-svc-cv8w4
Dec 14 13:29:12.262: INFO: Got endpoints: latency-svc-cv8w4 [230.965978ms]
Dec 14 13:29:12.262: INFO: Created: latency-svc-27bfq
Dec 14 13:29:12.269: INFO: Got endpoints: latency-svc-27bfq [229.46406ms]
Dec 14 13:29:12.284: INFO: Created: latency-svc-wrwtq
Dec 14 13:29:12.284: INFO: Got endpoints: latency-svc-wrwtq [230.059293ms]
Dec 14 13:29:12.290: INFO: Created: latency-svc-rk9vs
Dec 14 13:29:12.299: INFO: Created: latency-svc-f8gz9
Dec 14 13:29:12.300: INFO: Got endpoints: latency-svc-rk9vs [239.846466ms]
Dec 14 13:29:12.306: INFO: Got endpoints: latency-svc-f8gz9 [204.212731ms]
Dec 14 13:29:12.322: INFO: Created: latency-svc-fzhfb
Dec 14 13:29:12.330: INFO: Got endpoints: latency-svc-fzhfb [221.648628ms]
Dec 14 13:29:12.335: INFO: Created: latency-svc-z56n2
Dec 14 13:29:12.336: INFO: Got endpoints: latency-svc-z56n2 [211.591527ms]
Dec 14 13:29:12.343: INFO: Created: latency-svc-7qtl5
Dec 14 13:29:12.351: INFO: Got endpoints: latency-svc-7qtl5 [216.540618ms]
Dec 14 13:29:12.352: INFO: Created: latency-svc-pkzmx
Dec 14 13:29:12.359: INFO: Got endpoints: latency-svc-pkzmx [217.927664ms]
Dec 14 13:29:12.362: INFO: Created: latency-svc-q4jr4
Dec 14 13:29:12.370: INFO: Created: latency-svc-sc9q5
Dec 14 13:29:12.372: INFO: Got endpoints: latency-svc-q4jr4 [215.363696ms]
Dec 14 13:29:12.376: INFO: Got endpoints: latency-svc-sc9q5 [200.849108ms]
Dec 14 13:29:12.381: INFO: Created: latency-svc-2l6cc
Dec 14 13:29:12.389: INFO: Created: latency-svc-dg4n2
Dec 14 13:29:12.390: INFO: Got endpoints: latency-svc-2l6cc [196.567811ms]
Dec 14 13:29:12.393: INFO: Got endpoints: latency-svc-dg4n2 [168.637278ms]
Dec 14 13:29:12.399: INFO: Created: latency-svc-p9rdx
Dec 14 13:29:12.407: INFO: Got endpoints: latency-svc-p9rdx [164.210243ms]
Dec 14 13:29:12.408: INFO: Created: latency-svc-kq5w5
Dec 14 13:29:12.413: INFO: Got endpoints: latency-svc-kq5w5 [166.741004ms]
Dec 14 13:29:12.418: INFO: Created: latency-svc-j244f
Dec 14 13:29:12.441: INFO: Created: latency-svc-g89vd
Dec 14 13:29:12.442: INFO: Got endpoints: latency-svc-j244f [180.175399ms]
Dec 14 13:29:12.449: INFO: Created: latency-svc-kqf2q
Dec 14 13:29:12.449: INFO: Got endpoints: latency-svc-g89vd [180.485034ms]
Dec 14 13:29:12.457: INFO: Created: latency-svc-2f2mk
Dec 14 13:29:12.457: INFO: Got endpoints: latency-svc-kqf2q [173.055148ms]
Dec 14 13:29:12.465: INFO: Got endpoints: latency-svc-2f2mk [164.81937ms]
Dec 14 13:29:12.469: INFO: Created: latency-svc-ttksb
Dec 14 13:29:12.476: INFO: Got endpoints: latency-svc-ttksb [170.468788ms]
Dec 14 13:29:12.477: INFO: Created: latency-svc-twhgh
Dec 14 13:29:12.484: INFO: Created: latency-svc-vsvgr
Dec 14 13:29:12.485: INFO: Got endpoints: latency-svc-twhgh [155.069473ms]
Dec 14 13:29:12.491: INFO: Got endpoints: latency-svc-vsvgr [154.205194ms]
Dec 14 13:29:12.501: INFO: Created: latency-svc-z9vfk
Dec 14 13:29:12.525: INFO: Created: latency-svc-bg22k
Dec 14 13:29:12.532: INFO: Created: latency-svc-cs8mf
Dec 14 13:29:12.546: INFO: Got endpoints: latency-svc-z9vfk [195.158346ms]
Dec 14 13:29:12.565: INFO: Created: latency-svc-8d84w
Dec 14 13:29:12.576: INFO: Created: latency-svc-cdm4s
Dec 14 13:29:12.602: INFO: Created: latency-svc-gr6th
Dec 14 13:29:12.603: INFO: Got endpoints: latency-svc-bg22k [244.093153ms]
Dec 14 13:29:12.614: INFO: Created: latency-svc-qdqgp
Dec 14 13:29:12.622: INFO: Created: latency-svc-pb8l5
Dec 14 13:29:12.631: INFO: Created: latency-svc-7z2c8
Dec 14 13:29:12.654: INFO: Created: latency-svc-98xtk
Dec 14 13:29:12.655: INFO: Got endpoints: latency-svc-cs8mf [282.935442ms]
Dec 14 13:29:12.662: INFO: Created: latency-svc-2tkw5
Dec 14 13:29:12.670: INFO: Created: latency-svc-75q66
Dec 14 13:29:12.681: INFO: Created: latency-svc-b72hm
Dec 14 13:29:12.686: INFO: Created: latency-svc-nshm6
Dec 14 13:29:12.692: INFO: Got endpoints: latency-svc-8d84w [316.139702ms]
Dec 14 13:29:12.698: INFO: Created: latency-svc-55k8k
Dec 14 13:29:12.710: INFO: Created: latency-svc-bq8jv
Dec 14 13:29:12.718: INFO: Created: latency-svc-8pd7n
Dec 14 13:29:12.732: INFO: Created: latency-svc-mwdnb
Dec 14 13:29:12.741: INFO: Created: latency-svc-gnpkp
Dec 14 13:29:12.742: INFO: Got endpoints: latency-svc-cdm4s [351.834692ms]
Dec 14 13:29:12.774: INFO: Created: latency-svc-lj5rc
Dec 14 13:29:12.790: INFO: Got endpoints: latency-svc-gr6th [397.492592ms]
Dec 14 13:29:12.806: INFO: Created: latency-svc-jlx5q
Dec 14 13:29:12.844: INFO: Got endpoints: latency-svc-qdqgp [437.242524ms]
Dec 14 13:29:12.856: INFO: Created: latency-svc-5lgvd
Dec 14 13:29:12.895: INFO: Got endpoints: latency-svc-pb8l5 [481.289653ms]
Dec 14 13:29:12.909: INFO: Created: latency-svc-gffb6
Dec 14 13:29:12.939: INFO: Got endpoints: latency-svc-7z2c8 [497.021722ms]
Dec 14 13:29:12.951: INFO: Created: latency-svc-67h22
Dec 14 13:29:12.988: INFO: Got endpoints: latency-svc-98xtk [538.410941ms]
Dec 14 13:29:13.011: INFO: Created: latency-svc-w5xgv
Dec 14 13:29:13.041: INFO: Got endpoints: latency-svc-2tkw5 [584.162433ms]
Dec 14 13:29:13.053: INFO: Created: latency-svc-rfshn
Dec 14 13:29:13.090: INFO: Got endpoints: latency-svc-75q66 [625.132006ms]
Dec 14 13:29:13.101: INFO: Created: latency-svc-zx4bl
Dec 14 13:29:13.153: INFO: Got endpoints: latency-svc-b72hm [677.102199ms]
Dec 14 13:29:13.162: INFO: Created: latency-svc-zcpxk
Dec 14 13:29:13.188: INFO: Got endpoints: latency-svc-nshm6 [702.978042ms]
Dec 14 13:29:13.206: INFO: Created: latency-svc-gv25v
Dec 14 13:29:13.240: INFO: Got endpoints: latency-svc-55k8k [749.676797ms]
Dec 14 13:29:13.252: INFO: Created: latency-svc-fm2nk
Dec 14 13:29:13.291: INFO: Got endpoints: latency-svc-bq8jv [744.632765ms]
Dec 14 13:29:13.307: INFO: Created: latency-svc-8hnr7
Dec 14 13:29:13.341: INFO: Got endpoints: latency-svc-8pd7n [738.534342ms]
Dec 14 13:29:13.356: INFO: Created: latency-svc-cwdwh
Dec 14 13:29:13.391: INFO: Got endpoints: latency-svc-mwdnb [736.203744ms]
Dec 14 13:29:13.408: INFO: Created: latency-svc-p2nhw
Dec 14 13:29:13.442: INFO: Got endpoints: latency-svc-gnpkp [749.769493ms]
Dec 14 13:29:13.462: INFO: Created: latency-svc-pqggq
Dec 14 13:29:13.491: INFO: Got endpoints: latency-svc-lj5rc [749.498976ms]
Dec 14 13:29:13.523: INFO: Created: latency-svc-z9442
Dec 14 13:29:13.541: INFO: Got endpoints: latency-svc-jlx5q [750.716761ms]
Dec 14 13:29:13.552: INFO: Created: latency-svc-jxp4t
Dec 14 13:29:13.588: INFO: Got endpoints: latency-svc-5lgvd [743.863527ms]
Dec 14 13:29:13.601: INFO: Created: latency-svc-r99nc
Dec 14 13:29:13.639: INFO: Got endpoints: latency-svc-gffb6 [744.023199ms]
Dec 14 13:29:13.665: INFO: Created: latency-svc-9fnlz
Dec 14 13:29:13.691: INFO: Got endpoints: latency-svc-67h22 [751.393602ms]
Dec 14 13:29:13.711: INFO: Created: latency-svc-zkks9
Dec 14 13:29:13.742: INFO: Got endpoints: latency-svc-w5xgv [754.478299ms]
Dec 14 13:29:13.760: INFO: Created: latency-svc-pgljf
Dec 14 13:29:13.791: INFO: Got endpoints: latency-svc-rfshn [749.405021ms]
Dec 14 13:29:13.805: INFO: Created: latency-svc-vr4zf
Dec 14 13:29:13.849: INFO: Got endpoints: latency-svc-zx4bl [758.72461ms]
Dec 14 13:29:13.860: INFO: Created: latency-svc-thctc
Dec 14 13:29:13.891: INFO: Got endpoints: latency-svc-zcpxk [737.491459ms]
Dec 14 13:29:13.909: INFO: Created: latency-svc-sfvpc
Dec 14 13:29:13.938: INFO: Got endpoints: latency-svc-gv25v [750.127759ms]
Dec 14 13:29:13.956: INFO: Created: latency-svc-dtm42
Dec 14 13:29:14.001: INFO: Got endpoints: latency-svc-fm2nk [760.547531ms]
Dec 14 13:29:14.018: INFO: Created: latency-svc-rcm7b
Dec 14 13:29:14.041: INFO: Got endpoints: latency-svc-8hnr7 [750.257982ms]
Dec 14 13:29:14.056: INFO: Created: latency-svc-shs2m
Dec 14 13:29:14.089: INFO: Got endpoints: latency-svc-cwdwh [747.557839ms]
Dec 14 13:29:14.102: INFO: Created: latency-svc-bglht
Dec 14 13:29:14.143: INFO: Got endpoints: latency-svc-p2nhw [751.390319ms]
Dec 14 13:29:14.158: INFO: Created: latency-svc-kf8zs
Dec 14 13:29:14.190: INFO: Got endpoints: latency-svc-pqggq [748.19993ms]
Dec 14 13:29:14.204: INFO: Created: latency-svc-xn5j6
Dec 14 13:29:14.241: INFO: Got endpoints: latency-svc-z9442 [749.678262ms]
Dec 14 13:29:14.251: INFO: Created: latency-svc-nq8ks
Dec 14 13:29:14.288: INFO: Got endpoints: latency-svc-jxp4t [746.095655ms]
Dec 14 13:29:14.307: INFO: Created: latency-svc-ffnp6
Dec 14 13:29:14.344: INFO: Got endpoints: latency-svc-r99nc [756.00839ms]
Dec 14 13:29:14.357: INFO: Created: latency-svc-sqgkg
Dec 14 13:29:14.391: INFO: Got endpoints: latency-svc-9fnlz [752.348248ms]
Dec 14 13:29:14.409: INFO: Created: latency-svc-krdcm
Dec 14 13:29:14.453: INFO: Got endpoints: latency-svc-zkks9 [762.265642ms]
Dec 14 13:29:14.479: INFO: Created: latency-svc-54k9m
Dec 14 13:29:14.510: INFO: Got endpoints: latency-svc-pgljf [767.093436ms]
Dec 14 13:29:14.533: INFO: Created: latency-svc-xvkvt
Dec 14 13:29:14.542: INFO: Got endpoints: latency-svc-vr4zf [751.136432ms]
Dec 14 13:29:14.558: INFO: Created: latency-svc-jl97m
Dec 14 13:29:14.589: INFO: Got endpoints: latency-svc-thctc [740.314295ms]
Dec 14 13:29:14.604: INFO: Created: latency-svc-98gf8
Dec 14 13:29:14.640: INFO: Got endpoints: latency-svc-sfvpc [749.331797ms]
Dec 14 13:29:14.652: INFO: Created: latency-svc-stfcr
Dec 14 13:29:14.694: INFO: Got endpoints: latency-svc-dtm42 [755.29344ms]
Dec 14 13:29:14.704: INFO: Created: latency-svc-wwfvl
Dec 14 13:29:14.741: INFO: Got endpoints: latency-svc-rcm7b [739.379785ms]
Dec 14 13:29:14.753: INFO: Created: latency-svc-wm88t
Dec 14 13:29:14.790: INFO: Got endpoints: latency-svc-shs2m [748.49506ms]
Dec 14 13:29:14.799: INFO: Created: latency-svc-r72ln
Dec 14 13:29:14.840: INFO: Got endpoints: latency-svc-bglht [751.014824ms]
Dec 14 13:29:14.851: INFO: Created: latency-svc-4t5vh
Dec 14 13:29:14.894: INFO: Got endpoints: latency-svc-kf8zs [751.768429ms]
Dec 14 13:29:14.912: INFO: Created: latency-svc-fxj56
Dec 14 13:29:14.992: INFO: Got endpoints: latency-svc-xn5j6 [801.545883ms]
Dec 14 13:29:15.004: INFO: Created: latency-svc-wq2tn
Dec 14 13:29:15.048: INFO: Got endpoints: latency-svc-nq8ks [806.511017ms]
Dec 14 13:29:15.059: INFO: Created: latency-svc-pgjxl
Dec 14 13:29:15.096: INFO: Got endpoints: latency-svc-ffnp6 [807.826348ms]
Dec 14 13:29:15.109: INFO: Created: latency-svc-rzx7q
Dec 14 13:29:15.138: INFO: Got endpoints: latency-svc-sqgkg [794.201993ms]
Dec 14 13:29:15.159: INFO: Created: latency-svc-dmnl7
Dec 14 13:29:15.188: INFO: Got endpoints: latency-svc-krdcm [796.835364ms]
Dec 14 13:29:15.205: INFO: Created: latency-svc-sc79b
Dec 14 13:29:15.338: INFO: Got endpoints: latency-svc-54k9m [884.715562ms]
Dec 14 13:29:15.353: INFO: Created: latency-svc-77cvn
Dec 14 13:29:15.390: INFO: Got endpoints: latency-svc-xvkvt [879.748754ms]
Dec 14 13:29:15.413: INFO: Created: latency-svc-mblzj
Dec 14 13:29:15.443: INFO: Got endpoints: latency-svc-jl97m [900.606064ms]
Dec 14 13:29:15.453: INFO: Created: latency-svc-vcmrk
Dec 14 13:29:15.489: INFO: Got endpoints: latency-svc-98gf8 [899.663214ms]
Dec 14 13:29:15.502: INFO: Created: latency-svc-hsxx6
Dec 14 13:29:15.541: INFO: Got endpoints: latency-svc-stfcr [900.770193ms]
Dec 14 13:29:15.550: INFO: Created: latency-svc-96zg8
Dec 14 13:29:15.591: INFO: Got endpoints: latency-svc-wwfvl [897.197776ms]
Dec 14 13:29:15.605: INFO: Created: latency-svc-hwqwt
Dec 14 13:29:15.639: INFO: Got endpoints: latency-svc-wm88t [897.867251ms]
Dec 14 13:29:15.650: INFO: Created: latency-svc-8cd9k
Dec 14 13:29:15.697: INFO: Got endpoints: latency-svc-r72ln [907.480176ms]
Dec 14 13:29:15.709: INFO: Created: latency-svc-qzzzr
Dec 14 13:29:15.737: INFO: Got endpoints: latency-svc-4t5vh [897.093924ms]
Dec 14 13:29:15.767: INFO: Created: latency-svc-jxxgl
Dec 14 13:29:15.793: INFO: Got endpoints: latency-svc-fxj56 [898.838648ms]
Dec 14 13:29:15.814: INFO: Created: latency-svc-8bpm9
Dec 14 13:29:15.840: INFO: Got endpoints: latency-svc-wq2tn [847.981916ms]
Dec 14 13:29:15.850: INFO: Created: latency-svc-mhhm4
Dec 14 13:29:15.893: INFO: Got endpoints: latency-svc-pgjxl [845.539076ms]
Dec 14 13:29:15.902: INFO: Created: latency-svc-2xz86
Dec 14 13:29:15.940: INFO: Got endpoints: latency-svc-rzx7q [844.026281ms]
Dec 14 13:29:15.952: INFO: Created: latency-svc-kkw9w
Dec 14 13:29:15.989: INFO: Got endpoints: latency-svc-dmnl7 [850.609571ms]
Dec 14 13:29:16.003: INFO: Created: latency-svc-prgmv
Dec 14 13:29:16.038: INFO: Got endpoints: latency-svc-sc79b [849.899989ms]
Dec 14 13:29:16.050: INFO: Created: latency-svc-qwq4z
Dec 14 13:29:16.093: INFO: Got endpoints: latency-svc-77cvn [755.036583ms]
Dec 14 13:29:16.105: INFO: Created: latency-svc-wj8zq
Dec 14 13:29:16.140: INFO: Got endpoints: latency-svc-mblzj [750.681654ms]
Dec 14 13:29:16.152: INFO: Created: latency-svc-qbtkz
Dec 14 13:29:16.188: INFO: Got endpoints: latency-svc-vcmrk [744.729472ms]
Dec 14 13:29:16.200: INFO: Created: latency-svc-8xrs4
Dec 14 13:29:16.241: INFO: Got endpoints: latency-svc-hsxx6 [751.786222ms]
Dec 14 13:29:16.252: INFO: Created: latency-svc-hk4br
Dec 14 13:29:16.293: INFO: Got endpoints: latency-svc-96zg8 [751.756763ms]
Dec 14 13:29:16.304: INFO: Created: latency-svc-w796f
Dec 14 13:29:16.349: INFO: Got endpoints: latency-svc-hwqwt [757.652508ms]
Dec 14 13:29:16.359: INFO: Created: latency-svc-rv9f2
Dec 14 13:29:16.389: INFO: Got endpoints: latency-svc-8cd9k [750.528423ms]
Dec 14 13:29:16.400: INFO: Created: latency-svc-8pf8m
Dec 14 13:29:16.438: INFO: Got endpoints: latency-svc-qzzzr [740.373127ms]
Dec 14 13:29:16.464: INFO: Created: latency-svc-zjtvs
Dec 14 13:29:16.493: INFO: Got endpoints: latency-svc-jxxgl [755.65694ms]
Dec 14 13:29:16.504: INFO: Created: latency-svc-nslzr
Dec 14 13:29:16.537: INFO: Got endpoints: latency-svc-8bpm9 [743.958667ms]
Dec 14 13:29:16.553: INFO: Created: latency-svc-4ltqw
Dec 14 13:29:16.590: INFO: Got endpoints: latency-svc-mhhm4 [750.053334ms]
Dec 14 13:29:16.603: INFO: Created: latency-svc-vbdmk
Dec 14 13:29:16.643: INFO: Got endpoints: latency-svc-2xz86 [749.844903ms]
Dec 14 13:29:16.662: INFO: Created: latency-svc-v9fs6
Dec 14 13:29:16.693: INFO: Got endpoints: latency-svc-kkw9w [752.960531ms]
Dec 14 13:29:16.717: INFO: Created: latency-svc-tfg6l
Dec 14 13:29:16.738: INFO: Got endpoints: latency-svc-prgmv [748.413098ms]
Dec 14 13:29:16.770: INFO: Created: latency-svc-zw4pp
Dec 14 13:29:16.793: INFO: Got endpoints: latency-svc-qwq4z [755.201143ms]
Dec 14 13:29:16.803: INFO: Created: latency-svc-tlp6r
Dec 14 13:29:16.838: INFO: Got endpoints: latency-svc-wj8zq [744.928902ms]
Dec 14 13:29:16.850: INFO: Created: latency-svc-9bwd9
Dec 14 13:29:16.898: INFO: Got endpoints: latency-svc-qbtkz [757.582446ms]
Dec 14 13:29:16.909: INFO: Created: latency-svc-zhk2z
Dec 14 13:29:16.942: INFO: Got endpoints: latency-svc-8xrs4 [753.909552ms]
Dec 14 13:29:16.952: INFO: Created: latency-svc-zthcn
Dec 14 13:29:16.989: INFO: Got endpoints: latency-svc-hk4br [748.266394ms]
Dec 14 13:29:17.016: INFO: Created: latency-svc-z4mtq
Dec 14 13:29:17.041: INFO: Got endpoints: latency-svc-w796f [747.210301ms]
Dec 14 13:29:17.053: INFO: Created: latency-svc-4m964
Dec 14 13:29:17.088: INFO: Got endpoints: latency-svc-rv9f2 [739.277521ms]
Dec 14 13:29:17.100: INFO: Created: latency-svc-5h2r5
Dec 14 13:29:17.141: INFO: Got endpoints: latency-svc-8pf8m [751.206434ms]
Dec 14 13:29:17.152: INFO: Created: latency-svc-xz7lr
Dec 14 13:29:17.193: INFO: Got endpoints: latency-svc-zjtvs [754.632039ms]
Dec 14 13:29:17.204: INFO: Created: latency-svc-nc7z2
Dec 14 13:29:17.244: INFO: Got endpoints: latency-svc-nslzr [750.284231ms]
Dec 14 13:29:17.270: INFO: Created: latency-svc-78m2h
Dec 14 13:29:17.289: INFO: Got endpoints: latency-svc-4ltqw [750.985252ms]
Dec 14 13:29:17.302: INFO: Created: latency-svc-h7rx7
Dec 14 13:29:17.342: INFO: Got endpoints: latency-svc-vbdmk [751.370557ms]
Dec 14 13:29:17.363: INFO: Created: latency-svc-5j7b8
Dec 14 13:29:17.442: INFO: Got endpoints: latency-svc-v9fs6 [798.33216ms]
Dec 14 13:29:17.454: INFO: Created: latency-svc-5g2zg
Dec 14 13:29:17.489: INFO: Got endpoints: latency-svc-tfg6l [795.829064ms]
Dec 14 13:29:17.503: INFO: Created: latency-svc-slch7
Dec 14 13:29:17.542: INFO: Got endpoints: latency-svc-zw4pp [804.255978ms]
Dec 14 13:29:17.552: INFO: Created: latency-svc-755v2
Dec 14 13:29:17.593: INFO: Got endpoints: latency-svc-tlp6r [799.594116ms]
Dec 14 13:29:17.603: INFO: Created: latency-svc-42ml5
Dec 14 13:29:17.641: INFO: Got endpoints: latency-svc-9bwd9 [802.663939ms]
Dec 14 13:29:17.652: INFO: Created: latency-svc-ngnqw
Dec 14 13:29:17.693: INFO: Got endpoints: latency-svc-zhk2z [794.960825ms]
Dec 14 13:29:17.703: INFO: Created: latency-svc-4mn4t
Dec 14 13:29:17.742: INFO: Got endpoints: latency-svc-zthcn [800.389019ms]
Dec 14 13:29:17.751: INFO: Created: latency-svc-q7745
Dec 14 13:29:17.793: INFO: Got endpoints: latency-svc-z4mtq [803.973014ms]
Dec 14 13:29:17.808: INFO: Created: latency-svc-jbwxw
Dec 14 13:29:17.843: INFO: Got endpoints: latency-svc-4m964 [801.796542ms]
Dec 14 13:29:17.857: INFO: Created: latency-svc-78dmj
Dec 14 13:29:17.895: INFO: Got endpoints: latency-svc-5h2r5 [807.115653ms]
Dec 14 13:29:17.912: INFO: Created: latency-svc-d48cx
Dec 14 13:29:17.938: INFO: Got endpoints: latency-svc-xz7lr [797.168246ms]
Dec 14 13:29:17.950: INFO: Created: latency-svc-2bdsc
Dec 14 13:29:17.995: INFO: Got endpoints: latency-svc-nc7z2 [801.996105ms]
Dec 14 13:29:18.010: INFO: Created: latency-svc-f7ppp
Dec 14 13:29:18.044: INFO: Got endpoints: latency-svc-78m2h [800.197573ms]
Dec 14 13:29:18.056: INFO: Created: latency-svc-7964l
Dec 14 13:29:18.091: INFO: Got endpoints: latency-svc-h7rx7 [802.405116ms]
Dec 14 13:29:18.118: INFO: Created: latency-svc-5rvjd
Dec 14 13:29:18.137: INFO: Got endpoints: latency-svc-5j7b8 [795.526641ms]
Dec 14 13:29:18.152: INFO: Created: latency-svc-nr6sh
Dec 14 13:29:18.187: INFO: Got endpoints: latency-svc-5g2zg [745.610043ms]
Dec 14 13:29:18.200: INFO: Created: latency-svc-whlf2
Dec 14 13:29:18.244: INFO: Got endpoints: latency-svc-slch7 [754.532061ms]
Dec 14 13:29:18.254: INFO: Created: latency-svc-tqnks
Dec 14 13:29:18.288: INFO: Got endpoints: latency-svc-755v2 [745.553504ms]
Dec 14 13:29:18.301: INFO: Created: latency-svc-vbmwv
Dec 14 13:29:18.340: INFO: Got endpoints: latency-svc-42ml5 [746.770286ms]
Dec 14 13:29:18.355: INFO: Created: latency-svc-8gqwh
Dec 14 13:29:18.390: INFO: Got endpoints: latency-svc-ngnqw [748.856246ms]
Dec 14 13:29:18.403: INFO: Created: latency-svc-qwckm
Dec 14 13:29:18.446: INFO: Got endpoints: latency-svc-4mn4t [753.234395ms]
Dec 14 13:29:18.462: INFO: Created: latency-svc-xct6v
Dec 14 13:29:18.503: INFO: Got endpoints: latency-svc-q7745 [760.607029ms]
Dec 14 13:29:18.517: INFO: Created: latency-svc-ldmb9
Dec 14 13:29:18.540: INFO: Got endpoints: latency-svc-jbwxw [746.784559ms]
Dec 14 13:29:18.567: INFO: Created: latency-svc-w9zqv
Dec 14 13:29:18.591: INFO: Got endpoints: latency-svc-78dmj [748.634381ms]
Dec 14 13:29:18.605: INFO: Created: latency-svc-rrvm6
Dec 14 13:29:18.650: INFO: Got endpoints: latency-svc-d48cx [754.900982ms]
Dec 14 13:29:18.680: INFO: Created: latency-svc-tk2kb
Dec 14 13:29:18.694: INFO: Got endpoints: latency-svc-2bdsc [756.137828ms]
Dec 14 13:29:18.707: INFO: Created: latency-svc-7gbrd
Dec 14 13:29:18.740: INFO: Got endpoints: latency-svc-f7ppp [744.882458ms]
Dec 14 13:29:18.750: INFO: Created: latency-svc-rllr9
Dec 14 13:29:18.791: INFO: Got endpoints: latency-svc-7964l [747.315037ms]
Dec 14 13:29:18.808: INFO: Created: latency-svc-tlm25
Dec 14 13:29:18.843: INFO: Got endpoints: latency-svc-5rvjd [752.233955ms]
Dec 14 13:29:18.852: INFO: Created: latency-svc-fkzdh
Dec 14 13:29:18.890: INFO: Got endpoints: latency-svc-nr6sh [752.95734ms]
Dec 14 13:29:18.900: INFO: Created: latency-svc-8z8zx
Dec 14 13:29:18.937: INFO: Got endpoints: latency-svc-whlf2 [749.701316ms]
Dec 14 13:29:18.953: INFO: Created: latency-svc-d2np9
Dec 14 13:29:18.992: INFO: Got endpoints: latency-svc-tqnks [748.764845ms]
Dec 14 13:29:19.013: INFO: Created: latency-svc-hxnxk
Dec 14 13:29:19.038: INFO: Got endpoints: latency-svc-vbmwv [750.365022ms]
Dec 14 13:29:19.052: INFO: Created: latency-svc-86xfc
Dec 14 13:29:19.102: INFO: Got endpoints: latency-svc-8gqwh [761.851571ms]
Dec 14 13:29:19.112: INFO: Created: latency-svc-cfpc4
Dec 14 13:29:19.137: INFO: Got endpoints: latency-svc-qwckm [747.346521ms]
Dec 14 13:29:19.149: INFO: Created: latency-svc-l5x6w
Dec 14 13:29:19.189: INFO: Got endpoints: latency-svc-xct6v [742.812346ms]
Dec 14 13:29:19.197: INFO: Created: latency-svc-nsmwc
Dec 14 13:29:19.237: INFO: Got endpoints: latency-svc-ldmb9 [734.255982ms]
Dec 14 13:29:19.258: INFO: Created: latency-svc-zwbtb
Dec 14 13:29:19.290: INFO: Got endpoints: latency-svc-w9zqv [749.738419ms]
Dec 14 13:29:19.303: INFO: Created: latency-svc-gfw6j
Dec 14 13:29:19.338: INFO: Got endpoints: latency-svc-rrvm6 [746.612935ms]
Dec 14 13:29:19.353: INFO: Created: latency-svc-5z5z8
Dec 14 13:29:19.395: INFO: Got endpoints: latency-svc-tk2kb [744.282981ms]
Dec 14 13:29:19.406: INFO: Created: latency-svc-9j4t4
Dec 14 13:29:19.440: INFO: Got endpoints: latency-svc-7gbrd [746.087574ms]
Dec 14 13:29:19.453: INFO: Created: latency-svc-5684v
Dec 14 13:29:19.498: INFO: Got endpoints: latency-svc-rllr9 [758.10685ms]
Dec 14 13:29:19.513: INFO: Created: latency-svc-szbz6
Dec 14 13:29:19.541: INFO: Got endpoints: latency-svc-tlm25 [749.8814ms]
Dec 14 13:29:19.555: INFO: Created: latency-svc-s5ssq
Dec 14 13:29:19.587: INFO: Got endpoints: latency-svc-fkzdh [743.820562ms]
Dec 14 13:29:19.609: INFO: Created: latency-svc-tz9nv
Dec 14 13:29:19.644: INFO: Got endpoints: latency-svc-8z8zx [753.339743ms]
Dec 14 13:29:19.654: INFO: Created: latency-svc-xk9h6
Dec 14 13:29:19.693: INFO: Got endpoints: latency-svc-d2np9 [755.527045ms]
Dec 14 13:29:19.704: INFO: Created: latency-svc-vxcqf
Dec 14 13:29:19.738: INFO: Got endpoints: latency-svc-hxnxk [745.921515ms]
Dec 14 13:29:19.765: INFO: Created: latency-svc-g2xgt
Dec 14 13:29:19.796: INFO: Got endpoints: latency-svc-86xfc [757.990007ms]
Dec 14 13:29:19.812: INFO: Created: latency-svc-fl7mp
Dec 14 13:29:19.840: INFO: Got endpoints: latency-svc-cfpc4 [738.318888ms]
Dec 14 13:29:19.855: INFO: Created: latency-svc-phflc
Dec 14 13:29:19.892: INFO: Got endpoints: latency-svc-l5x6w [754.940626ms]
Dec 14 13:29:19.903: INFO: Created: latency-svc-2fcsl
Dec 14 13:29:19.943: INFO: Got endpoints: latency-svc-nsmwc [753.67567ms]
Dec 14 13:29:19.953: INFO: Created: latency-svc-csd4n
Dec 14 13:29:19.988: INFO: Got endpoints: latency-svc-zwbtb [750.488617ms]
Dec 14 13:29:20.010: INFO: Created: latency-svc-5pmmb
Dec 14 13:29:20.044: INFO: Got endpoints: latency-svc-gfw6j [753.844957ms]
Dec 14 13:29:20.058: INFO: Created: latency-svc-7bscs
Dec 14 13:29:20.091: INFO: Got endpoints: latency-svc-5z5z8 [752.577967ms]
Dec 14 13:29:20.140: INFO: Got endpoints: latency-svc-9j4t4 [744.529238ms]
Dec 14 13:29:20.190: INFO: Got endpoints: latency-svc-5684v [749.957743ms]
Dec 14 13:29:20.238: INFO: Got endpoints: latency-svc-szbz6 [739.557853ms]
Dec 14 13:29:20.296: INFO: Got endpoints: latency-svc-s5ssq [754.994044ms]
Dec 14 13:29:20.341: INFO: Got endpoints: latency-svc-tz9nv [753.51952ms]
Dec 14 13:29:20.391: INFO: Got endpoints: latency-svc-xk9h6 [746.784966ms]
Dec 14 13:29:20.440: INFO: Got endpoints: latency-svc-vxcqf [746.680503ms]
Dec 14 13:29:20.488: INFO: Got endpoints: latency-svc-g2xgt [748.993695ms]
Dec 14 13:29:20.540: INFO: Got endpoints: latency-svc-fl7mp [743.524083ms]
Dec 14 13:29:20.593: INFO: Got endpoints: latency-svc-phflc [752.173302ms]
Dec 14 13:29:20.638: INFO: Got endpoints: latency-svc-2fcsl [745.602545ms]
Dec 14 13:29:20.694: INFO: Got endpoints: latency-svc-csd4n [750.817496ms]
Dec 14 13:29:20.741: INFO: Got endpoints: latency-svc-5pmmb [752.679003ms]
Dec 14 13:29:20.794: INFO: Got endpoints: latency-svc-7bscs [750.143599ms]
Dec 14 13:29:20.795: INFO: Latencies: [25.91353ms 33.77845ms 43.263778ms 56.470398ms 62.512392ms 104.627207ms 111.647166ms 127.402344ms 137.338006ms 143.498585ms 154.205194ms 155.069473ms 159.604813ms 164.210243ms 164.81937ms 166.741004ms 168.637278ms 170.468788ms 173.055148ms 178.059321ms 180.175399ms 180.485034ms 195.158346ms 195.638792ms 196.567811ms 200.849108ms 204.212731ms 211.591527ms 215.363696ms 216.540618ms 217.927664ms 221.648628ms 224.356097ms 226.835859ms 229.46406ms 230.059293ms 230.965978ms 239.846466ms 244.093153ms 245.235006ms 282.935442ms 316.139702ms 351.834692ms 397.492592ms 437.242524ms 481.289653ms 497.021722ms 538.410941ms 584.162433ms 625.132006ms 677.102199ms 702.978042ms 734.255982ms 736.203744ms 737.491459ms 738.318888ms 738.534342ms 739.277521ms 739.379785ms 739.557853ms 740.314295ms 740.373127ms 742.812346ms 743.524083ms 743.820562ms 743.863527ms 743.958667ms 744.023199ms 744.282981ms 744.529238ms 744.632765ms 744.729472ms 744.882458ms 744.928902ms 745.553504ms 745.602545ms 745.610043ms 745.921515ms 746.087574ms 746.095655ms 746.612935ms 746.680503ms 746.770286ms 746.784559ms 746.784966ms 747.210301ms 747.315037ms 747.346521ms 747.557839ms 748.19993ms 748.266394ms 748.413098ms 748.49506ms 748.634381ms 748.764845ms 748.856246ms 748.993695ms 749.331797ms 749.405021ms 749.498976ms 749.676797ms 749.678262ms 749.701316ms 749.738419ms 749.769493ms 749.844903ms 749.8814ms 749.957743ms 750.053334ms 750.127759ms 750.143599ms 750.257982ms 750.284231ms 750.365022ms 750.488617ms 750.528423ms 750.681654ms 750.716761ms 750.817496ms 750.985252ms 751.014824ms 751.136432ms 751.206434ms 751.370557ms 751.390319ms 751.393602ms 751.756763ms 751.768429ms 751.786222ms 752.173302ms 752.233955ms 752.348248ms 752.577967ms 752.679003ms 752.95734ms 752.960531ms 753.234395ms 753.339743ms 753.51952ms 753.67567ms 753.844957ms 753.909552ms 754.478299ms 754.532061ms 754.632039ms 754.900982ms 754.940626ms 754.994044ms 755.036583ms 755.201143ms 755.29344ms 755.527045ms 755.65694ms 756.00839ms 756.137828ms 757.582446ms 757.652508ms 757.990007ms 758.10685ms 758.72461ms 760.547531ms 760.607029ms 761.851571ms 762.265642ms 767.093436ms 794.201993ms 794.960825ms 795.526641ms 795.829064ms 796.835364ms 797.168246ms 798.33216ms 799.594116ms 800.197573ms 800.389019ms 801.545883ms 801.796542ms 801.996105ms 802.405116ms 802.663939ms 803.973014ms 804.255978ms 806.511017ms 807.115653ms 807.826348ms 844.026281ms 845.539076ms 847.981916ms 849.899989ms 850.609571ms 879.748754ms 884.715562ms 897.093924ms 897.197776ms 897.867251ms 898.838648ms 899.663214ms 900.606064ms 900.770193ms 907.480176ms]
Dec 14 13:29:20.795: INFO: 50 %ile: 749.676797ms
Dec 14 13:29:20.795: INFO: 90 %ile: 803.973014ms
Dec 14 13:29:20.795: INFO: 99 %ile: 900.770193ms
Dec 14 13:29:20.795: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:20.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3171" for this suite.


• [SLOW TEST:12.063 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":-1,"completed":24,"skipped":538,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:18.550: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:29:19.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:29:21.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549359, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549359, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549359, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549359, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:29:24.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:24.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9525" for this suite.
STEP: Destroying namespace "webhook-9525-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":-1,"completed":28,"skipped":582,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:25.000: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Dec 14 13:29:25.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 cluster-info'
Dec 14 13:29:25.273: INFO: stderr: ""
Dec 14 13:29:25.273: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:25.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2684" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":-1,"completed":29,"skipped":587,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:21.068: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:25.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8506" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":-1,"completed":25,"skipped":669,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:18.758: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 14 13:29:19.986: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:29:20.028: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:29:22.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549360, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549360, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549360, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549360, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:29:25.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:26.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8230" for this suite.
STEP: Destroying namespace "webhook-8230-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:7.561 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-130
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-130
STEP: creating replication controller externalsvc in namespace services-130
I1214 13:29:14.478805      32 runners.go:190] Created replication controller with name: externalsvc, namespace: services-130, replica count: 2
I1214 13:29:17.529631      32 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 14 13:29:17.571: INFO: Creating new exec pod
Dec 14 13:29:21.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-130 execpodvjj5n -- /bin/sh -x -c nslookup nodeport-service.services-130.svc.cluster.local'
Dec 14 13:29:21.963: INFO: stderr: "+ nslookup nodeport-service.services-130.svc.cluster.local\n"
Dec 14 13:29:21.964: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-130.svc.cluster.local\tcanonical name = externalsvc.services-130.svc.cluster.local.\nName:\texternalsvc.services-130.svc.cluster.local\nAddress: 10.254.196.254\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-130, will wait for the garbage collector to delete the pods
Dec 14 13:29:22.029: INFO: Deleting ReplicationController externalsvc took: 11.732981ms
Dec 14 13:29:22.130: INFO: Terminating ReplicationController externalsvc pods took: 100.913762ms
Dec 14 13:29:28.766: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:28.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-130" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:14.434 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":-1,"completed":25,"skipped":377,"failed":0}

S
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:25.317: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6604" for this suite.


• [SLOW TEST:5.102 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":-1,"completed":30,"skipped":600,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:25.595: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:25.632: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:32.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6996" for this suite.


• [SLOW TEST:6.580 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":-1,"completed":26,"skipped":675,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:32.198: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:29:32.328: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 13:29:32.334: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:29:32.350: INFO: waiting for watch events with expected annotations
Dec 14 13:29:32.350: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:32.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3881" for this suite.

•
------------------------------
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":-1,"completed":27,"skipped":685,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:30.505: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:34.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1682" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":643,"failed":0}

S
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:27:27.492: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4789
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Dec 14 13:27:27.577: INFO: Found 0 stateful pods, waiting for 3
Dec 14 13:27:37.584: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:27:37.585: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:27:37.585: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 14 13:27:47.587: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:27:47.587: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:27:47.587: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 14 13:27:47.625: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 14 13:27:57.669: INFO: Updating stateful set ss2
Dec 14 13:27:57.677: INFO: Waiting for Pod statefulset-4789/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:28:07.685: INFO: Waiting for Pod statefulset-4789/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 14 13:28:17.775: INFO: Found 2 stateful pods, waiting for 3
Dec 14 13:28:27.781: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:28:27.781: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:28:27.781: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 14 13:28:27.811: INFO: Updating stateful set ss2
Dec 14 13:28:27.833: INFO: Waiting for Pod statefulset-4789/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:28:37.863: INFO: Updating stateful set ss2
Dec 14 13:28:37.876: INFO: Waiting for StatefulSet statefulset-4789/ss2 to complete update
Dec 14 13:28:37.876: INFO: Waiting for Pod statefulset-4789/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 14 13:28:47.892: INFO: Waiting for StatefulSet statefulset-4789/ss2 to complete update
Dec 14 13:28:47.893: INFO: Waiting for Pod statefulset-4789/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:28:57.887: INFO: Deleting all statefulset in ns statefulset-4789
Dec 14 13:28:57.890: INFO: Scaling statefulset ss2 to 0
Dec 14 13:29:37.907: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:29:37.912: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:37.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4789" for this suite.


• [SLOW TEST:130.449 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":-1,"completed":39,"skipped":684,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:34.606: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 14 13:29:39.233: INFO: Successfully updated pod "labelsupdatedee7d643-6348-4ae3-ae8b-753629f28f37"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:41.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7661" for this suite.


• [SLOW TEST:6.672 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":644,"failed":0}
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:41.280: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 14 13:29:43.345: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8471 PodName:pod-sharedvolume-91a7e558-0bc3-44d8-8761-86c8f4452913 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:29:43.345: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
Dec 14 13:29:43.529: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:43.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8471" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":-1,"completed":33,"skipped":644,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:43.572: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6815" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":-1,"completed":34,"skipped":651,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:37.958: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3296
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3296
I1214 13:29:38.029953      29 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3296, replica count: 2
I1214 13:29:41.080659      29 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:29:41.080: INFO: Creating new exec pod
Dec 14 13:29:46.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-3296 execpodp6xkz -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 14 13:29:46.596: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:29:46.596: INFO: stdout: ""
Dec 14 13:29:46.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 exec --namespace=services-3296 execpodp6xkz -- /bin/sh -x -c nc -zv -t -w 2 10.254.43.62 80'
Dec 14 13:29:47.042: INFO: stderr: "+ nc -zv -t -w 2 10.254.43.62 80\nConnection to 10.254.43.62 80 port [tcp/http] succeeded!\n"
Dec 14 13:29:47.042: INFO: stdout: ""
Dec 14 13:29:47.042: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3296" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:9.144 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":-1,"completed":40,"skipped":689,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:32.525: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-4504
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4504 to expose endpoints map[]
Dec 14 13:29:32.585: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:33.589: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:34.588: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:35.589: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:36.588: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:37.590: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:38.588: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:39.589: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:40.589: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 13:29:41.595: INFO: successfully validated that service endpoint-test2 in namespace services-4504 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4504
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4504 to expose endpoints map[pod1:[80]]
Dec 14 13:29:44.624: INFO: successfully validated that service endpoint-test2 in namespace services-4504 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4504
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4504 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 14 13:29:47.653: INFO: successfully validated that service endpoint-test2 in namespace services-4504 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4504
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4504 to expose endpoints map[pod2:[80]]
Dec 14 13:29:47.676: INFO: successfully validated that service endpoint-test2 in namespace services-4504 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4504
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4504 to expose endpoints map[]
Dec 14 13:29:47.700: INFO: successfully validated that service endpoint-test2 in namespace services-4504 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:47.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4504" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:15.217 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":-1,"completed":28,"skipped":733,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:47.203: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-19eec6dc-ae76-44c5-b63e-0dc56d9bdb3d
STEP: Creating a pod to test consume configMaps
Dec 14 13:29:47.266: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22" in namespace "projected-3888" to be "Succeeded or Failed"
Dec 14 13:29:47.270: INFO: Pod "pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136685ms
Dec 14 13:29:49.289: INFO: Pod "pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022900773s
Dec 14 13:29:51.294: INFO: Pod "pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027827105s
STEP: Saw pod success
Dec 14 13:29:51.294: INFO: Pod "pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22" satisfied condition "Succeeded or Failed"
Dec 14 13:29:51.297: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:29:51.319: INFO: Waiting for pod pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22 to disappear
Dec 14 13:29:51.323: INFO: Pod pod-projected-configmaps-1b57a3ad-ea3c-42df-83dc-1e5275a4fb22 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:51.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3888" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":41,"skipped":724,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:51.404: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:51.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9654" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":-1,"completed":42,"skipped":750,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:51.558: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:51.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8328" for this suite.

•
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":-1,"completed":43,"skipped":763,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:43.640: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:51.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7477" for this suite.


• [SLOW TEST:8.067 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":-1,"completed":35,"skipped":656,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:51.793: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:51.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8568" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

•
------------------------------
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":-1,"completed":36,"skipped":690,"failed":0}

S
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":-1,"completed":13,"skipped":222,"failed":0}
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:26.323: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1445
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:29:26.382: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:29:26.416: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:28.431: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:30.423: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:32.422: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:34.421: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:36.421: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:38.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:40.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:42.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:44.421: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:46.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:48.424: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:29:48.439: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:29:52.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.227.132:8080/dial?request=hostname&protocol=udp&host=10.100.196.185&port=8081&tries=1'] Namespace:pod-network-test-1445 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:29:52.470: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
Dec 14 13:29:52.704: INFO: Waiting for responses: map[]
Dec 14 13:29:52.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.227.132:8080/dial?request=hostname&protocol=udp&host=10.100.227.168&port=8081&tries=1'] Namespace:pod-network-test-1445 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:29:52.707: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
Dec 14 13:29:52.924: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1445" for this suite.


• [SLOW TEST:26.614 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":-1,"completed":14,"skipped":222,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:47.760: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 14 13:29:52.371: INFO: Successfully updated pod "annotationupdated2a6bf6f-f847-4bde-9b82-2e3e8808fa7a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:54.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2054" for this suite.


• [SLOW TEST:6.654 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":738,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:52.977: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:29:53.025: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 13:29:58.032: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 13:29:58.032: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 14 13:29:58.065: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9530 /apis/apps/v1/namespaces/deployment-9530/deployments/test-cleanup-deployment b4bc4e28-c3a9-44e3-a0ee-0aed5da2611d 21184 1 2020-12-14 13:29:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-12-14 13:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035400f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 13:29:58.074: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-9530 /apis/apps/v1/namespaces/deployment-9530/replicasets/test-cleanup-deployment-5d446bdd47 8e7d5d16-1ed3-4e22-bed3-d9931c27a7dc 21187 1 2020-12-14 13:29:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b4bc4e28-c3a9-44e3-a0ee-0aed5da2611d 0xc0035405d7 0xc0035405d8}] []  [{kube-controller-manager Update apps/v1 2020-12-14 13:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4bc4e28-c3a9-44e3-a0ee-0aed5da2611d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003540668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:29:58.074: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 14 13:29:58.074: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9530 /apis/apps/v1/namespaces/deployment-9530/replicasets/test-cleanup-controller d5b10b9a-9d87-4c0c-8469-4e4adda55fe4 21185 1 2020-12-14 13:29:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b4bc4e28-c3a9-44e3-a0ee-0aed5da2611d 0xc0035404c7 0xc0035404c8}] []  [{e2e.test Update apps/v1 2020-12-14 13:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-14 13:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b4bc4e28-c3a9-44e3-a0ee-0aed5da2611d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003540568 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:29:58.087: INFO: Pod "test-cleanup-controller-p9ttc" is available:
&Pod{ObjectMeta:{test-cleanup-controller-p9ttc test-cleanup-controller- deployment-9530 /api/v1/namespaces/deployment-9530/pods/test-cleanup-controller-p9ttc 040b0457-a93c-43f1-8ddd-6dc66d16e463 21077 0 2020-12-14 13:29:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.100.196.146/32 cni.projectcalico.org/podIPs:10.100.196.146/32] [{apps/v1 ReplicaSet test-cleanup-controller d5b10b9a-9d87-4c0c-8469-4e4adda55fe4 0xc003608b37 0xc003608b38}] []  [{kube-controller-manager Update v1 2020-12-14 13:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5b10b9a-9d87-4c0c-8469-4e4adda55fe4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-14 13:29:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-14 13:29:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.196.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d44zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d44zh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d44zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:29:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.200,PodIP:10.100.196.146,StartTime:2020-12-14 13:29:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-14 13:29:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c5ec8cdef73a762408f24da2567f8ba9cc6bce0f9cbf058a2dbac7812338fc89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.196.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:29:58.087: INFO: Pod "test-cleanup-deployment-5d446bdd47-jljm2" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-jljm2 test-cleanup-deployment-5d446bdd47- deployment-9530 /api/v1/namespaces/deployment-9530/pods/test-cleanup-deployment-5d446bdd47-jljm2 154a0584-4e7e-45e7-a13a-132dd565514c 21191 0 2020-12-14 13:29:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 8e7d5d16-1ed3-4e22-bed3-d9931c27a7dc 0xc003608cd7 0xc003608cd8}] []  [{kube-controller-manager Update v1 2020-12-14 13:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e7d5d16-1ed3-4e22-bed3-d9931c27a7dc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d44zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d44zh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d44zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-v1-19-pwlsgdsa6hrh-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-14 13:29:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:29:58.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9530" for this suite.


• [SLOW TEST:5.139 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":-1,"completed":15,"skipped":236,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:51.690: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 14 13:29:51.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-1049'
Dec 14 13:29:51.961: INFO: stderr: ""
Dec 14 13:29:51.961: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 14 13:29:57.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get pod e2e-test-httpd-pod --namespace=kubectl-1049 -o json'
Dec 14 13:29:57.256: INFO: stderr: ""
Dec 14 13:29:57.256: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.100.196.148/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.196.148/32\"\n        },\n        \"creationTimestamp\": \"2020-12-14T13:29:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-14T13:29:51Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-14T13:29:53Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.100.196.148\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-14T13:29:54Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1049\",\n        \"resourceVersion\": \"21019\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1049/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ee1baef8-b4f5-4a7a-96be-f22a98cfaec6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fpvcv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance-v1-19-pwlsgdsa6hrh-node-0\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fpvcv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fpvcv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:29:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:29:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-14T13:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9cf1303e803f9ddd1b9dc1e37c33cb45b4771c8fcd604421e6f6a19704980c70\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-12-14T13:29:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.200\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.196.148\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.196.148\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-14T13:29:51Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 14 13:29:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 replace -f - --namespace=kubectl-1049'
Dec 14 13:29:57.864: INFO: stderr: ""
Dec 14 13:29:57.864: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Dec 14 13:29:57.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 delete pods e2e-test-httpd-pod --namespace=kubectl-1049'
Dec 14 13:30:00.850: INFO: stderr: ""
Dec 14 13:30:00.851: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:00.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1049" for this suite.


• [SLOW TEST:9.179 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":-1,"completed":44,"skipped":774,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:28.816: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Dec 14 13:29:28.860: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:03.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6012" for this suite.


• [SLOW TEST:35.155 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:54.431: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:05.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3767" for this suite.


• [SLOW TEST:11.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":-1,"completed":30,"skipped":741,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Dec 14 13:30:00.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 create -f - --namespace=kubectl-4287'
Dec 14 13:30:01.554: INFO: stderr: ""
Dec 14 13:30:01.554: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 13:30:02.561: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:02.561: INFO: Found 0 / 1
Dec 14 13:30:03.560: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:03.560: INFO: Found 0 / 1
Dec 14 13:30:04.560: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:04.560: INFO: Found 0 / 1
Dec 14 13:30:05.558: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:05.559: INFO: Found 1 / 1
Dec 14 13:30:05.559: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 14 13:30:05.562: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:05.562: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 13:30:05.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 patch pod agnhost-primary-4wm65 --namespace=kubectl-4287 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 13:30:05.774: INFO: stderr: ""
Dec 14 13:30:05.775: INFO: stdout: "pod/agnhost-primary-4wm65 patched\n"
STEP: checking annotations
Dec 14 13:30:05.786: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:05.786: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:05.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4287" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":-1,"completed":45,"skipped":778,"failed":0}

SS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:05.813: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Dec 14 13:30:05.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 create -f -'
Dec 14 13:30:06.322: INFO: stderr: ""
Dec 14 13:30:06.322: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 14 13:30:06.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 diff -f -'
Dec 14 13:30:07.092: INFO: rc: 1
Dec 14 13:30:07.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 delete -f -'
Dec 14 13:30:07.325: INFO: stderr: ""
Dec 14 13:30:07.325: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:07.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-525" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":-1,"completed":46,"skipped":780,"failed":0}

SSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":-1,"completed":26,"skipped":378,"failed":0}
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:03.975: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:08.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2777" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":-1,"completed":27,"skipped":378,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:38.038: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7459
STEP: creating service affinity-nodeport-transition in namespace services-7459
STEP: creating replication controller affinity-nodeport-transition in namespace services-7459
I1214 13:28:38.107718      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7459, replica count: 3
I1214 13:28:41.158526      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:28:44.159070      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:28:44.173: INFO: Creating new exec pod
Dec 14 13:28:49.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Dec 14 13:28:49.654: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 14 13:28:49.654: INFO: stdout: ""
Dec 14 13:28:49.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 10.254.125.212 80'
Dec 14 13:28:50.117: INFO: stderr: "+ nc -zv -t -w 2 10.254.125.212 80\nConnection to 10.254.125.212 80 port [tcp/http] succeeded!\n"
Dec 14 13:28:50.117: INFO: stdout: ""
Dec 14 13:28:50.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.200 31193'
Dec 14 13:28:50.574: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.200 31193\nConnection to 10.0.0.200 31193 port [tcp/31193] succeeded!\n"
Dec 14 13:28:50.574: INFO: stdout: ""
Dec 14 13:28:50.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.55 31193'
Dec 14 13:28:51.148: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.55 31193\nConnection to 10.0.0.55 31193 port [tcp/31193] succeeded!\n"
Dec 14 13:28:51.148: INFO: stdout: ""
Dec 14 13:28:51.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.93 31193'
Dec 14 13:28:51.716: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.93 31193\nConnection to 38.108.68.93 31193 port [tcp/31193] succeeded!\n"
Dec 14 13:28:51.717: INFO: stdout: ""
Dec 14 13:28:51.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.247 31193'
Dec 14 13:28:52.289: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.247 31193\nConnection to 38.108.68.247 31193 port [tcp/31193] succeeded!\n"
Dec 14 13:28:52.289: INFO: stdout: ""
Dec 14 13:28:52.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:31193/ ; done'
Dec 14 13:28:52.986: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n"
Dec 14 13:28:52.986: INFO: stdout: "\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59"
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:28:52.986: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:22.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:31193/ ; done'
Dec 14 13:29:23.461: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n"
Dec 14 13:29:23.461: INFO: stdout: "\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg"
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:23.461: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:23.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:31193/ ; done'
Dec 14 13:29:24.054: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n"
Dec 14 13:29:24.054: INFO: stdout: "\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-fqtdc\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-q9w59\naffinity-nodeport-transition-fqtdc"
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-q9w59
Dec 14 13:29:24.054: INFO: Received response from host: affinity-nodeport-transition-fqtdc
Dec 14 13:29:54.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7459 execpod-affinity669px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:31193/ ; done'
Dec 14 13:29:54.663: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:31193/\n"
Dec 14 13:29:54.664: INFO: stdout: "\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg\naffinity-nodeport-transition-npjlg"
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.664: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Received response from host: affinity-nodeport-transition-npjlg
Dec 14 13:29:54.665: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7459, will wait for the garbage collector to delete the pods
Dec 14 13:29:54.747: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.332502ms
Dec 14 13:29:54.847: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.272402ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:08.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7459" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:90.751 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":38,"skipped":753,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:05.605: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:05.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6" in namespace "projected-335" to be "Succeeded or Failed"
Dec 14 13:30:05.643: INFO: Pod "downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973959ms
Dec 14 13:30:07.648: INFO: Pod "downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007902229s
Dec 14 13:30:09.651: INFO: Pod "downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011496218s
STEP: Saw pod success
Dec 14 13:30:09.651: INFO: Pod "downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6" satisfied condition "Succeeded or Failed"
Dec 14 13:30:09.658: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6 container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:09.694: INFO: Waiting for pod downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6 to disappear
Dec 14 13:30:09.706: INFO: Pod downwardapi-volume-ceab1fac-1cf8-4f81-8608-7a6551b806f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:09.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-335" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":782,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 13:30:06.380: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 13:30:06.385: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 13:30:08.386: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 13:30:08.390: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 13:30:10.385: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 13:30:10.388: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:10.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6222" for this suite.


• [SLOW TEST:12.164 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":-1,"completed":16,"skipped":274,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:29:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3036
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3036
STEP: Creating statefulset with conflicting port in namespace statefulset-3036
STEP: Waiting until pod test-pod will start running in namespace statefulset-3036
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3036
Dec 14 13:29:55.936: INFO: Observed stateful pod in namespace: statefulset-3036, name: ss-0, uid: 7439f7d6-3851-44d8-af50-6cc9b0e39d9d, status phase: Pending. Waiting for statefulset controller to delete.
Dec 14 13:29:56.317: INFO: Observed stateful pod in namespace: statefulset-3036, name: ss-0, uid: 7439f7d6-3851-44d8-af50-6cc9b0e39d9d, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 13:29:56.322: INFO: Observed stateful pod in namespace: statefulset-3036, name: ss-0, uid: 7439f7d6-3851-44d8-af50-6cc9b0e39d9d, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 13:29:56.327: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3036
STEP: Removing pod with conflicting port in namespace statefulset-3036
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3036 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:30:02.376: INFO: Deleting all statefulset in ns statefulset-3036
Dec 14 13:30:02.380: INFO: Scaling statefulset ss to 0
Dec 14 13:30:12.399: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:30:12.402: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:12.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3036" for this suite.


• [SLOW TEST:20.592 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":-1,"completed":37,"skipped":691,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:07.393: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:07.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb" in namespace "downward-api-840" to be "Succeeded or Failed"
Dec 14 13:30:07.463: INFO: Pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.156294ms
Dec 14 13:30:09.469: INFO: Pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015956703s
Dec 14 13:30:11.482: INFO: Pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029399953s
Dec 14 13:30:13.486: INFO: Pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033716361s
STEP: Saw pod success
Dec 14 13:30:13.487: INFO: Pod "downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb" satisfied condition "Succeeded or Failed"
Dec 14 13:30:13.489: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:13.531: INFO: Waiting for pod downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb to disappear
Dec 14 13:30:13.535: INFO: Pod downwardapi-volume-3bf893d8-e982-4a79-8407-ef5b61a0adbb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:13.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-840" for this suite.


• [SLOW TEST:6.153 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":47,"skipped":792,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:12.458: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 13:30:12.515: INFO: Waiting up to 5m0s for pod "pod-2329c324-27a8-4db7-8d0b-a3277f580c1d" in namespace "emptydir-2858" to be "Succeeded or Failed"
Dec 14 13:30:12.532: INFO: Pod "pod-2329c324-27a8-4db7-8d0b-a3277f580c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.627896ms
Dec 14 13:30:14.535: INFO: Pod "pod-2329c324-27a8-4db7-8d0b-a3277f580c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019817221s
Dec 14 13:30:16.539: INFO: Pod "pod-2329c324-27a8-4db7-8d0b-a3277f580c1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023896671s
STEP: Saw pod success
Dec 14 13:30:16.540: INFO: Pod "pod-2329c324-27a8-4db7-8d0b-a3277f580c1d" satisfied condition "Succeeded or Failed"
Dec 14 13:30:16.542: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-2329c324-27a8-4db7-8d0b-a3277f580c1d container test-container: <nil>
STEP: delete the pod
Dec 14 13:30:16.572: INFO: Waiting for pod pod-2329c324-27a8-4db7-8d0b-a3277f580c1d to disappear
Dec 14 13:30:16.576: INFO: Pod pod-2329c324-27a8-4db7-8d0b-a3277f580c1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:16.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2858" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":701,"failed":0}

SS
------------------------------
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:08.155: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 14 13:30:12.732: INFO: Successfully updated pod "adopt-release-kdzn2"
STEP: Checking that the Job readopts the Pod
Dec 14 13:30:12.732: INFO: Waiting up to 15m0s for pod "adopt-release-kdzn2" in namespace "job-6123" to be "adopted"
Dec 14 13:30:12.739: INFO: Pod "adopt-release-kdzn2": Phase="Running", Reason="", readiness=true. Elapsed: 6.831159ms
Dec 14 13:30:14.749: INFO: Pod "adopt-release-kdzn2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017359504s
Dec 14 13:30:14.749: INFO: Pod "adopt-release-kdzn2" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 14 13:30:15.265: INFO: Successfully updated pod "adopt-release-kdzn2"
STEP: Checking that the Job releases the Pod
Dec 14 13:30:15.265: INFO: Waiting up to 15m0s for pod "adopt-release-kdzn2" in namespace "job-6123" to be "released"
Dec 14 13:30:15.275: INFO: Pod "adopt-release-kdzn2": Phase="Running", Reason="", readiness=true. Elapsed: 9.401724ms
Dec 14 13:30:17.282: INFO: Pod "adopt-release-kdzn2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016488728s
Dec 14 13:30:17.282: INFO: Pod "adopt-release-kdzn2" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:17.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6123" for this suite.


• [SLOW TEST:9.139 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":-1,"completed":28,"skipped":391,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:13.641: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:13.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2" in namespace "downward-api-3781" to be "Succeeded or Failed"
Dec 14 13:30:13.695: INFO: Pod "downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.940614ms
Dec 14 13:30:15.699: INFO: Pod "downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011234708s
Dec 14 13:30:17.705: INFO: Pod "downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017111187s
STEP: Saw pod success
Dec 14 13:30:17.705: INFO: Pod "downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2" satisfied condition "Succeeded or Failed"
Dec 14 13:30:17.707: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2 container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:17.731: INFO: Waiting for pod downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2 to disappear
Dec 14 13:30:17.734: INFO: Pod downwardapi-volume-6a0e78f2-5da4-4346-acf6-ce5919dc6eb2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3781" for this suite.

•
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:09.765: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8473.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8473.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:30:13.864: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf: the server could not find the requested resource (get pods dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf)
Dec 14 13:30:13.868: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf: the server could not find the requested resource (get pods dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf)
Dec 14 13:30:13.881: INFO: Unable to read jessie_udp@PodARecord from pod dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf: the server could not find the requested resource (get pods dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf)
Dec 14 13:30:13.887: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf: the server could not find the requested resource (get pods dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf)
Dec 14 13:30:13.887: INFO: Lookups using dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:30:18.918: INFO: DNS probes using dns-8473/dns-test-1bd433fe-ff4f-454d-a9ae-62101cacddaf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:18.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8473" for this suite.


• [SLOW TEST:9.193 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":-1,"completed":32,"skipped":792,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:08.861: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:30:14.944: INFO: Waiting up to 5m0s for pod "client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d" in namespace "pods-7369" to be "Succeeded or Failed"
Dec 14 13:30:14.954: INFO: Pod "client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.75367ms
Dec 14 13:30:16.959: INFO: Pod "client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0148259s
Dec 14 13:30:18.964: INFO: Pod "client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020217657s
STEP: Saw pod success
Dec 14 13:30:18.964: INFO: Pod "client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d" satisfied condition "Succeeded or Failed"
Dec 14 13:30:18.968: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d container env3cont: <nil>
STEP: delete the pod
Dec 14 13:30:18.987: INFO: Waiting for pod client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d to disappear
Dec 14 13:30:18.990: INFO: Pod client-envvars-c94f91aa-dcb2-4050-92b1-fe30149cd57d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:18.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7369" for this suite.


• [SLOW TEST:10.139 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":776,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:18.995: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Dec 14 13:30:19.035: INFO: created test-pod-1
Dec 14 13:30:19.045: INFO: created test-pod-2
Dec 14 13:30:19.058: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:19.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-88" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":-1,"completed":33,"skipped":803,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:16.594: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:16.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1" in namespace "downward-api-9425" to be "Succeeded or Failed"
Dec 14 13:30:16.639: INFO: Pod "downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.825353ms
Dec 14 13:30:18.654: INFO: Pod "downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02308253s
Dec 14 13:30:20.666: INFO: Pod "downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035350377s
STEP: Saw pod success
Dec 14 13:30:20.666: INFO: Pod "downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1" satisfied condition "Succeeded or Failed"
Dec 14 13:30:20.669: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1 container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:20.692: INFO: Waiting for pod downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1 to disappear
Dec 14 13:30:20.696: INFO: Pod downwardapi-volume-0d6c974d-4aa5-4bf6-93b4-dffdf483f7c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:20.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9425" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":703,"failed":0}
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:20.711: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:30:23.802: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:23.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5650" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":40,"skipped":703,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:23.909: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:30:24.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:30:26.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549424, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549424, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549424, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549424, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:30:29.626: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1844" for this suite.
STEP: Destroying namespace "webhook-1844-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:5.881 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":-1,"completed":41,"skipped":737,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:29.827: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-284" for this suite.


• [SLOW TEST:11.149 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":-1,"completed":42,"skipped":748,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:19.075: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-klm9
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:30:19.143: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-klm9" in namespace "subpath-1614" to be "Succeeded or Failed"
Dec 14 13:30:19.156: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.042178ms
Dec 14 13:30:21.161: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017534431s
Dec 14 13:30:23.168: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 4.024683965s
Dec 14 13:30:25.174: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 6.03042092s
Dec 14 13:30:27.180: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 8.036793656s
Dec 14 13:30:29.185: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 10.041862482s
Dec 14 13:30:31.189: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 12.046162154s
Dec 14 13:30:33.195: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 14.051411623s
Dec 14 13:30:35.203: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 16.059662502s
Dec 14 13:30:37.207: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 18.063594267s
Dec 14 13:30:39.211: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 20.068180466s
Dec 14 13:30:41.216: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Running", Reason="", readiness=true. Elapsed: 22.072268929s
Dec 14 13:30:43.221: INFO: Pod "pod-subpath-test-configmap-klm9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.077382759s
STEP: Saw pod success
Dec 14 13:30:43.221: INFO: Pod "pod-subpath-test-configmap-klm9" satisfied condition "Succeeded or Failed"
Dec 14 13:30:43.223: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-subpath-test-configmap-klm9 container test-container-subpath-configmap-klm9: <nil>
STEP: delete the pod
Dec 14 13:30:43.249: INFO: Waiting for pod pod-subpath-test-configmap-klm9 to disappear
Dec 14 13:30:43.254: INFO: Pod pod-subpath-test-configmap-klm9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-klm9
Dec 14 13:30:43.254: INFO: Deleting pod "pod-subpath-test-configmap-klm9" in namespace "subpath-1614"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:43.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1614" for this suite.


• [SLOW TEST:24.192 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:40.987: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Dec 14 13:30:41.042: INFO: Waiting up to 5m0s for pod "client-containers-0a6663df-3812-45d1-be01-8ee8eb698478" in namespace "containers-7956" to be "Succeeded or Failed"
Dec 14 13:30:41.045: INFO: Pod "client-containers-0a6663df-3812-45d1-be01-8ee8eb698478": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316049ms
Dec 14 13:30:43.050: INFO: Pod "client-containers-0a6663df-3812-45d1-be01-8ee8eb698478": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007556589s
Dec 14 13:30:45.069: INFO: Pod "client-containers-0a6663df-3812-45d1-be01-8ee8eb698478": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026604397s
STEP: Saw pod success
Dec 14 13:30:45.069: INFO: Pod "client-containers-0a6663df-3812-45d1-be01-8ee8eb698478" satisfied condition "Succeeded or Failed"
Dec 14 13:30:45.072: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod client-containers-0a6663df-3812-45d1-be01-8ee8eb698478 container test-container: <nil>
STEP: delete the pod
Dec 14 13:30:45.091: INFO: Waiting for pod client-containers-0a6663df-3812-45d1-be01-8ee8eb698478 to disappear
Dec 14 13:30:45.095: INFO: Pod client-containers-0a6663df-3812-45d1-be01-8ee8eb698478 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:45.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7956" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":-1,"completed":43,"skipped":751,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Dec 14 13:30:10.457: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:46.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2685" for this suite.


• [SLOW TEST:35.745 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":-1,"completed":17,"skipped":277,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":-1,"completed":40,"skipped":799,"failed":0}
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Dec 14 13:30:43.310: INFO: Waiting up to 5m0s for pod "var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13" in namespace "var-expansion-3166" to be "Succeeded or Failed"
Dec 14 13:30:43.316: INFO: Pod "var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644398ms
Dec 14 13:30:45.324: INFO: Pod "var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013933551s
Dec 14 13:30:47.328: INFO: Pod "var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018466s
STEP: Saw pod success
Dec 14 13:30:47.328: INFO: Pod "var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13" satisfied condition "Succeeded or Failed"
Dec 14 13:30:47.332: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:30:47.362: INFO: Waiting for pod var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13 to disappear
Dec 14 13:30:47.366: INFO: Pod var-expansion-c38497cb-dfba-42b2-972e-e3b0f3df2d13 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:47.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3166" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":-1,"completed":41,"skipped":799,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:45.138: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:30:45.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-6025'
Dec 14 13:30:45.739: INFO: stderr: ""
Dec 14 13:30:45.739: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 14 13:30:45.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 create -f - --namespace=kubectl-6025'
Dec 14 13:30:46.255: INFO: stderr: ""
Dec 14 13:30:46.255: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 13:30:47.260: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:47.260: INFO: Found 0 / 1
Dec 14 13:30:48.260: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:48.260: INFO: Found 1 / 1
Dec 14 13:30:48.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 13:30:48.263: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 13:30:48.263: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 13:30:48.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 describe pod agnhost-primary-hhsgq --namespace=kubectl-6025'
Dec 14 13:30:48.504: INFO: stderr: ""
Dec 14 13:30:48.505: INFO: stdout: "Name:         agnhost-primary-hhsgq\nNamespace:    kubectl-6025\nNode:         conformance-v1-19-pwlsgdsa6hrh-node-1/10.0.0.55\nStart Time:   Mon, 14 Dec 2020 13:30:45 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.100.227.184/32\n              cni.projectcalico.org/podIPs: 10.100.227.184/32\nStatus:       Running\nIP:           10.100.227.184\nIPs:\n  IP:           10.100.227.184\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://65d8dbd5eceacf4a4e4735df6339aaaac3abe4f901ff6f72b9a2f9a6217c13e7\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 14 Dec 2020 13:30:47 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nz66q (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nz66q:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nz66q\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6025/agnhost-primary-hhsgq to conformance-v1-19-pwlsgdsa6hrh-node-1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 14 13:30:48.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 describe rc agnhost-primary --namespace=kubectl-6025'
Dec 14 13:30:48.746: INFO: stderr: ""
Dec 14 13:30:48.747: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6025\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hhsgq\n"
Dec 14 13:30:48.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 describe service agnhost-primary --namespace=kubectl-6025'
Dec 14 13:30:48.955: INFO: stderr: ""
Dec 14 13:30:48.956: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6025\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.254.95.217\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.227.184:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 13:30:48.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 describe node conformance-v1-19-pwlsgdsa6hrh-master-0'
Dec 14 13:30:49.280: INFO: stderr: ""
Dec 14 13:30:49.281: INFO: stdout: "Name:               conformance-v1-19-pwlsgdsa6hrh-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=221de281-95ec-414f-8e42-c86c9e0b318d\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=sjc1\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance-v1-19-pwlsgdsa6hrh-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=221de281-95ec-414f-8e42-c86c9e0b318d\n                    topology.kubernetes.io/region=sjc1\n                    topology.kubernetes.io/zone=nova\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.227/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 14 Dec 2020 13:03:53 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  conformance-v1-19-pwlsgdsa6hrh-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 14 Dec 2020 13:30:40 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 14 Dec 2020 13:04:56 +0000   Mon, 14 Dec 2020 13:04:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 14 Dec 2020 13:26:50 +0000   Mon, 14 Dec 2020 13:03:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 14 Dec 2020 13:26:50 +0000   Mon, 14 Dec 2020 13:03:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 14 Dec 2020 13:26:50 +0000   Mon, 14 Dec 2020 13:03:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 14 Dec 2020 13:26:50 +0000   Mon, 14 Dec 2020 13:04:44 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.227\n  ExternalIP:  38.108.68.21\n  Hostname:    conformance-v1-19-pwlsgdsa6hrh-master-0\nCapacity:\n  cpu:                8\n  ephemeral-storage:  62378988Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8143540Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  57488475246\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8041140Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 043dbe1cb2424dd4a458053556c911f1\n  System UUID:                043dbe1c-b242-4dd4-a458-053556c911f1\n  Boot ID:                    977aefdc-f465-45f8-8b1c-82d89b1314ac\n  Kernel Version:             5.6.14-300.fc32.x86_64\n  OS Image:                   Fedora CoreOS 32.20200601.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.8\n  Kubelet Version:            v1.19.4\n  Kube-Proxy Version:         v1.19.4\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///043dbe1c-b242-4dd4-a458-053556c911f1\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6c8d565f86-ngnrm                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 calico-node-6kvk7                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 cluster-autoscaler-66f986bfb-tnw8v                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 coredns-66b8cb74db-6ppmc                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     26m\n  kube-system                 coredns-66b8cb74db-vvdsc                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     26m\n  kube-system                 dashboard-metrics-scraper-7bd7c99b45-fk2hn                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 k8s-keystone-auth-2vsgl                                    200m (2%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kubernetes-dashboard-7b85f97959-9rvzw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 magnum-auto-healer-pwv9k                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 magnum-prometheus-node-exporter-86wks                      20m (0%)      20m (0%)    20M (0%)         20M (0%)       23m\n  kube-system                 openstack-cloud-controller-manager-2nrnt                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9664d730602a4738-cjwv8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m37s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                670m (8%)       20m (0%)\n  memory             166800640 (2%)  376515840 (4%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type    Reason     Age   From        Message\n  ----    ------     ----  ----        -------\n  Normal  Starting   26m   kube-proxy  Starting kube-proxy.\n  Normal  NodeReady  26m   kubelet     Node conformance-v1-19-pwlsgdsa6hrh-master-0 status is now: NodeReady\n"
Dec 14 13:30:49.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-517469967 describe namespace kubectl-6025'
Dec 14 13:30:49.474: INFO: stderr: ""
Dec 14 13:30:49.474: INFO: stdout: "Name:         kubectl-6025\nLabels:       e2e-framework=kubectl\n              e2e-run=6c11a018-a261-4fa4-ab1f-442df8b5418e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:49.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6025" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":-1,"completed":44,"skipped":763,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:19.129: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 14 13:30:19.190: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:30:24.800: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4572" for this suite.


• [SLOW TEST:31.022 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":-1,"completed":34,"skipped":804,"failed":0}

S
------------------------------
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:47.499: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 14 13:30:47.545: INFO: Waiting up to 5m0s for pod "downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb" in namespace "downward-api-660" to be "Succeeded or Failed"
Dec 14 13:30:47.557: INFO: Pod "downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.129036ms
Dec 14 13:30:49.569: INFO: Pod "downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023200813s
Dec 14 13:30:51.575: INFO: Pod "downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029251895s
STEP: Saw pod success
Dec 14 13:30:51.575: INFO: Pod "downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb" satisfied condition "Succeeded or Failed"
Dec 14 13:30:51.577: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:30:51.606: INFO: Waiting for pod downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb to disappear
Dec 14 13:30:51.610: INFO: Pod downward-api-d981341b-f6dd-4ad8-95e9-3d4595c15dbb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:51.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-660" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":-1,"completed":42,"skipped":835,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:46.208: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 14 13:30:48.812: INFO: Successfully updated pod "annotationupdate5ee3130b-ea51-4e54-8a9d-0422073c59a0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:52.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2985" for this suite.


• [SLOW TEST:6.647 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":293,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Dec 14 13:30:52.948: INFO: Waiting up to 5m0s for pod "client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4" in namespace "containers-606" to be "Succeeded or Failed"
Dec 14 13:30:52.966: INFO: Pod "client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.111664ms
Dec 14 13:30:54.970: INFO: Pod "client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021913894s
Dec 14 13:30:56.974: INFO: Pod "client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026091587s
STEP: Saw pod success
Dec 14 13:30:56.974: INFO: Pod "client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4" satisfied condition "Succeeded or Failed"
Dec 14 13:30:56.977: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4 container test-container: <nil>
STEP: delete the pod
Dec 14 13:30:57.011: INFO: Waiting for pod client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4 to disappear
Dec 14 13:30:57.016: INFO: Pod client-containers-21af40fb-30dc-4995-8b06-aac55f5832b4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:30:57.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-606" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":311,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:49.538: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:06.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3361" for this suite.


• [SLOW TEST:17.098 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":-1,"completed":45,"skipped":780,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:51.698: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 13:30:59.793: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:30:59.806: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:31:01.807: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:31:01.810: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:31:03.807: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:31:03.811: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:31:05.807: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:31:05.811: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:31:07.807: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:31:07.811: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:31:09.807: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:31:09.813: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1797" for this suite.


• [SLOW TEST:18.130 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":43,"skipped":866,"failed":0}

S
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":822,"failed":0}
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 14 13:30:17.810: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 21922 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:30:17.811: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 21922 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 14 13:30:27.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22232 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:30:27.825: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22232 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 14 13:30:37.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22359 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:30:37.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22359 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 14 13:30:47.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22533 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:30:47.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-a 15b63db1-1264-4cfa-b8e2-aa59e1d3047e 22533 0 2020-12-14 13:30:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 14 13:30:57.859: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-b a1654a9d-801e-419e-bae4-851df102cda8 22806 0 2020-12-14 13:30:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:30:57.859: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-b a1654a9d-801e-419e-bae4-851df102cda8 22806 0 2020-12-14 13:30:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 14 13:31:07.867: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-b a1654a9d-801e-419e-bae4-851df102cda8 22996 0 2020-12-14 13:30:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:31:07.867: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-760 /api/v1/namespaces/watch-760/configmaps/e2e-watch-test-configmap-b a1654a9d-801e-419e-bae4-851df102cda8 22996 0 2020-12-14 13:30:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-14 13:30:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:17.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-760" for this suite.


• [SLOW TEST:60.136 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":-1,"completed":49,"skipped":822,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4536
STEP: creating service affinity-clusterip in namespace services-4536
STEP: creating replication controller affinity-clusterip in namespace services-4536
I1214 13:30:57.128370      27 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4536, replica count: 3
I1214 13:31:00.178970      27 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:31:00.186: INFO: Creating new exec pod
Dec 14 13:31:05.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-4536 execpod-affinityqm4jd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Dec 14 13:31:05.564: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 14 13:31:05.564: INFO: stdout: ""
Dec 14 13:31:05.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-4536 execpod-affinityqm4jd -- /bin/sh -x -c nc -zv -t -w 2 10.254.64.9 80'
Dec 14 13:31:05.908: INFO: stderr: "+ nc -zv -t -w 2 10.254.64.9 80\nConnection to 10.254.64.9 80 port [tcp/http] succeeded!\n"
Dec 14 13:31:05.908: INFO: stdout: ""
Dec 14 13:31:05.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-4536 execpod-affinityqm4jd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.64.9:80/ ; done'
Dec 14 13:31:06.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.64.9:80/\n"
Dec 14 13:31:06.341: INFO: stdout: "\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd\naffinity-clusterip-ch9xd"
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Received response from host: affinity-clusterip-ch9xd
Dec 14 13:31:06.341: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4536, will wait for the garbage collector to delete the pods
Dec 14 13:31:06.423: INFO: Deleting ReplicationController affinity-clusterip took: 6.900957ms
Dec 14 13:31:07.429: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.005471792s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:18.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4536" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:21.703 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:17.908: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-528" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":-1,"completed":50,"skipped":827,"failed":0}

SSSS
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":20,"skipped":330,"failed":0}
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-5f3222bf-b1d5-4abc-a9ac-19b9241ee609
STEP: Creating a pod to test consume configMaps
Dec 14 13:31:18.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021" in namespace "projected-6211" to be "Succeeded or Failed"
Dec 14 13:31:18.842: INFO: Pod "pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28558ms
Dec 14 13:31:20.847: INFO: Pod "pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008651807s
Dec 14 13:31:22.852: INFO: Pod "pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014432124s
STEP: Saw pod success
Dec 14 13:31:22.852: INFO: Pod "pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021" satisfied condition "Succeeded or Failed"
Dec 14 13:31:22.855: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:31:22.877: INFO: Waiting for pod pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021 to disappear
Dec 14 13:31:22.881: INFO: Pod pod-projected-configmaps-17274a94-c4a9-41a0-9f2f-7862fa45d021 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:22.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6211" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":330,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:09.836: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7407
STEP: creating service affinity-nodeport in namespace services-7407
STEP: creating replication controller affinity-nodeport in namespace services-7407
I1214 13:31:09.921254      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7407, replica count: 3
I1214 13:31:12.972029      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:31:12.989: INFO: Creating new exec pod
Dec 14 13:31:18.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Dec 14 13:31:18.446: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 14 13:31:18.446: INFO: stdout: ""
Dec 14 13:31:18.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 10.254.147.10 80'
Dec 14 13:31:18.838: INFO: stderr: "+ nc -zv -t -w 2 10.254.147.10 80\nConnection to 10.254.147.10 80 port [tcp/http] succeeded!\n"
Dec 14 13:31:18.838: INFO: stdout: ""
Dec 14 13:31:18.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.200 30534'
Dec 14 13:31:19.209: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.200 30534\nConnection to 10.0.0.200 30534 port [tcp/30534] succeeded!\n"
Dec 14 13:31:19.209: INFO: stdout: ""
Dec 14 13:31:19.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.55 30534'
Dec 14 13:31:19.578: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.55 30534\nConnection to 10.0.0.55 30534 port [tcp/30534] succeeded!\n"
Dec 14 13:31:19.579: INFO: stdout: ""
Dec 14 13:31:19.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.93 30534'
Dec 14 13:31:19.991: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.93 30534\nConnection to 38.108.68.93 30534 port [tcp/30534] succeeded!\n"
Dec 14 13:31:19.991: INFO: stdout: ""
Dec 14 13:31:19.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.247 30534'
Dec 14 13:31:20.388: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.247 30534\nConnection to 38.108.68.247 30534 port [tcp/30534] succeeded!\n"
Dec 14 13:31:20.388: INFO: stdout: ""
Dec 14 13:31:20.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-782519938 exec --namespace=services-7407 execpod-affinity7zlxm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.200:30534/ ; done'
Dec 14 13:31:20.893: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.200:30534/\n"
Dec 14 13:31:20.893: INFO: stdout: "\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch\naffinity-nodeport-zrwch"
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Received response from host: affinity-nodeport-zrwch
Dec 14 13:31:20.893: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7407, will wait for the garbage collector to delete the pods
Dec 14 13:31:20.977: INFO: Deleting ReplicationController affinity-nodeport took: 7.515723ms
Dec 14 13:31:21.078: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.44778ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:32.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7407" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:22.293 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":44,"skipped":867,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:31:32.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682" in namespace "projected-8229" to be "Succeeded or Failed"
Dec 14 13:31:32.260: INFO: Pod "downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682": Phase="Pending", Reason="", readiness=false. Elapsed: 18.667687ms
Dec 14 13:31:34.264: INFO: Pod "downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023558621s
Dec 14 13:31:36.268: INFO: Pod "downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027207375s
STEP: Saw pod success
Dec 14 13:31:36.268: INFO: Pod "downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682" satisfied condition "Succeeded or Failed"
Dec 14 13:31:36.271: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682 container client-container: <nil>
STEP: delete the pod
Dec 14 13:31:36.295: INFO: Waiting for pod downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682 to disappear
Dec 14 13:31:36.298: INFO: Pod downwardapi-volume-6afb99be-0a75-44bb-b996-c8ee40b31682 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:36.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8229" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":877,"failed":0}

SS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:38.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8240" for this suite.


• [SLOW TEST:16.110 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":-1,"completed":51,"skipped":831,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:17.315: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7596
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7596
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7596
Dec 14 13:30:17.404: INFO: Found 0 stateful pods, waiting for 1
Dec 14 13:30:27.410: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 14 13:30:27.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:30:27.887: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:30:27.888: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:30:27.888: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:30:27.892: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 13:30:37.897: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:30:37.897: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:30:37.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999689s
Dec 14 13:30:38.920: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997093592s
Dec 14 13:30:39.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992738254s
Dec 14 13:30:40.930: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987236942s
Dec 14 13:30:41.935: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982419437s
Dec 14 13:30:42.938: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97771486s
Dec 14 13:30:43.943: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.974454233s
Dec 14 13:30:44.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96993506s
Dec 14 13:30:45.959: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958915205s
Dec 14 13:30:46.965: INFO: Verifying statefulset ss doesn't scale past 1 for another 953.993245ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7596
Dec 14 13:30:47.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:48.420: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:30:48.421: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:30:48.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:30:48.424: INFO: Found 1 stateful pods, waiting for 3
Dec 14 13:30:58.429: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:30:58.429: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:30:58.429: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 14 13:30:58.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:30:58.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:30:58.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:30:58.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:30:58.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:30:59.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:30:59.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:30:59.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:30:59.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:30:59.705: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:30:59.705: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:30:59.705: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:30:59.705: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:30:59.709: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 13:31:09.721: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:31:09.721: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:31:09.721: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:31:09.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999602s
Dec 14 13:31:10.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992229068s
Dec 14 13:31:11.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979377251s
Dec 14 13:31:12.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97402111s
Dec 14 13:31:13.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969477862s
Dec 14 13:31:14.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955830152s
Dec 14 13:31:15.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949762562s
Dec 14 13:31:16.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941298139s
Dec 14 13:31:17.811: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.935843877s
Dec 14 13:31:18.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.391207ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7596
Dec 14 13:31:19.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:20.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:31:20.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:31:20.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:31:20.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:20.582: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:31:20.582: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:31:20.582: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:31:20.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=statefulset-7596 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:21.015: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:31:21.015: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:31:21.015: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:31:21.015: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:31:41.055: INFO: Deleting all statefulset in ns statefulset-7596
Dec 14 13:31:41.059: INFO: Scaling statefulset ss to 0
Dec 14 13:31:41.071: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:31:41.074: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:41.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7596" for this suite.


• [SLOW TEST:83.783 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":-1,"completed":29,"skipped":401,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:38.129: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Dec 14 13:31:38.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 create -f - --namespace=kubectl-6881'
Dec 14 13:31:38.818: INFO: stderr: ""
Dec 14 13:31:38.818: INFO: stdout: "pod/pause created\n"
Dec 14 13:31:38.818: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 13:31:38.818: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6881" to be "running and ready"
Dec 14 13:31:38.824: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899899ms
Dec 14 13:31:40.828: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009502862s
Dec 14 13:31:42.834: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015327895s
Dec 14 13:31:42.834: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 13:31:42.834: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 14 13:31:42.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 label pods pause testing-label=testing-label-value --namespace=kubectl-6881'
Dec 14 13:31:43.035: INFO: stderr: ""
Dec 14 13:31:43.036: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 14 13:31:43.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get pod pause -L testing-label --namespace=kubectl-6881'
Dec 14 13:31:43.216: INFO: stderr: ""
Dec 14 13:31:43.216: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 14 13:31:43.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 label pods pause testing-label- --namespace=kubectl-6881'
Dec 14 13:31:43.399: INFO: stderr: ""
Dec 14 13:31:43.399: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 14 13:31:43.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get pod pause -L testing-label --namespace=kubectl-6881'
Dec 14 13:31:43.593: INFO: stderr: ""
Dec 14 13:31:43.593: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Dec 14 13:31:43.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 delete --grace-period=0 --force -f - --namespace=kubectl-6881'
Dec 14 13:31:43.759: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:31:43.759: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 13:31:43.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get rc,svc -l name=pause --no-headers --namespace=kubectl-6881'
Dec 14 13:31:43.971: INFO: stderr: "No resources found in kubectl-6881 namespace.\n"
Dec 14 13:31:43.971: INFO: stdout: ""
Dec 14 13:31:43.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-669464615 get pods -l name=pause --namespace=kubectl-6881 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 13:31:44.138: INFO: stderr: ""
Dec 14 13:31:44.138: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:44.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6881" for this suite.


• [SLOW TEST:6.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":-1,"completed":52,"skipped":835,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-qp8v
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:31:22.978: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qp8v" in namespace "subpath-1305" to be "Succeeded or Failed"
Dec 14 13:31:22.988: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Pending", Reason="", readiness=false. Elapsed: 9.895808ms
Dec 14 13:31:24.991: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.013682415s
Dec 14 13:31:26.996: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 4.018307671s
Dec 14 13:31:29.001: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 6.023480097s
Dec 14 13:31:31.006: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 8.027911647s
Dec 14 13:31:33.010: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 10.03190183s
Dec 14 13:31:35.020: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 12.041913173s
Dec 14 13:31:37.026: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 14.048136335s
Dec 14 13:31:39.030: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 16.052530137s
Dec 14 13:31:41.036: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 18.058197017s
Dec 14 13:31:43.040: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 20.062817183s
Dec 14 13:31:45.046: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Running", Reason="", readiness=true. Elapsed: 22.06801458s
Dec 14 13:31:47.050: INFO: Pod "pod-subpath-test-projected-qp8v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.072511174s
STEP: Saw pod success
Dec 14 13:31:47.050: INFO: Pod "pod-subpath-test-projected-qp8v" satisfied condition "Succeeded or Failed"
Dec 14 13:31:47.053: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-subpath-test-projected-qp8v container test-container-subpath-projected-qp8v: <nil>
STEP: delete the pod
Dec 14 13:31:47.121: INFO: Waiting for pod pod-subpath-test-projected-qp8v to disappear
Dec 14 13:31:47.125: INFO: Pod pod-subpath-test-projected-qp8v no longer exists
STEP: Deleting pod pod-subpath-test-projected-qp8v
Dec 14 13:31:47.125: INFO: Deleting pod "pod-subpath-test-projected-qp8v" in namespace "subpath-1305"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:47.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1305" for this suite.


• [SLOW TEST:24.227 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":-1,"completed":22,"skipped":334,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:44.202: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:48.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7316" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":854,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:30:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:30:50.203: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Creating first CR 
Dec 14 13:30:50.770: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:30:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:30:50Z]] name:name1 resourceVersion:22610 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:abb37588-2881-45ed-b2b5-cbad69a248b7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 14 13:31:00.777: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:31:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:31:00Z]] name:name2 resourceVersion:22895 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f0c37906-71d7-478e-a699-952e4ce58ee9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 14 13:31:10.785: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:31:10Z]] name:name1 resourceVersion:23069 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:abb37588-2881-45ed-b2b5-cbad69a248b7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 14 13:31:20.795: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:31:20Z]] name:name2 resourceVersion:23257 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f0c37906-71d7-478e-a699-952e4ce58ee9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 14 13:31:30.806: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:31:10Z]] name:name1 resourceVersion:23447 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:abb37588-2881-45ed-b2b5-cbad69a248b7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 14 13:31:40.818: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-14T13:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-14T13:31:20Z]] name:name2 resourceVersion:23643 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f0c37906-71d7-478e-a699-952e4ce58ee9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:51.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9144" for this suite.


• [SLOW TEST:61.189 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":-1,"completed":35,"skipped":805,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:48.303: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-f7e76366-ea67-4d27-9e32-cb10eb7ac687
STEP: Creating secret with name secret-projected-all-test-volume-6a7a2395-10a8-463f-b072-d6fffe46549b
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 14 13:31:48.352: INFO: Waiting up to 5m0s for pod "projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674" in namespace "projected-2391" to be "Succeeded or Failed"
Dec 14 13:31:48.357: INFO: Pod "projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674": Phase="Pending", Reason="", readiness=false. Elapsed: 5.920452ms
Dec 14 13:31:50.362: INFO: Pod "projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010716165s
Dec 14 13:31:52.367: INFO: Pod "projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015327724s
STEP: Saw pod success
Dec 14 13:31:52.367: INFO: Pod "projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674" satisfied condition "Succeeded or Failed"
Dec 14 13:31:52.370: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 14 13:31:52.391: INFO: Waiting for pod projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674 to disappear
Dec 14 13:31:52.403: INFO: Pod projected-volume-6d3a569a-0a8b-4718-a274-d7dff5b04674 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:52.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2391" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":-1,"completed":54,"skipped":859,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:47.219: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 14 13:31:47.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 23779 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:31:47.279: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 23780 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:31:47.279: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 23781 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 14 13:31:57.312: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 24002 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:31:57.312: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 24003 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:31:57.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-139 /api/v1/namespaces/watch-139/configmaps/e2e-watch-test-label-changed b385166e-f9cc-41d0-b1ea-7b4810de10b3 24004 0 2020-12-14 13:31:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-14 13:31:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:57.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-139" for this suite.


• [SLOW TEST:10.105 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:51.392: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 14 13:31:57.491: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:57.492: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:57.712: INFO: Exec stderr: ""
Dec 14 13:31:57.712: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:57.892: INFO: Exec stderr: ""
Dec 14 13:31:57.892: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:57.892: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:58.108: INFO: Exec stderr: ""
Dec 14 13:31:58.108: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:58.294: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 14 13:31:58.294: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:58.294: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:58.480: INFO: Exec stderr: ""
Dec 14 13:31:58.481: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:58.481: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:58.657: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 14 13:31:58.657: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:58.657: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:58.836: INFO: Exec stderr: ""
Dec 14 13:31:58.836: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:58.836: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:59.024: INFO: Exec stderr: ""
Dec 14 13:31:59.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:59.200: INFO: Exec stderr: ""
Dec 14 13:31:59.200: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:31:59.200: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
Dec 14 13:31:59.385: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:31:59.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5078" for this suite.


• [SLOW TEST:8.003 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":820,"failed":0}

SS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:59.404: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Dec 14 13:31:59.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 create -f - --namespace=kubectl-8857'
Dec 14 13:31:59.933: INFO: stderr: ""
Dec 14 13:31:59.933: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 13:31:59.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8857'
Dec 14 13:32:00.099: INFO: stderr: ""
Dec 14 13:32:00.099: INFO: stdout: "update-demo-nautilus-m2nc2 update-demo-nautilus-qzvbh "
Dec 14 13:32:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods update-demo-nautilus-m2nc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8857'
Dec 14 13:32:00.375: INFO: stderr: ""
Dec 14 13:32:00.375: INFO: stdout: ""
Dec 14 13:32:00.375: INFO: update-demo-nautilus-m2nc2 is created but not running
Dec 14 13:32:05.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8857'
Dec 14 13:32:05.569: INFO: stderr: ""
Dec 14 13:32:05.569: INFO: stdout: "update-demo-nautilus-m2nc2 update-demo-nautilus-qzvbh "
Dec 14 13:32:05.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods update-demo-nautilus-m2nc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8857'
Dec 14 13:32:05.730: INFO: stderr: ""
Dec 14 13:32:05.730: INFO: stdout: "true"
Dec 14 13:32:05.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods update-demo-nautilus-m2nc2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8857'
Dec 14 13:32:05.896: INFO: stderr: ""
Dec 14 13:32:05.896: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:32:05.896: INFO: validating pod update-demo-nautilus-m2nc2
Dec 14 13:32:05.906: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:32:05.906: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:32:05.906: INFO: update-demo-nautilus-m2nc2 is verified up and running
Dec 14 13:32:05.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods update-demo-nautilus-qzvbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8857'
Dec 14 13:32:06.072: INFO: stderr: ""
Dec 14 13:32:06.072: INFO: stdout: "true"
Dec 14 13:32:06.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods update-demo-nautilus-qzvbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8857'
Dec 14 13:32:06.221: INFO: stderr: ""
Dec 14 13:32:06.221: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 14 13:32:06.221: INFO: validating pod update-demo-nautilus-qzvbh
Dec 14 13:32:06.240: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 13:32:06.240: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 13:32:06.240: INFO: update-demo-nautilus-qzvbh is verified up and running
STEP: using delete to clean up resources
Dec 14 13:32:06.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 delete --grace-period=0 --force -f - --namespace=kubectl-8857'
Dec 14 13:32:06.380: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:32:06.380: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 13:32:06.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8857'
Dec 14 13:32:06.595: INFO: stderr: "No resources found in kubectl-8857 namespace.\n"
Dec 14 13:32:06.595: INFO: stdout: ""
Dec 14 13:32:06.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods -l name=update-demo --namespace=kubectl-8857 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 13:32:06.785: INFO: stderr: ""
Dec 14 13:32:06.785: INFO: stdout: "update-demo-nautilus-m2nc2\nupdate-demo-nautilus-qzvbh\n"
Dec 14 13:32:07.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8857'
Dec 14 13:32:07.576: INFO: stderr: "No resources found in kubectl-8857 namespace.\n"
Dec 14 13:32:07.576: INFO: stdout: ""
Dec 14 13:32:07.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-404816711 get pods -l name=update-demo --namespace=kubectl-8857 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 13:32:07.754: INFO: stderr: ""
Dec 14 13:32:07.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8857" for this suite.


• [SLOW TEST:8.362 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":-1,"completed":37,"skipped":822,"failed":0}

SSSSSS
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":-1,"completed":23,"skipped":367,"failed":0}
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:57.328: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 14 13:32:03.447: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:32:03.452: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:32:05.452: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:32:05.456: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:32:07.452: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:32:07.456: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:32:09.452: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:32:09.457: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:09.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5206" for this suite.


• [SLOW TEST:12.214 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":367,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:09.573: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Dec 14 13:32:09.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 api-versions'
Dec 14 13:32:09.778: INFO: stderr: ""
Dec 14 13:32:09.778: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:09.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7709" for this suite.

•
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":-1,"completed":25,"skipped":380,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:36.321: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8619, will wait for the garbage collector to delete the pods
Dec 14 13:31:40.423: INFO: Deleting Job.batch foo took: 6.288392ms
Dec 14 13:31:40.524: INFO: Terminating Job.batch foo pods took: 100.396975ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:12.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8619" for this suite.


• [SLOW TEST:36.219 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":-1,"completed":46,"skipped":879,"failed":0}

SS
------------------------------
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:09.823: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 14 13:32:09.874: INFO: Waiting up to 5m0s for pod "downward-api-b6aba473-770c-44fb-a261-50329161cb37" in namespace "downward-api-8108" to be "Succeeded or Failed"
Dec 14 13:32:09.898: INFO: Pod "downward-api-b6aba473-770c-44fb-a261-50329161cb37": Phase="Pending", Reason="", readiness=false. Elapsed: 24.215093ms
Dec 14 13:32:11.904: INFO: Pod "downward-api-b6aba473-770c-44fb-a261-50329161cb37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02983967s
Dec 14 13:32:13.908: INFO: Pod "downward-api-b6aba473-770c-44fb-a261-50329161cb37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033851209s
STEP: Saw pod success
Dec 14 13:32:13.908: INFO: Pod "downward-api-b6aba473-770c-44fb-a261-50329161cb37" satisfied condition "Succeeded or Failed"
Dec 14 13:32:13.913: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downward-api-b6aba473-770c-44fb-a261-50329161cb37 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:32:13.939: INFO: Waiting for pod downward-api-b6aba473-770c-44fb-a261-50329161cb37 to disappear
Dec 14 13:32:13.943: INFO: Pod downward-api-b6aba473-770c-44fb-a261-50329161cb37 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:13.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8108" for this suite.

•
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":-1,"completed":26,"skipped":389,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:52.451: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-lfzs
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:31:52.503: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lfzs" in namespace "subpath-3792" to be "Succeeded or Failed"
Dec 14 13:31:52.514: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Pending", Reason="", readiness=false. Elapsed: 10.642829ms
Dec 14 13:31:54.519: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015427164s
Dec 14 13:31:56.522: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.018552493s
Dec 14 13:31:58.525: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.021532047s
Dec 14 13:32:00.530: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.026187352s
Dec 14 13:32:02.538: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.034667341s
Dec 14 13:32:04.542: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.038738557s
Dec 14 13:32:06.546: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.042581395s
Dec 14 13:32:08.552: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.048323336s
Dec 14 13:32:10.557: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.05407203s
Dec 14 13:32:12.564: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.060433407s
Dec 14 13:32:14.572: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Running", Reason="", readiness=true. Elapsed: 22.068676405s
Dec 14 13:32:16.577: INFO: Pod "pod-subpath-test-configmap-lfzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.073441483s
STEP: Saw pod success
Dec 14 13:32:16.577: INFO: Pod "pod-subpath-test-configmap-lfzs" satisfied condition "Succeeded or Failed"
Dec 14 13:32:16.581: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-subpath-test-configmap-lfzs container test-container-subpath-configmap-lfzs: <nil>
STEP: delete the pod
Dec 14 13:32:16.603: INFO: Waiting for pod pod-subpath-test-configmap-lfzs to disappear
Dec 14 13:32:16.615: INFO: Pod pod-subpath-test-configmap-lfzs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lfzs
Dec 14 13:32:16.615: INFO: Deleting pod "pod-subpath-test-configmap-lfzs" in namespace "subpath-3792"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:16.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3792" for this suite.


• [SLOW TEST:24.176 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":-1,"completed":55,"skipped":870,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Dec 14 13:32:18.643: INFO: 0 pods remaining
Dec 14 13:32:18.644: INFO: 0 pods has nil DeletionTimestamp
Dec 14 13:32:18.644: INFO: 
STEP: Gathering metrics
W1214 13:32:19.661307      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1214 13:32:19.661432      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1214 13:32:19.661445      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 14 13:32:19.661: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:19.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1332" for this suite.


• [SLOW TEST:7.123 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":-1,"completed":47,"skipped":881,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:16.755: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-54bda698-71ba-4fb8-9343-037013a6c91c
STEP: Creating a pod to test consume secrets
Dec 14 13:32:16.814: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d" in namespace "projected-1588" to be "Succeeded or Failed"
Dec 14 13:32:16.822: INFO: Pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.670061ms
Dec 14 13:32:18.827: INFO: Pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013344903s
Dec 14 13:32:20.834: INFO: Pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019687731s
Dec 14 13:32:22.839: INFO: Pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02486371s
STEP: Saw pod success
Dec 14 13:32:22.839: INFO: Pod "pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d" satisfied condition "Succeeded or Failed"
Dec 14 13:32:22.842: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:32:22.868: INFO: Waiting for pod pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d to disappear
Dec 14 13:32:22.871: INFO: Pod pod-projected-secrets-83265b40-f45a-4135-8f36-22f882c2018d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:22.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1588" for this suite.


• [SLOW TEST:6.125 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":56,"skipped":922,"failed":0}
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:22.883: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:22.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2530" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":-1,"completed":57,"skipped":922,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:07.786: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:23.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4964" for this suite.


• [SLOW TEST:16.156 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":-1,"completed":38,"skipped":828,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:22.982: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:32:23.030: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6bf12084-93e3-4e74-acb4-67112a7b6583" in namespace "security-context-test-9722" to be "Succeeded or Failed"
Dec 14 13:32:23.044: INFO: Pod "busybox-user-65534-6bf12084-93e3-4e74-acb4-67112a7b6583": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150914ms
Dec 14 13:32:25.049: INFO: Pod "busybox-user-65534-6bf12084-93e3-4e74-acb4-67112a7b6583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019789592s
Dec 14 13:32:27.054: INFO: Pod "busybox-user-65534-6bf12084-93e3-4e74-acb4-67112a7b6583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024683254s
Dec 14 13:32:27.054: INFO: Pod "busybox-user-65534-6bf12084-93e3-4e74-acb4-67112a7b6583" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:27.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9722" for this suite.

•
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:13.975: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-2734
STEP: creating replication controller nodeport-test in namespace services-2734
I1214 13:32:14.072142      27 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2734, replica count: 2
I1214 13:32:17.123463      27 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:32:20.123823      27 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:32:20.123: INFO: Creating new exec pod
Dec 14 13:32:27.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 14 13:32:27.569: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 13:32:27.569: INFO: stdout: ""
Dec 14 13:32:27.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 10.254.62.124 80'
Dec 14 13:32:27.997: INFO: stderr: "+ nc -zv -t -w 2 10.254.62.124 80\nConnection to 10.254.62.124 80 port [tcp/http] succeeded!\n"
Dec 14 13:32:27.998: INFO: stdout: ""
Dec 14 13:32:27.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.200 30828'
Dec 14 13:32:28.400: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.200 30828\nConnection to 10.0.0.200 30828 port [tcp/30828] succeeded!\n"
Dec 14 13:32:28.400: INFO: stdout: ""
Dec 14 13:32:28.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.55 30828'
Dec 14 13:32:28.797: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.55 30828\nConnection to 10.0.0.55 30828 port [tcp/30828] succeeded!\n"
Dec 14 13:32:28.797: INFO: stdout: ""
Dec 14 13:32:28.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.93 30828'
Dec 14 13:32:29.173: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.93 30828\nConnection to 38.108.68.93 30828 port [tcp/30828] succeeded!\n"
Dec 14 13:32:29.173: INFO: stdout: ""
Dec 14 13:32:29.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-334331672 exec --namespace=services-2734 execpod6ctb4 -- /bin/sh -x -c nc -zv -t -w 2 38.108.68.247 30828'
Dec 14 13:32:29.554: INFO: stderr: "+ nc -zv -t -w 2 38.108.68.247 30828\nConnection to 38.108.68.247 30828 port [tcp/30828] succeeded!\n"
Dec 14 13:32:29.554: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:29.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2734" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:15.594 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":-1,"completed":27,"skipped":394,"failed":0}

SSSSSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":58,"skipped":929,"failed":0}
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:27.070: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d91c377d-bb90-4145-80f6-94eeca1cc10a
STEP: Creating a pod to test consume secrets
Dec 14 13:32:27.114: INFO: Waiting up to 5m0s for pod "pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c" in namespace "secrets-5253" to be "Succeeded or Failed"
Dec 14 13:32:27.118: INFO: Pod "pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020653ms
Dec 14 13:32:29.127: INFO: Pod "pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013488846s
Dec 14 13:32:31.132: INFO: Pod "pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018251166s
STEP: Saw pod success
Dec 14 13:32:31.132: INFO: Pod "pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c" satisfied condition "Succeeded or Failed"
Dec 14 13:32:31.145: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c container secret-env-test: <nil>
STEP: delete the pod
Dec 14 13:32:31.169: INFO: Waiting for pod pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c to disappear
Dec 14 13:32:31.172: INFO: Pod pod-secrets-a89bc3f8-0932-48b1-a123-83566364e40c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:31.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5253" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":-1,"completed":59,"skipped":929,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:41.203: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4479
Dec 14 13:31:43.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 13:31:43.640: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 13:31:43.640: INFO: stdout: "iptables"
Dec 14 13:31:43.640: INFO: proxyMode: iptables
Dec 14 13:31:43.647: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:43.651: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:31:45.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:45.656: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:31:47.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:47.659: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:31:49.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:49.655: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:31:51.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:51.655: INFO: Pod kube-proxy-mode-detector still exists
Dec 14 13:31:53.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:31:53.662: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4479
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4479
I1214 13:31:53.685985      32 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4479, replica count: 3
I1214 13:31:56.736767      32 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:31:56.746: INFO: Creating new exec pod
Dec 14 13:32:01.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 execpod-affinityzfvvh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Dec 14 13:32:02.229: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 13:32:02.229: INFO: stdout: ""
Dec 14 13:32:02.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 execpod-affinityzfvvh -- /bin/sh -x -c nc -zv -t -w 2 10.254.7.170 80'
Dec 14 13:32:02.622: INFO: stderr: "+ nc -zv -t -w 2 10.254.7.170 80\nConnection to 10.254.7.170 80 port [tcp/http] succeeded!\n"
Dec 14 13:32:02.622: INFO: stdout: ""
Dec 14 13:32:02.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 execpod-affinityzfvvh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.7.170:80/ ; done'
Dec 14 13:32:03.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n"
Dec 14 13:32:03.088: INFO: stdout: "\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk\naffinity-clusterip-timeout-7qxpk"
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.088: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.089: INFO: Received response from host: affinity-clusterip-timeout-7qxpk
Dec 14 13:32:03.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 execpod-affinityzfvvh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.7.170:80/'
Dec 14 13:32:03.468: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n"
Dec 14 13:32:03.468: INFO: stdout: "affinity-clusterip-timeout-7qxpk"
Dec 14 13:32:18.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-214741859 exec --namespace=services-4479 execpod-affinityzfvvh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.7.170:80/'
Dec 14 13:32:19.002: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.7.170:80/\n"
Dec 14 13:32:19.002: INFO: stdout: "affinity-clusterip-timeout-6jj9w"
Dec 14 13:32:19.002: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4479, will wait for the garbage collector to delete the pods
Dec 14 13:32:19.099: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.183273ms
Dec 14 13:32:19.200: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.071397ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:32.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4479" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


• [SLOW TEST:50.947 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":30,"skipped":447,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 13:32:29.643: INFO: Waiting up to 5m0s for pod "pod-ad5ed2b7-8b38-4a85-984a-4086163817b6" in namespace "emptydir-3932" to be "Succeeded or Failed"
Dec 14 13:32:29.648: INFO: Pod "pod-ad5ed2b7-8b38-4a85-984a-4086163817b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198214ms
Dec 14 13:32:31.654: INFO: Pod "pod-ad5ed2b7-8b38-4a85-984a-4086163817b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010345692s
Dec 14 13:32:33.657: INFO: Pod "pod-ad5ed2b7-8b38-4a85-984a-4086163817b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013385882s
STEP: Saw pod success
Dec 14 13:32:33.657: INFO: Pod "pod-ad5ed2b7-8b38-4a85-984a-4086163817b6" satisfied condition "Succeeded or Failed"
Dec 14 13:32:33.659: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-ad5ed2b7-8b38-4a85-984a-4086163817b6 container test-container: <nil>
STEP: delete the pod
Dec 14 13:32:33.682: INFO: Waiting for pod pod-ad5ed2b7-8b38-4a85-984a-4086163817b6 to disappear
Dec 14 13:32:33.685: INFO: Pod pod-ad5ed2b7-8b38-4a85-984a-4086163817b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:33.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3932" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":28,"skipped":405,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:32.176: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-6878df79-a935-41d9-9aac-19b75de85f77
STEP: Creating a pod to test consume secrets
Dec 14 13:32:32.244: INFO: Waiting up to 5m0s for pod "pod-secrets-c9f58c07-1491-4279-ae95-098546d14193" in namespace "secrets-1013" to be "Succeeded or Failed"
Dec 14 13:32:32.258: INFO: Pod "pod-secrets-c9f58c07-1491-4279-ae95-098546d14193": Phase="Pending", Reason="", readiness=false. Elapsed: 13.502768ms
Dec 14 13:32:34.262: INFO: Pod "pod-secrets-c9f58c07-1491-4279-ae95-098546d14193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017779505s
Dec 14 13:32:36.266: INFO: Pod "pod-secrets-c9f58c07-1491-4279-ae95-098546d14193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021545034s
STEP: Saw pod success
Dec 14 13:32:36.266: INFO: Pod "pod-secrets-c9f58c07-1491-4279-ae95-098546d14193" satisfied condition "Succeeded or Failed"
Dec 14 13:32:36.269: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-secrets-c9f58c07-1491-4279-ae95-098546d14193 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:32:36.296: INFO: Waiting for pod pod-secrets-c9f58c07-1491-4279-ae95-098546d14193 to disappear
Dec 14 13:32:36.300: INFO: Pod pod-secrets-c9f58c07-1491-4279-ae95-098546d14193 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1013" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":451,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:33.720: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:32:36.819: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:36.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2871" for this suite.

•
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":416,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:24.072: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:37.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1595" for this suite.


• [SLOW TEST:13.132 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":-1,"completed":39,"skipped":870,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:31.204: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:32:31.870: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:32:33.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549551, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549551, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549551, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549551, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:32:36.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:38.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8962" for this suite.
STEP: Destroying namespace "webhook-8962-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:6.923 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":-1,"completed":60,"skipped":931,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:38.144: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-f029b6ec-23b1-4e14-a84c-700774e0b835
STEP: Creating a pod to test consume configMaps
Dec 14 13:32:38.219: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf" in namespace "projected-9866" to be "Succeeded or Failed"
Dec 14 13:32:38.227: INFO: Pod "pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.263263ms
Dec 14 13:32:40.230: INFO: Pod "pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955511s
Dec 14 13:32:42.235: INFO: Pod "pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015009237s
STEP: Saw pod success
Dec 14 13:32:42.235: INFO: Pod "pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf" satisfied condition "Succeeded or Failed"
Dec 14 13:32:42.239: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 13:32:42.284: INFO: Waiting for pod pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf to disappear
Dec 14 13:32:42.288: INFO: Pod pod-projected-configmaps-010c61a3-20e2-4e62-b7e5-bfd32e61e9cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:42.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9866" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":61,"skipped":936,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 14 13:32:37.640: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:32:37.664: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:32:39.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549557, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549557, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549557, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549557, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:32:42.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:42.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3827" for this suite.
STEP: Destroying namespace "webhook-3827-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:5.930 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":-1,"completed":30,"skipped":450,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:42.917: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 13:32:42.966: INFO: Waiting up to 5m0s for pod "pod-d4d43c08-45db-4065-a619-aef163a546c3" in namespace "emptydir-9480" to be "Succeeded or Failed"
Dec 14 13:32:42.976: INFO: Pod "pod-d4d43c08-45db-4065-a619-aef163a546c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.812065ms
Dec 14 13:32:44.980: INFO: Pod "pod-d4d43c08-45db-4065-a619-aef163a546c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013945174s
Dec 14 13:32:46.985: INFO: Pod "pod-d4d43c08-45db-4065-a619-aef163a546c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018638749s
STEP: Saw pod success
Dec 14 13:32:46.985: INFO: Pod "pod-d4d43c08-45db-4065-a619-aef163a546c3" satisfied condition "Succeeded or Failed"
Dec 14 13:32:46.987: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-d4d43c08-45db-4065-a619-aef163a546c3 container test-container: <nil>
STEP: delete the pod
Dec 14 13:32:47.014: INFO: Waiting for pod pod-d4d43c08-45db-4065-a619-aef163a546c3 to disappear
Dec 14 13:32:47.018: INFO: Pod pod-d4d43c08-45db-4065-a619-aef163a546c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:47.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9480" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":456,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:47.063: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-0a6d0c87-1265-4cdc-b91a-dfab7a0e3f6b
STEP: Creating a pod to test consume secrets
Dec 14 13:32:47.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4" in namespace "projected-2865" to be "Succeeded or Failed"
Dec 14 13:32:47.115: INFO: Pod "pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24397ms
Dec 14 13:32:49.130: INFO: Pod "pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019645764s
Dec 14 13:32:51.135: INFO: Pod "pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024353802s
STEP: Saw pod success
Dec 14 13:32:51.135: INFO: Pod "pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4" satisfied condition "Succeeded or Failed"
Dec 14 13:32:51.138: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:32:51.164: INFO: Waiting for pod pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4 to disappear
Dec 14 13:32:51.167: INFO: Pod pod-projected-secrets-9f104a04-f648-4d0b-ba86-898c4c9202b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2865" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":472,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:51.208: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:32:52.046: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 14 13:32:52.049: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 14 13:32:52.049: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.049: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 14 13:32:52.049: INFO: Checking APIGroup: extensions
Dec 14 13:32:52.051: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Dec 14 13:32:52.051: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Dec 14 13:32:52.051: INFO: extensions/v1beta1 matches extensions/v1beta1
Dec 14 13:32:52.051: INFO: Checking APIGroup: apps
Dec 14 13:32:52.054: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 14 13:32:52.054: INFO: Versions found [{apps/v1 v1}]
Dec 14 13:32:52.054: INFO: apps/v1 matches apps/v1
Dec 14 13:32:52.054: INFO: Checking APIGroup: events.k8s.io
Dec 14 13:32:52.056: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 14 13:32:52.056: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.056: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 14 13:32:52.056: INFO: Checking APIGroup: authentication.k8s.io
Dec 14 13:32:52.059: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 14 13:32:52.059: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.059: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 14 13:32:52.059: INFO: Checking APIGroup: authorization.k8s.io
Dec 14 13:32:52.061: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 14 13:32:52.061: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.061: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 14 13:32:52.061: INFO: Checking APIGroup: autoscaling
Dec 14 13:32:52.063: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec 14 13:32:52.063: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 14 13:32:52.063: INFO: autoscaling/v1 matches autoscaling/v1
Dec 14 13:32:52.063: INFO: Checking APIGroup: batch
Dec 14 13:32:52.066: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 14 13:32:52.066: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
Dec 14 13:32:52.066: INFO: batch/v1 matches batch/v1
Dec 14 13:32:52.066: INFO: Checking APIGroup: certificates.k8s.io
Dec 14 13:32:52.068: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 14 13:32:52.068: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.068: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 14 13:32:52.068: INFO: Checking APIGroup: networking.k8s.io
Dec 14 13:32:52.070: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 14 13:32:52.070: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.070: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 14 13:32:52.070: INFO: Checking APIGroup: policy
Dec 14 13:32:52.072: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Dec 14 13:32:52.073: INFO: Versions found [{policy/v1beta1 v1beta1}]
Dec 14 13:32:52.073: INFO: policy/v1beta1 matches policy/v1beta1
Dec 14 13:32:52.073: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 14 13:32:52.076: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 14 13:32:52.076: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.076: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 14 13:32:52.077: INFO: Checking APIGroup: settings.k8s.io
Dec 14 13:32:52.079: INFO: PreferredVersion.GroupVersion: settings.k8s.io/v1alpha1
Dec 14 13:32:52.079: INFO: Versions found [{settings.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.079: INFO: settings.k8s.io/v1alpha1 matches settings.k8s.io/v1alpha1
Dec 14 13:32:52.079: INFO: Checking APIGroup: storage.k8s.io
Dec 14 13:32:52.082: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 14 13:32:52.082: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.082: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 14 13:32:52.082: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 14 13:32:52.084: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 14 13:32:52.084: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.084: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 14 13:32:52.084: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 14 13:32:52.086: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 14 13:32:52.086: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.086: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 14 13:32:52.086: INFO: Checking APIGroup: scheduling.k8s.io
Dec 14 13:32:52.089: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 14 13:32:52.089: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.089: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 14 13:32:52.089: INFO: Checking APIGroup: coordination.k8s.io
Dec 14 13:32:52.091: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 14 13:32:52.091: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.091: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 14 13:32:52.091: INFO: Checking APIGroup: node.k8s.io
Dec 14 13:32:52.093: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Dec 14 13:32:52.093: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.093: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Dec 14 13:32:52.093: INFO: Checking APIGroup: discovery.k8s.io
Dec 14 13:32:52.095: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Dec 14 13:32:52.095: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.095: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Dec 14 13:32:52.095: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 14 13:32:52.097: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1alpha1
Dec 14 13:32:52.097: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Dec 14 13:32:52.097: INFO: flowcontrol.apiserver.k8s.io/v1alpha1 matches flowcontrol.apiserver.k8s.io/v1alpha1
Dec 14 13:32:52.097: INFO: Checking APIGroup: crd.projectcalico.org
Dec 14 13:32:52.100: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 14 13:32:52.100: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 14 13:32:52.100: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Dec 14 13:32:52.100: INFO: Checking APIGroup: monitoring.coreos.com
Dec 14 13:32:52.102: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Dec 14 13:32:52.102: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Dec 14 13:32:52.102: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Dec 14 13:32:52.102: INFO: Checking APIGroup: custom.metrics.k8s.io
Dec 14 13:32:52.104: INFO: PreferredVersion.GroupVersion: custom.metrics.k8s.io/v1beta1
Dec 14 13:32:52.104: INFO: Versions found [{custom.metrics.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.104: INFO: custom.metrics.k8s.io/v1beta1 matches custom.metrics.k8s.io/v1beta1
Dec 14 13:32:52.104: INFO: Checking APIGroup: metrics.k8s.io
Dec 14 13:32:52.107: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec 14 13:32:52.107: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec 14 13:32:52.107: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:52.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6242" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":-1,"completed":33,"skipped":481,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:52.152: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:32:52.223: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8cafab58-1303-4063-ac40-b1b7b4b15841", Controller:(*bool)(0xc0026aaae6), BlockOwnerDeletion:(*bool)(0xc0026aaae7)}}
Dec 14 13:32:52.232: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a3dc938f-dd4c-485f-a180-c84ea85f7753", Controller:(*bool)(0xc0039076a6), BlockOwnerDeletion:(*bool)(0xc0039076a7)}}
Dec 14 13:32:52.253: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"56658236-8c39-4188-8322-e46c739b2ae8", Controller:(*bool)(0xc003907866), BlockOwnerDeletion:(*bool)(0xc003907867)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9261" for this suite.


• [SLOW TEST:5.121 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":-1,"completed":34,"skipped":496,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:57.308: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-eab5cec8-99e4-440c-9fc8-d14326e91d0d
STEP: Creating a pod to test consume secrets
Dec 14 13:32:57.384: INFO: Waiting up to 5m0s for pod "pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72" in namespace "secrets-7402" to be "Succeeded or Failed"
Dec 14 13:32:57.391: INFO: Pod "pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190253ms
Dec 14 13:32:59.396: INFO: Pod "pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01204341s
STEP: Saw pod success
Dec 14 13:32:59.396: INFO: Pod "pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72" satisfied condition "Succeeded or Failed"
Dec 14 13:32:59.400: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:32:59.421: INFO: Waiting for pod pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72 to disappear
Dec 14 13:32:59.424: INFO: Pod pod-secrets-bf06786b-216f-4087-af75-b93c4a553d72 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:59.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7402" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":35,"skipped":512,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:36.357: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 14 13:32:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Dec 14 13:32:36.791: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 14 13:32:38.839: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:40.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:42.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:44.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:46.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:48.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:50.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:52.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549556, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:32:58.489: INFO: Waited 3.623377577s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:59.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9516" for this suite.


• [SLOW TEST:23.294 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":-1,"completed":32,"skipped":469,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 14 13:32:59.739: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7518 /api/v1/namespaces/watch-7518/configmaps/e2e-watch-test-watch-closed c432b97f-90bb-4c62-9495-318d422d8d23 25802 0 2020-12-14 13:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-14 13:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:32:59.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7518 /api/v1/namespaces/watch-7518/configmaps/e2e-watch-test-watch-closed c432b97f-90bb-4c62-9495-318d422d8d23 25805 0 2020-12-14 13:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-14 13:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 14 13:32:59.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7518 /api/v1/namespaces/watch-7518/configmaps/e2e-watch-test-watch-closed c432b97f-90bb-4c62-9495-318d422d8d23 25806 0 2020-12-14 13:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-14 13:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:32:59.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7518 /api/v1/namespaces/watch-7518/configmaps/e2e-watch-test-watch-closed c432b97f-90bb-4c62-9495-318d422d8d23 25807 0 2020-12-14 13:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-14 13:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:32:59.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7518" for this suite.

•
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":-1,"completed":33,"skipped":474,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:19.684: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 14 13:32:25.763: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5486 PodName:var-expansion-3e09e04a-ab94-4d2b-b884-c29ae72f1b85 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:32:25.763: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: test for file in mounted path
Dec 14 13:32:25.984: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5486 PodName:var-expansion-3e09e04a-ab94-4d2b-b884-c29ae72f1b85 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 14 13:32:25.984: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: updating the annotation value
Dec 14 13:32:26.738: INFO: Successfully updated pod "var-expansion-3e09e04a-ab94-4d2b-b884-c29ae72f1b85"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 14 13:32:26.743: INFO: Deleting pod "var-expansion-3e09e04a-ab94-4d2b-b884-c29ae72f1b85" in namespace "var-expansion-5486"
Dec 14 13:32:26.751: INFO: Wait up to 5m0s for pod "var-expansion-3e09e04a-ab94-4d2b-b884-c29ae72f1b85" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:00.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5486" for this suite.


• [SLOW TEST:41.084 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":-1,"completed":48,"skipped":885,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:42.310: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1119
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-1119
Dec 14 13:32:42.372: INFO: Found 0 stateful pods, waiting for 1
Dec 14 13:32:52.378: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:32:52.400: INFO: Deleting all statefulset in ns statefulset-1119
Dec 14 13:32:52.404: INFO: Scaling statefulset ss to 0
Dec 14 13:33:02.432: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:33:02.435: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:02.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1119" for this suite.


• [SLOW TEST:20.151 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":-1,"completed":62,"skipped":939,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:59.453: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:32:59.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11" in namespace "projected-8422" to be "Succeeded or Failed"
Dec 14 13:32:59.517: INFO: Pod "downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11": Phase="Pending", Reason="", readiness=false. Elapsed: 11.744563ms
Dec 14 13:33:01.522: INFO: Pod "downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01610573s
Dec 14 13:33:03.527: INFO: Pod "downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021038558s
STEP: Saw pod success
Dec 14 13:33:03.527: INFO: Pod "downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11" satisfied condition "Succeeded or Failed"
Dec 14 13:33:03.530: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-1 pod downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11 container client-container: <nil>
STEP: delete the pod
Dec 14 13:33:03.550: INFO: Waiting for pod downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11 to disappear
Dec 14 13:33:03.558: INFO: Pod downwardapi-volume-789e245e-b92f-4fd8-9144-69640ab72a11 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:03.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8422" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":520,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:59.819: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-16c53f1e-c67f-4fa8-8716-82f74f6053a1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:03.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7907" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":34,"skipped":487,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:02.502: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:33:02.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884" in namespace "downward-api-6985" to be "Succeeded or Failed"
Dec 14 13:33:02.558: INFO: Pod "downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884": Phase="Pending", Reason="", readiness=false. Elapsed: 13.631937ms
Dec 14 13:33:04.567: INFO: Pod "downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022238986s
Dec 14 13:33:06.572: INFO: Pod "downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027176333s
STEP: Saw pod success
Dec 14 13:33:06.572: INFO: Pod "downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884" satisfied condition "Succeeded or Failed"
Dec 14 13:33:06.575: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884 container client-container: <nil>
STEP: delete the pod
Dec 14 13:33:06.599: INFO: Waiting for pod downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884 to disappear
Dec 14 13:33:06.602: INFO: Pod downwardapi-volume-7dfc7a55-1311-497a-a9e8-de69be53c884 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:06.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6985" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":63,"skipped":957,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:03.631: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:33:04.198: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:33:06.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:33:09.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4472" for this suite.
STEP: Destroying namespace "webhook-4472-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:5.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":-1,"completed":37,"skipped":541,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:00.792: INFO: >>> kubeConfig: /tmp/kubeconfig-782519938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5732.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5732.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5732.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5732.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5732.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5732.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:33:04.887: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.891: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.910: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.924: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.927: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.935: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.939: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.945: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.949: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.952: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5732.svc.cluster.local from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.956: INFO: Unable to read jessie_udp@PodARecord from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.960: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa: the server could not find the requested resource (get pods dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa)
Dec 14 13:33:04.960: INFO: Lookups using dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5732.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5732.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5732.svc.cluster.local jessie_udp@dns-test-service-2.dns-5732.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5732.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec 14 13:33:10.012: INFO: DNS probes using dns-5732/dns-test-fc9de322-f0b2-4b4a-b15a-6780a05d9faa succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5732" for this suite.


• [SLOW TEST:9.276 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":-1,"completed":49,"skipped":896,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:06.641: INFO: >>> kubeConfig: /tmp/kubeconfig-669464615
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-6e7f3267-f585-4a5f-a38e-1957f7432be5
STEP: Creating a pod to test consume secrets
Dec 14 13:33:06.685: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d" in namespace "projected-1419" to be "Succeeded or Failed"
Dec 14 13:33:06.688: INFO: Pod "pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.387507ms
Dec 14 13:33:08.692: INFO: Pod "pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006860963s
Dec 14 13:33:10.699: INFO: Pod "pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013673236s
STEP: Saw pod success
Dec 14 13:33:10.699: INFO: Pod "pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d" satisfied condition "Succeeded or Failed"
Dec 14 13:33:10.702: INFO: Trying to get logs from node conformance-v1-19-pwlsgdsa6hrh-node-0 pod pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:33:10.726: INFO: Waiting for pod pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d to disappear
Dec 14 13:33:10.739: INFO: Pod pod-projected-secrets-6893b55e-cb52-4655-a819-765c3888e59d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:10.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1419" for this suite.

•
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":969,"failed":0}
Dec 14 13:33:10.753: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:04.054: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 14 13:33:04.856: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:33:04.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:33:06.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743549584, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:33:09.915: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:33:09.921: INFO: >>> kubeConfig: /tmp/kubeconfig-214741859
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-422-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:11.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9513" for this suite.
STEP: Destroying namespace "webhook-9513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


• [SLOW TEST:7.047 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":-1,"completed":35,"skipped":540,"failed":0}
Dec 14 13:33:11.105: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:31:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-517469967
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-722832ff-7811-4470-bc47-19ec24ade553 in namespace container-probe-5113
Dec 14 13:31:10.731: INFO: Started pod liveness-722832ff-7811-4470-bc47-19ec24ade553 in namespace container-probe-5113
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:31:10.734: INFO: Initial restart count of pod liveness-722832ff-7811-4470-bc47-19ec24ade553 is 0
Dec 14 13:31:28.787: INFO: Restart count of pod container-probe-5113/liveness-722832ff-7811-4470-bc47-19ec24ade553 is now 1 (18.05345492s elapsed)
Dec 14 13:31:48.849: INFO: Restart count of pod container-probe-5113/liveness-722832ff-7811-4470-bc47-19ec24ade553 is now 2 (38.11488498s elapsed)
Dec 14 13:32:08.922: INFO: Restart count of pod container-probe-5113/liveness-722832ff-7811-4470-bc47-19ec24ade553 is now 3 (58.18784362s elapsed)
Dec 14 13:32:26.974: INFO: Restart count of pod container-probe-5113/liveness-722832ff-7811-4470-bc47-19ec24ade553 is now 4 (1m16.2395001s elapsed)
Dec 14 13:33:29.126: INFO: Restart count of pod container-probe-5113/liveness-722832ff-7811-4470-bc47-19ec24ade553 is now 5 (2m18.392094666s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:33:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5113" for this suite.


• [SLOW TEST:142.481 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":-1,"completed":46,"skipped":797,"failed":0}
Dec 14 13:33:29.159: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:28:15.089: INFO: >>> kubeConfig: /tmp/kubeconfig-315446123
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6726
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6726
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6726
Dec 14 13:28:15.140: INFO: Found 0 stateful pods, waiting for 1
Dec 14 13:28:25.144: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 14 13:28:25.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:28:25.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:28:25.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:28:25.595: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:28:25.599: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 13:28:35.607: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:28:35.608: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:28:35.629: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:28:35.629: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:28:35.630: INFO: 
Dec 14 13:28:35.630: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 13:28:36.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993190388s
Dec 14 13:28:37.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987175842s
Dec 14 13:28:38.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979707879s
Dec 14 13:28:39.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970299576s
Dec 14 13:28:40.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964307886s
Dec 14 13:28:41.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956813851s
Dec 14 13:28:42.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951493612s
Dec 14 13:28:43.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94592872s
Dec 14 13:28:44.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.341508ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6726
Dec 14 13:28:45.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:28:46.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:28:46.251: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:28:46.251: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:28:46.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:28:46.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 13:28:46.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:28:46.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:28:46.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:28:47.297: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 13:28:47.297: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:28:47.297: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:28:47.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:28:47.302: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:28:47.302: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 14 13:28:47.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:28:47.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:28:47.707: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:28:47.707: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:28:47.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:28:48.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:28:48.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:28:48.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:28:48.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:28:48.572: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:28:48.572: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:28:48.572: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:28:48.572: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:28:48.577: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 13:28:58.591: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:28:58.591: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:28:58.591: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:28:58.605: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:28:58.605: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:28:58.606: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:28:58.606: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:28:58.606: INFO: 
Dec 14 13:28:58.606: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:28:59.610: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:28:59.610: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:28:59.610: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:28:59.610: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:28:59.611: INFO: 
Dec 14 13:28:59.611: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:00.621: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:00.621: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:00.621: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:00.621: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:00.621: INFO: 
Dec 14 13:29:00.621: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:01.626: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:01.626: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:01.626: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:01.627: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:01.627: INFO: 
Dec 14 13:29:01.627: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:02.632: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:02.632: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:02.632: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:02.632: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:02.632: INFO: 
Dec 14 13:29:02.632: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:03.639: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:03.639: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:03.639: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:03.639: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:03.639: INFO: 
Dec 14 13:29:03.639: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:04.653: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:04.653: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:04.653: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:04.653: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:04.653: INFO: 
Dec 14 13:29:04.653: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:05.658: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:05.658: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:05.658: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:05.658: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:05.658: INFO: 
Dec 14 13:29:05.658: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:06.663: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:06.663: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:06.663: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:06.663: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:06.663: INFO: 
Dec 14 13:29:06.663: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:29:07.669: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Dec 14 13:29:07.669: INFO: ss-0  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:15 +0000 UTC  }]
Dec 14 13:29:07.669: INFO: ss-1  conformance-v1-19-pwlsgdsa6hrh-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:07.669: INFO: ss-2  conformance-v1-19-pwlsgdsa6hrh-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-14 13:28:35 +0000 UTC  }]
Dec 14 13:29:07.669: INFO: 
Dec 14 13:29:07.669: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6726
Dec 14 13:29:08.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:29:08.982: INFO: rc: 1
Dec 14 13:29:08.982: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 14 13:29:18.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:29:19.177: INFO: rc: 1
Dec 14 13:29:19.177: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:29:29.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:29:29.410: INFO: rc: 1
Dec 14 13:29:29.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:29:39.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:29:39.700: INFO: rc: 1
Dec 14 13:29:39.700: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:29:49.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:29:49.920: INFO: rc: 1
Dec 14 13:29:49.921: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:29:59.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:00.141: INFO: rc: 1
Dec 14 13:30:00.141: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:30:10.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:10.372: INFO: rc: 1
Dec 14 13:30:10.373: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:30:20.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:20.605: INFO: rc: 1
Dec 14 13:30:20.605: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:30:30.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:30.809: INFO: rc: 1
Dec 14 13:30:30.810: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:30:40.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:40.995: INFO: rc: 1
Dec 14 13:30:40.995: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:30:50.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:30:51.192: INFO: rc: 1
Dec 14 13:30:51.192: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:01.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:01.376: INFO: rc: 1
Dec 14 13:31:01.376: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:11.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:11.563: INFO: rc: 1
Dec 14 13:31:11.564: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:21.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:21.755: INFO: rc: 1
Dec 14 13:31:21.755: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:31.917: INFO: rc: 1
Dec 14 13:31:31.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:41.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:42.084: INFO: rc: 1
Dec 14 13:31:42.084: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:31:52.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:31:52.315: INFO: rc: 1
Dec 14 13:31:52.315: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:02.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:02.500: INFO: rc: 1
Dec 14 13:32:02.500: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:12.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:12.734: INFO: rc: 1
Dec 14 13:32:12.734: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:22.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:22.919: INFO: rc: 1
Dec 14 13:32:22.919: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:33.156: INFO: rc: 1
Dec 14 13:32:33.157: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:43.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:43.391: INFO: rc: 1
Dec 14 13:32:43.391: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:32:53.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:32:53.709: INFO: rc: 1
Dec 14 13:32:53.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:03.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:03.878: INFO: rc: 1
Dec 14 13:33:03.878: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:13.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:14.094: INFO: rc: 1
Dec 14 13:33:14.094: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:24.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:24.269: INFO: rc: 1
Dec 14 13:33:24.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:34.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:34.438: INFO: rc: 1
Dec 14 13:33:34.438: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:44.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:44.631: INFO: rc: 1
Dec 14 13:33:44.631: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:33:54.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:33:54.813: INFO: rc: 1
Dec 14 13:33:54.813: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:34:04.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:34:04.995: INFO: rc: 1
Dec 14 13:34:04.995: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:34:14.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-315446123 exec --namespace=statefulset-6726 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:34:15.176: INFO: rc: 1
Dec 14 13:34:15.177: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Dec 14 13:34:15.177: INFO: Scaling statefulset ss to 0
Dec 14 13:34:15.227: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 14 13:34:15.230: INFO: Deleting all statefulset in ns statefulset-6726
Dec 14 13:34:15.234: INFO: Scaling statefulset ss to 0
Dec 14 13:34:15.243: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:34:15.246: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:34:15.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6726" for this suite.


• [SLOW TEST:360.189 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":-1,"completed":14,"skipped":273,"failed":0}
Dec 14 13:34:15.286: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:33:09.472: INFO: >>> kubeConfig: /tmp/kubeconfig-334331672
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-b415cd03-3cb2-4720-91ec-d8fc67c16347
STEP: Creating secret with name s-test-opt-upd-551d6035-b4f9-4417-b133-05828a9dde22
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b415cd03-3cb2-4720-91ec-d8fc67c16347
STEP: Updating secret s-test-opt-upd-551d6035-b4f9-4417-b133-05828a9dde22
STEP: Creating secret with name s-test-opt-create-c7e0f297-575c-4789-b5e3-cc9bd013ad42
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:34:26.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3581" for this suite.


• [SLOW TEST:76.603 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":560,"failed":0}
Dec 14 13:34:26.078: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 14 13:32:37.280: INFO: >>> kubeConfig: /tmp/kubeconfig-404816711
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 14 13:34:37.378: INFO: Deleting pod "var-expansion-26ea87df-0d2e-40c1-9013-40de090e4239" in namespace "var-expansion-9647"
Dec 14 13:34:37.387: INFO: Wait up to 5m0s for pod "var-expansion-26ea87df-0d2e-40c1-9013-40de090e4239" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 14 13:34:39.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9647" for this suite.


• [SLOW TEST:122.129 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":-1,"completed":40,"skipped":906,"failed":0}
Dec 14 13:34:39.413: INFO: Running AfterSuite actions on all nodes


Dec 14 13:33:10.088: INFO: Running AfterSuite actions on all nodes
Dec 14 13:34:39.485: INFO: Running AfterSuite actions on node 1
Dec 14 13:34:39.486: INFO: Skipping dumping logs from cluster


Ran 286 of 5234 Specs in 786.405 seconds
SUCCESS! -- 286 Passed | 0 Failed | 0 Pending | 4948 Skipped


Ginkgo ran 1 suite in 13m8.992579307s
Test Suite Passed
