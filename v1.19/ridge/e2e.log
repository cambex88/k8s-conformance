I0521 09:45:58.693944      23 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-234519559
I0521 09:45:58.694003      23 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0521 09:45:58.694233      23 e2e.go:129] Starting e2e run "cbb716f9-0954-492c-aa58-112d17dc96e2" on Ginkgo node 1
{"msg":"Test Suite starting","total":303,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621590356 - Will randomize all specs
Will run 303 of 5484 specs

May 21 09:45:58.772: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 09:45:58.778: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 21 09:45:58.801: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 21 09:45:58.879: INFO: The status of Pod wait-for-etcd-slff4 is Succeeded, skipping waiting
May 21 09:45:58.879: INFO: 46 / 47 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 21 09:45:58.879: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 21 09:45:58.879: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 21 09:45:58.896: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-ridge-node' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'fluent-bit' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'meta' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ridge-auth' (0 seconds elapsed)
May 21 09:45:58.897: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
May 21 09:45:58.897: INFO: e2e test version: v1.19.11
May 21 09:45:58.899: INFO: kube-apiserver version: v1.19.11
May 21 09:45:58.899: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 09:45:58.906: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:45:58.908: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename namespaces
May 21 09:45:58.995: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:46:05.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3738" for this suite.
STEP: Destroying namespace "nsdeletetest-3170" for this suite.
May 21 09:46:05.252: INFO: Namespace nsdeletetest-3170 was already deleted
STEP: Destroying namespace "nsdeletetest-5436" for this suite.

• [SLOW TEST:6.356 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":303,"completed":1,"skipped":13,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:46:05.267: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:46:05.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4534" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":303,"completed":2,"skipped":15,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:46:05.372: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
May 21 09:46:05.435: INFO: Major version: 1
STEP: Confirm minor version
May 21 09:46:05.435: INFO: cleanMinorVersion: 19
May 21 09:46:05.436: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:46:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-481" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":303,"completed":3,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:46:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3273
STEP: creating service affinity-clusterip in namespace services-3273
STEP: creating replication controller affinity-clusterip in namespace services-3273
I0521 09:46:05.611926      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-3273, replica count: 3
I0521 09:46:08.662913      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:46:11.664838      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:46:11.676: INFO: Creating new exec pod
May 21 09:46:16.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-3273 exec execpod-affinitydxj69 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 21 09:46:17.333: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 21 09:46:17.333: INFO: stdout: ""
May 21 09:46:17.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-3273 exec execpod-affinitydxj69 -- /bin/sh -x -c nc -zv -t -w 2 10.103.185.50 80'
May 21 09:46:17.619: INFO: stderr: "+ nc -zv -t -w 2 10.103.185.50 80\nConnection to 10.103.185.50 80 port [tcp/http] succeeded!\n"
May 21 09:46:17.619: INFO: stdout: ""
May 21 09:46:17.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-3273 exec execpod-affinitydxj69 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.185.50:80/ ; done'
May 21 09:46:18.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.185.50:80/\n"
May 21 09:46:18.001: INFO: stdout: "\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd\naffinity-clusterip-f4lqd"
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Received response from host: affinity-clusterip-f4lqd
May 21 09:46:18.001: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3273, will wait for the garbage collector to delete the pods
May 21 09:46:18.110: INFO: Deleting ReplicationController affinity-clusterip took: 11.522535ms
May 21 09:46:18.211: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.764793ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:46:27.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3273" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.544 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":4,"skipped":68,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:46:28.003: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2166
May 21 09:46:32.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 09:46:32.481: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 09:46:32.481: INFO: stdout: "iptables"
May 21 09:46:32.481: INFO: proxyMode: iptables
May 21 09:46:32.497: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:46:32.503: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:46:34.503: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:46:34.510: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:46:36.503: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:46:36.511: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:46:38.503: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:46:38.513: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2166
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2166
I0521 09:46:38.589930      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2166, replica count: 3
I0521 09:46:41.640478      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:46:44.641583      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:46:44.664: INFO: Creating new exec pod
May 21 09:46:49.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 21 09:46:50.025: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 21 09:46:50.025: INFO: stdout: ""
May 21 09:46:50.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c nc -zv -t -w 2 10.109.252.213 80'
May 21 09:46:50.319: INFO: stderr: "+ nc -zv -t -w 2 10.109.252.213 80\nConnection to 10.109.252.213 80 port [tcp/http] succeeded!\n"
May 21 09:46:50.319: INFO: stdout: ""
May 21 09:46:50.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.36 30437'
May 21 09:46:50.588: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.36 30437\nConnection to 172.30.52.36 30437 port [tcp/30437] succeeded!\n"
May 21 09:46:50.588: INFO: stdout: ""
May 21 09:46:50.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.10 30437'
May 21 09:46:50.867: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.10 30437\nConnection to 172.30.52.10 30437 port [tcp/30437] succeeded!\n"
May 21 09:46:50.867: INFO: stdout: ""
May 21 09:46:50.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.52.36:30437/ ; done'
May 21 09:46:51.276: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n"
May 21 09:46:51.276: INFO: stdout: "\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt\naffinity-nodeport-timeout-vptnt"
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Received response from host: affinity-nodeport-timeout-vptnt
May 21 09:46:51.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.52.36:30437/'
May 21 09:46:51.544: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n"
May 21 09:46:51.544: INFO: stdout: "affinity-nodeport-timeout-vptnt"
May 21 09:47:06.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2166 exec execpod-affinitynbvw8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.52.36:30437/'
May 21 09:47:06.911: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.52.36:30437/\n"
May 21 09:47:06.912: INFO: stdout: "affinity-nodeport-timeout-fjc9p"
May 21 09:47:06.912: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2166, will wait for the garbage collector to delete the pods
May 21 09:47:07.015: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.48248ms
May 21 09:47:07.416: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 400.30688ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:17.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2166" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:50.020 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":5,"skipped":68,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:18.024: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-12
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-12
STEP: creating replication controller externalsvc in namespace services-12
I0521 09:47:18.246990      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-12, replica count: 2
I0521 09:47:21.297839      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:47:24.298725      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 21 09:47:24.359: INFO: Creating new exec pod
May 21 09:47:28.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-12 exec execpod2tkf9 -- /bin/sh -x -c nslookup clusterip-service.services-12.svc.cluster.local'
May 21 09:47:28.719: INFO: stderr: "+ nslookup clusterip-service.services-12.svc.cluster.local\n"
May 21 09:47:28.719: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-12.svc.cluster.local\tcanonical name = externalsvc.services-12.svc.cluster.local.\nName:\texternalsvc.services-12.svc.cluster.local\nAddress: 10.105.68.18\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-12, will wait for the garbage collector to delete the pods
May 21 09:47:28.794: INFO: Deleting ReplicationController externalsvc took: 17.163714ms
May 21 09:47:29.194: INFO: Terminating ReplicationController externalsvc pods took: 400.185866ms
May 21 09:47:37.982: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:38.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-12" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.016 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":303,"completed":6,"skipped":73,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:38.044: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
May 21 09:47:38.133: INFO: created test-event-1
May 21 09:47:38.140: INFO: created test-event-2
May 21 09:47:38.149: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 21 09:47:38.152: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 21 09:47:38.189: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:38.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9635" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":303,"completed":7,"skipped":102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:38.215: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:38.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4568" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":303,"completed":8,"skipped":143,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:38.302: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 09:47:38.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55" in namespace "downward-api-6305" to be "Succeeded or Failed"
May 21 09:47:38.418: INFO: Pod "downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.465012ms
May 21 09:47:40.424: INFO: Pod "downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012368007s
May 21 09:47:42.436: INFO: Pod "downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023821674s
STEP: Saw pod success
May 21 09:47:42.436: INFO: Pod "downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55" satisfied condition "Succeeded or Failed"
May 21 09:47:42.442: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55 container client-container: <nil>
STEP: delete the pod
May 21 09:47:42.505: INFO: Waiting for pod downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55 to disappear
May 21 09:47:42.510: INFO: Pod downwardapi-volume-fad69fc1-b1a6-42b3-bfce-3eeb58530d55 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6305" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":9,"skipped":156,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 09:47:43.176: INFO: Checking APIGroup: apiregistration.k8s.io
May 21 09:47:43.177: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 21 09:47:43.177: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.177: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 21 09:47:43.177: INFO: Checking APIGroup: extensions
May 21 09:47:43.178: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 21 09:47:43.178: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 21 09:47:43.178: INFO: extensions/v1beta1 matches extensions/v1beta1
May 21 09:47:43.178: INFO: Checking APIGroup: apps
May 21 09:47:43.180: INFO: PreferredVersion.GroupVersion: apps/v1
May 21 09:47:43.180: INFO: Versions found [{apps/v1 v1}]
May 21 09:47:43.180: INFO: apps/v1 matches apps/v1
May 21 09:47:43.180: INFO: Checking APIGroup: events.k8s.io
May 21 09:47:43.181: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 21 09:47:43.181: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.181: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 21 09:47:43.181: INFO: Checking APIGroup: authentication.k8s.io
May 21 09:47:43.182: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 21 09:47:43.182: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.182: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 21 09:47:43.182: INFO: Checking APIGroup: authorization.k8s.io
May 21 09:47:43.183: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 21 09:47:43.183: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.183: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 21 09:47:43.183: INFO: Checking APIGroup: autoscaling
May 21 09:47:43.184: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 21 09:47:43.184: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 21 09:47:43.185: INFO: autoscaling/v1 matches autoscaling/v1
May 21 09:47:43.185: INFO: Checking APIGroup: batch
May 21 09:47:43.186: INFO: PreferredVersion.GroupVersion: batch/v1
May 21 09:47:43.186: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 21 09:47:43.186: INFO: batch/v1 matches batch/v1
May 21 09:47:43.186: INFO: Checking APIGroup: certificates.k8s.io
May 21 09:47:43.187: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 21 09:47:43.187: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.187: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 21 09:47:43.187: INFO: Checking APIGroup: networking.k8s.io
May 21 09:47:43.188: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 21 09:47:43.188: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.188: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 21 09:47:43.188: INFO: Checking APIGroup: policy
May 21 09:47:43.189: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 21 09:47:43.189: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 21 09:47:43.189: INFO: policy/v1beta1 matches policy/v1beta1
May 21 09:47:43.189: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 21 09:47:43.190: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 21 09:47:43.190: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.190: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 21 09:47:43.190: INFO: Checking APIGroup: storage.k8s.io
May 21 09:47:43.191: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 21 09:47:43.191: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.191: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 21 09:47:43.191: INFO: Checking APIGroup: admissionregistration.k8s.io
May 21 09:47:43.192: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 21 09:47:43.192: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.192: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 21 09:47:43.192: INFO: Checking APIGroup: apiextensions.k8s.io
May 21 09:47:43.194: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 21 09:47:43.194: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.194: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 21 09:47:43.194: INFO: Checking APIGroup: scheduling.k8s.io
May 21 09:47:43.195: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 21 09:47:43.195: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.195: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 21 09:47:43.195: INFO: Checking APIGroup: coordination.k8s.io
May 21 09:47:43.196: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 21 09:47:43.196: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.197: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 21 09:47:43.197: INFO: Checking APIGroup: node.k8s.io
May 21 09:47:43.198: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
May 21 09:47:43.198: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.198: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
May 21 09:47:43.198: INFO: Checking APIGroup: discovery.k8s.io
May 21 09:47:43.199: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 21 09:47:43.199: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 21 09:47:43.199: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:43.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6991" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":303,"completed":10,"skipped":158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:43.231: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-02fe4e66-a3f9-4424-ae8b-c74922e8ba0c
STEP: Creating a pod to test consume secrets
May 21 09:47:43.422: INFO: Waiting up to 5m0s for pod "pod-secrets-300b750a-3abc-4583-906f-36e14251528b" in namespace "secrets-4909" to be "Succeeded or Failed"
May 21 09:47:43.429: INFO: Pod "pod-secrets-300b750a-3abc-4583-906f-36e14251528b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.090608ms
May 21 09:47:45.436: INFO: Pod "pod-secrets-300b750a-3abc-4583-906f-36e14251528b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013808157s
May 21 09:47:47.445: INFO: Pod "pod-secrets-300b750a-3abc-4583-906f-36e14251528b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023013291s
STEP: Saw pod success
May 21 09:47:47.446: INFO: Pod "pod-secrets-300b750a-3abc-4583-906f-36e14251528b" satisfied condition "Succeeded or Failed"
May 21 09:47:47.452: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-300b750a-3abc-4583-906f-36e14251528b container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:47:47.496: INFO: Waiting for pod pod-secrets-300b750a-3abc-4583-906f-36e14251528b to disappear
May 21 09:47:47.505: INFO: Pod pod-secrets-300b750a-3abc-4583-906f-36e14251528b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:47.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4909" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":11,"skipped":224,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:47.524: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:47:47.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8299" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":303,"completed":12,"skipped":227,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:47:47.728: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
May 21 09:47:47.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 21 09:47:47.960: INFO: stderr: ""
May 21 09:47:47.960: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
May 21 09:47:47.960: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 21 09:47:47.960: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8712" to be "running and ready, or succeeded"
May 21 09:47:47.968: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011562ms
May 21 09:47:49.973: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012224852s
May 21 09:47:51.978: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.017782838s
May 21 09:47:51.978: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 21 09:47:51.978: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 21 09:47:51.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator'
May 21 09:47:52.141: INFO: stderr: ""
May 21 09:47:52.141: INFO: stdout: "I0521 09:47:50.029311       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/q9d 589\nI0521 09:47:50.229428       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/4wr8 562\nI0521 09:47:50.429446       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/qnr 340\nI0521 09:47:50.629509       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7g8r 255\nI0521 09:47:50.829505       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/f62t 220\nI0521 09:47:51.029373       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4wq7 377\nI0521 09:47:51.229392       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/t5p 464\nI0521 09:47:51.429598       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/lmc 410\nI0521 09:47:51.629418       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/mhk5 298\nI0521 09:47:51.829422       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/qvk9 203\nI0521 09:47:52.029381       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/dnd 301\n"
STEP: limiting log lines
May 21 09:47:52.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator --tail=1'
May 21 09:47:52.265: INFO: stderr: ""
May 21 09:47:52.265: INFO: stdout: "I0521 09:47:52.229378       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/bkz4 240\n"
May 21 09:47:52.265: INFO: got output "I0521 09:47:52.229378       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/bkz4 240\n"
STEP: limiting log bytes
May 21 09:47:52.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator --limit-bytes=1'
May 21 09:47:52.387: INFO: stderr: ""
May 21 09:47:52.387: INFO: stdout: "I"
May 21 09:47:52.387: INFO: got output "I"
STEP: exposing timestamps
May 21 09:47:52.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator --tail=1 --timestamps'
May 21 09:47:52.530: INFO: stderr: ""
May 21 09:47:52.530: INFO: stdout: "2021-05-21T09:47:52.429710325Z I0521 09:47:52.429431       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/sjg 597\n"
May 21 09:47:52.530: INFO: got output "2021-05-21T09:47:52.429710325Z I0521 09:47:52.429431       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/sjg 597\n"
STEP: restricting to a time range
May 21 09:47:55.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator --since=1s'
May 21 09:47:55.204: INFO: stderr: ""
May 21 09:47:55.204: INFO: stdout: "I0521 09:47:54.229462       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/s56m 233\nI0521 09:47:54.429458       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/mtrl 463\nI0521 09:47:54.629451       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/wvbl 566\nI0521 09:47:54.829406       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2rm 562\nI0521 09:47:55.029474       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/9t9 496\n"
May 21 09:47:55.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 logs logs-generator logs-generator --since=24h'
May 21 09:47:55.338: INFO: stderr: ""
May 21 09:47:55.338: INFO: stdout: "I0521 09:47:50.029311       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/q9d 589\nI0521 09:47:50.229428       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/4wr8 562\nI0521 09:47:50.429446       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/qnr 340\nI0521 09:47:50.629509       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7g8r 255\nI0521 09:47:50.829505       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/f62t 220\nI0521 09:47:51.029373       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4wq7 377\nI0521 09:47:51.229392       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/t5p 464\nI0521 09:47:51.429598       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/lmc 410\nI0521 09:47:51.629418       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/mhk5 298\nI0521 09:47:51.829422       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/qvk9 203\nI0521 09:47:52.029381       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/dnd 301\nI0521 09:47:52.229378       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/bkz4 240\nI0521 09:47:52.429431       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/sjg 597\nI0521 09:47:52.629406       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/zjb7 234\nI0521 09:47:52.829467       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/ktpm 387\nI0521 09:47:53.029394       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/k7hl 570\nI0521 09:47:53.229456       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/4fq 539\nI0521 09:47:53.429795       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nn9 274\nI0521 09:47:53.629583       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/ppx4 373\nI0521 09:47:53.829416       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/dn2x 340\nI0521 09:47:54.029354       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/nn9m 517\nI0521 09:47:54.229462       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/s56m 233\nI0521 09:47:54.429458       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/mtrl 463\nI0521 09:47:54.629451       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/wvbl 566\nI0521 09:47:54.829406       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2rm 562\nI0521 09:47:55.029474       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/9t9 496\nI0521 09:47:55.229457       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/hgdc 338\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
May 21 09:47:55.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-8712 delete pod logs-generator'
May 21 09:48:07.843: INFO: stderr: ""
May 21 09:48:07.843: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:07.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8712" for this suite.

• [SLOW TEST:20.138 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":303,"completed":13,"skipped":234,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:07.866: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 09:48:07.963: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a" in namespace "projected-6765" to be "Succeeded or Failed"
May 21 09:48:07.968: INFO: Pod "downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805654ms
May 21 09:48:09.976: INFO: Pod "downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012302743s
May 21 09:48:11.981: INFO: Pod "downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017936481s
STEP: Saw pod success
May 21 09:48:11.981: INFO: Pod "downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a" satisfied condition "Succeeded or Failed"
May 21 09:48:11.986: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a container client-container: <nil>
STEP: delete the pod
May 21 09:48:12.025: INFO: Waiting for pod downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a to disappear
May 21 09:48:12.031: INFO: Pod downwardapi-volume-69ce7b3f-6283-40cf-973d-13054f279e4a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:12.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6765" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":303,"completed":14,"skipped":234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:12.067: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7271 to expose endpoints map[]
May 21 09:48:12.215: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 21 09:48:13.229: INFO: successfully validated that service multi-endpoint-test in namespace services-7271 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7271 to expose endpoints map[pod1:[100]]
May 21 09:48:17.267: INFO: successfully validated that service multi-endpoint-test in namespace services-7271 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7271 to expose endpoints map[pod1:[100] pod2:[101]]
May 21 09:48:21.308: INFO: successfully validated that service multi-endpoint-test in namespace services-7271 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7271 to expose endpoints map[pod2:[101]]
May 21 09:48:21.369: INFO: successfully validated that service multi-endpoint-test in namespace services-7271 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7271 to expose endpoints map[]
May 21 09:48:21.433: INFO: successfully validated that service multi-endpoint-test in namespace services-7271 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:21.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7271" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.455 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":303,"completed":15,"skipped":389,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:21.524: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:21.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2806" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":303,"completed":16,"skipped":390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:21.684: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 21 09:48:21.769: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:42.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4670" for this suite.

• [SLOW TEST:20.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":303,"completed":17,"skipped":428,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:42.322: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-2b5a433f-1464-4fd7-9586-7c4804379d11
STEP: Creating a pod to test consume configMaps
May 21 09:48:42.453: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6" in namespace "projected-8243" to be "Succeeded or Failed"
May 21 09:48:42.468: INFO: Pod "pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.031963ms
May 21 09:48:44.476: INFO: Pod "pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023177618s
May 21 09:48:46.483: INFO: Pod "pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030081192s
STEP: Saw pod success
May 21 09:48:46.483: INFO: Pod "pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6" satisfied condition "Succeeded or Failed"
May 21 09:48:46.489: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 09:48:46.565: INFO: Waiting for pod pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6 to disappear
May 21 09:48:46.572: INFO: Pod pod-projected-configmaps-7a2e0103-f795-43a6-b4cb-c3bc85f1fbb6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:48:46.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8243" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":18,"skipped":433,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:48:46.609: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:02.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1152" for this suite.

• [SLOW TEST:16.302 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":303,"completed":19,"skipped":448,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:02.912: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:20.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1630" for this suite.

• [SLOW TEST:17.286 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":303,"completed":20,"skipped":459,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 09:49:20.330: INFO: Waiting up to 5m0s for pod "pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1" in namespace "emptydir-5427" to be "Succeeded or Failed"
May 21 09:49:20.341: INFO: Pod "pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.780604ms
May 21 09:49:22.351: INFO: Pod "pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02042662s
May 21 09:49:24.377: INFO: Pod "pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047015586s
STEP: Saw pod success
May 21 09:49:24.378: INFO: Pod "pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1" satisfied condition "Succeeded or Failed"
May 21 09:49:24.383: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1 container test-container: <nil>
STEP: delete the pod
May 21 09:49:24.529: INFO: Waiting for pod pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1 to disappear
May 21 09:49:24.536: INFO: Pod pod-fca7c1f8-deb5-48e9-90b8-51037ae66ad1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5427" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":21,"skipped":460,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:24.576: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a751c7a5-fbbe-4e5f-8212-5f76c9feeb63
STEP: Creating a pod to test consume configMaps
May 21 09:49:24.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7" in namespace "configmap-5963" to be "Succeeded or Failed"
May 21 09:49:24.752: INFO: Pod "pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.12542ms
May 21 09:49:26.763: INFO: Pod "pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033033908s
May 21 09:49:28.776: INFO: Pod "pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046264317s
STEP: Saw pod success
May 21 09:49:28.776: INFO: Pod "pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7" satisfied condition "Succeeded or Failed"
May 21 09:49:28.785: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 09:49:28.865: INFO: Waiting for pod pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7 to disappear
May 21 09:49:28.873: INFO: Pod pod-configmaps-c15ca211-00e0-49fa-a395-c9694c3906b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5963" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":22,"skipped":475,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:28.902: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d3b8b23e-2987-47c3-ba95-a51c2d214695
STEP: Creating a pod to test consume configMaps
May 21 09:49:29.027: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a" in namespace "configmap-3792" to be "Succeeded or Failed"
May 21 09:49:29.044: INFO: Pod "pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.17013ms
May 21 09:49:31.052: INFO: Pod "pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025030271s
May 21 09:49:33.063: INFO: Pod "pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036511154s
STEP: Saw pod success
May 21 09:49:33.063: INFO: Pod "pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a" satisfied condition "Succeeded or Failed"
May 21 09:49:33.069: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a container configmap-volume-test: <nil>
STEP: delete the pod
May 21 09:49:33.140: INFO: Waiting for pod pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a to disappear
May 21 09:49:33.148: INFO: Pod pod-configmaps-a0d9c336-e28e-4405-bb45-ce297619d48a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:33.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3792" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":23,"skipped":479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:33.177: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 09:49:33.261: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 09:49:33.281: INFO: Waiting for terminating namespaces to be deleted...
May 21 09:49:33.290: INFO: 
Logging pods the apiserver thinks is on node p1-maou9az9wksf7qjxozd15h7j6r before test
May 21 09:49:33.314: INFO: coredns-f9fd979d6-jnqgf from kube-system started at 2021-05-20 22:00:08 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container coredns ready: true, restart count 0
May 21 09:49:33.314: INFO: csi-ridge-node-x5xqt from kube-system started at 2021-05-21 08:39:57 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 09:49:33.314: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 09:49:33.314: INFO: fluent-bit-w7b5s from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 09:49:33.314: INFO: kube-proxy-n6n5f from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:49:33.314: INFO: meta-rltsl from kube-system started at 2021-05-21 08:38:47 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container meta ready: true, restart count 0
May 21 09:49:33.314: INFO: weave-net-jq22f from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container weave ready: true, restart count 1
May 21 09:49:33.314: INFO: 	Container weave-npc ready: true, restart count 0
May 21 09:49:33.314: INFO: sonobuoy-e2e-job-d98dbb7be59843f5 from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container e2e ready: true, restart count 0
May 21 09:49:33.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 09:49:33.314: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 09:49:33.314: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 09:49:33.314: INFO: 
Logging pods the apiserver thinks is on node p1-zq5sznp3cxrb94t9rscobyn6ny before test
May 21 09:49:33.347: INFO: csi-ridge-node-p4gmp from kube-system started at 2021-05-21 08:41:20 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.347: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 09:49:33.347: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 09:49:33.347: INFO: fluent-bit-v27pz from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.347: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 09:49:33.347: INFO: kube-proxy-bf9ms from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.347: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 09:49:33.347: INFO: meta-xbc48 from kube-system started at 2021-05-21 08:38:37 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.347: INFO: 	Container meta ready: true, restart count 0
May 21 09:49:33.347: INFO: weave-net-4ql9j from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.348: INFO: 	Container weave ready: true, restart count 1
May 21 09:49:33.348: INFO: 	Container weave-npc ready: true, restart count 0
May 21 09:49:33.348: INFO: sonobuoy from sonobuoy started at 2021-05-21 09:45:50 +0000 UTC (1 container statuses recorded)
May 21 09:49:33.348: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 09:49:33.348: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 09:49:33.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 09:49:33.348: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aaea4001-6a51-4d7a-9200-d966ff149b38 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-aaea4001-6a51-4d7a-9200-d966ff149b38 off the node p1-zq5sznp3cxrb94t9rscobyn6ny
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaea4001-6a51-4d7a-9200-d966ff149b38
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:49:49.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8203" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.652 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":303,"completed":24,"skipped":512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:49:49.831: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 09:49:50.028: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:50.028: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:50.028: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:50.034: INFO: Number of nodes with available pods: 0
May 21 09:49:50.034: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:51.050: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:51.050: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:51.050: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:51.055: INFO: Number of nodes with available pods: 0
May 21 09:49:51.055: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:52.054: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:52.055: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:52.055: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:52.063: INFO: Number of nodes with available pods: 0
May 21 09:49:52.063: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:53.053: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:53.053: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:53.053: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:53.061: INFO: Number of nodes with available pods: 0
May 21 09:49:53.061: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:54.050: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:54.050: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:54.050: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:54.060: INFO: Number of nodes with available pods: 0
May 21 09:49:54.060: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:55.054: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:55.054: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:55.055: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:55.065: INFO: Number of nodes with available pods: 0
May 21 09:49:55.065: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:49:56.062: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.062: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.062: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.071: INFO: Number of nodes with available pods: 2
May 21 09:49:56.071: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 21 09:49:56.149: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.149: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.149: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:49:56.156: INFO: Number of nodes with available pods: 2
May 21 09:49:56.156: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6192, will wait for the garbage collector to delete the pods
May 21 09:49:57.264: INFO: Deleting DaemonSet.extensions daemon-set took: 33.437477ms
May 21 09:49:57.765: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.301706ms
May 21 09:50:07.974: INFO: Number of nodes with available pods: 0
May 21 09:50:07.975: INFO: Number of running nodes: 0, number of available pods: 0
May 21 09:50:07.983: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6192/daemonsets","resourceVersion":"158427"},"items":null}

May 21 09:50:07.990: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6192/pods","resourceVersion":"158427"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:08.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6192" for this suite.

• [SLOW TEST:18.252 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":303,"completed":25,"skipped":550,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:50:09.083: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:50:11.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187409, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187409, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187409, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187409, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:50:14.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:24.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-732" for this suite.
STEP: Destroying namespace "webhook-732-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.721 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":303,"completed":26,"skipped":551,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:24.807: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-33819b5f-cb52-4e32-a4ae-915d8f76fe5a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-33819b5f-cb52-4e32-a4ae-915d8f76fe5a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9299" for this suite.

• [SLOW TEST:6.315 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":27,"skipped":565,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:31.123: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8339" for this suite.

• [SLOW TEST:20.187 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":303,"completed":28,"skipped":566,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:51.311: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-691" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":303,"completed":29,"skipped":569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:51.458: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 09:50:51.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9" in namespace "downward-api-8117" to be "Succeeded or Failed"
May 21 09:50:51.596: INFO: Pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823743ms
May 21 09:50:53.604: INFO: Pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014419715s
May 21 09:50:55.617: INFO: Pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026606513s
May 21 09:50:57.630: INFO: Pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040272133s
STEP: Saw pod success
May 21 09:50:57.630: INFO: Pod "downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9" satisfied condition "Succeeded or Failed"
May 21 09:50:57.642: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9 container client-container: <nil>
STEP: delete the pod
May 21 09:50:57.717: INFO: Waiting for pod downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9 to disappear
May 21 09:50:57.724: INFO: Pod downwardapi-volume-eb1e926a-625e-4e4f-a6e0-1dc25816dac9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:57.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8117" for this suite.

• [SLOW TEST:6.290 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":303,"completed":30,"skipped":619,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:57.751: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
May 21 09:50:57.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-4244 create -f -'
May 21 09:50:58.307: INFO: stderr: ""
May 21 09:50:58.307: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 21 09:50:58.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-4244 diff -f -'
May 21 09:50:58.798: INFO: rc: 1
May 21 09:50:58.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-4244 delete -f -'
May 21 09:50:58.927: INFO: stderr: ""
May 21 09:50:58.927: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:50:58.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4244" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":303,"completed":31,"skipped":623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:50:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 09:50:59.081: INFO: Waiting up to 5m0s for pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7" in namespace "downward-api-1418" to be "Succeeded or Failed"
May 21 09:50:59.117: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7": Phase="Pending", Reason="", readiness=false. Elapsed: 35.770783ms
May 21 09:51:01.145: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064632668s
May 21 09:51:03.167: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086156491s
May 21 09:51:05.175: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094590191s
May 21 09:51:07.185: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.103931719s
STEP: Saw pod success
May 21 09:51:07.185: INFO: Pod "downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7" satisfied condition "Succeeded or Failed"
May 21 09:51:07.192: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7 container dapi-container: <nil>
STEP: delete the pod
May 21 09:51:07.276: INFO: Waiting for pod downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7 to disappear
May 21 09:51:07.282: INFO: Pod downward-api-1403bc8e-797a-4f9d-a9cc-67cb9a77bad7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:51:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1418" for this suite.

• [SLOW TEST:8.348 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":303,"completed":32,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:51:07.321: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 21 09:51:07.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 create -f -'
May 21 09:51:07.782: INFO: stderr: ""
May 21 09:51:07.782: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 09:51:07.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:51:07.915: INFO: stderr: ""
May 21 09:51:07.915: INFO: stdout: "update-demo-nautilus-dvlfn update-demo-nautilus-rpr9t "
May 21 09:51:07.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-dvlfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:51:08.022: INFO: stderr: ""
May 21 09:51:08.022: INFO: stdout: ""
May 21 09:51:08.022: INFO: update-demo-nautilus-dvlfn is created but not running
May 21 09:51:13.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:51:13.139: INFO: stderr: ""
May 21 09:51:13.139: INFO: stdout: "update-demo-nautilus-dvlfn update-demo-nautilus-rpr9t "
May 21 09:51:13.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-dvlfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:51:13.268: INFO: stderr: ""
May 21 09:51:13.268: INFO: stdout: ""
May 21 09:51:13.268: INFO: update-demo-nautilus-dvlfn is created but not running
May 21 09:51:18.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 09:51:18.394: INFO: stderr: ""
May 21 09:51:18.395: INFO: stdout: "update-demo-nautilus-dvlfn update-demo-nautilus-rpr9t "
May 21 09:51:18.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-dvlfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:51:18.490: INFO: stderr: ""
May 21 09:51:18.490: INFO: stdout: "true"
May 21 09:51:18.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-dvlfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:51:18.594: INFO: stderr: ""
May 21 09:51:18.594: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 09:51:18.594: INFO: validating pod update-demo-nautilus-dvlfn
May 21 09:51:18.610: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:51:18.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:51:18.610: INFO: update-demo-nautilus-dvlfn is verified up and running
May 21 09:51:18.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-rpr9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 09:51:18.709: INFO: stderr: ""
May 21 09:51:18.709: INFO: stdout: "true"
May 21 09:51:18.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods update-demo-nautilus-rpr9t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 09:51:18.813: INFO: stderr: ""
May 21 09:51:18.813: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 09:51:18.813: INFO: validating pod update-demo-nautilus-rpr9t
May 21 09:51:18.827: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 09:51:18.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 09:51:18.827: INFO: update-demo-nautilus-rpr9t is verified up and running
STEP: using delete to clean up resources
May 21 09:51:18.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 delete --grace-period=0 --force -f -'
May 21 09:51:18.943: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 09:51:18.943: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 09:51:18.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get rc,svc -l name=update-demo --no-headers'
May 21 09:51:19.088: INFO: stderr: "No resources found in kubectl-6664 namespace.\n"
May 21 09:51:19.088: INFO: stdout: ""
May 21 09:51:19.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6664 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 09:51:19.235: INFO: stderr: ""
May 21 09:51:19.235: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:51:19.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6664" for this suite.

• [SLOW TEST:11.971 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":303,"completed":33,"skipped":708,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:51:19.292: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 09:51:19.399: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:51:37.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5880" for this suite.

• [SLOW TEST:18.296 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":303,"completed":34,"skipped":718,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:51:37.590: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8426 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8426;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8426 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8426;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8426.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8426.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8426.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8426.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.99.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.99.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.99.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.99.172_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8426 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8426;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8426 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8426;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8426.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8426.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8426.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8426.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.99.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.99.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.99.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.99.172_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 09:51:49.843: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.852: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.860: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.879: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.885: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.894: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.901: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.906: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.979: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.985: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.993: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:49.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:50.005: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:50.022: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:50.028: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:50.044: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:50.084: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:51:55.096: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.105: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.113: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.124: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.152: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.162: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.171: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.226: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.231: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.239: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.245: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.252: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.259: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.265: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.275: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:51:55.318: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:52:00.095: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.102: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.111: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.141: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.157: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.254: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.260: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.265: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.272: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.281: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.288: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.294: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.300: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:00.363: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:52:05.106: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.115: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.140: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.148: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.155: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.162: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.213: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.221: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.228: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.241: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.255: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.263: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:05.308: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:52:10.094: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.103: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.110: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.133: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.140: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.146: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.194: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.200: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.207: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.220: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.225: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.232: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.238: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:10.277: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:52:15.096: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.105: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.114: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.128: INFO: Unable to read wheezy_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.136: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.144: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.222: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.230: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.237: INFO: Unable to read jessie_udp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426 from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.252: INFO: Unable to read jessie_udp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.258: INFO: Unable to read jessie_tcp@dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.265: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.270: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc from pod dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528: the server could not find the requested resource (get pods dns-test-adf554ba-240c-49db-b464-64057109c528)
May 21 09:52:15.311: INFO: Lookups using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8426 wheezy_tcp@dns-test-service.dns-8426 wheezy_udp@dns-test-service.dns-8426.svc wheezy_tcp@dns-test-service.dns-8426.svc wheezy_udp@_http._tcp.dns-test-service.dns-8426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8426 jessie_tcp@dns-test-service.dns-8426 jessie_udp@dns-test-service.dns-8426.svc jessie_tcp@dns-test-service.dns-8426.svc jessie_udp@_http._tcp.dns-test-service.dns-8426.svc jessie_tcp@_http._tcp.dns-test-service.dns-8426.svc]

May 21 09:52:20.294: INFO: DNS probes using dns-8426/dns-test-adf554ba-240c-49db-b464-64057109c528 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:20.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8426" for this suite.

• [SLOW TEST:43.029 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":303,"completed":35,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:20.621: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-e04f0f49-f5c0-4458-922e-1bb0c2a39d3c
STEP: Creating a pod to test consume secrets
May 21 09:52:20.774: INFO: Waiting up to 5m0s for pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7" in namespace "secrets-2877" to be "Succeeded or Failed"
May 21 09:52:20.800: INFO: Pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.748327ms
May 21 09:52:22.811: INFO: Pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036671736s
May 21 09:52:24.824: INFO: Pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050326755s
May 21 09:52:26.834: INFO: Pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059732407s
STEP: Saw pod success
May 21 09:52:26.834: INFO: Pod "pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7" satisfied condition "Succeeded or Failed"
May 21 09:52:26.840: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7 container secret-volume-test: <nil>
STEP: delete the pod
May 21 09:52:26.922: INFO: Waiting for pod pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7 to disappear
May 21 09:52:26.930: INFO: Pod pod-secrets-19bb8a9d-1874-460c-95a3-a60fdf2d0ab7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:26.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2877" for this suite.

• [SLOW TEST:6.340 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":36,"skipped":795,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 09:52:33.703: INFO: Successfully updated pod "annotationupdate7c283242-4761-4346-9760-be8ea2628134"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:35.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4971" for this suite.

• [SLOW TEST:8.845 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":37,"skipped":816,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:35.809: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 21 09:52:44.023: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8640 PodName:pod-sharedvolume-c56a23a0-b830-4e0d-b9c3-5433ca7989a0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 09:52:44.023: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 09:52:44.169: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:44.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8640" for this suite.

• [SLOW TEST:8.392 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":303,"completed":38,"skipped":821,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:44.205: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88
May 21 09:52:44.373: INFO: Pod name my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88: Found 0 pods out of 1
May 21 09:52:49.386: INFO: Pod name my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88: Found 1 pods out of 1
May 21 09:52:49.386: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88" are running
May 21 09:52:51.404: INFO: Pod "my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88-kcmsl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 09:52:44 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 09:52:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 09:52:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 09:52:44 +0000 UTC Reason: Message:}])
May 21 09:52:51.406: INFO: Trying to dial the pod
May 21 09:52:56.441: INFO: Controller my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88: Got expected result from replica 1 [my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88-kcmsl]: "my-hostname-basic-8d2cccaa-7f91-4acc-8b61-231299b47a88-kcmsl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:52:56.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2902" for this suite.

• [SLOW TEST:12.272 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":39,"skipped":822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:52:56.480: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6226
May 21 09:53:02.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 09:53:02.968: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 21 09:53:02.968: INFO: stdout: "iptables"
May 21 09:53:02.968: INFO: proxyMode: iptables
May 21 09:53:03.006: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:53:03.016: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:53:05.017: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:53:05.026: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:53:07.017: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:53:07.027: INFO: Pod kube-proxy-mode-detector still exists
May 21 09:53:09.017: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 09:53:09.026: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6226
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6226
I0521 09:53:09.134388      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6226, replica count: 3
I0521 09:53:12.185098      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:53:15.185569      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 09:53:18.186111      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 09:53:18.202: INFO: Creating new exec pod
May 21 09:53:25.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec execpod-affinitydhtl6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 21 09:53:25.637: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 21 09:53:25.638: INFO: stdout: ""
May 21 09:53:25.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec execpod-affinitydhtl6 -- /bin/sh -x -c nc -zv -t -w 2 10.102.195.150 80'
May 21 09:53:25.927: INFO: stderr: "+ nc -zv -t -w 2 10.102.195.150 80\nConnection to 10.102.195.150 80 port [tcp/http] succeeded!\n"
May 21 09:53:25.927: INFO: stdout: ""
May 21 09:53:25.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec execpod-affinitydhtl6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.195.150:80/ ; done'
May 21 09:53:26.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n"
May 21 09:53:26.301: INFO: stdout: "\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb\naffinity-clusterip-timeout-hb6zb"
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.301: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Received response from host: affinity-clusterip-timeout-hb6zb
May 21 09:53:26.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec execpod-affinitydhtl6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.195.150:80/'
May 21 09:53:26.567: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n"
May 21 09:53:26.567: INFO: stdout: "affinity-clusterip-timeout-hb6zb"
May 21 09:53:41.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6226 exec execpod-affinitydhtl6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.195.150:80/'
May 21 09:53:41.907: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.195.150:80/\n"
May 21 09:53:41.907: INFO: stdout: "affinity-clusterip-timeout-bv2k9"
May 21 09:53:41.907: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6226, will wait for the garbage collector to delete the pods
May 21 09:53:42.089: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 22.068618ms
May 21 09:53:42.590: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 500.622422ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:53:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:61.641 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":40,"skipped":847,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:53:58.126: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
May 21 09:53:58.260: INFO: Waiting up to 5m0s for pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a" in namespace "containers-8366" to be "Succeeded or Failed"
May 21 09:53:58.276: INFO: Pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.516995ms
May 21 09:54:00.286: INFO: Pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0254408s
May 21 09:54:02.299: INFO: Pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038800391s
May 21 09:54:04.309: INFO: Pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049273754s
STEP: Saw pod success
May 21 09:54:04.310: INFO: Pod "client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a" satisfied condition "Succeeded or Failed"
May 21 09:54:04.317: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a container test-container: <nil>
STEP: delete the pod
May 21 09:54:04.411: INFO: Waiting for pod client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a to disappear
May 21 09:54:04.445: INFO: Pod client-containers-8df039e8-fa98-4e35-bd85-99c0573d546a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:04.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8366" for this suite.

• [SLOW TEST:6.366 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":303,"completed":41,"skipped":860,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:10.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9430" for this suite.

• [SLOW TEST:6.293 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":303,"completed":42,"skipped":869,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 09:54:10.913: INFO: Waiting up to 5m0s for pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842" in namespace "emptydir-6166" to be "Succeeded or Failed"
May 21 09:54:10.918: INFO: Pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842": Phase="Pending", Reason="", readiness=false. Elapsed: 4.874692ms
May 21 09:54:12.929: INFO: Pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015881599s
May 21 09:54:14.937: INFO: Pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023768796s
May 21 09:54:16.947: INFO: Pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034204829s
STEP: Saw pod success
May 21 09:54:16.947: INFO: Pod "pod-c3d72bfc-733d-4a36-a7cc-42648b883842" satisfied condition "Succeeded or Failed"
May 21 09:54:16.956: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-c3d72bfc-733d-4a36-a7cc-42648b883842 container test-container: <nil>
STEP: delete the pod
May 21 09:54:17.041: INFO: Waiting for pod pod-c3d72bfc-733d-4a36-a7cc-42648b883842 to disappear
May 21 09:54:17.050: INFO: Pod pod-c3d72bfc-733d-4a36-a7cc-42648b883842 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:17.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6166" for this suite.

• [SLOW TEST:6.294 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":43,"skipped":869,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:17.082: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-2867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2867 to expose endpoints map[]
May 21 09:54:17.244: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 21 09:54:18.280: INFO: successfully validated that service endpoint-test2 in namespace services-2867 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2867 to expose endpoints map[pod1:[80]]
May 21 09:54:22.364: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
May 21 09:54:24.370: INFO: successfully validated that service endpoint-test2 in namespace services-2867 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2867 to expose endpoints map[pod1:[80] pod2:[80]]
May 21 09:54:28.431: INFO: Unexpected endpoints: found map[74a24daa-e2e7-4b0d-a489-6e2b0c928c3a:[80]], expected map[pod1:[80] pod2:[80]], will retry
May 21 09:54:30.439: INFO: successfully validated that service endpoint-test2 in namespace services-2867 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2867 to expose endpoints map[pod2:[80]]
May 21 09:54:30.565: INFO: successfully validated that service endpoint-test2 in namespace services-2867 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2867 to expose endpoints map[]
May 21 09:54:30.628: INFO: successfully validated that service endpoint-test2 in namespace services-2867 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:30.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2867" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.662 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":303,"completed":44,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:30.745: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:30.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2352" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":303,"completed":45,"skipped":907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:31.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3065" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":303,"completed":46,"skipped":933,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 09:54:31.300: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06" in namespace "projected-8057" to be "Succeeded or Failed"
May 21 09:54:31.321: INFO: Pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06": Phase="Pending", Reason="", readiness=false. Elapsed: 21.309786ms
May 21 09:54:33.340: INFO: Pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039875929s
May 21 09:54:35.357: INFO: Pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057707929s
May 21 09:54:37.373: INFO: Pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073296115s
STEP: Saw pod success
May 21 09:54:37.373: INFO: Pod "downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06" satisfied condition "Succeeded or Failed"
May 21 09:54:37.383: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06 container client-container: <nil>
STEP: delete the pod
May 21 09:54:37.460: INFO: Waiting for pod downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06 to disappear
May 21 09:54:37.471: INFO: Pod downwardapi-volume-21c62dd0-1327-4974-b057-be11679e8f06 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:37.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8057" for this suite.

• [SLOW TEST:6.305 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":47,"skipped":947,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:37.503: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 09:54:37.602: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 21 09:54:39.697: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:40.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1939" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":303,"completed":48,"skipped":960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:40.756: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 09:54:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:42.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-605" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":303,"completed":49,"skipped":997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 09:54:49.233: INFO: Successfully updated pod "pod-update-cdb3e531-a239-4385-a6c4-15cae1c96032"
STEP: verifying the updated pod is in kubernetes
May 21 09:54:49.257: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6593" for this suite.

• [SLOW TEST:7.027 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":303,"completed":50,"skipped":1031,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:49.295: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 09:54:49.426: INFO: Waiting up to 5m0s for pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8" in namespace "emptydir-5367" to be "Succeeded or Failed"
May 21 09:54:49.448: INFO: Pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.634463ms
May 21 09:54:51.467: INFO: Pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040926478s
May 21 09:54:53.478: INFO: Pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052181316s
May 21 09:54:55.485: INFO: Pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059360348s
STEP: Saw pod success
May 21 09:54:55.485: INFO: Pod "pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8" satisfied condition "Succeeded or Failed"
May 21 09:54:55.493: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8 container test-container: <nil>
STEP: delete the pod
May 21 09:54:55.579: INFO: Waiting for pod pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8 to disappear
May 21 09:54:55.584: INFO: Pod pod-7e86be73-0a22-4cd0-9d30-fced1483e3e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:54:55.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5367" for this suite.

• [SLOW TEST:6.313 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":51,"skipped":1043,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:54:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0521 09:55:01.843269      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 09:56:03.898: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:56:03.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5037" for this suite.

• [SLOW TEST:68.337 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":303,"completed":52,"skipped":1050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:56:03.950: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:56:05.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:56:07.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 09:56:09.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 09:56:11.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187765, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:56:14.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:56:14.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6548" for this suite.
STEP: Destroying namespace "webhook-6548-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":303,"completed":53,"skipped":1095,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:56:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-9562/secret-test-2695d602-858d-4be4-9bea-913af7fad90f
STEP: Creating a pod to test consume secrets
May 21 09:56:14.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a" in namespace "secrets-9562" to be "Succeeded or Failed"
May 21 09:56:14.641: INFO: Pod "pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.927005ms
May 21 09:56:16.650: INFO: Pod "pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027616422s
May 21 09:56:18.663: INFO: Pod "pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039933076s
STEP: Saw pod success
May 21 09:56:18.663: INFO: Pod "pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a" satisfied condition "Succeeded or Failed"
May 21 09:56:18.670: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a container env-test: <nil>
STEP: delete the pod
May 21 09:56:18.749: INFO: Waiting for pod pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a to disappear
May 21 09:56:18.754: INFO: Pod pod-configmaps-e0efcb0f-6848-4b13-95e7-280db6be5c8a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:56:18.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9562" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":54,"skipped":1096,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:56:18.781: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 09:56:18.902: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 21 09:56:18.930: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:18.931: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:18.931: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:18.938: INFO: Number of nodes with available pods: 0
May 21 09:56:18.938: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:56:19.960: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:19.960: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:19.960: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:19.975: INFO: Number of nodes with available pods: 0
May 21 09:56:19.975: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:56:20.950: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:20.950: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:20.950: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:20.959: INFO: Number of nodes with available pods: 0
May 21 09:56:20.960: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:56:21.952: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:21.952: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:21.952: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:21.960: INFO: Number of nodes with available pods: 0
May 21 09:56:21.960: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 09:56:22.962: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:22.962: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:22.962: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:22.971: INFO: Number of nodes with available pods: 2
May 21 09:56:22.971: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 21 09:56:23.062: INFO: Wrong image for pod: daemon-set-fnqqd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:23.062: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:23.072: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:23.072: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:23.072: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:24.092: INFO: Wrong image for pod: daemon-set-fnqqd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:24.092: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:24.104: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:24.104: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:24.104: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:25.085: INFO: Wrong image for pod: daemon-set-fnqqd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:25.085: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:25.103: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:25.103: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:25.103: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:26.083: INFO: Wrong image for pod: daemon-set-fnqqd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:26.083: INFO: Pod daemon-set-fnqqd is not available
May 21 09:56:26.083: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:26.099: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:26.099: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:26.100: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:27.085: INFO: Pod daemon-set-5s62q is not available
May 21 09:56:27.085: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:27.096: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:27.096: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:27.096: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:28.083: INFO: Pod daemon-set-5s62q is not available
May 21 09:56:28.083: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:28.094: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:28.094: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:28.094: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:29.082: INFO: Pod daemon-set-5s62q is not available
May 21 09:56:29.082: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:29.093: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:29.093: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:29.093: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:30.086: INFO: Pod daemon-set-5s62q is not available
May 21 09:56:30.086: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:30.099: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:30.100: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:30.100: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:31.081: INFO: Pod daemon-set-5s62q is not available
May 21 09:56:31.081: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:31.099: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:31.099: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:31.099: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:32.082: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:32.091: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:32.091: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:32.091: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:33.085: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:33.085: INFO: Pod daemon-set-h74kz is not available
May 21 09:56:33.096: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:33.096: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:33.097: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:34.094: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:34.094: INFO: Pod daemon-set-h74kz is not available
May 21 09:56:34.106: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:34.106: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:34.106: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:35.089: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:35.089: INFO: Pod daemon-set-h74kz is not available
May 21 09:56:35.109: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:35.109: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:35.110: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:36.085: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:36.085: INFO: Pod daemon-set-h74kz is not available
May 21 09:56:36.098: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:36.098: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:36.098: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:37.080: INFO: Wrong image for pod: daemon-set-h74kz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 09:56:37.081: INFO: Pod daemon-set-h74kz is not available
May 21 09:56:37.093: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:37.094: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:37.094: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.080: INFO: Pod daemon-set-bb4b9 is not available
May 21 09:56:38.088: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.089: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.089: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 21 09:56:38.099: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.099: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.100: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:38.108: INFO: Number of nodes with available pods: 1
May 21 09:56:38.108: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:39.123: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:39.124: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:39.124: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:39.137: INFO: Number of nodes with available pods: 1
May 21 09:56:39.138: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:40.123: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:40.124: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:40.124: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:40.130: INFO: Number of nodes with available pods: 1
May 21 09:56:40.130: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:41.125: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:41.125: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:41.125: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:41.142: INFO: Number of nodes with available pods: 1
May 21 09:56:41.142: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:42.124: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:42.124: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:42.124: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:42.133: INFO: Number of nodes with available pods: 1
May 21 09:56:42.133: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:43.120: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:43.121: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:43.121: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:43.129: INFO: Number of nodes with available pods: 1
May 21 09:56:43.130: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 09:56:44.125: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:44.125: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:44.125: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 09:56:44.131: INFO: Number of nodes with available pods: 2
May 21 09:56:44.131: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8230, will wait for the garbage collector to delete the pods
May 21 09:56:44.277: INFO: Deleting DaemonSet.extensions daemon-set took: 25.485465ms
May 21 09:56:44.777: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.299109ms
May 21 09:56:57.894: INFO: Number of nodes with available pods: 0
May 21 09:56:57.894: INFO: Number of running nodes: 0, number of available pods: 0
May 21 09:56:57.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8230/daemonsets","resourceVersion":"161344"},"items":null}

May 21 09:56:57.907: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8230/pods","resourceVersion":"161344"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:56:57.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8230" for this suite.

• [SLOW TEST:39.197 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":303,"completed":55,"skipped":1107,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:56:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:57:03.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7552" for this suite.

• [SLOW TEST:5.257 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":303,"completed":56,"skipped":1128,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:57:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 09:57:04.123: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 09:57:06.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 09:57:08.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757187824, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 09:57:11.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 09:57:23.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7483" for this suite.
STEP: Destroying namespace "webhook-7483-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.669 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":303,"completed":57,"skipped":1133,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 09:57:23.911: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-c02a6610-47fd-4381-9afb-c78a2b8dcf09 in namespace container-probe-8456
May 21 09:57:30.087: INFO: Started pod test-webserver-c02a6610-47fd-4381-9afb-c78a2b8dcf09 in namespace container-probe-8456
STEP: checking the pod's current state and verifying that restartCount is present
May 21 09:57:30.094: INFO: Initial restart count of pod test-webserver-c02a6610-47fd-4381-9afb-c78a2b8dcf09 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:31.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8456" for this suite.

• [SLOW TEST:247.752 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":58,"skipped":1135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-073a34c3-315a-4718-a15f-e9fcab50af3b
STEP: Creating a pod to test consume secrets
May 21 10:01:31.836: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8" in namespace "projected-983" to be "Succeeded or Failed"
May 21 10:01:31.854: INFO: Pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.000918ms
May 21 10:01:33.861: INFO: Pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024862087s
May 21 10:01:35.870: INFO: Pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034336659s
May 21 10:01:37.889: INFO: Pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05240915s
STEP: Saw pod success
May 21 10:01:37.889: INFO: Pod "pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8" satisfied condition "Succeeded or Failed"
May 21 10:01:37.895: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 10:01:37.996: INFO: Waiting for pod pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8 to disappear
May 21 10:01:38.004: INFO: Pod pod-projected-secrets-63d7061b-7c59-4454-91c7-60d88ce74dd8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-983" for this suite.

• [SLOW TEST:6.374 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":59,"skipped":1181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 21 10:01:38.240: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4456 /api/v1/namespaces/watch-4456/configmaps/e2e-watch-test-resource-version 3dbc6a78-5e19-4e8a-a846-7d0be69a4f60 162463 0 2021-05-21 10:01:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 10:01:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:01:38.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4456 /api/v1/namespaces/watch-4456/configmaps/e2e-watch-test-resource-version 3dbc6a78-5e19-4e8a-a846-7d0be69a4f60 162464 0 2021-05-21 10:01:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 10:01:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:38.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4456" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":303,"completed":60,"skipped":1224,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:38.271: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 10:01:44.987: INFO: Successfully updated pod "annotationupdate7341e99a-3914-4a2e-9801-af9cc4b0cd44"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:47.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3382" for this suite.

• [SLOW TEST:8.798 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":61,"skipped":1228,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:47.070: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
May 21 10:01:47.144: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9923 proxy --unix-socket=/tmp/kubectl-proxy-unix780598410/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:47.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9923" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":303,"completed":62,"skipped":1231,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:47.284: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 10:01:47.395: INFO: Waiting up to 5m0s for pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42" in namespace "downward-api-7700" to be "Succeeded or Failed"
May 21 10:01:47.421: INFO: Pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42": Phase="Pending", Reason="", readiness=false. Elapsed: 26.591718ms
May 21 10:01:49.429: INFO: Pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03441113s
May 21 10:01:51.436: INFO: Pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040940136s
May 21 10:01:53.444: INFO: Pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049531786s
STEP: Saw pod success
May 21 10:01:53.444: INFO: Pod "downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42" satisfied condition "Succeeded or Failed"
May 21 10:01:53.450: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42 container dapi-container: <nil>
STEP: delete the pod
May 21 10:01:53.527: INFO: Waiting for pod downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42 to disappear
May 21 10:01:53.534: INFO: Pod downward-api-408a4db5-e6d8-4567-9ec7-ddd8aa718a42 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:01:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7700" for this suite.

• [SLOW TEST:6.275 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":303,"completed":63,"skipped":1232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:01:53.563: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3396
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 10:01:53.668: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 10:01:53.797: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:01:55.806: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:01:57.804: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:01:59.827: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:02:01.810: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:02:03.807: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:02:05.812: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:02:07.806: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:02:09.810: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 10:02:09.826: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 10:02:17.993: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.128.4:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3396 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:02:17.994: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:02:18.189: INFO: Found all expected endpoints: [netserver-0]
May 21 10:02:18.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.96.3:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3396 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:02:18.197: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:02:18.403: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:02:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3396" for this suite.

• [SLOW TEST:24.889 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":64,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:02:18.454: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lqj46 in namespace proxy-7489
I0521 10:02:18.644160      23 runners.go:190] Created replication controller with name: proxy-service-lqj46, namespace: proxy-7489, replica count: 1
I0521 10:02:19.695983      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:02:20.696523      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:02:21.697062      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:02:22.697834      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:02:23.698128      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:02:24.698578      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 10:02:25.698957      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 10:02:26.699390      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 10:02:27.699839      23 runners.go:190] proxy-service-lqj46 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:02:27.711: INFO: setup took 9.173044688s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 21 10:02:27.732: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 21.425838ms)
May 21 10:02:27.747: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 36.357768ms)
May 21 10:02:27.750: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 39.020679ms)
May 21 10:02:27.751: INFO: (0) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 39.805766ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 49.295511ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 49.260813ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 49.082251ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 49.324166ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 49.710411ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 49.543138ms)
May 21 10:02:27.761: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 49.727926ms)
May 21 10:02:27.764: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 52.35151ms)
May 21 10:02:27.764: INFO: (0) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 52.526971ms)
May 21 10:02:27.766: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 54.327844ms)
May 21 10:02:27.768: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 56.564857ms)
May 21 10:02:27.768: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 57.135933ms)
May 21 10:02:27.779: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 10.658221ms)
May 21 10:02:27.780: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 10.332227ms)
May 21 10:02:27.792: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 22.220109ms)
May 21 10:02:27.793: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 23.287718ms)
May 21 10:02:27.793: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 23.581815ms)
May 21 10:02:27.794: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 24.081419ms)
May 21 10:02:27.796: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 26.690132ms)
May 21 10:02:27.798: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 28.600542ms)
May 21 10:02:27.798: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 28.750716ms)
May 21 10:02:27.799: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 29.630833ms)
May 21 10:02:27.800: INFO: (1) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 30.920009ms)
May 21 10:02:27.801: INFO: (1) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 31.309082ms)
May 21 10:02:27.801: INFO: (1) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 31.570009ms)
May 21 10:02:27.803: INFO: (1) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 34.113781ms)
May 21 10:02:27.804: INFO: (1) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 34.44279ms)
May 21 10:02:27.804: INFO: (1) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 34.886638ms)
May 21 10:02:27.817: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 12.673044ms)
May 21 10:02:27.840: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 35.43246ms)
May 21 10:02:27.846: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 41.06402ms)
May 21 10:02:27.855: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 50.339261ms)
May 21 10:02:27.855: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 50.245313ms)
May 21 10:02:27.855: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 50.241772ms)
May 21 10:02:27.855: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 50.605954ms)
May 21 10:02:27.858: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 52.734501ms)
May 21 10:02:27.858: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 52.985793ms)
May 21 10:02:27.862: INFO: (2) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 57.575252ms)
May 21 10:02:27.862: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 57.638238ms)
May 21 10:02:27.863: INFO: (2) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 58.062072ms)
May 21 10:02:27.863: INFO: (2) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 58.103839ms)
May 21 10:02:27.863: INFO: (2) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 58.125166ms)
May 21 10:02:27.863: INFO: (2) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 58.371145ms)
May 21 10:02:27.863: INFO: (2) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 58.528363ms)
May 21 10:02:27.878: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 14.831891ms)
May 21 10:02:27.888: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 24.553505ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 24.752033ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 25.471583ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 25.468431ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 25.551901ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 25.256214ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 25.417451ms)
May 21 10:02:27.889: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 25.674892ms)
May 21 10:02:27.891: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 27.181362ms)
May 21 10:02:27.893: INFO: (3) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 28.948985ms)
May 21 10:02:27.894: INFO: (3) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 30.458988ms)
May 21 10:02:27.895: INFO: (3) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 31.062181ms)
May 21 10:02:27.895: INFO: (3) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 31.404276ms)
May 21 10:02:27.895: INFO: (3) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 31.54223ms)
May 21 10:02:27.897: INFO: (3) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 33.040323ms)
May 21 10:02:27.914: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 16.527307ms)
May 21 10:02:27.915: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 16.498738ms)
May 21 10:02:27.915: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 17.510629ms)
May 21 10:02:27.915: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 17.82597ms)
May 21 10:02:27.915: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.812411ms)
May 21 10:02:27.915: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 17.715433ms)
May 21 10:02:27.919: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 20.947895ms)
May 21 10:02:27.919: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 20.948616ms)
May 21 10:02:27.919: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 21.195581ms)
May 21 10:02:27.920: INFO: (4) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 22.095069ms)
May 21 10:02:27.920: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 22.040845ms)
May 21 10:02:27.929: INFO: (4) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 31.481259ms)
May 21 10:02:27.930: INFO: (4) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 31.930409ms)
May 21 10:02:27.930: INFO: (4) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 32.672843ms)
May 21 10:02:27.930: INFO: (4) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 32.859882ms)
May 21 10:02:27.931: INFO: (4) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 32.593309ms)
May 21 10:02:27.942: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 11.37326ms)
May 21 10:02:27.959: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 27.813486ms)
May 21 10:02:27.959: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 28.770189ms)
May 21 10:02:27.960: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 29.250027ms)
May 21 10:02:27.960: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 29.354335ms)
May 21 10:02:27.961: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 29.612985ms)
May 21 10:02:27.961: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 29.531886ms)
May 21 10:02:27.961: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 29.920995ms)
May 21 10:02:27.961: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 29.744805ms)
May 21 10:02:27.961: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 29.986718ms)
May 21 10:02:27.963: INFO: (5) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 32.154596ms)
May 21 10:02:27.971: INFO: (5) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 39.770725ms)
May 21 10:02:27.971: INFO: (5) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 39.587682ms)
May 21 10:02:27.972: INFO: (5) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 41.352521ms)
May 21 10:02:27.973: INFO: (5) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 41.332392ms)
May 21 10:02:27.974: INFO: (5) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 42.744521ms)
May 21 10:02:27.985: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 10.400028ms)
May 21 10:02:27.985: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 10.651613ms)
May 21 10:02:27.990: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 15.519507ms)
May 21 10:02:27.990: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 15.119668ms)
May 21 10:02:27.990: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 15.271635ms)
May 21 10:02:27.990: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 15.682469ms)
May 21 10:02:27.990: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 15.465047ms)
May 21 10:02:27.992: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 17.135229ms)
May 21 10:02:27.992: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 17.904436ms)
May 21 10:02:28.000: INFO: (6) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 25.006736ms)
May 21 10:02:28.000: INFO: (6) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 25.400988ms)
May 21 10:02:28.000: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 25.284924ms)
May 21 10:02:28.000: INFO: (6) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 25.808472ms)
May 21 10:02:28.001: INFO: (6) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 26.181745ms)
May 21 10:02:28.001: INFO: (6) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 26.397963ms)
May 21 10:02:28.001: INFO: (6) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 26.6517ms)
May 21 10:02:28.013: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 11.44424ms)
May 21 10:02:28.031: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 29.398368ms)
May 21 10:02:28.032: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 29.994089ms)
May 21 10:02:28.032: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 30.805456ms)
May 21 10:02:28.034: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 31.78289ms)
May 21 10:02:28.034: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 31.583894ms)
May 21 10:02:28.037: INFO: (7) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 35.57579ms)
May 21 10:02:28.037: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 35.005994ms)
May 21 10:02:28.037: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 35.434239ms)
May 21 10:02:28.040: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 37.919206ms)
May 21 10:02:28.040: INFO: (7) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 37.622545ms)
May 21 10:02:28.040: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 38.091622ms)
May 21 10:02:28.040: INFO: (7) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 38.688981ms)
May 21 10:02:28.042: INFO: (7) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 40.094539ms)
May 21 10:02:28.043: INFO: (7) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 41.026393ms)
May 21 10:02:28.043: INFO: (7) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 40.999812ms)
May 21 10:02:28.054: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 10.792729ms)
May 21 10:02:28.058: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 14.192062ms)
May 21 10:02:28.058: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 14.819229ms)
May 21 10:02:28.059: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 14.938328ms)
May 21 10:02:28.061: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.114241ms)
May 21 10:02:28.061: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 17.148919ms)
May 21 10:02:28.061: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 17.328979ms)
May 21 10:02:28.061: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.93601ms)
May 21 10:02:28.062: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 18.123826ms)
May 21 10:02:28.062: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 18.613753ms)
May 21 10:02:28.064: INFO: (8) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 20.323911ms)
May 21 10:02:28.068: INFO: (8) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 24.092785ms)
May 21 10:02:28.068: INFO: (8) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 24.098885ms)
May 21 10:02:28.068: INFO: (8) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 24.207518ms)
May 21 10:02:28.068: INFO: (8) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 24.133934ms)
May 21 10:02:28.068: INFO: (8) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 24.119664ms)
May 21 10:02:28.078: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 9.54053ms)
May 21 10:02:28.079: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 10.199082ms)
May 21 10:02:28.086: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 17.065654ms)
May 21 10:02:28.086: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.177572ms)
May 21 10:02:28.086: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.14895ms)
May 21 10:02:28.086: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 17.634918ms)
May 21 10:02:28.087: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 18.149066ms)
May 21 10:02:28.088: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 19.086367ms)
May 21 10:02:28.088: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 19.740864ms)
May 21 10:02:28.089: INFO: (9) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 20.597278ms)
May 21 10:02:28.089: INFO: (9) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 21.216161ms)
May 21 10:02:28.090: INFO: (9) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 21.286964ms)
May 21 10:02:28.090: INFO: (9) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 21.465174ms)
May 21 10:02:28.090: INFO: (9) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 21.687527ms)
May 21 10:02:28.090: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 22.134976ms)
May 21 10:02:28.093: INFO: (9) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 24.305469ms)
May 21 10:02:28.103: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 9.621553ms)
May 21 10:02:28.110: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 16.775506ms)
May 21 10:02:28.110: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 16.976317ms)
May 21 10:02:28.110: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 17.319593ms)
May 21 10:02:28.111: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 18.528756ms)
May 21 10:02:28.111: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 18.617168ms)
May 21 10:02:28.112: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 18.542177ms)
May 21 10:02:28.112: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 18.73133ms)
May 21 10:02:28.112: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 18.723238ms)
May 21 10:02:28.113: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 20.108436ms)
May 21 10:02:28.115: INFO: (10) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 21.686544ms)
May 21 10:02:28.119: INFO: (10) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 25.596766ms)
May 21 10:02:28.119: INFO: (10) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 25.984862ms)
May 21 10:02:28.119: INFO: (10) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 26.397622ms)
May 21 10:02:28.120: INFO: (10) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 26.486918ms)
May 21 10:02:28.120: INFO: (10) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 26.954596ms)
May 21 10:02:28.131: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 10.749333ms)
May 21 10:02:28.144: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 24.209672ms)
May 21 10:02:28.144: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 24.232446ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 24.505267ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 25.219493ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 25.094339ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 25.186942ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 25.05213ms)
May 21 10:02:28.145: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 25.470699ms)
May 21 10:02:28.146: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 25.808599ms)
May 21 10:02:28.146: INFO: (11) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 25.695201ms)
May 21 10:02:28.146: INFO: (11) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 26.029849ms)
May 21 10:02:28.146: INFO: (11) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 26.239516ms)
May 21 10:02:28.147: INFO: (11) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 27.132591ms)
May 21 10:02:28.148: INFO: (11) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 27.289701ms)
May 21 10:02:28.148: INFO: (11) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 28.010878ms)
May 21 10:02:28.164: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 15.149798ms)
May 21 10:02:28.166: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 17.533134ms)
May 21 10:02:28.166: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 17.510355ms)
May 21 10:02:28.166: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 17.795865ms)
May 21 10:02:28.166: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 17.807075ms)
May 21 10:02:28.166: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 17.746874ms)
May 21 10:02:28.167: INFO: (12) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 18.441966ms)
May 21 10:02:28.169: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 20.83352ms)
May 21 10:02:28.169: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 20.717762ms)
May 21 10:02:28.169: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 20.986849ms)
May 21 10:02:28.169: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 21.178825ms)
May 21 10:02:28.175: INFO: (12) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 26.334396ms)
May 21 10:02:28.178: INFO: (12) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 29.193884ms)
May 21 10:02:28.178: INFO: (12) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 29.605201ms)
May 21 10:02:28.178: INFO: (12) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 29.681563ms)
May 21 10:02:28.179: INFO: (12) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 31.131871ms)
May 21 10:02:28.198: INFO: (13) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 18.035075ms)
May 21 10:02:28.201: INFO: (13) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 21.324152ms)
May 21 10:02:28.201: INFO: (13) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 21.566255ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 21.700665ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 22.296067ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 22.284593ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 22.638733ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 22.125507ms)
May 21 10:02:28.202: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 22.544979ms)
May 21 10:02:28.203: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 23.142911ms)
May 21 10:02:28.204: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 24.676831ms)
May 21 10:02:28.205: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 25.450947ms)
May 21 10:02:28.205: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 25.700959ms)
May 21 10:02:28.206: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 26.43239ms)
May 21 10:02:28.206: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 26.485369ms)
May 21 10:02:28.206: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 26.700345ms)
May 21 10:02:28.216: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 9.105838ms)
May 21 10:02:28.217: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 10.611044ms)
May 21 10:02:28.217: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 10.757139ms)
May 21 10:02:28.217: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 10.963807ms)
May 21 10:02:28.223: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 14.813076ms)
May 21 10:02:28.223: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 15.538961ms)
May 21 10:02:28.223: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 15.352387ms)
May 21 10:02:28.223: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 15.527963ms)
May 21 10:02:28.223: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 15.078969ms)
May 21 10:02:28.227: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 19.654909ms)
May 21 10:02:28.227: INFO: (14) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 20.09649ms)
May 21 10:02:28.228: INFO: (14) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 19.969932ms)
May 21 10:02:28.231: INFO: (14) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 23.373199ms)
May 21 10:02:28.232: INFO: (14) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 24.608016ms)
May 21 10:02:28.232: INFO: (14) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 24.990654ms)
May 21 10:02:28.232: INFO: (14) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 24.086027ms)
May 21 10:02:28.242: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 9.945857ms)
May 21 10:02:28.247: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 14.062398ms)
May 21 10:02:28.247: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 14.563717ms)
May 21 10:02:28.249: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 16.819241ms)
May 21 10:02:28.249: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 16.828292ms)
May 21 10:02:28.250: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 17.122164ms)
May 21 10:02:28.253: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 20.533663ms)
May 21 10:02:28.254: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 21.049014ms)
May 21 10:02:28.256: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 23.144775ms)
May 21 10:02:28.260: INFO: (15) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 27.424791ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 28.754252ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 28.675977ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 28.686354ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 29.0288ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 28.982861ms)
May 21 10:02:28.261: INFO: (15) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 28.826661ms)
May 21 10:02:28.270: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 8.330672ms)
May 21 10:02:28.277: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 15.034899ms)
May 21 10:02:28.277: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 15.276041ms)
May 21 10:02:28.277: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 15.413996ms)
May 21 10:02:28.279: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 17.64419ms)
May 21 10:02:28.280: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 17.971956ms)
May 21 10:02:28.280: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 18.39982ms)
May 21 10:02:28.282: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 20.037402ms)
May 21 10:02:28.282: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 20.451356ms)
May 21 10:02:28.286: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 23.802735ms)
May 21 10:02:28.287: INFO: (16) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 24.379932ms)
May 21 10:02:28.287: INFO: (16) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 24.819879ms)
May 21 10:02:28.289: INFO: (16) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 26.721899ms)
May 21 10:02:28.290: INFO: (16) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 27.890189ms)
May 21 10:02:28.291: INFO: (16) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 29.050588ms)
May 21 10:02:28.291: INFO: (16) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 29.507159ms)
May 21 10:02:28.301: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 9.093213ms)
May 21 10:02:28.301: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 9.506262ms)
May 21 10:02:28.302: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 10.249956ms)
May 21 10:02:28.302: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 10.559367ms)
May 21 10:02:28.307: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 15.19405ms)
May 21 10:02:28.308: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 15.66446ms)
May 21 10:02:28.310: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 17.725043ms)
May 21 10:02:28.311: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 18.999918ms)
May 21 10:02:28.313: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 20.869014ms)
May 21 10:02:28.313: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 20.947191ms)
May 21 10:02:28.319: INFO: (17) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 27.447851ms)
May 21 10:02:28.319: INFO: (17) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 27.428631ms)
May 21 10:02:28.320: INFO: (17) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 28.007142ms)
May 21 10:02:28.320: INFO: (17) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 28.064088ms)
May 21 10:02:28.320: INFO: (17) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 28.026959ms)
May 21 10:02:28.321: INFO: (17) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 29.23257ms)
May 21 10:02:28.335: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 12.43175ms)
May 21 10:02:28.335: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 12.409458ms)
May 21 10:02:28.340: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 17.650198ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 20.617058ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 22.604908ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 22.301023ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 21.891617ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 22.507542ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 21.837673ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 21.737832ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 21.132166ms)
May 21 10:02:28.344: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 21.140727ms)
May 21 10:02:28.345: INFO: (18) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 21.916021ms)
May 21 10:02:28.345: INFO: (18) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 23.345877ms)
May 21 10:02:28.345: INFO: (18) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 22.494753ms)
May 21 10:02:28.346: INFO: (18) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 22.866351ms)
May 21 10:02:28.354: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">... (200; 8.314654ms)
May 21 10:02:28.354: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:462/proxy/: tls qux (200; 8.242055ms)
May 21 10:02:28.357: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:443/proxy/tlsrewritem... (200; 10.610219ms)
May 21 10:02:28.362: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 16.37211ms)
May 21 10:02:28.363: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:162/proxy/: bar (200; 16.515502ms)
May 21 10:02:28.363: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6/proxy/rewriteme">test</a> (200; 16.784351ms)
May 21 10:02:28.366: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-lqj46-g2ws6:460/proxy/: tls baz (200; 19.804269ms)
May 21 10:02:28.366: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:1080/proxy/rewriteme">test<... (200; 20.226422ms)
May 21 10:02:28.375: INFO: (19) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname1/proxy/: foo (200; 29.376031ms)
May 21 10:02:28.376: INFO: (19) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname1/proxy/: foo (200; 30.30399ms)
May 21 10:02:28.377: INFO: (19) /api/v1/namespaces/proxy-7489/services/http:proxy-service-lqj46:portname2/proxy/: bar (200; 30.474011ms)
May 21 10:02:28.377: INFO: (19) /api/v1/namespaces/proxy-7489/services/proxy-service-lqj46:portname2/proxy/: bar (200; 30.571337ms)
May 21 10:02:28.378: INFO: (19) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname2/proxy/: tls qux (200; 31.785991ms)
May 21 10:02:28.378: INFO: (19) /api/v1/namespaces/proxy-7489/services/https:proxy-service-lqj46:tlsportname1/proxy/: tls baz (200; 31.720376ms)
May 21 10:02:28.378: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 32.096544ms)
May 21 10:02:28.379: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-lqj46-g2ws6:160/proxy/: foo (200; 33.213199ms)
STEP: deleting ReplicationController proxy-service-lqj46 in namespace proxy-7489, will wait for the garbage collector to delete the pods
May 21 10:02:28.467: INFO: Deleting ReplicationController proxy-service-lqj46 took: 33.536999ms
May 21 10:02:30.768: INFO: Terminating ReplicationController proxy-service-lqj46 pods took: 2.300250582s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:02:37.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7489" for this suite.

• [SLOW TEST:19.150 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":303,"completed":65,"skipped":1321,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:02:37.605: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8293
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-8293
May 21 10:02:37.768: INFO: Found 0 stateful pods, waiting for 1
May 21 10:02:47.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:02:47.904: INFO: Deleting all statefulset in ns statefulset-8293
May 21 10:02:47.917: INFO: Scaling statefulset ss to 0
May 21 10:02:57.984: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:02:57.989: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:02:58.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8293" for this suite.

• [SLOW TEST:20.462 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":303,"completed":66,"skipped":1335,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:02:58.068: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-204fa4e0-17c3-4b48-be79-903139adb0c7 in namespace container-probe-4539
May 21 10:03:04.196: INFO: Started pod liveness-204fa4e0-17c3-4b48-be79-903139adb0c7 in namespace container-probe-4539
STEP: checking the pod's current state and verifying that restartCount is present
May 21 10:03:04.210: INFO: Initial restart count of pod liveness-204fa4e0-17c3-4b48-be79-903139adb0c7 is 0
May 21 10:03:26.349: INFO: Restart count of pod container-probe-4539/liveness-204fa4e0-17c3-4b48-be79-903139adb0c7 is now 1 (22.138242296s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:03:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4539" for this suite.

• [SLOW TEST:28.380 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":67,"skipped":1353,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:03:26.451: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-7d2e6939-8935-42ff-9080-4a5b96e723e5
STEP: Creating a pod to test consume secrets
May 21 10:03:26.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741" in namespace "projected-1011" to be "Succeeded or Failed"
May 21 10:03:26.614: INFO: Pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.915732ms
May 21 10:03:28.628: INFO: Pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01913815s
May 21 10:03:30.638: INFO: Pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028353032s
May 21 10:03:32.647: INFO: Pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037928135s
STEP: Saw pod success
May 21 10:03:32.647: INFO: Pod "pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741" satisfied condition "Succeeded or Failed"
May 21 10:03:32.655: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 10:03:32.761: INFO: Waiting for pod pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741 to disappear
May 21 10:03:32.769: INFO: Pod pod-projected-secrets-f1e1f14f-7ec3-47e6-973b-816b26053741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:03:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1011" for this suite.

• [SLOW TEST:6.364 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":68,"skipped":1355,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:03:32.815: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 21 10:03:32.945: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163194 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:03:32.945: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163194 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 21 10:03:42.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163239 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:03:42.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163239 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 21 10:03:53.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163273 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:03:53.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163273 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 21 10:04:03.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163307 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:04:03.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-a 7756a778-32a6-47ec-b172-f4830add4f55 163307 0 2021-05-21 10:03:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 10:03:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 21 10:04:13.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-b a2561dac-e5fc-4e9b-8ae6-055676b84cbc 163341 0 2021-05-21 10:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:04:13.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-b a2561dac-e5fc-4e9b-8ae6-055676b84cbc 163341 0 2021-05-21 10:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 21 10:04:23.132: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-b a2561dac-e5fc-4e9b-8ae6-055676b84cbc 163371 0 2021-05-21 10:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 10:04:23.133: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7893 /api/v1/namespaces/watch-7893/configmaps/e2e-watch-test-configmap-b a2561dac-e5fc-4e9b-8ae6-055676b84cbc 163371 0 2021-05-21 10:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:04:33.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7893" for this suite.

• [SLOW TEST:60.377 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":303,"completed":69,"skipped":1365,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:04:33.194: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 10:04:33.317: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 10:05:33.415: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 21 10:05:33.471: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 10:05:33.542: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:05:53.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8195" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:80.626 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":303,"completed":70,"skipped":1371,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:05:53.820: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 10:06:00.068: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:06:00.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4493" for this suite.

• [SLOW TEST:6.340 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":71,"skipped":1372,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:06:00.161: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:06:00.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05" in namespace "projected-7014" to be "Succeeded or Failed"
May 21 10:06:00.304: INFO: Pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05": Phase="Pending", Reason="", readiness=false. Elapsed: 13.255067ms
May 21 10:06:02.314: INFO: Pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022795875s
May 21 10:06:04.323: INFO: Pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032605389s
May 21 10:06:06.334: INFO: Pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043485067s
STEP: Saw pod success
May 21 10:06:06.334: INFO: Pod "downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05" satisfied condition "Succeeded or Failed"
May 21 10:06:06.340: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05 container client-container: <nil>
STEP: delete the pod
May 21 10:06:06.438: INFO: Waiting for pod downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05 to disappear
May 21 10:06:06.448: INFO: Pod downwardapi-volume-621d7886-55bc-4970-bcc0-b6c7c98c8a05 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:06:06.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7014" for this suite.

• [SLOW TEST:6.315 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":72,"skipped":1382,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:06:06.477: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-ec70e95c-48b8-4df2-b78c-2f97b01688b5
STEP: Creating a pod to test consume secrets
May 21 10:06:06.666: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5" in namespace "projected-6168" to be "Succeeded or Failed"
May 21 10:06:06.686: INFO: Pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.897907ms
May 21 10:06:08.700: INFO: Pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033887071s
May 21 10:06:10.709: INFO: Pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042934263s
May 21 10:06:12.718: INFO: Pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051524945s
STEP: Saw pod success
May 21 10:06:12.718: INFO: Pod "pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5" satisfied condition "Succeeded or Failed"
May 21 10:06:12.725: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 10:06:12.806: INFO: Waiting for pod pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5 to disappear
May 21 10:06:12.822: INFO: Pod pod-projected-secrets-19d5fc5c-3e08-470f-95e5-17ecbf9cb0a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:06:12.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6168" for this suite.

• [SLOW TEST:6.379 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":73,"skipped":1382,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:06:12.858: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:06:13.491: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 10:06:15.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:17.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188373, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:06:20.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:06:20.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2160" for this suite.
STEP: Destroying namespace "webhook-2160-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.120 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":303,"completed":74,"skipped":1388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:06:20.987: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 10:06:21.092: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:06:33.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6899" for this suite.

• [SLOW TEST:12.701 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":303,"completed":75,"skipped":1423,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:06:33.689: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:06:33.823: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 21 10:06:38.837: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 10:06:40.867: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 21 10:06:42.882: INFO: Creating deployment "test-rollover-deployment"
May 21 10:06:42.913: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 21 10:06:44.930: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 21 10:06:44.949: INFO: Ensure that both replica sets have 1 created replica
May 21 10:06:44.969: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 21 10:06:45.000: INFO: Updating deployment test-rollover-deployment
May 21 10:06:45.000: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 21 10:06:47.029: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 21 10:06:47.043: INFO: Make sure deployment "test-rollover-deployment" is complete
May 21 10:06:47.060: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:47.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188405, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:49.082: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:49.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188405, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:51.085: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:51.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188410, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:53.081: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:53.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188410, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:55.077: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:55.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188410, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:57.082: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:57.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188410, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:06:59.075: INFO: all replica sets need to contain the pod-template-hash label
May 21 10:06:59.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188403, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188410, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:07:01.079: INFO: 
May 21 10:07:01.079: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 10:07:01.098: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4134 /apis/apps/v1/namespaces/deployment-4134/deployments/test-rollover-deployment ab6dd059-8bc2-4bfa-90e0-deaaba098df2 164257 2 2021-05-21 10:06:42 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 10:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 10:07:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004321dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 10:06:43 +0000 UTC,LastTransitionTime:2021-05-21 10:06:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-05-21 10:07:00 +0000 UTC,LastTransitionTime:2021-05-21 10:06:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 10:07:01.105: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-4134 /apis/apps/v1/namespaces/deployment-4134/replicasets/test-rollover-deployment-5797c7764 79d15a2e-6729-4123-8eb8-a7bf4ddf1628 164246 2 2021-05-21 10:06:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ab6dd059-8bc2-4bfa-90e0-deaaba098df2 0xc003232920 0xc003232921}] []  [{kube-controller-manager Update apps/v1 2021-05-21 10:07:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab6dd059-8bc2-4bfa-90e0-deaaba098df2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003232a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 10:07:01.106: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 21 10:07:01.106: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4134 /apis/apps/v1/namespaces/deployment-4134/replicasets/test-rollover-controller 46fcb9bf-8e0d-4099-bc15-8e25ddbd6610 164256 2 2021-05-21 10:06:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ab6dd059-8bc2-4bfa-90e0-deaaba098df2 0xc0032325e7 0xc0032325e8}] []  [{e2e.test Update apps/v1 2021-05-21 10:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 10:07:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab6dd059-8bc2-4bfa-90e0-deaaba098df2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003232838 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 10:07:01.106: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4134 /apis/apps/v1/namespaces/deployment-4134/replicasets/test-rollover-deployment-78bc8b888c 7f67afa6-8f9b-40c6-a1a5-e126b3b0b183 164189 2 2021-05-21 10:06:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ab6dd059-8bc2-4bfa-90e0-deaaba098df2 0xc003232be7 0xc003232be8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 10:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab6dd059-8bc2-4bfa-90e0-deaaba098df2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003232d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 10:07:01.112: INFO: Pod "test-rollover-deployment-5797c7764-vg97k" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-vg97k test-rollover-deployment-5797c7764- deployment-4134 /api/v1/namespaces/deployment-4134/pods/test-rollover-deployment-5797c7764-vg97k f86c6373-7a7c-49a0-b050-3f3d385e90f3 164210 0 2021-05-21 10:06:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 79d15a2e-6729-4123-8eb8-a7bf4ddf1628 0xc0032339c0 0xc0032339c1}] []  [{kube-controller-manager Update v1 2021-05-21 10:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79d15a2e-6729-4123-8eb8-a7bf4ddf1628\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:06:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xsvtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xsvtc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xsvtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:06:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:06:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.4,StartTime:2021-05-21 10:06:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:06:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://31e6224ee731376ab61a9147c203cf80f95a1a490d2f05be4971f6c3111cbee5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:07:01.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4134" for this suite.

• [SLOW TEST:27.460 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":303,"completed":76,"skipped":1424,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:07:01.150: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 10:07:13.434: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:13.443: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:15.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:15.458: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:17.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:17.460: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:19.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:19.451: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:21.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:21.452: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:23.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:23.454: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:25.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:25.452: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:27.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:27.456: INFO: Pod pod-with-poststart-http-hook still exists
May 21 10:07:29.443: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 10:07:29.461: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:07:29.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6303" for this suite.

• [SLOW TEST:28.355 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":303,"completed":77,"skipped":1431,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:07:29.507: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-42a37646-f79c-45b0-8064-96e14039c62b
STEP: Creating a pod to test consume configMaps
May 21 10:07:29.690: INFO: Waiting up to 5m0s for pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355" in namespace "configmap-6248" to be "Succeeded or Failed"
May 21 10:07:29.696: INFO: Pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355": Phase="Pending", Reason="", readiness=false. Elapsed: 6.413209ms
May 21 10:07:31.711: INFO: Pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021394366s
May 21 10:07:33.742: INFO: Pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052218691s
May 21 10:07:35.753: INFO: Pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062641438s
STEP: Saw pod success
May 21 10:07:35.753: INFO: Pod "pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355" satisfied condition "Succeeded or Failed"
May 21 10:07:35.760: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:07:35.869: INFO: Waiting for pod pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355 to disappear
May 21 10:07:35.886: INFO: Pod pod-configmaps-da51b8af-dc6f-4fd0-a20e-055a387d0355 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:07:35.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6248" for this suite.

• [SLOW TEST:6.405 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":78,"skipped":1444,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:07:35.913: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1119
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1119
I0521 10:07:36.145067      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1119, replica count: 2
I0521 10:07:39.195535      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:07:42.195991      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:07:42.196: INFO: Creating new exec pod
May 21 10:07:49.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-1119 exec execpod95d9j -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 10:07:49.799: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 10:07:49.799: INFO: stdout: ""
May 21 10:07:49.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-1119 exec execpod95d9j -- /bin/sh -x -c nc -zv -t -w 2 10.106.106.77 80'
May 21 10:07:50.102: INFO: stderr: "+ nc -zv -t -w 2 10.106.106.77 80\nConnection to 10.106.106.77 80 port [tcp/http] succeeded!\n"
May 21 10:07:50.102: INFO: stdout: ""
May 21 10:07:50.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-1119 exec execpod95d9j -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.36 31882'
May 21 10:07:50.371: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.36 31882\nConnection to 172.30.52.36 31882 port [tcp/31882] succeeded!\n"
May 21 10:07:50.371: INFO: stdout: ""
May 21 10:07:50.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-1119 exec execpod95d9j -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.10 31882'
May 21 10:07:50.645: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.10 31882\nConnection to 172.30.52.10 31882 port [tcp/31882] succeeded!\n"
May 21 10:07:50.645: INFO: stdout: ""
May 21 10:07:50.645: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:07:50.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1119" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.858 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":303,"completed":79,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:07:50.773: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-126/configmap-test-8de9e65d-9485-4253-b9e7-a584a7e3278e
STEP: Creating a pod to test consume configMaps
May 21 10:07:50.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417" in namespace "configmap-126" to be "Succeeded or Failed"
May 21 10:07:50.926: INFO: Pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417": Phase="Pending", Reason="", readiness=false. Elapsed: 13.112418ms
May 21 10:07:52.934: INFO: Pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02111666s
May 21 10:07:54.954: INFO: Pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040763464s
May 21 10:07:56.966: INFO: Pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053419792s
STEP: Saw pod success
May 21 10:07:56.966: INFO: Pod "pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417" satisfied condition "Succeeded or Failed"
May 21 10:07:56.974: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417 container env-test: <nil>
STEP: delete the pod
May 21 10:07:57.052: INFO: Waiting for pod pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417 to disappear
May 21 10:07:57.067: INFO: Pod pod-configmaps-8aff1a02-a00a-465a-b2d6-335afaae3417 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:07:57.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-126" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":303,"completed":80,"skipped":1469,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:07:57.102: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:07:57.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe" in namespace "downward-api-3049" to be "Succeeded or Failed"
May 21 10:07:57.220: INFO: Pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.428273ms
May 21 10:07:59.228: INFO: Pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021165472s
May 21 10:08:01.242: INFO: Pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034387224s
May 21 10:08:03.252: INFO: Pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0451377s
STEP: Saw pod success
May 21 10:08:03.253: INFO: Pod "downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe" satisfied condition "Succeeded or Failed"
May 21 10:08:03.259: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe container client-container: <nil>
STEP: delete the pod
May 21 10:08:03.342: INFO: Waiting for pod downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe to disappear
May 21 10:08:03.350: INFO: Pod downwardapi-volume-ad078d31-7c6b-42d9-82ba-510cfb38cfbe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:08:03.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3049" for this suite.

• [SLOW TEST:6.277 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":81,"skipped":1470,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:08:03.382: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:08:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5604" for this suite.
STEP: Destroying namespace "nsdeletetest-462" for this suite.
May 21 10:08:37.773: INFO: Namespace nsdeletetest-462 was already deleted
STEP: Destroying namespace "nsdeletetest-4482" for this suite.

• [SLOW TEST:34.412 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":303,"completed":82,"skipped":1484,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:08:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:08:49.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7805" for this suite.

• [SLOW TEST:11.318 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":303,"completed":83,"skipped":1489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:08:49.116: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 21 10:09:11.362: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:11.526: INFO: Exec stderr: ""
May 21 10:09:11.527: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:11.527: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:11.691: INFO: Exec stderr: ""
May 21 10:09:11.691: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:11.859: INFO: Exec stderr: ""
May 21 10:09:11.859: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:11.859: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.008: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 21 10:09:12.008: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.153: INFO: Exec stderr: ""
May 21 10:09:12.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.153: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.300: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 21 10:09:12.300: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.300: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.500: INFO: Exec stderr: ""
May 21 10:09:12.500: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.501: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.648: INFO: Exec stderr: ""
May 21 10:09:12.648: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.648: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:12.841: INFO: Exec stderr: ""
May 21 10:09:12.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2483 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:09:12.841: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:09:13.003: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:09:13.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2483" for this suite.

• [SLOW TEST:23.924 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":84,"skipped":1600,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:09:13.041: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-893e23db-91e7-4132-ae3e-35e96b71e33e
STEP: Creating a pod to test consume configMaps
May 21 10:09:13.181: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c" in namespace "projected-6316" to be "Succeeded or Failed"
May 21 10:09:13.224: INFO: Pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c": Phase="Pending", Reason="", readiness=false. Elapsed: 43.465235ms
May 21 10:09:15.235: INFO: Pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054891774s
May 21 10:09:17.248: INFO: Pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067377845s
May 21 10:09:19.272: INFO: Pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091808834s
STEP: Saw pod success
May 21 10:09:19.273: INFO: Pod "pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c" satisfied condition "Succeeded or Failed"
May 21 10:09:19.286: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:09:19.385: INFO: Waiting for pod pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c to disappear
May 21 10:09:19.394: INFO: Pod pod-projected-configmaps-d55660c1-777d-4836-a288-f1b85d19ec0c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:09:19.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6316" for this suite.

• [SLOW TEST:6.383 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":85,"skipped":1604,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:09:19.425: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
May 21 10:09:19.587: INFO: Waiting up to 5m0s for pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88" in namespace "containers-793" to be "Succeeded or Failed"
May 21 10:09:19.594: INFO: Pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41825ms
May 21 10:09:21.606: INFO: Pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017908185s
May 21 10:09:23.615: INFO: Pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02756681s
May 21 10:09:25.625: INFO: Pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037289209s
STEP: Saw pod success
May 21 10:09:25.625: INFO: Pod "client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88" satisfied condition "Succeeded or Failed"
May 21 10:09:25.634: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88 container test-container: <nil>
STEP: delete the pod
May 21 10:09:25.715: INFO: Waiting for pod client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88 to disappear
May 21 10:09:25.729: INFO: Pod client-containers-e0949cc6-2bfc-4521-b256-8d48bf79db88 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:09:25.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-793" for this suite.

• [SLOW TEST:6.333 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":303,"completed":86,"skipped":1608,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:09:25.760: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:09:37.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4611" for this suite.

• [SLOW TEST:11.293 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":303,"completed":87,"skipped":1611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:09:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:09:37.143: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3295
I0521 10:09:37.179695      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3295, replica count: 1
I0521 10:09:38.231394      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:09:39.231940      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:09:40.232551      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:09:41.233223      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:09:42.233674      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:09:43.234564      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:09:43.431: INFO: Created: latency-svc-jxswm
May 21 10:09:43.467: INFO: Got endpoints: latency-svc-jxswm [131.620942ms]
May 21 10:09:43.547: INFO: Created: latency-svc-vvfjq
May 21 10:09:43.595: INFO: Got endpoints: latency-svc-vvfjq [127.40185ms]
May 21 10:09:43.610: INFO: Created: latency-svc-8bx7t
May 21 10:09:43.621: INFO: Got endpoints: latency-svc-8bx7t [153.985792ms]
May 21 10:09:43.682: INFO: Created: latency-svc-8r4tm
May 21 10:09:43.724: INFO: Got endpoints: latency-svc-8r4tm [255.610353ms]
May 21 10:09:43.780: INFO: Created: latency-svc-4hvtt
May 21 10:09:43.785: INFO: Got endpoints: latency-svc-4hvtt [317.546222ms]
May 21 10:09:43.834: INFO: Created: latency-svc-445rn
May 21 10:09:43.904: INFO: Got endpoints: latency-svc-445rn [436.921519ms]
May 21 10:09:43.911: INFO: Created: latency-svc-dmqc4
May 21 10:09:43.931: INFO: Got endpoints: latency-svc-dmqc4 [463.440245ms]
May 21 10:09:43.966: INFO: Created: latency-svc-x55zh
May 21 10:09:44.029: INFO: Got endpoints: latency-svc-x55zh [561.302326ms]
May 21 10:09:44.061: INFO: Created: latency-svc-g6vgq
May 21 10:09:44.071: INFO: Got endpoints: latency-svc-g6vgq [602.681815ms]
May 21 10:09:44.139: INFO: Created: latency-svc-f9xfd
May 21 10:09:44.164: INFO: Got endpoints: latency-svc-f9xfd [696.761135ms]
May 21 10:09:44.200: INFO: Created: latency-svc-hcvtv
May 21 10:09:44.216: INFO: Got endpoints: latency-svc-hcvtv [747.216469ms]
May 21 10:09:44.256: INFO: Created: latency-svc-cpgkh
May 21 10:09:44.276: INFO: Got endpoints: latency-svc-cpgkh [807.556517ms]
May 21 10:09:44.316: INFO: Created: latency-svc-rb5ml
May 21 10:09:44.345: INFO: Got endpoints: latency-svc-rb5ml [876.377677ms]
May 21 10:09:44.386: INFO: Created: latency-svc-bsrqj
May 21 10:09:44.392: INFO: Got endpoints: latency-svc-bsrqj [923.342692ms]
May 21 10:09:44.431: INFO: Created: latency-svc-wkgzd
May 21 10:09:44.447: INFO: Got endpoints: latency-svc-wkgzd [979.167551ms]
May 21 10:09:44.508: INFO: Created: latency-svc-vlg8l
May 21 10:09:44.511: INFO: Got endpoints: latency-svc-vlg8l [1.042369136s]
May 21 10:09:44.555: INFO: Created: latency-svc-4lxr5
May 21 10:09:44.585: INFO: Got endpoints: latency-svc-4lxr5 [989.339446ms]
May 21 10:09:44.601: INFO: Created: latency-svc-7fw5g
May 21 10:09:44.643: INFO: Got endpoints: latency-svc-7fw5g [1.021957907s]
May 21 10:09:44.652: INFO: Created: latency-svc-s6jv8
May 21 10:09:44.665: INFO: Got endpoints: latency-svc-s6jv8 [941.24206ms]
May 21 10:09:44.693: INFO: Created: latency-svc-xwxr4
May 21 10:09:44.755: INFO: Got endpoints: latency-svc-xwxr4 [969.282089ms]
May 21 10:09:44.767: INFO: Created: latency-svc-bzfcr
May 21 10:09:44.782: INFO: Got endpoints: latency-svc-bzfcr [877.960379ms]
May 21 10:09:44.812: INFO: Created: latency-svc-jz267
May 21 10:09:44.831: INFO: Got endpoints: latency-svc-jz267 [899.238489ms]
May 21 10:09:44.877: INFO: Created: latency-svc-67lpj
May 21 10:09:44.915: INFO: Got endpoints: latency-svc-67lpj [886.024245ms]
May 21 10:09:44.924: INFO: Created: latency-svc-l5r6b
May 21 10:09:44.955: INFO: Got endpoints: latency-svc-l5r6b [883.56769ms]
May 21 10:09:45.008: INFO: Created: latency-svc-m9s8b
May 21 10:09:45.035: INFO: Got endpoints: latency-svc-m9s8b [870.268702ms]
May 21 10:09:45.070: INFO: Created: latency-svc-5vvrn
May 21 10:09:45.079: INFO: Got endpoints: latency-svc-5vvrn [863.571905ms]
May 21 10:09:45.126: INFO: Created: latency-svc-q4gbf
May 21 10:09:45.161: INFO: Created: latency-svc-8tvck
May 21 10:09:45.195: INFO: Got endpoints: latency-svc-q4gbf [918.859784ms]
May 21 10:09:45.196: INFO: Got endpoints: latency-svc-8tvck [850.908659ms]
May 21 10:09:45.236: INFO: Created: latency-svc-n447c
May 21 10:09:45.236: INFO: Got endpoints: latency-svc-n447c [843.4007ms]
May 21 10:09:45.274: INFO: Created: latency-svc-xght8
May 21 10:09:45.284: INFO: Got endpoints: latency-svc-xght8 [836.280167ms]
May 21 10:09:45.330: INFO: Created: latency-svc-jxb4k
May 21 10:09:45.356: INFO: Got endpoints: latency-svc-jxb4k [844.87892ms]
May 21 10:09:45.397: INFO: Created: latency-svc-q4l52
May 21 10:09:45.407: INFO: Got endpoints: latency-svc-q4l52 [822.359049ms]
May 21 10:09:45.464: INFO: Created: latency-svc-q775q
May 21 10:09:45.498: INFO: Got endpoints: latency-svc-q775q [854.907594ms]
May 21 10:09:45.528: INFO: Created: latency-svc-79s58
May 21 10:09:45.547: INFO: Got endpoints: latency-svc-79s58 [882.19706ms]
May 21 10:09:45.601: INFO: Created: latency-svc-92986
May 21 10:09:45.619: INFO: Got endpoints: latency-svc-92986 [864.015401ms]
May 21 10:09:45.660: INFO: Created: latency-svc-l9ldx
May 21 10:09:45.673: INFO: Got endpoints: latency-svc-l9ldx [891.036017ms]
May 21 10:09:45.739: INFO: Created: latency-svc-gjdf9
May 21 10:09:45.749: INFO: Got endpoints: latency-svc-gjdf9 [917.996285ms]
May 21 10:09:45.799: INFO: Created: latency-svc-g4466
May 21 10:09:45.804: INFO: Got endpoints: latency-svc-g4466 [888.29418ms]
May 21 10:09:45.845: INFO: Created: latency-svc-dqhww
May 21 10:09:45.884: INFO: Created: latency-svc-tjqfr
May 21 10:09:45.895: INFO: Got endpoints: latency-svc-dqhww [939.890605ms]
May 21 10:09:45.897: INFO: Got endpoints: latency-svc-tjqfr [861.78961ms]
May 21 10:09:45.968: INFO: Created: latency-svc-sn2sq
May 21 10:09:45.968: INFO: Got endpoints: latency-svc-sn2sq [889.03463ms]
May 21 10:09:46.014: INFO: Created: latency-svc-j669f
May 21 10:09:46.030: INFO: Got endpoints: latency-svc-j669f [835.281237ms]
May 21 10:09:46.079: INFO: Created: latency-svc-tr87q
May 21 10:09:46.093: INFO: Got endpoints: latency-svc-tr87q [897.287063ms]
May 21 10:09:46.127: INFO: Created: latency-svc-j9756
May 21 10:09:46.140: INFO: Got endpoints: latency-svc-j9756 [904.662297ms]
May 21 10:09:46.194: INFO: Created: latency-svc-77pg5
May 21 10:09:46.209: INFO: Got endpoints: latency-svc-77pg5 [924.573319ms]
May 21 10:09:46.250: INFO: Created: latency-svc-s6qwq
May 21 10:09:46.268: INFO: Got endpoints: latency-svc-s6qwq [912.21897ms]
May 21 10:09:46.308: INFO: Created: latency-svc-9cw4b
May 21 10:09:46.322: INFO: Got endpoints: latency-svc-9cw4b [914.140311ms]
May 21 10:09:46.352: INFO: Created: latency-svc-t4cgh
May 21 10:09:46.371: INFO: Got endpoints: latency-svc-t4cgh [872.657162ms]
May 21 10:09:46.427: INFO: Created: latency-svc-7m4nr
May 21 10:09:46.429: INFO: Got endpoints: latency-svc-7m4nr [881.65409ms]
May 21 10:09:46.472: INFO: Created: latency-svc-7cwp9
May 21 10:09:46.503: INFO: Got endpoints: latency-svc-7cwp9 [883.568852ms]
May 21 10:09:46.515: INFO: Created: latency-svc-bt9mp
May 21 10:09:46.578: INFO: Got endpoints: latency-svc-bt9mp [904.312664ms]
May 21 10:09:46.593: INFO: Created: latency-svc-qvmdk
May 21 10:09:46.603: INFO: Got endpoints: latency-svc-qvmdk [853.815707ms]
May 21 10:09:46.640: INFO: Created: latency-svc-7j9w5
May 21 10:09:46.674: INFO: Got endpoints: latency-svc-7j9w5 [870.743997ms]
May 21 10:09:46.693: INFO: Created: latency-svc-8cssd
May 21 10:09:46.723: INFO: Got endpoints: latency-svc-8cssd [828.110357ms]
May 21 10:09:46.742: INFO: Created: latency-svc-98vdf
May 21 10:09:46.758: INFO: Got endpoints: latency-svc-98vdf [861.477584ms]
May 21 10:09:46.799: INFO: Created: latency-svc-lm7sg
May 21 10:09:46.835: INFO: Created: latency-svc-5bgwj
May 21 10:09:46.839: INFO: Got endpoints: latency-svc-lm7sg [870.070495ms]
May 21 10:09:46.924: INFO: Got endpoints: latency-svc-5bgwj [893.699496ms]
May 21 10:09:46.928: INFO: Created: latency-svc-wf44c
May 21 10:09:46.945: INFO: Got endpoints: latency-svc-wf44c [851.708499ms]
May 21 10:09:46.988: INFO: Created: latency-svc-2pq4g
May 21 10:09:46.999: INFO: Got endpoints: latency-svc-2pq4g [858.981719ms]
May 21 10:09:47.043: INFO: Created: latency-svc-rl4dh
May 21 10:09:47.053: INFO: Got endpoints: latency-svc-rl4dh [843.964577ms]
May 21 10:09:47.099: INFO: Created: latency-svc-bfd27
May 21 10:09:47.114: INFO: Got endpoints: latency-svc-bfd27 [845.215458ms]
May 21 10:09:47.157: INFO: Created: latency-svc-zg4r6
May 21 10:09:47.197: INFO: Got endpoints: latency-svc-zg4r6 [874.792089ms]
May 21 10:09:47.231: INFO: Created: latency-svc-jbwr2
May 21 10:09:47.273: INFO: Got endpoints: latency-svc-jbwr2 [902.105724ms]
May 21 10:09:47.286: INFO: Created: latency-svc-wxtwg
May 21 10:09:47.293: INFO: Got endpoints: latency-svc-wxtwg [864.014131ms]
May 21 10:09:47.336: INFO: Created: latency-svc-jdv72
May 21 10:09:47.336: INFO: Got endpoints: latency-svc-jdv72 [833.548207ms]
May 21 10:09:47.389: INFO: Created: latency-svc-8bdqs
May 21 10:09:47.402: INFO: Got endpoints: latency-svc-8bdqs [823.627565ms]
May 21 10:09:47.453: INFO: Created: latency-svc-ss2fm
May 21 10:09:47.484: INFO: Got endpoints: latency-svc-ss2fm [881.108349ms]
May 21 10:09:47.500: INFO: Created: latency-svc-jc7h9
May 21 10:09:47.516: INFO: Got endpoints: latency-svc-jc7h9 [841.885247ms]
May 21 10:09:47.589: INFO: Created: latency-svc-rbzfw
May 21 10:09:47.597: INFO: Got endpoints: latency-svc-rbzfw [874.415982ms]
May 21 10:09:47.790: INFO: Created: latency-svc-7gz2w
May 21 10:09:47.806: INFO: Got endpoints: latency-svc-7gz2w [1.047369083s]
May 21 10:09:47.877: INFO: Created: latency-svc-jvkhv
May 21 10:09:47.877: INFO: Got endpoints: latency-svc-jvkhv [1.038490291s]
May 21 10:09:47.933: INFO: Created: latency-svc-6n7dx
May 21 10:09:47.945: INFO: Got endpoints: latency-svc-6n7dx [1.020172621s]
May 21 10:09:47.998: INFO: Created: latency-svc-m6q9c
May 21 10:09:48.019: INFO: Got endpoints: latency-svc-m6q9c [1.07397161s]
May 21 10:09:48.060: INFO: Created: latency-svc-pwpts
May 21 10:09:48.099: INFO: Got endpoints: latency-svc-pwpts [1.099211841s]
May 21 10:09:48.118: INFO: Created: latency-svc-6qsc6
May 21 10:09:48.126: INFO: Got endpoints: latency-svc-6qsc6 [1.073669239s]
May 21 10:09:48.186: INFO: Created: latency-svc-6tfbj
May 21 10:09:48.234: INFO: Got endpoints: latency-svc-6tfbj [1.119784133s]
May 21 10:09:48.251: INFO: Created: latency-svc-gd2qt
May 21 10:09:48.288: INFO: Got endpoints: latency-svc-gd2qt [1.090933165s]
May 21 10:09:48.323: INFO: Created: latency-svc-mfhg4
May 21 10:09:48.329: INFO: Got endpoints: latency-svc-mfhg4 [1.055375858s]
May 21 10:09:48.377: INFO: Created: latency-svc-qt6lv
May 21 10:09:48.397: INFO: Got endpoints: latency-svc-qt6lv [1.103242413s]
May 21 10:09:48.471: INFO: Created: latency-svc-8zgwn
May 21 10:09:48.471: INFO: Got endpoints: latency-svc-8zgwn [1.13401397s]
May 21 10:09:48.535: INFO: Created: latency-svc-dlnhd
May 21 10:09:48.548: INFO: Got endpoints: latency-svc-dlnhd [1.146055686s]
May 21 10:09:48.591: INFO: Created: latency-svc-wjtbt
May 21 10:09:48.645: INFO: Got endpoints: latency-svc-wjtbt [1.16068756s]
May 21 10:09:48.655: INFO: Created: latency-svc-xqgln
May 21 10:09:48.667: INFO: Got endpoints: latency-svc-xqgln [1.150561025s]
May 21 10:09:48.704: INFO: Created: latency-svc-kr98l
May 21 10:09:48.744: INFO: Got endpoints: latency-svc-kr98l [1.146581972s]
May 21 10:09:48.757: INFO: Created: latency-svc-qzxfr
May 21 10:09:48.785: INFO: Got endpoints: latency-svc-qzxfr [978.828879ms]
May 21 10:09:48.812: INFO: Created: latency-svc-dc7t9
May 21 10:09:48.836: INFO: Got endpoints: latency-svc-dc7t9 [958.394275ms]
May 21 10:09:48.886: INFO: Created: latency-svc-xq7sq
May 21 10:09:48.887: INFO: Got endpoints: latency-svc-xq7sq [942.633338ms]
May 21 10:09:48.931: INFO: Created: latency-svc-ls4h2
May 21 10:09:48.974: INFO: Got endpoints: latency-svc-ls4h2 [954.865661ms]
May 21 10:09:48.982: INFO: Created: latency-svc-c2hdn
May 21 10:09:48.996: INFO: Got endpoints: latency-svc-c2hdn [896.864429ms]
May 21 10:09:49.028: INFO: Created: latency-svc-7pr45
May 21 10:09:49.042: INFO: Got endpoints: latency-svc-7pr45 [915.404331ms]
May 21 10:09:49.106: INFO: Created: latency-svc-6d69s
May 21 10:09:49.106: INFO: Got endpoints: latency-svc-6d69s [872.292275ms]
May 21 10:09:49.162: INFO: Created: latency-svc-hg6z7
May 21 10:09:49.174: INFO: Got endpoints: latency-svc-hg6z7 [885.780897ms]
May 21 10:09:49.215: INFO: Created: latency-svc-khdfb
May 21 10:09:49.232: INFO: Got endpoints: latency-svc-khdfb [903.162594ms]
May 21 10:09:49.279: INFO: Created: latency-svc-z9vrc
May 21 10:09:49.310: INFO: Got endpoints: latency-svc-z9vrc [913.307165ms]
May 21 10:09:49.330: INFO: Created: latency-svc-4wkkb
May 21 10:09:49.353: INFO: Got endpoints: latency-svc-4wkkb [882.701629ms]
May 21 10:09:49.382: INFO: Created: latency-svc-tlmg8
May 21 10:09:49.399: INFO: Got endpoints: latency-svc-tlmg8 [851.010585ms]
May 21 10:09:49.453: INFO: Created: latency-svc-pl5rk
May 21 10:09:49.453: INFO: Got endpoints: latency-svc-pl5rk [808.268593ms]
May 21 10:09:49.510: INFO: Created: latency-svc-ppq4w
May 21 10:09:49.562: INFO: Got endpoints: latency-svc-ppq4w [894.873034ms]
May 21 10:09:49.577: INFO: Created: latency-svc-rrs2p
May 21 10:09:49.610: INFO: Got endpoints: latency-svc-rrs2p [865.866795ms]
May 21 10:09:49.619: INFO: Created: latency-svc-jfgpk
May 21 10:09:49.661: INFO: Got endpoints: latency-svc-jfgpk [876.369183ms]
May 21 10:09:49.690: INFO: Created: latency-svc-c66kq
May 21 10:09:49.702: INFO: Got endpoints: latency-svc-c66kq [866.324672ms]
May 21 10:09:49.745: INFO: Created: latency-svc-c5hz8
May 21 10:09:49.779: INFO: Got endpoints: latency-svc-c5hz8 [891.490704ms]
May 21 10:09:49.823: INFO: Created: latency-svc-dp7ns
May 21 10:09:49.830: INFO: Got endpoints: latency-svc-dp7ns [855.958059ms]
May 21 10:09:49.868: INFO: Created: latency-svc-wds7j
May 21 10:09:49.879: INFO: Got endpoints: latency-svc-wds7j [883.12081ms]
May 21 10:09:49.920: INFO: Created: latency-svc-7wcbs
May 21 10:09:49.932: INFO: Got endpoints: latency-svc-7wcbs [890.237399ms]
May 21 10:09:49.968: INFO: Created: latency-svc-47khh
May 21 10:09:49.981: INFO: Got endpoints: latency-svc-47khh [874.321198ms]
May 21 10:09:50.048: INFO: Created: latency-svc-crw5x
May 21 10:09:50.048: INFO: Got endpoints: latency-svc-crw5x [874.427275ms]
May 21 10:09:50.082: INFO: Created: latency-svc-8r6b2
May 21 10:09:50.093: INFO: Got endpoints: latency-svc-8r6b2 [860.849463ms]
May 21 10:09:50.128: INFO: Created: latency-svc-7r2qd
May 21 10:09:50.173: INFO: Got endpoints: latency-svc-7r2qd [862.2268ms]
May 21 10:09:50.182: INFO: Created: latency-svc-lh5bg
May 21 10:09:50.198: INFO: Got endpoints: latency-svc-lh5bg [844.273571ms]
May 21 10:09:50.239: INFO: Created: latency-svc-wdkph
May 21 10:09:50.243: INFO: Got endpoints: latency-svc-wdkph [843.13251ms]
May 21 10:09:50.310: INFO: Created: latency-svc-qxtxb
May 21 10:09:50.329: INFO: Got endpoints: latency-svc-qxtxb [875.607277ms]
May 21 10:09:50.369: INFO: Created: latency-svc-w2ktm
May 21 10:09:50.392: INFO: Got endpoints: latency-svc-w2ktm [829.681824ms]
May 21 10:09:50.418: INFO: Created: latency-svc-nmlqb
May 21 10:09:50.446: INFO: Got endpoints: latency-svc-nmlqb [835.367909ms]
May 21 10:09:50.456: INFO: Created: latency-svc-7lmnn
May 21 10:09:50.514: INFO: Got endpoints: latency-svc-7lmnn [853.016633ms]
May 21 10:09:50.523: INFO: Created: latency-svc-j7cvr
May 21 10:09:50.554: INFO: Got endpoints: latency-svc-j7cvr [851.971129ms]
May 21 10:09:50.566: INFO: Created: latency-svc-jcgn6
May 21 10:09:50.613: INFO: Got endpoints: latency-svc-jcgn6 [834.048987ms]
May 21 10:09:50.635: INFO: Created: latency-svc-x8b5b
May 21 10:09:50.653: INFO: Got endpoints: latency-svc-x8b5b [822.491029ms]
May 21 10:09:50.685: INFO: Created: latency-svc-xlkcw
May 21 10:09:50.768: INFO: Created: latency-svc-z4xj7
May 21 10:09:50.782: INFO: Got endpoints: latency-svc-xlkcw [902.987742ms]
May 21 10:09:50.787: INFO: Got endpoints: latency-svc-z4xj7 [854.246345ms]
May 21 10:09:50.803: INFO: Created: latency-svc-85q6n
May 21 10:09:50.810: INFO: Got endpoints: latency-svc-85q6n [829.374211ms]
May 21 10:09:50.860: INFO: Created: latency-svc-jgqqg
May 21 10:09:50.908: INFO: Got endpoints: latency-svc-jgqqg [859.700387ms]
May 21 10:09:50.931: INFO: Created: latency-svc-5ljjs
May 21 10:09:50.975: INFO: Created: latency-svc-t4m85
May 21 10:09:50.986: INFO: Got endpoints: latency-svc-5ljjs [892.804471ms]
May 21 10:09:50.990: INFO: Got endpoints: latency-svc-t4m85 [817.197187ms]
May 21 10:09:51.047: INFO: Created: latency-svc-x9zbc
May 21 10:09:51.055: INFO: Got endpoints: latency-svc-x9zbc [857.196189ms]
May 21 10:09:51.102: INFO: Created: latency-svc-lqlw4
May 21 10:09:51.107: INFO: Got endpoints: latency-svc-lqlw4 [864.36609ms]
May 21 10:09:51.139: INFO: Created: latency-svc-zxwjp
May 21 10:09:51.154: INFO: Got endpoints: latency-svc-zxwjp [824.972007ms]
May 21 10:09:51.190: INFO: Created: latency-svc-ws26c
May 21 10:09:51.261: INFO: Got endpoints: latency-svc-ws26c [868.624573ms]
May 21 10:09:51.265: INFO: Created: latency-svc-d9hn2
May 21 10:09:51.279: INFO: Got endpoints: latency-svc-d9hn2 [832.905126ms]
May 21 10:09:51.324: INFO: Created: latency-svc-c2stk
May 21 10:09:51.330: INFO: Got endpoints: latency-svc-c2stk [815.477523ms]
May 21 10:09:51.393: INFO: Created: latency-svc-c962l
May 21 10:09:51.422: INFO: Got endpoints: latency-svc-c962l [867.58851ms]
May 21 10:09:51.498: INFO: Created: latency-svc-skzkk
May 21 10:09:51.518: INFO: Got endpoints: latency-svc-skzkk [904.871252ms]
May 21 10:09:51.520: INFO: Created: latency-svc-t5cp9
May 21 10:09:51.536: INFO: Got endpoints: latency-svc-t5cp9 [883.505581ms]
May 21 10:09:51.573: INFO: Created: latency-svc-jzphz
May 21 10:09:51.616: INFO: Got endpoints: latency-svc-jzphz [833.685852ms]
May 21 10:09:51.631: INFO: Created: latency-svc-f4tlh
May 21 10:09:51.655: INFO: Got endpoints: latency-svc-f4tlh [868.565856ms]
May 21 10:09:51.670: INFO: Created: latency-svc-kvm9l
May 21 10:09:51.677: INFO: Got endpoints: latency-svc-kvm9l [866.915796ms]
May 21 10:09:51.711: INFO: Created: latency-svc-27gcr
May 21 10:09:51.730: INFO: Got endpoints: latency-svc-27gcr [821.755275ms]
May 21 10:09:51.769: INFO: Created: latency-svc-xsph6
May 21 10:09:51.783: INFO: Got endpoints: latency-svc-xsph6 [796.354956ms]
May 21 10:09:51.817: INFO: Created: latency-svc-5l8n4
May 21 10:09:51.847: INFO: Got endpoints: latency-svc-5l8n4 [857.477185ms]
May 21 10:09:51.857: INFO: Created: latency-svc-xkh6f
May 21 10:09:51.870: INFO: Got endpoints: latency-svc-xkh6f [815.226854ms]
May 21 10:09:51.911: INFO: Created: latency-svc-2tnsv
May 21 10:09:51.916: INFO: Got endpoints: latency-svc-2tnsv [808.890095ms]
May 21 10:09:51.974: INFO: Created: latency-svc-pn7hf
May 21 10:09:51.974: INFO: Got endpoints: latency-svc-pn7hf [819.606217ms]
May 21 10:09:52.060: INFO: Created: latency-svc-b9g4d
May 21 10:09:52.061: INFO: Got endpoints: latency-svc-b9g4d [799.624666ms]
May 21 10:09:52.124: INFO: Created: latency-svc-l49s5
May 21 10:09:52.149: INFO: Got endpoints: latency-svc-l49s5 [869.969396ms]
May 21 10:09:52.165: INFO: Created: latency-svc-7xsk6
May 21 10:09:52.167: INFO: Got endpoints: latency-svc-7xsk6 [837.43833ms]
May 21 10:09:52.214: INFO: Created: latency-svc-2hpgq
May 21 10:09:52.225: INFO: Got endpoints: latency-svc-2hpgq [803.425448ms]
May 21 10:09:52.254: INFO: Created: latency-svc-2vz47
May 21 10:09:52.270: INFO: Got endpoints: latency-svc-2vz47 [752.05361ms]
May 21 10:09:52.313: INFO: Created: latency-svc-9jfpb
May 21 10:09:52.324: INFO: Got endpoints: latency-svc-9jfpb [787.947405ms]
May 21 10:09:52.369: INFO: Created: latency-svc-bf56x
May 21 10:09:52.380: INFO: Got endpoints: latency-svc-bf56x [764.513813ms]
May 21 10:09:52.423: INFO: Created: latency-svc-fvwbl
May 21 10:09:52.439: INFO: Got endpoints: latency-svc-fvwbl [784.030314ms]
May 21 10:09:52.476: INFO: Created: latency-svc-sszq5
May 21 10:09:52.508: INFO: Got endpoints: latency-svc-sszq5 [830.936178ms]
May 21 10:09:52.692: INFO: Created: latency-svc-mw67l
May 21 10:09:52.715: INFO: Got endpoints: latency-svc-mw67l [984.591101ms]
May 21 10:09:52.742: INFO: Created: latency-svc-q779j
May 21 10:09:52.769: INFO: Got endpoints: latency-svc-q779j [986.541879ms]
May 21 10:09:52.786: INFO: Created: latency-svc-7p6hp
May 21 10:09:52.799: INFO: Got endpoints: latency-svc-7p6hp [951.345694ms]
May 21 10:09:52.833: INFO: Created: latency-svc-wnqb4
May 21 10:09:52.870: INFO: Got endpoints: latency-svc-wnqb4 [999.933572ms]
May 21 10:09:52.883: INFO: Created: latency-svc-fs7hv
May 21 10:09:52.912: INFO: Got endpoints: latency-svc-fs7hv [996.336332ms]
May 21 10:09:52.926: INFO: Created: latency-svc-snl4x
May 21 10:09:52.967: INFO: Got endpoints: latency-svc-snl4x [992.726862ms]
May 21 10:09:52.981: INFO: Created: latency-svc-7ncqx
May 21 10:09:53.007: INFO: Got endpoints: latency-svc-7ncqx [946.35366ms]
May 21 10:09:53.022: INFO: Created: latency-svc-wkfw6
May 21 10:09:53.064: INFO: Got endpoints: latency-svc-wkfw6 [915.061658ms]
May 21 10:09:53.097: INFO: Created: latency-svc-twbpr
May 21 10:09:53.097: INFO: Got endpoints: latency-svc-twbpr [928.842105ms]
May 21 10:09:53.136: INFO: Created: latency-svc-spbgg
May 21 10:09:53.167: INFO: Got endpoints: latency-svc-spbgg [942.16586ms]
May 21 10:09:53.183: INFO: Created: latency-svc-z6hxc
May 21 10:09:53.221: INFO: Got endpoints: latency-svc-z6hxc [950.474291ms]
May 21 10:09:53.235: INFO: Created: latency-svc-b8zxx
May 21 10:09:53.272: INFO: Got endpoints: latency-svc-b8zxx [947.529875ms]
May 21 10:09:53.286: INFO: Created: latency-svc-m752b
May 21 10:09:53.302: INFO: Got endpoints: latency-svc-m752b [921.164044ms]
May 21 10:09:53.332: INFO: Created: latency-svc-vzb9s
May 21 10:09:53.384: INFO: Got endpoints: latency-svc-vzb9s [944.821793ms]
May 21 10:09:53.389: INFO: Created: latency-svc-8js9q
May 21 10:09:53.406: INFO: Got endpoints: latency-svc-8js9q [898.091241ms]
May 21 10:09:53.457: INFO: Created: latency-svc-5ptp9
May 21 10:09:53.491: INFO: Got endpoints: latency-svc-5ptp9 [775.951277ms]
May 21 10:09:53.520: INFO: Created: latency-svc-lz9m7
May 21 10:09:53.539: INFO: Got endpoints: latency-svc-lz9m7 [769.3772ms]
May 21 10:09:53.590: INFO: Created: latency-svc-tvjv2
May 21 10:09:53.620: INFO: Got endpoints: latency-svc-tvjv2 [821.179642ms]
May 21 10:09:53.638: INFO: Created: latency-svc-jwtt4
May 21 10:09:53.649: INFO: Got endpoints: latency-svc-jwtt4 [778.941709ms]
May 21 10:09:53.697: INFO: Created: latency-svc-9vb6p
May 21 10:09:53.703: INFO: Got endpoints: latency-svc-9vb6p [790.512173ms]
May 21 10:09:53.768: INFO: Created: latency-svc-jshkk
May 21 10:09:53.802: INFO: Got endpoints: latency-svc-jshkk [835.228939ms]
May 21 10:09:53.844: INFO: Created: latency-svc-sdlmd
May 21 10:09:53.882: INFO: Got endpoints: latency-svc-sdlmd [874.511585ms]
May 21 10:09:53.926: INFO: Created: latency-svc-dwshz
May 21 10:09:53.956: INFO: Got endpoints: latency-svc-dwshz [892.154537ms]
May 21 10:09:53.973: INFO: Created: latency-svc-j2jhk
May 21 10:09:54.015: INFO: Got endpoints: latency-svc-j2jhk [918.657262ms]
May 21 10:09:54.015: INFO: Created: latency-svc-fqn92
May 21 10:09:54.070: INFO: Got endpoints: latency-svc-fqn92 [902.002533ms]
May 21 10:09:54.088: INFO: Created: latency-svc-m2qsk
May 21 10:09:54.115: INFO: Got endpoints: latency-svc-m2qsk [894.142338ms]
May 21 10:09:54.126: INFO: Created: latency-svc-9642q
May 21 10:09:54.139: INFO: Got endpoints: latency-svc-9642q [867.399252ms]
May 21 10:09:54.171: INFO: Created: latency-svc-52799
May 21 10:09:54.197: INFO: Got endpoints: latency-svc-52799 [895.451498ms]
May 21 10:09:54.233: INFO: Created: latency-svc-7qpff
May 21 10:09:54.242: INFO: Got endpoints: latency-svc-7qpff [857.909456ms]
May 21 10:09:54.271: INFO: Created: latency-svc-vgwh6
May 21 10:09:54.315: INFO: Got endpoints: latency-svc-vgwh6 [908.220159ms]
May 21 10:09:54.325: INFO: Created: latency-svc-ssbxp
May 21 10:09:54.334: INFO: Got endpoints: latency-svc-ssbxp [843.713497ms]
May 21 10:09:54.374: INFO: Created: latency-svc-6mqvl
May 21 10:09:54.388: INFO: Got endpoints: latency-svc-6mqvl [849.736108ms]
May 21 10:09:54.433: INFO: Created: latency-svc-9v4nf
May 21 10:09:54.439: INFO: Got endpoints: latency-svc-9v4nf [818.776791ms]
May 21 10:09:54.483: INFO: Created: latency-svc-kvwlw
May 21 10:09:54.545: INFO: Got endpoints: latency-svc-kvwlw [895.565963ms]
May 21 10:09:54.563: INFO: Created: latency-svc-j8p8s
May 21 10:09:54.566: INFO: Got endpoints: latency-svc-j8p8s [863.300165ms]
May 21 10:09:54.602: INFO: Created: latency-svc-9gdxz
May 21 10:09:54.643: INFO: Got endpoints: latency-svc-9gdxz [840.514181ms]
May 21 10:09:54.660: INFO: Created: latency-svc-qhhvq
May 21 10:09:54.692: INFO: Got endpoints: latency-svc-qhhvq [810.527508ms]
May 21 10:09:54.707: INFO: Created: latency-svc-kxcwx
May 21 10:09:54.738: INFO: Got endpoints: latency-svc-kxcwx [782.017013ms]
May 21 10:09:54.753: INFO: Created: latency-svc-qt8t5
May 21 10:09:54.794: INFO: Created: latency-svc-zbq2b
May 21 10:09:54.825: INFO: Created: latency-svc-q82jk
May 21 10:09:54.839: INFO: Got endpoints: latency-svc-qt8t5 [823.887275ms]
May 21 10:09:54.859: INFO: Got endpoints: latency-svc-zbq2b [789.418743ms]
May 21 10:09:54.878: INFO: Got endpoints: latency-svc-q82jk [762.922275ms]
May 21 10:09:54.895: INFO: Created: latency-svc-tprhj
May 21 10:09:54.910: INFO: Got endpoints: latency-svc-tprhj [770.80522ms]
May 21 10:09:54.958: INFO: Created: latency-svc-qh7m6
May 21 10:09:54.963: INFO: Got endpoints: latency-svc-qh7m6 [766.233066ms]
May 21 10:09:55.009: INFO: Created: latency-svc-69dck
May 21 10:09:55.017: INFO: Got endpoints: latency-svc-69dck [774.229654ms]
May 21 10:09:55.063: INFO: Created: latency-svc-hp5rw
May 21 10:09:55.075: INFO: Got endpoints: latency-svc-hp5rw [760.131854ms]
May 21 10:09:55.128: INFO: Created: latency-svc-qjl5n
May 21 10:09:55.147: INFO: Got endpoints: latency-svc-qjl5n [812.196421ms]
May 21 10:09:55.178: INFO: Created: latency-svc-6fdxj
May 21 10:09:55.230: INFO: Got endpoints: latency-svc-6fdxj [841.103267ms]
May 21 10:09:55.240: INFO: Created: latency-svc-c54kc
May 21 10:09:55.266: INFO: Got endpoints: latency-svc-c54kc [827.082114ms]
May 21 10:09:55.281: INFO: Created: latency-svc-56s52
May 21 10:09:55.307: INFO: Got endpoints: latency-svc-56s52 [761.271043ms]
May 21 10:09:55.317: INFO: Created: latency-svc-5ntzl
May 21 10:09:55.343: INFO: Got endpoints: latency-svc-5ntzl [776.725989ms]
May 21 10:09:55.343: INFO: Latencies: [127.40185ms 153.985792ms 255.610353ms 317.546222ms 436.921519ms 463.440245ms 561.302326ms 602.681815ms 696.761135ms 747.216469ms 752.05361ms 760.131854ms 761.271043ms 762.922275ms 764.513813ms 766.233066ms 769.3772ms 770.80522ms 774.229654ms 775.951277ms 776.725989ms 778.941709ms 782.017013ms 784.030314ms 787.947405ms 789.418743ms 790.512173ms 796.354956ms 799.624666ms 803.425448ms 807.556517ms 808.268593ms 808.890095ms 810.527508ms 812.196421ms 815.226854ms 815.477523ms 817.197187ms 818.776791ms 819.606217ms 821.179642ms 821.755275ms 822.359049ms 822.491029ms 823.627565ms 823.887275ms 824.972007ms 827.082114ms 828.110357ms 829.374211ms 829.681824ms 830.936178ms 832.905126ms 833.548207ms 833.685852ms 834.048987ms 835.228939ms 835.281237ms 835.367909ms 836.280167ms 837.43833ms 840.514181ms 841.103267ms 841.885247ms 843.13251ms 843.4007ms 843.713497ms 843.964577ms 844.273571ms 844.87892ms 845.215458ms 849.736108ms 850.908659ms 851.010585ms 851.708499ms 851.971129ms 853.016633ms 853.815707ms 854.246345ms 854.907594ms 855.958059ms 857.196189ms 857.477185ms 857.909456ms 858.981719ms 859.700387ms 860.849463ms 861.477584ms 861.78961ms 862.2268ms 863.300165ms 863.571905ms 864.014131ms 864.015401ms 864.36609ms 865.866795ms 866.324672ms 866.915796ms 867.399252ms 867.58851ms 868.565856ms 868.624573ms 869.969396ms 870.070495ms 870.268702ms 870.743997ms 872.292275ms 872.657162ms 874.321198ms 874.415982ms 874.427275ms 874.511585ms 874.792089ms 875.607277ms 876.369183ms 876.377677ms 877.960379ms 881.108349ms 881.65409ms 882.19706ms 882.701629ms 883.12081ms 883.505581ms 883.56769ms 883.568852ms 885.780897ms 886.024245ms 888.29418ms 889.03463ms 890.237399ms 891.036017ms 891.490704ms 892.154537ms 892.804471ms 893.699496ms 894.142338ms 894.873034ms 895.451498ms 895.565963ms 896.864429ms 897.287063ms 898.091241ms 899.238489ms 902.002533ms 902.105724ms 902.987742ms 903.162594ms 904.312664ms 904.662297ms 904.871252ms 908.220159ms 912.21897ms 913.307165ms 914.140311ms 915.061658ms 915.404331ms 917.996285ms 918.657262ms 918.859784ms 921.164044ms 923.342692ms 924.573319ms 928.842105ms 939.890605ms 941.24206ms 942.16586ms 942.633338ms 944.821793ms 946.35366ms 947.529875ms 950.474291ms 951.345694ms 954.865661ms 958.394275ms 969.282089ms 978.828879ms 979.167551ms 984.591101ms 986.541879ms 989.339446ms 992.726862ms 996.336332ms 999.933572ms 1.020172621s 1.021957907s 1.038490291s 1.042369136s 1.047369083s 1.055375858s 1.073669239s 1.07397161s 1.090933165s 1.099211841s 1.103242413s 1.119784133s 1.13401397s 1.146055686s 1.146581972s 1.150561025s 1.16068756s]
May 21 10:09:55.344: INFO: 50 %ile: 868.565856ms
May 21 10:09:55.344: INFO: 90 %ile: 992.726862ms
May 21 10:09:55.344: INFO: 99 %ile: 1.150561025s
May 21 10:09:55.344: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:09:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3295" for this suite.

• [SLOW TEST:18.333 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":303,"completed":88,"skipped":1676,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:09:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
May 21 10:09:55.502: INFO: Waiting up to 5m0s for pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a" in namespace "var-expansion-9200" to be "Succeeded or Failed"
May 21 10:09:55.508: INFO: Pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.530896ms
May 21 10:09:57.517: INFO: Pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015576468s
May 21 10:09:59.527: INFO: Pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025031396s
May 21 10:10:01.535: INFO: Pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033589489s
STEP: Saw pod success
May 21 10:10:01.536: INFO: Pod "var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a" satisfied condition "Succeeded or Failed"
May 21 10:10:01.544: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a container dapi-container: <nil>
STEP: delete the pod
May 21 10:10:01.653: INFO: Waiting for pod var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a to disappear
May 21 10:10:01.668: INFO: Pod var-expansion-ea33aba7-0f1b-4e14-914f-bb9a253ce84a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:10:01.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9200" for this suite.

• [SLOW TEST:6.370 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":303,"completed":89,"skipped":1680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:10:01.775: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2500
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2500
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2500
May 21 10:10:02.041: INFO: Found 0 stateful pods, waiting for 1
May 21 10:10:12.068: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 21 10:10:12.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:10:12.387: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:10:12.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:10:12.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:10:12.398: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 10:10:22.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:10:22.407: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:10:22.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999455s
May 21 10:10:23.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98653094s
May 21 10:10:24.502: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.96742996s
May 21 10:10:25.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.954398713s
May 21 10:10:26.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.944930422s
May 21 10:10:27.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.933262985s
May 21 10:10:28.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.924022269s
May 21 10:10:29.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.913695656s
May 21 10:10:30.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.901623549s
May 21 10:10:31.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 894.147592ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2500
May 21 10:10:32.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:10:32.909: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:10:32.909: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:10:32.909: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:10:32.920: INFO: Found 1 stateful pods, waiting for 3
May 21 10:10:42.935: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:10:42.935: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:10:42.935: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 21 10:10:52.942: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:10:52.942: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:10:52.942: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 21 10:10:52.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:10:53.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:10:53.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:10:53.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:10:53.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:10:53.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:10:53.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:10:53.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:10:53.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:10:53.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:10:53.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:10:53.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:10:53.898: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:10:53.912: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 21 10:11:03.959: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:11:03.959: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:11:03.960: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:11:04.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999646s
May 21 10:11:05.023: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981845495s
May 21 10:11:06.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970539807s
May 21 10:11:07.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955361725s
May 21 10:11:08.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942129364s
May 21 10:11:09.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.928485169s
May 21 10:11:10.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91078858s
May 21 10:11:11.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900245965s
May 21 10:11:12.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.891418647s
May 21 10:11:13.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 881.43824ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2500
May 21 10:11:14.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:11:14.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:11:14.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:11:14.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:11:14.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:11:14.900: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:11:14.900: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:11:14.900: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:11:14.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-2500 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:11:15.216: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:11:15.216: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:11:15.216: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:11:15.216: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:11:35.253: INFO: Deleting all statefulset in ns statefulset-2500
May 21 10:11:35.262: INFO: Scaling statefulset ss to 0
May 21 10:11:35.294: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:11:35.306: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:11:35.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2500" for this suite.

• [SLOW TEST:93.616 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":303,"completed":90,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:11:35.400: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:11:35.510: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 21 10:11:40.524: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 10:11:42.720: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 10:11:42.757: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7895 /apis/apps/v1/namespaces/deployment-7895/deployments/test-cleanup-deployment aaa25d1b-0257-446c-a7cc-e4e32eff09e2 167815 1 2021-05-21 10:11:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-21 10:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d56dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 21 10:11:42.769: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 21 10:11:42.769: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 21 10:11:42.769: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7895 /apis/apps/v1/namespaces/deployment-7895/replicasets/test-cleanup-controller 11e7d20d-883a-40bd-875c-b3acb8291f00 167816 1 2021-05-21 10:11:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment aaa25d1b-0257-446c-a7cc-e4e32eff09e2 0xc003d571a7 0xc003d571a8}] []  [{e2e.test Update apps/v1 2021-05-21 10:11:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 10:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa25d1b-0257-446c-a7cc-e4e32eff09e2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d57248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 10:11:42.778: INFO: Pod "test-cleanup-controller-9nhxs" is available:
&Pod{ObjectMeta:{test-cleanup-controller-9nhxs test-cleanup-controller- deployment-7895 /api/v1/namespaces/deployment-7895/pods/test-cleanup-controller-9nhxs 958f7230-eae2-4d46-8f39-835e363b67d9 167794 0 2021-05-21 10:11:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 11e7d20d-883a-40bd-875c-b3acb8291f00 0xc003d575d7 0xc003d575d8}] []  [{kube-controller-manager Update v1 2021-05-21 10:11:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e7d20d-883a-40bd-875c-b3acb8291f00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s9vvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s9vvf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s9vvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:11:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:11:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.3,StartTime:2021-05-21 10:11:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:11:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://354e9392ad18f5f35c14982567b31ef676b87b344c9f815085168d22eabdd98b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:11:42.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7895" for this suite.

• [SLOW TEST:7.409 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":303,"completed":91,"skipped":1769,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:11:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 in namespace container-probe-9230
May 21 10:11:49.040: INFO: Started pod liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 in namespace container-probe-9230
STEP: checking the pod's current state and verifying that restartCount is present
May 21 10:11:49.051: INFO: Initial restart count of pod liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is 0
May 21 10:12:07.180: INFO: Restart count of pod container-probe-9230/liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is now 1 (18.128454995s elapsed)
May 21 10:12:27.371: INFO: Restart count of pod container-probe-9230/liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is now 2 (38.319952486s elapsed)
May 21 10:12:47.460: INFO: Restart count of pod container-probe-9230/liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is now 3 (58.408257192s elapsed)
May 21 10:13:07.580: INFO: Restart count of pod container-probe-9230/liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is now 4 (1m18.528186908s elapsed)
May 21 10:14:15.995: INFO: Restart count of pod container-probe-9230/liveness-c9e46a68-166e-434e-a0c7-2577d0f038a6 is now 5 (2m26.943996353s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:14:16.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9230" for this suite.

• [SLOW TEST:153.281 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":303,"completed":92,"skipped":1772,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:14:16.091: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-19b2a6bc-1e54-42cf-8350-86fecafe8cd4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:14:26.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8103" for this suite.

• [SLOW TEST:10.324 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":93,"skipped":1780,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:14:26.420: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8590.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8590.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8590.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8590.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 10:14:48.702: INFO: DNS probes using dns-8590/dns-test-c223618f-5525-4800-84e3-164ce3bed188 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:14:48.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8590" for this suite.

• [SLOW TEST:22.419 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":303,"completed":94,"skipped":1799,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:14:48.840: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 21 10:14:55.026: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9775 PodName:var-expansion-8d36e4d2-d4f8-4264-bf75-2ae4a79cc2a6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:14:55.027: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: test for file in mounted path
May 21 10:14:55.215: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9775 PodName:var-expansion-8d36e4d2-d4f8-4264-bf75-2ae4a79cc2a6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:14:55.215: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: updating the annotation value
May 21 10:14:55.895: INFO: Successfully updated pod "var-expansion-8d36e4d2-d4f8-4264-bf75-2ae4a79cc2a6"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 21 10:14:55.905: INFO: Deleting pod "var-expansion-8d36e4d2-d4f8-4264-bf75-2ae4a79cc2a6" in namespace "var-expansion-9775"
May 21 10:14:55.935: INFO: Wait up to 5m0s for pod "var-expansion-8d36e4d2-d4f8-4264-bf75-2ae4a79cc2a6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:15:37.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9775" for this suite.

• [SLOW TEST:49.166 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":303,"completed":95,"skipped":1811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:15:38.008: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:15:38.083: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:15:44.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6342" for this suite.

• [SLOW TEST:6.234 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":303,"completed":96,"skipped":1837,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:15:44.244: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2583.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2583.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2583.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 10:15:56.459: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.467: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.475: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.482: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.508: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.515: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.522: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.528: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:15:56.546: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:01.558: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.567: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.573: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.580: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.601: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.608: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.615: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.625: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:01.643: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:06.560: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.572: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.579: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.588: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.627: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.640: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.646: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.651: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:06.664: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:11.564: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.573: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.580: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.588: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.618: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.626: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.634: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.641: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:11.655: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:16.560: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.570: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.583: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.601: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.627: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.635: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.642: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.649: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:16.663: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:21.558: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.568: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.575: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.581: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.603: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.609: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.617: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.625: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local from pod dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25: the server could not find the requested resource (get pods dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25)
May 21 10:16:21.639: INFO: Lookups using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2583.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2583.svc.cluster.local jessie_udp@dns-test-service-2.dns-2583.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2583.svc.cluster.local]

May 21 10:16:26.687: INFO: DNS probes using dns-2583/dns-test-45a0130c-f53c-447c-97cc-efd9d56c7c25 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:16:26.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2583" for this suite.

• [SLOW TEST:42.633 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":303,"completed":97,"skipped":1842,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:16:26.880: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:16:28.057: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 10:16:30.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:16:32.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757188988, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:16:35.154: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:16:35.167: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:16:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3566" for this suite.
STEP: Destroying namespace "webhook-3566-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.889 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":303,"completed":98,"skipped":1845,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:16:36.773: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3125.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3125.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3125.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3125.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 25.84.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.84.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.84.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.84.25_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3125.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3125.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3125.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3125.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3125.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3125.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 25.84.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.84.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.84.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.84.25_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 10:16:49.077: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.086: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.112: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.168: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.173: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.180: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.186: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:49.235: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:16:54.249: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.267: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.275: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.324: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.345: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:54.394: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:16:59.249: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.256: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.263: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.271: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.340: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.360: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:16:59.434: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:17:04.252: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.262: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.270: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.341: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:04.411: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:17:09.250: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.283: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.337: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.369: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.391: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:09.440: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:17:14.249: INFO: Unable to read wheezy_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.332: INFO: Unable to read jessie_udp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.340: INFO: Unable to read jessie_tcp@dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.347: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.353: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local from pod dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0: the server could not find the requested resource (get pods dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0)
May 21 10:17:14.442: INFO: Lookups using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 failed for: [wheezy_udp@dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@dns-test-service.dns-3125.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_udp@dns-test-service.dns-3125.svc.cluster.local jessie_tcp@dns-test-service.dns-3125.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3125.svc.cluster.local]

May 21 10:17:19.423: INFO: DNS probes using dns-3125/dns-test-13bf9e8d-c968-45ab-aaff-69e8c8a323a0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:17:19.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3125" for this suite.

• [SLOW TEST:42.962 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":303,"completed":99,"skipped":1859,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:17:19.736: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:17:20.957: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 10:17:22.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189041, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:17:24.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189041, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189040, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:17:28.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 21 10:17:34.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=webhook-6869 attach --namespace=webhook-6869 to-be-attached-pod -i -c=container1'
May 21 10:17:34.320: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:17:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6869" for this suite.
STEP: Destroying namespace "webhook-6869-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.822 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":303,"completed":100,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:17:34.564: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-jdf9
STEP: Creating a pod to test atomic-volume-subpath
May 21 10:17:34.702: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jdf9" in namespace "subpath-4733" to be "Succeeded or Failed"
May 21 10:17:34.726: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.814886ms
May 21 10:17:36.737: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034752324s
May 21 10:17:38.747: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044541052s
May 21 10:17:40.758: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 6.055913222s
May 21 10:17:42.769: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 8.06650498s
May 21 10:17:44.776: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.073935285s
May 21 10:17:46.789: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.086572857s
May 21 10:17:48.799: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 14.096902373s
May 21 10:17:50.807: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 16.104295793s
May 21 10:17:52.817: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 18.114356253s
May 21 10:17:54.828: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 20.125677299s
May 21 10:17:56.838: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 22.135611684s
May 21 10:17:58.851: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Running", Reason="", readiness=true. Elapsed: 24.14865289s
May 21 10:18:00.862: INFO: Pod "pod-subpath-test-secret-jdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.15918646s
STEP: Saw pod success
May 21 10:18:00.862: INFO: Pod "pod-subpath-test-secret-jdf9" satisfied condition "Succeeded or Failed"
May 21 10:18:00.867: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod pod-subpath-test-secret-jdf9 container test-container-subpath-secret-jdf9: <nil>
STEP: delete the pod
May 21 10:18:00.974: INFO: Waiting for pod pod-subpath-test-secret-jdf9 to disappear
May 21 10:18:00.983: INFO: Pod pod-subpath-test-secret-jdf9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-jdf9
May 21 10:18:00.983: INFO: Deleting pod "pod-subpath-test-secret-jdf9" in namespace "subpath-4733"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:18:00.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4733" for this suite.

• [SLOW TEST:26.470 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":303,"completed":101,"skipped":1915,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:18:01.035: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 10:18:01.168: INFO: Waiting up to 5m0s for pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e" in namespace "emptydir-6360" to be "Succeeded or Failed"
May 21 10:18:01.190: INFO: Pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.876769ms
May 21 10:18:03.197: INFO: Pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029412281s
May 21 10:18:05.223: INFO: Pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054580176s
May 21 10:18:07.232: INFO: Pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063865401s
STEP: Saw pod success
May 21 10:18:07.232: INFO: Pod "pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e" satisfied condition "Succeeded or Failed"
May 21 10:18:07.238: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e container test-container: <nil>
STEP: delete the pod
May 21 10:18:07.337: INFO: Waiting for pod pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e to disappear
May 21 10:18:07.344: INFO: Pod pod-5808649c-f7ea-4b7f-9e27-9b4a8e5b758e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:18:07.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6360" for this suite.

• [SLOW TEST:6.343 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":102,"skipped":1918,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:18:07.378: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 10:18:07.481: INFO: Waiting up to 5m0s for pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb" in namespace "emptydir-4018" to be "Succeeded or Failed"
May 21 10:18:07.508: INFO: Pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.810276ms
May 21 10:18:09.520: INFO: Pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038816026s
May 21 10:18:11.532: INFO: Pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050240413s
May 21 10:18:13.542: INFO: Pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060364899s
STEP: Saw pod success
May 21 10:18:13.542: INFO: Pod "pod-d6245a50-d491-4f87-a93d-cbc364d446bb" satisfied condition "Succeeded or Failed"
May 21 10:18:13.551: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-d6245a50-d491-4f87-a93d-cbc364d446bb container test-container: <nil>
STEP: delete the pod
May 21 10:18:13.620: INFO: Waiting for pod pod-d6245a50-d491-4f87-a93d-cbc364d446bb to disappear
May 21 10:18:13.625: INFO: Pod pod-d6245a50-d491-4f87-a93d-cbc364d446bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:18:13.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4018" for this suite.

• [SLOW TEST:6.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":103,"skipped":1922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:18:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 10:18:13.730: INFO: PodSpec: initContainers in spec.initContainers
May 21 10:19:11.182: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cf7c64da-8b8c-4272-80dd-70256f5b3623", GenerateName:"", Namespace:"init-container-2268", SelfLink:"/api/v1/namespaces/init-container-2268/pods/pod-init-cf7c64da-8b8c-4272-80dd-70256f5b3623", UID:"6042f7ca-05a4-486e-9db7-ed5528568b1f", ResourceVersion:"170367", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757189093, loc:(*time.Location)(0x770e980)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"730700147"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001cf6240), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001cf6260)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001cf6280), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001cf62a0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-x4nkz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005a56100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x4nkz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x4nkz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x4nkz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003d984f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"p1-zq5sznp3cxrb94t9rscobyn6ny", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c8e230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d985e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d98640)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003d98648), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003d9864c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000b78110), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189093, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189093, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189093, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189093, loc:(*time.Location)(0x770e980)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.30.52.10", PodIP:"172.28.96.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.28.96.3"}}, StartTime:(*v1.Time)(0xc001cf62c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c8e310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://7c75df83e3e0adec40a06a9a290da472afd75749ba353a039462c8510e2a7d77", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cf6300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cf62e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003d9883f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:19:11.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2268" for this suite.

• [SLOW TEST:57.580 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":303,"completed":104,"skipped":1971,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:19:11.235: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 10:19:18.034: INFO: Successfully updated pod "labelsupdate8010ae9a-41f6-4f76-94a7-4014001e1bf3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:19:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-406" for this suite.

• [SLOW TEST:8.911 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":105,"skipped":1972,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:19:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:19:21.564: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 10:19:23.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:19:25.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189161, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:19:28.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:19:29.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7354" for this suite.
STEP: Destroying namespace "webhook-7354-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.307 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":303,"completed":106,"skipped":1980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:19:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 21 10:19:29.548: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:19:51.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2" for this suite.

• [SLOW TEST:22.531 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":303,"completed":107,"skipped":2034,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:19:51.988: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9745
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 21 10:19:52.158: INFO: Found 0 stateful pods, waiting for 3
May 21 10:20:02.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:02.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:02.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
May 21 10:20:12.174: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:12.174: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:12.174: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 10:20:12.257: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 21 10:20:22.352: INFO: Updating stateful set ss2
May 21 10:20:22.374: INFO: Waiting for Pod statefulset-9745/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
May 21 10:20:32.594: INFO: Found 2 stateful pods, waiting for 3
May 21 10:20:42.612: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:42.612: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:20:42.612: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 21 10:20:42.688: INFO: Updating stateful set ss2
May 21 10:20:42.702: INFO: Waiting for Pod statefulset-9745/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 10:20:52.772: INFO: Updating stateful set ss2
May 21 10:20:52.784: INFO: Waiting for StatefulSet statefulset-9745/ss2 to complete update
May 21 10:20:52.784: INFO: Waiting for Pod statefulset-9745/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:21:02.807: INFO: Deleting all statefulset in ns statefulset-9745
May 21 10:21:02.813: INFO: Scaling statefulset ss2 to 0
May 21 10:21:32.900: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:21:32.908: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:21:32.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9745" for this suite.

• [SLOW TEST:101.016 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":303,"completed":108,"skipped":2035,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:21:33.005: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 10:21:33.153: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 21 10:21:33.160: INFO: starting watch
STEP: patching
STEP: updating
May 21 10:21:33.227: INFO: waiting for watch events with expected annotations
May 21 10:21:33.227: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:21:33.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8243" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":303,"completed":109,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:21:33.455: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:21:34.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 10:21:36.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:21:38.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757189294, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:21:41.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:21:41.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-506" for this suite.
STEP: Destroying namespace "webhook-506-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.453 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":303,"completed":110,"skipped":2067,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:21:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 10:21:42.054: INFO: Waiting up to 5m0s for pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587" in namespace "emptydir-9556" to be "Succeeded or Failed"
May 21 10:21:42.058: INFO: Pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.717444ms
May 21 10:21:44.068: INFO: Pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014275154s
May 21 10:21:46.079: INFO: Pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025878207s
May 21 10:21:48.089: INFO: Pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035093807s
STEP: Saw pod success
May 21 10:21:48.089: INFO: Pod "pod-4fdf9419-4826-4c50-a052-b4b8bcf77587" satisfied condition "Succeeded or Failed"
May 21 10:21:48.095: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-4fdf9419-4826-4c50-a052-b4b8bcf77587 container test-container: <nil>
STEP: delete the pod
May 21 10:21:48.372: INFO: Waiting for pod pod-4fdf9419-4826-4c50-a052-b4b8bcf77587 to disappear
May 21 10:21:48.379: INFO: Pod pod-4fdf9419-4826-4c50-a052-b4b8bcf77587 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:21:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9556" for this suite.

• [SLOW TEST:6.495 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":111,"skipped":2074,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:21:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-96000a9f-acd6-4ae5-ac1c-869a32d5621c
STEP: Creating configMap with name cm-test-opt-upd-f7950003-8fea-4a1e-81c0-96ddaf2218ea
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-96000a9f-acd6-4ae5-ac1c-869a32d5621c
STEP: Updating configmap cm-test-opt-upd-f7950003-8fea-4a1e-81c0-96ddaf2218ea
STEP: Creating configMap with name cm-test-opt-create-3056c4cd-0bb0-4c3e-9066-febb7bfafd82
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:04.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2928" for this suite.

• [SLOW TEST:16.452 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":112,"skipped":2083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:04.863: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:05.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-686" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":303,"completed":113,"skipped":2119,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:05.062: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:09.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1474" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":303,"completed":114,"skipped":2135,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:09.250: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:22:09.408: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Pending, waiting for it to be Running (with Ready = true)
May 21 10:22:11.417: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Pending, waiting for it to be Running (with Ready = true)
May 21 10:22:13.424: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Pending, waiting for it to be Running (with Ready = true)
May 21 10:22:15.417: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:17.417: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:19.418: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:21.428: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:23.421: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:25.422: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:27.416: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:29.421: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:31.417: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = false)
May 21 10:22:33.429: INFO: The status of Pod test-webserver-c11d7221-6e26-4814-899c-45610f96587f is Running (Ready = true)
May 21 10:22:33.444: INFO: Container started at 2021-05-21 10:22:13 +0000 UTC, pod became ready at 2021-05-21 10:22:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:33.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2709" for this suite.

• [SLOW TEST:24.238 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":303,"completed":115,"skipped":2154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:33.494: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:37.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-87" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":116,"skipped":2177,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 10:22:49.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 10:22:49.970: INFO: Pod pod-with-prestop-http-hook still exists
May 21 10:22:51.971: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 10:22:51.982: INFO: Pod pod-with-prestop-http-hook still exists
May 21 10:22:53.971: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 10:22:53.981: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:54.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8326" for this suite.

• [SLOW TEST:16.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":303,"completed":117,"skipped":2191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:54.057: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:22:54.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2814" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":303,"completed":118,"skipped":2215,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:22:54.242: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
May 21 10:22:54.383: INFO: Waiting up to 5m0s for pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465" in namespace "var-expansion-2624" to be "Succeeded or Failed"
May 21 10:22:54.391: INFO: Pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465": Phase="Pending", Reason="", readiness=false. Elapsed: 8.714234ms
May 21 10:22:56.403: INFO: Pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020640307s
May 21 10:22:58.413: INFO: Pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030387545s
May 21 10:23:00.440: INFO: Pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056886478s
STEP: Saw pod success
May 21 10:23:00.440: INFO: Pod "var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465" satisfied condition "Succeeded or Failed"
May 21 10:23:00.447: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465 container dapi-container: <nil>
STEP: delete the pod
May 21 10:23:00.574: INFO: Waiting for pod var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465 to disappear
May 21 10:23:00.580: INFO: Pod var-expansion-c0ef39a9-cbea-47ab-a163-5f5e61a43465 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:23:00.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2624" for this suite.

• [SLOW TEST:6.379 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":303,"completed":119,"skipped":2219,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:23:00.621: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7413
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 21 10:23:00.789: INFO: Found 0 stateful pods, waiting for 3
May 21 10:23:10.805: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:23:10.805: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:23:10.805: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:23:10.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-7413 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:23:11.352: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:23:11.352: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:23:11.352: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 10:23:21.483: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 21 10:23:31.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-7413 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:23:31.888: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:23:31.888: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:23:31.889: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
May 21 10:23:51.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-7413 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:23:52.294: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:23:52.294: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:23:52.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:24:02.401: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 21 10:24:12.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-7413 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:24:12.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:24:12.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:24:12.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:24:32.891: INFO: Waiting for StatefulSet statefulset-7413/ss2 to complete update
May 21 10:24:32.891: INFO: Waiting for Pod statefulset-7413/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:24:42.911: INFO: Deleting all statefulset in ns statefulset-7413
May 21 10:24:42.919: INFO: Scaling statefulset ss2 to 0
May 21 10:25:03.004: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:25:03.015: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:03.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7413" for this suite.

• [SLOW TEST:122.490 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":303,"completed":120,"skipped":2233,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9703
STEP: creating service affinity-nodeport-transition in namespace services-9703
STEP: creating replication controller affinity-nodeport-transition in namespace services-9703
I0521 10:25:03.317688      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9703, replica count: 3
I0521 10:25:06.369191      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:25:09.369811      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:25:12.370591      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:25:12.403: INFO: Creating new exec pod
May 21 10:25:19.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 21 10:25:19.792: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 21 10:25:19.792: INFO: stdout: ""
May 21 10:25:19.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c nc -zv -t -w 2 10.102.179.50 80'
May 21 10:25:20.074: INFO: stderr: "+ nc -zv -t -w 2 10.102.179.50 80\nConnection to 10.102.179.50 80 port [tcp/http] succeeded!\n"
May 21 10:25:20.074: INFO: stdout: ""
May 21 10:25:20.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.36 32027'
May 21 10:25:20.336: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.36 32027\nConnection to 172.30.52.36 32027 port [tcp/32027] succeeded!\n"
May 21 10:25:20.336: INFO: stdout: ""
May 21 10:25:20.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.10 32027'
May 21 10:25:20.617: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.10 32027\nConnection to 172.30.52.10 32027 port [tcp/32027] succeeded!\n"
May 21 10:25:20.617: INFO: stdout: ""
May 21 10:25:20.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.52.36:32027/ ; done'
May 21 10:25:21.064: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n"
May 21 10:25:21.064: INFO: stdout: "\naffinity-nodeport-transition-pqhxt\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-pqhxt\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-pqhxt\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-pqhxt\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-rvplf\naffinity-nodeport-transition-pqhxt\naffinity-nodeport-transition-rvplf"
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-pqhxt
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-pqhxt
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-pqhxt
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.064: INFO: Received response from host: affinity-nodeport-transition-pqhxt
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-pqhxt
May 21 10:25:21.065: INFO: Received response from host: affinity-nodeport-transition-rvplf
May 21 10:25:21.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-9703 exec execpod-affinityvrxlc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.52.36:32027/ ; done'
May 21 10:25:21.485: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:32027/\n"
May 21 10:25:21.485: INFO: stdout: "\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4\naffinity-nodeport-transition-gl5w4"
May 21 10:25:21.485: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.485: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Received response from host: affinity-nodeport-transition-gl5w4
May 21 10:25:21.486: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9703, will wait for the garbage collector to delete the pods
May 21 10:25:21.613: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.517904ms
May 21 10:25:21.713: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.25614ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:28.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9703" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.960 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":121,"skipped":2245,"failed":0}
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:28.074: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
May 21 10:25:28.199: INFO: created test-podtemplate-1
May 21 10:25:28.212: INFO: created test-podtemplate-2
May 21 10:25:28.224: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 21 10:25:28.231: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 21 10:25:28.296: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:28.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3609" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":303,"completed":122,"skipped":2245,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:28.339: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:28.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8976" for this suite.
STEP: Destroying namespace "nspatchtest-e7a57412-c1ee-4f27-8dbb-2f228aa95804-3750" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":303,"completed":123,"skipped":2256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:28.571: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 10:25:28.707: INFO: starting watch
STEP: patching
STEP: updating
May 21 10:25:28.750: INFO: waiting for watch events with expected annotations
May 21 10:25:28.750: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:28.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8052" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":303,"completed":124,"skipped":2309,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:28.868: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:40.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3064" for this suite.

• [SLOW TEST:11.455 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":303,"completed":125,"skipped":2317,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:40.326: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-21a2ec73-db83-411e-87e3-da9f1b943730
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:40.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8833" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":303,"completed":126,"skipped":2336,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:40.446: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:25:40.552: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3deea28c-0dcc-442c-9051-a380636187b9" in namespace "security-context-test-1713" to be "Succeeded or Failed"
May 21 10:25:40.557: INFO: Pod "busybox-user-65534-3deea28c-0dcc-442c-9051-a380636187b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532654ms
May 21 10:25:42.580: INFO: Pod "busybox-user-65534-3deea28c-0dcc-442c-9051-a380636187b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027520745s
May 21 10:25:44.589: INFO: Pod "busybox-user-65534-3deea28c-0dcc-442c-9051-a380636187b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036981314s
May 21 10:25:44.589: INFO: Pod "busybox-user-65534-3deea28c-0dcc-442c-9051-a380636187b9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:44.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1713" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":127,"skipped":2346,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:44.619: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0a50a919-ab82-46fc-ae77-0fb05e810870
STEP: Creating a pod to test consume configMaps
May 21 10:25:44.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd" in namespace "configmap-6303" to be "Succeeded or Failed"
May 21 10:25:44.738: INFO: Pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73477ms
May 21 10:25:46.750: INFO: Pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017239724s
May 21 10:25:48.766: INFO: Pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032768132s
May 21 10:25:50.775: INFO: Pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041924186s
STEP: Saw pod success
May 21 10:25:50.775: INFO: Pod "pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd" satisfied condition "Succeeded or Failed"
May 21 10:25:50.780: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd container configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:25:50.858: INFO: Waiting for pod pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd to disappear
May 21 10:25:50.864: INFO: Pod pod-configmaps-9f30d629-b60b-4b68-8344-401076d49afd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:25:50.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6303" for this suite.

• [SLOW TEST:6.280 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":128,"skipped":2347,"failed":0}
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:25:50.900: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 10:25:51.084: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:51.084: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:51.084: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:51.089: INFO: Number of nodes with available pods: 0
May 21 10:25:51.089: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 10:25:52.102: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:52.103: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:52.103: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:52.109: INFO: Number of nodes with available pods: 0
May 21 10:25:52.109: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 10:25:53.112: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:53.112: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:53.112: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:53.122: INFO: Number of nodes with available pods: 0
May 21 10:25:53.122: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 10:25:54.107: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:54.107: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:54.107: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:54.120: INFO: Number of nodes with available pods: 1
May 21 10:25:54.120: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:25:55.110: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.110: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.110: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.120: INFO: Number of nodes with available pods: 2
May 21 10:25:55.120: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 21 10:25:55.190: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.191: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.191: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:55.196: INFO: Number of nodes with available pods: 1
May 21 10:25:55.196: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:25:56.211: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:56.211: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:56.211: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:56.218: INFO: Number of nodes with available pods: 1
May 21 10:25:56.218: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:25:57.218: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:57.219: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:57.219: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:57.227: INFO: Number of nodes with available pods: 1
May 21 10:25:57.227: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:25:58.213: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:58.214: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:58.214: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:58.221: INFO: Number of nodes with available pods: 1
May 21 10:25:58.222: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:25:59.217: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:59.218: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:59.218: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:25:59.224: INFO: Number of nodes with available pods: 1
May 21 10:25:59.224: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:26:00.214: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:00.214: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:00.214: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:00.220: INFO: Number of nodes with available pods: 1
May 21 10:26:00.220: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:26:01.210: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:01.210: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:01.210: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:01.219: INFO: Number of nodes with available pods: 1
May 21 10:26:01.219: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 10:26:02.218: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:02.219: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:02.223: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 10:26:02.235: INFO: Number of nodes with available pods: 2
May 21 10:26:02.235: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1829, will wait for the garbage collector to delete the pods
May 21 10:26:02.335: INFO: Deleting DaemonSet.extensions daemon-set took: 30.980302ms
May 21 10:26:02.835: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.225659ms
May 21 10:26:07.943: INFO: Number of nodes with available pods: 0
May 21 10:26:07.944: INFO: Number of running nodes: 0, number of available pods: 0
May 21 10:26:07.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1829/daemonsets","resourceVersion":"173286"},"items":null}

May 21 10:26:07.959: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1829/pods","resourceVersion":"173286"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:26:07.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1829" for this suite.

• [SLOW TEST:17.129 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":303,"completed":129,"skipped":2347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:26:08.033: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 10:26:08.214: INFO: Waiting up to 5m0s for pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81" in namespace "emptydir-149" to be "Succeeded or Failed"
May 21 10:26:08.221: INFO: Pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81": Phase="Pending", Reason="", readiness=false. Elapsed: 7.548624ms
May 21 10:26:10.235: INFO: Pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021487387s
May 21 10:26:12.244: INFO: Pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030864099s
May 21 10:26:14.257: INFO: Pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043170184s
STEP: Saw pod success
May 21 10:26:14.257: INFO: Pod "pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81" satisfied condition "Succeeded or Failed"
May 21 10:26:14.265: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81 container test-container: <nil>
STEP: delete the pod
May 21 10:26:14.328: INFO: Waiting for pod pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81 to disappear
May 21 10:26:14.334: INFO: Pod pod-d5593aef-f30b-4e58-8ad2-e6ff8968cd81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:26:14.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-149" for this suite.

• [SLOW TEST:6.329 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":130,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:26:14.366: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0521 10:26:15.590471      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 10:27:17.656: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:27:17.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6074" for this suite.

• [SLOW TEST:63.342 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":303,"completed":131,"skipped":2417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:27:17.710: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
May 21 10:29:18.415: INFO: Successfully updated pod "var-expansion-383b50f9-791d-43c3-b5ae-41f8a0dca3c7"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 21 10:29:24.442: INFO: Deleting pod "var-expansion-383b50f9-791d-43c3-b5ae-41f8a0dca3c7" in namespace "var-expansion-677"
May 21 10:29:24.465: INFO: Wait up to 5m0s for pod "var-expansion-383b50f9-791d-43c3-b5ae-41f8a0dca3c7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:29:56.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-677" for this suite.

• [SLOW TEST:158.831 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":303,"completed":132,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:29:56.542: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3ede37b7-47d0-44f1-a4c0-ecbd6d60c71e
STEP: Creating a pod to test consume configMaps
May 21 10:29:56.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54" in namespace "configmap-9607" to be "Succeeded or Failed"
May 21 10:29:56.737: INFO: Pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040432ms
May 21 10:29:58.748: INFO: Pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014591006s
May 21 10:30:00.758: INFO: Pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02472952s
May 21 10:30:02.767: INFO: Pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034007885s
STEP: Saw pod success
May 21 10:30:02.767: INFO: Pod "pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54" satisfied condition "Succeeded or Failed"
May 21 10:30:02.773: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:30:02.885: INFO: Waiting for pod pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54 to disappear
May 21 10:30:02.902: INFO: Pod pod-configmaps-a2895dca-9c21-4828-8b08-d3bcc6452a54 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:02.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9607" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":133,"skipped":2465,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:02.951: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-4377b7af-6aca-4cae-a738-134273780c9b
STEP: Creating a pod to test consume configMaps
May 21 10:30:03.135: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f" in namespace "configmap-3717" to be "Succeeded or Failed"
May 21 10:30:03.148: INFO: Pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.609112ms
May 21 10:30:05.163: INFO: Pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027424205s
May 21 10:30:07.182: INFO: Pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046454698s
May 21 10:30:09.190: INFO: Pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055179836s
STEP: Saw pod success
May 21 10:30:09.190: INFO: Pod "pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f" satisfied condition "Succeeded or Failed"
May 21 10:30:09.196: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f container configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:30:09.266: INFO: Waiting for pod pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f to disappear
May 21 10:30:09.271: INFO: Pod pod-configmaps-0d25ab7c-b85f-4aff-b0d5-b320ce2aad0f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3717" for this suite.

• [SLOW TEST:6.342 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":134,"skipped":2471,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:30:09.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097" in namespace "projected-5859" to be "Succeeded or Failed"
May 21 10:30:09.414: INFO: Pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097": Phase="Pending", Reason="", readiness=false. Elapsed: 4.643168ms
May 21 10:30:11.422: INFO: Pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0126192s
May 21 10:30:13.438: INFO: Pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028573925s
May 21 10:30:15.447: INFO: Pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038222947s
STEP: Saw pod success
May 21 10:30:15.448: INFO: Pod "downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097" satisfied condition "Succeeded or Failed"
May 21 10:30:15.456: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097 container client-container: <nil>
STEP: delete the pod
May 21 10:30:15.527: INFO: Waiting for pod downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097 to disappear
May 21 10:30:15.538: INFO: Pod downwardapi-volume-cb5d3446-9f51-41fb-ba07-dcb28ce45097 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:15.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5859" for this suite.

• [SLOW TEST:6.273 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":135,"skipped":2478,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:15.568: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 21 10:30:15.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-3636 create -f -'
May 21 10:30:15.981: INFO: stderr: ""
May 21 10:30:15.981: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 10:30:16.993: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:16.993: INFO: Found 0 / 1
May 21 10:30:17.993: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:17.993: INFO: Found 0 / 1
May 21 10:30:18.996: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:18.997: INFO: Found 0 / 1
May 21 10:30:19.989: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:19.989: INFO: Found 0 / 1
May 21 10:30:20.990: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:20.990: INFO: Found 0 / 1
May 21 10:30:21.994: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:21.994: INFO: Found 1 / 1
May 21 10:30:21.994: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 21 10:30:22.002: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:22.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 10:30:22.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-3636 patch pod agnhost-primary-qvr52 -p {"metadata":{"annotations":{"x":"y"}}}'
May 21 10:30:22.142: INFO: stderr: ""
May 21 10:30:22.142: INFO: stdout: "pod/agnhost-primary-qvr52 patched\n"
STEP: checking annotations
May 21 10:30:22.151: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:30:22.151: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:22.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3636" for this suite.

• [SLOW TEST:6.621 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":303,"completed":136,"skipped":2485,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:22.190: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:30:22.266: INFO: Creating deployment "webserver-deployment"
May 21 10:30:22.290: INFO: Waiting for observed generation 1
May 21 10:30:24.309: INFO: Waiting for all required pods to come up
May 21 10:30:24.319: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 21 10:30:40.355: INFO: Waiting for deployment "webserver-deployment" to complete
May 21 10:30:40.378: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 21 10:30:40.411: INFO: Updating deployment webserver-deployment
May 21 10:30:40.411: INFO: Waiting for observed generation 2
May 21 10:30:42.427: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 21 10:30:42.434: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 21 10:30:42.442: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 10:30:42.467: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 21 10:30:42.467: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 21 10:30:42.472: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 10:30:42.486: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 21 10:30:42.486: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 21 10:30:42.515: INFO: Updating deployment webserver-deployment
May 21 10:30:42.515: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 21 10:30:42.538: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 21 10:30:42.552: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 10:30:42.578: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5164 /apis/apps/v1/namespaces/deployment-5164/deployments/webserver-deployment 935c6f7a-55f1-44f5-9123-ed56d95c5f1f 174667 3 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d89868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 10:30:36 +0000 UTC,LastTransitionTime:2021-05-21 10:30:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-21 10:30:40 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 21 10:30:42.622: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5164 /apis/apps/v1/namespaces/deployment-5164/replicasets/webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 174671 3 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 935c6f7a-55f1-44f5-9123-ed56d95c5f1f 0xc0005583c7 0xc0005583c8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"935c6f7a-55f1-44f5-9123-ed56d95c5f1f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000558638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 10:30:42.622: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 21 10:30:42.622: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-5164 /apis/apps/v1/namespaces/deployment-5164/replicasets/webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 174668 3 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 935c6f7a-55f1-44f5-9123-ed56d95c5f1f 0xc0005588b7 0xc0005588b8}] []  [{kube-controller-manager Update apps/v1 2021-05-21 10:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"935c6f7a-55f1-44f5-9123-ed56d95c5f1f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000558a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 21 10:30:42.655: INFO: Pod "webserver-deployment-795d758f88-5827g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5827g webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-5827g 9f15f409-4548-4fe1-a985-b4de0897436e 174616 0 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379c3e7 0xc00379c3e8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:,StartTime:2021-05-21 10:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.656: INFO: Pod "webserver-deployment-795d758f88-62sst" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-62sst webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-62sst 7cb09b91-cb28-43a3-914e-0eb71a10dc4d 174646 0 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379c587 0xc00379c588}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:,StartTime:2021-05-21 10:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.656: INFO: Pod "webserver-deployment-795d758f88-9cpnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9cpnc webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-9cpnc 76485384-f605-4faa-aed1-5893c1cbd2af 174617 0 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379c727 0xc00379c728}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:,StartTime:2021-05-21 10:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.657: INFO: Pod "webserver-deployment-795d758f88-ltngv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ltngv webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-ltngv d0cd8727-f32b-44a3-a847-8b85fc96c754 174654 0 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379c8c7 0xc00379c8c8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:,StartTime:2021-05-21 10:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.658: INFO: Pod "webserver-deployment-795d758f88-m66hz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m66hz webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-m66hz f9c0fab7-214b-42eb-8460-260458c6ab41 174624 0 2021-05-21 10:30:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379ca57 0xc00379ca58}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:,StartTime:2021-05-21 10:30:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.658: INFO: Pod "webserver-deployment-795d758f88-tgnkk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tgnkk webserver-deployment-795d758f88- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-795d758f88-tgnkk 2911332c-9914-4a35-89e6-e88fcae29e97 174675 0 2021-05-21 10:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 694e79e0-b99d-4704-b4c9-7fc2dab2c062 0xc00379cbe7 0xc00379cbe8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"694e79e0-b99d-4704-b4c9-7fc2dab2c062\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.659: INFO: Pod "webserver-deployment-867f44f6fb-28mzz" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-28mzz webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-28mzz 2cd0c605-d865-47c0-be0b-fe547bf9e3d5 174673 0 2021-05-21 10:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379cd60 0xc00379cd61}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.660: INFO: Pod "webserver-deployment-867f44f6fb-5fvcz" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-5fvcz webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-5fvcz c83070c8-9d13-415d-8ab0-d006baa37536 174672 0 2021-05-21 10:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379cf50 0xc00379cf51}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.661: INFO: Pod "webserver-deployment-867f44f6fb-9f8l7" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-9f8l7 webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-9f8l7 230806b9-c28d-4082-827d-534f6eed79d9 174547 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379d070 0xc00379d071}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.6,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://281ff61985610b64b80e836a74557468dfa930eaa793c0254abdcc32f0bc0230,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.661: INFO: Pod "webserver-deployment-867f44f6fb-d8hqp" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-d8hqp webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-d8hqp be051fdd-d1b3-4e37-a921-addbdabb4427 174595 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379d207 0xc00379d208}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.128.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:172.28.128.8,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d829cb608cbbbd3f8b067ad47d04e0939f9b3d3be6b0f4b38108138b04be2942,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.128.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.662: INFO: Pod "webserver-deployment-867f44f6fb-kxlz5" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-kxlz5 webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-kxlz5 df7fb2ed-9949-4f6f-8a07-6f4e854bc00a 174489 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379d417 0xc00379d418}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.4,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://aac854c7593ed70abf44a87f4fb89a8bd9fc74fe88bd72fad77e6a07951d6ccc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.662: INFO: Pod "webserver-deployment-867f44f6fb-lfch5" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-lfch5 webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-lfch5 ba5242d6-dade-4226-8550-e0c3c4fbf374 174569 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379d7c7 0xc00379d7c8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.128.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:172.28.128.7,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c3f3f3492eb3b893c4cff256d40a04c8cbe9cd706d9a951ab007aa910b572647,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.128.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.663: INFO: Pod "webserver-deployment-867f44f6fb-nd8kp" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-nd8kp webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-nd8kp 99228671-9d2d-4ffb-a822-42979ea5d12a 174525 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc00379daa7 0xc00379daa8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.128.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:172.28.128.5,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b2cbc1a41e92ef42bc3c3d64d664786880bba7b7648dbe4885c897d832ebe33f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.128.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.664: INFO: Pod "webserver-deployment-867f44f6fb-r996v" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-r996v webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-r996v f78bc6df-6d47-43fd-b8d0-b116f64357b2 174486 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc0001381f7 0xc0001381f8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.128.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:172.28.128.4,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://dc50372925cac4fff49533f5a0dec8bfecc45e344607c6163f3950e971011710,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.128.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.664: INFO: Pod "webserver-deployment-867f44f6fb-s9qzd" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-s9qzd webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-s9qzd 8639b31b-8dae-4e36-928d-50689010ab6e 174528 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc000138867 0xc000138868}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.5,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://44bfe950308ca4f029016abf5723eedbf31a887d6a326afe19b9e674c1dccf1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.665: INFO: Pod "webserver-deployment-867f44f6fb-sdfrg" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-sdfrg webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-sdfrg e163cbf8-99e0-497d-8090-5bde82f44903 174544 0 2021-05-21 10:30:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc0033da0c7 0xc0033da0c8}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 10:30:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.128.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-maou9az9wksf7qjxozd15h7j6r,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.36,PodIP:172.28.128.6,StartTime:2021-05-21 10:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 10:30:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1e11c1e19520324e0d9889e362cc8183e6d18ee7eff3b58be90c11cf5d0416ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.128.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:30:42.665: INFO: Pod "webserver-deployment-867f44f6fb-t2jg5" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-t2jg5 webserver-deployment-867f44f6fb- deployment-5164 /api/v1/namespaces/deployment-5164/pods/webserver-deployment-867f44f6fb-t2jg5 4e242863-48f2-4585-b1c3-01615d033442 174674 0 2021-05-21 10:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 222f0fe0-ecd6-4ace-b811-962a6e954cb9 0xc0033da257 0xc0033da258}] []  [{kube-controller-manager Update v1 2021-05-21 10:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222f0fe0-ecd6-4ace-b811-962a6e954cb9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szctj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szctj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szctj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 10:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:42.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5164" for this suite.

• [SLOW TEST:20.621 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":303,"completed":137,"skipped":2497,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:42.813: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
May 21 10:30:43.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5174 cluster-info'
May 21 10:30:43.395: INFO: stderr: ""
May 21 10:30:43.395: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:30:43.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5174" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":303,"completed":138,"skipped":2505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:30:43.443: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:31:09.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8008" for this suite.

• [SLOW TEST:26.314 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":139,"skipped":2573,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:31:09.766: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 10:31:09.930: INFO: Waiting up to 5m0s for pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12" in namespace "emptydir-7263" to be "Succeeded or Failed"
May 21 10:31:09.971: INFO: Pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12": Phase="Pending", Reason="", readiness=false. Elapsed: 41.244089ms
May 21 10:31:11.984: INFO: Pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05419817s
May 21 10:31:13.993: INFO: Pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062859137s
May 21 10:31:16.008: INFO: Pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078463069s
STEP: Saw pod success
May 21 10:31:16.008: INFO: Pod "pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12" satisfied condition "Succeeded or Failed"
May 21 10:31:16.019: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12 container test-container: <nil>
STEP: delete the pod
May 21 10:31:16.110: INFO: Waiting for pod pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12 to disappear
May 21 10:31:16.116: INFO: Pod pod-cc186d6f-e8df-4b62-a0a7-8233b25a1a12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:31:16.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7263" for this suite.

• [SLOW TEST:6.374 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":140,"skipped":2592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:31:16.144: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:31:16.250: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473" in namespace "downward-api-6537" to be "Succeeded or Failed"
May 21 10:31:16.255: INFO: Pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750353ms
May 21 10:31:18.265: INFO: Pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014697458s
May 21 10:31:20.274: INFO: Pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023924953s
May 21 10:31:22.285: INFO: Pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035381427s
STEP: Saw pod success
May 21 10:31:22.285: INFO: Pod "downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473" satisfied condition "Succeeded or Failed"
May 21 10:31:22.295: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473 container client-container: <nil>
STEP: delete the pod
May 21 10:31:22.378: INFO: Waiting for pod downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473 to disappear
May 21 10:31:22.385: INFO: Pod downwardapi-volume-05715b0b-9f18-4240-b100-8afb16140473 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:31:22.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6537" for this suite.

• [SLOW TEST:6.276 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":141,"skipped":2636,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:31:22.422: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-08aa18e6-e996-480c-bc18-8f33b5d98e94
STEP: Creating a pod to test consume secrets
May 21 10:31:22.570: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc" in namespace "projected-534" to be "Succeeded or Failed"
May 21 10:31:22.585: INFO: Pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.619118ms
May 21 10:31:24.597: INFO: Pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026271582s
May 21 10:31:26.622: INFO: Pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051764222s
May 21 10:31:28.632: INFO: Pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062206719s
STEP: Saw pod success
May 21 10:31:28.633: INFO: Pod "pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc" satisfied condition "Succeeded or Failed"
May 21 10:31:28.640: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 10:31:28.725: INFO: Waiting for pod pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc to disappear
May 21 10:31:28.734: INFO: Pod pod-projected-secrets-74910867-537f-4723-b5c2-e964ec56c5dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:31:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-534" for this suite.

• [SLOW TEST:6.341 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":142,"skipped":2644,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:31:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 10:31:28.903: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 10:32:29.018: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 21 10:32:29.116: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 10:32:29.178: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:32:57.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3636" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:88.702 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":303,"completed":143,"skipped":2660,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:32:57.467: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 21 10:32:57.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 create -f -'
May 21 10:32:57.898: INFO: stderr: ""
May 21 10:32:57.898: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 10:32:57.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:32:58.010: INFO: stderr: ""
May 21 10:32:58.010: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-qprj2 "
May 21 10:32:58.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:32:58.114: INFO: stderr: ""
May 21 10:32:58.114: INFO: stdout: ""
May 21 10:32:58.114: INFO: update-demo-nautilus-cjr9l is created but not running
May 21 10:33:03.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:03.253: INFO: stderr: ""
May 21 10:33:03.254: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-qprj2 "
May 21 10:33:03.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:03.356: INFO: stderr: ""
May 21 10:33:03.356: INFO: stdout: "true"
May 21 10:33:03.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:03.454: INFO: stderr: ""
May 21 10:33:03.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:03.454: INFO: validating pod update-demo-nautilus-cjr9l
May 21 10:33:03.468: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:03.468: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:03.468: INFO: update-demo-nautilus-cjr9l is verified up and running
May 21 10:33:03.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-qprj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:03.593: INFO: stderr: ""
May 21 10:33:03.593: INFO: stdout: "true"
May 21 10:33:03.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-qprj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:03.704: INFO: stderr: ""
May 21 10:33:03.704: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:03.704: INFO: validating pod update-demo-nautilus-qprj2
May 21 10:33:03.717: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:03.717: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:03.717: INFO: update-demo-nautilus-qprj2 is verified up and running
STEP: scaling down the replication controller
May 21 10:33:03.722: INFO: scanned /root for discovery docs: <nil>
May 21 10:33:03.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 21 10:33:04.872: INFO: stderr: ""
May 21 10:33:04.872: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 10:33:04.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:04.983: INFO: stderr: ""
May 21 10:33:04.983: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-qprj2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 10:33:09.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:10.182: INFO: stderr: ""
May 21 10:33:10.182: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-qprj2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 10:33:15.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:15.499: INFO: stderr: ""
May 21 10:33:15.499: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-qprj2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 10:33:20.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:20.629: INFO: stderr: ""
May 21 10:33:20.629: INFO: stdout: "update-demo-nautilus-cjr9l "
May 21 10:33:20.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:20.751: INFO: stderr: ""
May 21 10:33:20.751: INFO: stdout: "true"
May 21 10:33:20.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:20.855: INFO: stderr: ""
May 21 10:33:20.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:20.855: INFO: validating pod update-demo-nautilus-cjr9l
May 21 10:33:20.868: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:20.868: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:20.868: INFO: update-demo-nautilus-cjr9l is verified up and running
STEP: scaling up the replication controller
May 21 10:33:20.873: INFO: scanned /root for discovery docs: <nil>
May 21 10:33:20.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 21 10:33:22.074: INFO: stderr: ""
May 21 10:33:22.074: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 10:33:22.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:22.185: INFO: stderr: ""
May 21 10:33:22.185: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-m72c4 "
May 21 10:33:22.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:22.289: INFO: stderr: ""
May 21 10:33:22.289: INFO: stdout: "true"
May 21 10:33:22.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:22.404: INFO: stderr: ""
May 21 10:33:22.404: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:22.404: INFO: validating pod update-demo-nautilus-cjr9l
May 21 10:33:22.413: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:22.413: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:22.413: INFO: update-demo-nautilus-cjr9l is verified up and running
May 21 10:33:22.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-m72c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:22.513: INFO: stderr: ""
May 21 10:33:22.513: INFO: stdout: ""
May 21 10:33:22.513: INFO: update-demo-nautilus-m72c4 is created but not running
May 21 10:33:27.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 10:33:27.636: INFO: stderr: ""
May 21 10:33:27.636: INFO: stdout: "update-demo-nautilus-cjr9l update-demo-nautilus-m72c4 "
May 21 10:33:27.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:27.740: INFO: stderr: ""
May 21 10:33:27.741: INFO: stdout: "true"
May 21 10:33:27.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-cjr9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:27.855: INFO: stderr: ""
May 21 10:33:27.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:27.855: INFO: validating pod update-demo-nautilus-cjr9l
May 21 10:33:27.865: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:27.865: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:27.865: INFO: update-demo-nautilus-cjr9l is verified up and running
May 21 10:33:27.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-m72c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 10:33:27.976: INFO: stderr: ""
May 21 10:33:27.976: INFO: stdout: "true"
May 21 10:33:27.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods update-demo-nautilus-m72c4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 10:33:28.072: INFO: stderr: ""
May 21 10:33:28.072: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 10:33:28.072: INFO: validating pod update-demo-nautilus-m72c4
May 21 10:33:28.085: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 10:33:28.085: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 10:33:28.085: INFO: update-demo-nautilus-m72c4 is verified up and running
STEP: using delete to clean up resources
May 21 10:33:28.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 delete --grace-period=0 --force -f -'
May 21 10:33:28.222: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 10:33:28.222: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 10:33:28.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get rc,svc -l name=update-demo --no-headers'
May 21 10:33:28.342: INFO: stderr: "No resources found in kubectl-7312 namespace.\n"
May 21 10:33:28.342: INFO: stdout: ""
May 21 10:33:28.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 10:33:28.455: INFO: stderr: ""
May 21 10:33:28.455: INFO: stdout: "update-demo-nautilus-cjr9l\nupdate-demo-nautilus-m72c4\n"
May 21 10:33:28.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get rc,svc -l name=update-demo --no-headers'
May 21 10:33:29.090: INFO: stderr: "No resources found in kubectl-7312 namespace.\n"
May 21 10:33:29.090: INFO: stdout: ""
May 21 10:33:29.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7312 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 10:33:29.215: INFO: stderr: ""
May 21 10:33:29.215: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:33:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7312" for this suite.

• [SLOW TEST:31.817 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":303,"completed":144,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:33:29.286: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0521 10:33:39.669677      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 10:34:41.741: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 21 10:34:41.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-24zsz" in namespace "gc-4050"
May 21 10:34:41.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jxzh" in namespace "gc-4050"
May 21 10:34:41.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z966" in namespace "gc-4050"
May 21 10:34:41.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z4vh" in namespace "gc-4050"
May 21 10:34:41.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ts55" in namespace "gc-4050"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:34:42.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4050" for this suite.

• [SLOW TEST:72.793 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":303,"completed":145,"skipped":2681,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:34:42.085: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:34:42.185: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 10:34:46.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 create -f -'
May 21 10:34:46.667: INFO: stderr: ""
May 21 10:34:46.667: INFO: stdout: "e2e-test-crd-publish-openapi-5130-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 10:34:46.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 delete e2e-test-crd-publish-openapi-5130-crds test-cr'
May 21 10:34:46.915: INFO: stderr: ""
May 21 10:34:46.915: INFO: stdout: "e2e-test-crd-publish-openapi-5130-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 21 10:34:46.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 apply -f -'
May 21 10:34:47.167: INFO: stderr: ""
May 21 10:34:47.167: INFO: stdout: "e2e-test-crd-publish-openapi-5130-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 10:34:47.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 delete e2e-test-crd-publish-openapi-5130-crds test-cr'
May 21 10:34:47.285: INFO: stderr: ""
May 21 10:34:47.285: INFO: stdout: "e2e-test-crd-publish-openapi-5130-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 10:34:47.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-1596 explain e2e-test-crd-publish-openapi-5130-crds'
May 21 10:34:47.611: INFO: stderr: ""
May 21 10:34:47.611: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5130-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:34:51.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1596" for this suite.

• [SLOW TEST:9.464 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":303,"completed":146,"skipped":2685,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:34:51.550: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 10:34:52.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 10:34:54.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:34:56.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757190092, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 10:34:59.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:34:59.610: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9632-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:00.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5900" for this suite.
STEP: Destroying namespace "webhook-5900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.515 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":303,"completed":147,"skipped":2690,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:01.071: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 10:35:05.245: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1970" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":148,"skipped":2707,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 21 10:35:05.500: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:05.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4713" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":303,"completed":149,"skipped":2721,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:05.595: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-af5e923e-3753-4b9d-907c-2412ab55c241
STEP: Creating a pod to test consume configMaps
May 21 10:35:05.755: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a" in namespace "projected-961" to be "Succeeded or Failed"
May 21 10:35:05.762: INFO: Pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913088ms
May 21 10:35:07.777: INFO: Pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022394776s
May 21 10:35:09.796: INFO: Pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041221223s
May 21 10:35:11.812: INFO: Pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056901709s
STEP: Saw pod success
May 21 10:35:11.812: INFO: Pod "pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a" satisfied condition "Succeeded or Failed"
May 21 10:35:11.825: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:35:11.930: INFO: Waiting for pod pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a to disappear
May 21 10:35:11.937: INFO: Pod pod-projected-configmaps-e8fff547-ba88-4de7-bc48-9bf9b7b6300a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:11.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-961" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":150,"skipped":2734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:11.969: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7727.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7727.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7727.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7727.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7727.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7727.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 10:35:26.203: INFO: DNS probes using dns-7727/dns-test-a4632681-8acd-4b13-9a6f-a1a8021ae2d0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:26.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7727" for this suite.

• [SLOW TEST:14.335 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":303,"completed":151,"skipped":2757,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:26.305: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6742
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6742
STEP: Creating statefulset with conflicting port in namespace statefulset-6742
STEP: Waiting until pod test-pod will start running in namespace statefulset-6742
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6742
May 21 10:35:30.507: INFO: Observed stateful pod in namespace: statefulset-6742, name: ss-0, uid: 82d3109a-c866-4b5c-aed1-6be74bd1f1fb, status phase: Pending. Waiting for statefulset controller to delete.
May 21 10:35:30.864: INFO: Observed stateful pod in namespace: statefulset-6742, name: ss-0, uid: 82d3109a-c866-4b5c-aed1-6be74bd1f1fb, status phase: Failed. Waiting for statefulset controller to delete.
May 21 10:35:30.898: INFO: Observed stateful pod in namespace: statefulset-6742, name: ss-0, uid: 82d3109a-c866-4b5c-aed1-6be74bd1f1fb, status phase: Failed. Waiting for statefulset controller to delete.
May 21 10:35:30.926: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6742
STEP: Removing pod with conflicting port in namespace statefulset-6742
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6742 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:35:35.045: INFO: Deleting all statefulset in ns statefulset-6742
May 21 10:35:35.051: INFO: Scaling statefulset ss to 0
May 21 10:35:55.091: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:35:55.100: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:35:55.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6742" for this suite.

• [SLOW TEST:28.868 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":303,"completed":152,"skipped":2771,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:35:55.173: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0521 10:35:56.458692      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 10:36:58.548: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:36:58.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3215" for this suite.

• [SLOW TEST:63.426 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":303,"completed":153,"skipped":2787,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:36:58.602: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:36:58.703: INFO: Creating ReplicaSet my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9
May 21 10:36:58.737: INFO: Pod name my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9: Found 0 pods out of 1
May 21 10:37:03.745: INFO: Pod name my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9: Found 1 pods out of 1
May 21 10:37:03.746: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9" is running
May 21 10:37:05.763: INFO: Pod "my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9-8flzx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 10:36:58 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 10:36:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 10:36:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 10:36:58 +0000 UTC Reason: Message:}])
May 21 10:37:05.764: INFO: Trying to dial the pod
May 21 10:37:10.812: INFO: Controller my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9: Got expected result from replica 1 [my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9-8flzx]: "my-hostname-basic-a82de293-274b-4597-8ce2-f43e7d96cac9-8flzx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:37:10.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8015" for this suite.

• [SLOW TEST:12.266 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":154,"skipped":2795,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:37:10.868: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-20a2b51d-a43b-428f-89b2-5eba437e8e01
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-20a2b51d-a43b-428f-89b2-5eba437e8e01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:37:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5823" for this suite.

• [SLOW TEST:10.382 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":155,"skipped":2802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:37:21.252: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 10:37:21.342: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 10:37:21.367: INFO: Waiting for terminating namespaces to be deleted...
May 21 10:37:21.374: INFO: 
Logging pods the apiserver thinks is on node p1-maou9az9wksf7qjxozd15h7j6r before test
May 21 10:37:21.397: INFO: coredns-f9fd979d6-jnqgf from kube-system started at 2021-05-20 22:00:08 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.397: INFO: 	Container coredns ready: true, restart count 0
May 21 10:37:21.397: INFO: csi-ridge-node-76q9w from kube-system started at 2021-05-21 10:15:47 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 10:37:21.398: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 10:37:21.398: INFO: fluent-bit-w7b5s from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 10:37:21.398: INFO: kube-proxy-n6n5f from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 10:37:21.398: INFO: meta-bsns4 from kube-system started at 2021-05-21 10:13:03 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container meta ready: true, restart count 0
May 21 10:37:21.398: INFO: weave-net-jq22f from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container weave ready: true, restart count 1
May 21 10:37:21.398: INFO: 	Container weave-npc ready: true, restart count 0
May 21 10:37:21.398: INFO: sonobuoy-e2e-job-d98dbb7be59843f5 from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container e2e ready: true, restart count 0
May 21 10:37:21.398: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 10:37:21.398: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.398: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 10:37:21.398: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 10:37:21.398: INFO: 
Logging pods the apiserver thinks is on node p1-zq5sznp3cxrb94t9rscobyn6ny before test
May 21 10:37:21.425: INFO: csi-ridge-node-8gs7s from kube-system started at 2021-05-21 10:15:07 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 10:37:21.425: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 10:37:21.425: INFO: fluent-bit-v27pz from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 10:37:21.425: INFO: kube-proxy-bf9ms from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 10:37:21.425: INFO: meta-jwhdl from kube-system started at 2021-05-21 10:12:26 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container meta ready: true, restart count 0
May 21 10:37:21.425: INFO: weave-net-4ql9j from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container weave ready: true, restart count 1
May 21 10:37:21.425: INFO: 	Container weave-npc ready: true, restart count 0
May 21 10:37:21.425: INFO: pod-projected-configmaps-92b9df08-1a93-4f68-b8f2-80a1da3ca5a7 from projected-5823 started at 2021-05-21 10:37:11 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container projected-configmap-volume-test ready: true, restart count 0
May 21 10:37:21.425: INFO: sonobuoy from sonobuoy started at 2021-05-21 09:45:50 +0000 UTC (1 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 10:37:21.425: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:37:21.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 10:37:21.425: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cefe450e-de6e-4d91-8832-15e5edcf1a9c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-cefe450e-de6e-4d91-8832-15e5edcf1a9c off the node p1-zq5sznp3cxrb94t9rscobyn6ny
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cefe450e-de6e-4d91-8832-15e5edcf1a9c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:42:33.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2226" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:312.539 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":303,"completed":156,"skipped":2828,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:42:33.797: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:42:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1908" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":303,"completed":157,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:42:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 10:42:34.064: INFO: Waiting up to 5m0s for pod "pod-eaeb2327-b000-4019-9f4d-e3a18246e22a" in namespace "emptydir-1835" to be "Succeeded or Failed"
May 21 10:42:34.093: INFO: Pod "pod-eaeb2327-b000-4019-9f4d-e3a18246e22a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.497467ms
May 21 10:42:36.103: INFO: Pod "pod-eaeb2327-b000-4019-9f4d-e3a18246e22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038984147s
May 21 10:42:38.117: INFO: Pod "pod-eaeb2327-b000-4019-9f4d-e3a18246e22a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053305131s
STEP: Saw pod success
May 21 10:42:38.118: INFO: Pod "pod-eaeb2327-b000-4019-9f4d-e3a18246e22a" satisfied condition "Succeeded or Failed"
May 21 10:42:38.124: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-eaeb2327-b000-4019-9f4d-e3a18246e22a container test-container: <nil>
STEP: delete the pod
May 21 10:42:38.224: INFO: Waiting for pod pod-eaeb2327-b000-4019-9f4d-e3a18246e22a to disappear
May 21 10:42:38.240: INFO: Pod pod-eaeb2327-b000-4019-9f4d-e3a18246e22a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:42:38.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1835" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":158,"skipped":2851,"failed":0}
SSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:42:38.278: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:42:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9999" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":303,"completed":159,"skipped":2854,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:42:38.561: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
May 21 10:42:38.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-35 api-versions'
May 21 10:42:38.774: INFO: stderr: ""
May 21 10:42:38.774: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:42:38.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-35" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":303,"completed":160,"skipped":2854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:42:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:42:38.897: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Creating first CR 
May 21 10:42:39.551: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:42:39Z]] name:name1 resourceVersion:178395 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2fcbed9c-6f3d-46d5-8ee4-1b1fe94de47f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 21 10:42:49.580: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:42:49Z]] name:name2 resourceVersion:178454 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:86d40e4c-0149-480a-a165-0a82fb70fd73] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 21 10:42:59.603: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:42:59Z]] name:name1 resourceVersion:178488 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2fcbed9c-6f3d-46d5-8ee4-1b1fe94de47f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 21 10:43:09.635: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:43:09Z]] name:name2 resourceVersion:178523 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:86d40e4c-0149-480a-a165-0a82fb70fd73] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 21 10:43:19.684: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:42:59Z]] name:name1 resourceVersion:178556 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2fcbed9c-6f3d-46d5-8ee4-1b1fe94de47f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 21 10:43:29.719: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T10:42:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T10:43:09Z]] name:name2 resourceVersion:178590 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:86d40e4c-0149-480a-a165-0a82fb70fd73] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:43:40.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9854" for this suite.

• [SLOW TEST:61.515 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":303,"completed":161,"skipped":2878,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:43:40.323: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:43:46.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4650" for this suite.

• [SLOW TEST:6.212 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":303,"completed":162,"skipped":2883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:43:46.547: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 21 10:43:53.771: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:43:53.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3221" for this suite.

• [SLOW TEST:7.404 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":303,"completed":163,"skipped":2923,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:43:53.952: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-74a8e6c1-d06d-45f1-9509-2e6b7c47f5fd
STEP: Creating a pod to test consume configMaps
May 21 10:43:54.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1" in namespace "projected-6557" to be "Succeeded or Failed"
May 21 10:43:54.079: INFO: Pod "pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.776612ms
May 21 10:43:56.099: INFO: Pod "pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025362556s
May 21 10:43:58.108: INFO: Pod "pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034210197s
STEP: Saw pod success
May 21 10:43:58.108: INFO: Pod "pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1" satisfied condition "Succeeded or Failed"
May 21 10:43:58.115: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 10:43:58.177: INFO: Waiting for pod pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1 to disappear
May 21 10:43:58.182: INFO: Pod pod-projected-configmaps-374abadd-cc02-475e-8692-4f80e2129cb1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:43:58.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6557" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":164,"skipped":2931,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:43:58.233: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 10:43:58.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-2678 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 10:43:58.468: INFO: stderr: ""
May 21 10:43:58.468: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 21 10:44:08.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-2678 get pod e2e-test-httpd-pod -o json'
May 21 10:44:08.664: INFO: stderr: ""
May 21 10:44:08.664: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-05-21T10:43:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T10:43:58Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.28.96.4\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T10:44:03Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2678\",\n        \"resourceVersion\": \"178822\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2678/pods/e2e-test-httpd-pod\",\n        \"uid\": \"0dc4491c-237f-4f35-8e7d-395c534ddd59\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kxn7d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"p1-zq5sznp3cxrb94t9rscobyn6ny\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kxn7d\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kxn7d\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T10:43:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T10:44:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T10:44:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T10:43:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://26fe70b403cdb19af3259bb4f966989aadb6162c8799817f5d56d9ac71b733d5\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-21T10:44:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.30.52.10\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.28.96.4\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.28.96.4\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-21T10:43:58Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 21 10:44:08.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-2678 replace -f -'
May 21 10:44:09.004: INFO: stderr: ""
May 21 10:44:09.004: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
May 21 10:44:09.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-2678 delete pods e2e-test-httpd-pod'
May 21 10:44:14.154: INFO: stderr: ""
May 21 10:44:14.154: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:44:14.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2678" for this suite.

• [SLOW TEST:15.985 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":303,"completed":165,"skipped":2933,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:44:14.219: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
May 21 10:44:14.328: INFO: Waiting up to 5m0s for pod "client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064" in namespace "containers-4247" to be "Succeeded or Failed"
May 21 10:44:14.334: INFO: Pod "client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064": Phase="Pending", Reason="", readiness=false. Elapsed: 5.646691ms
May 21 10:44:16.343: INFO: Pod "client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01455117s
May 21 10:44:18.355: INFO: Pod "client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0263255s
STEP: Saw pod success
May 21 10:44:18.355: INFO: Pod "client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064" satisfied condition "Succeeded or Failed"
May 21 10:44:18.374: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064 container test-container: <nil>
STEP: delete the pod
May 21 10:44:18.451: INFO: Waiting for pod client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064 to disappear
May 21 10:44:18.458: INFO: Pod client-containers-db4e771f-a6fe-4840-89d1-fd78851f5064 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:44:18.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4247" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":303,"completed":166,"skipped":2942,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:44:18.492: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8424
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 10:44:18.624: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 10:44:18.683: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:44:20.700: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:44:22.699: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:24.691: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:26.703: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:28.692: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:30.692: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:32.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:44:34.702: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 10:44:34.717: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 10:44:38.874: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.128.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8424 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:44:38.874: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:44:40.043: INFO: Found all expected endpoints: [netserver-0]
May 21 10:44:40.052: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.96.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8424 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:44:40.052: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:44:41.201: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:44:41.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8424" for this suite.

• [SLOW TEST:22.754 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":167,"skipped":2950,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:44:41.246: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 10:44:41.378: INFO: Waiting up to 5m0s for pod "pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b" in namespace "emptydir-5041" to be "Succeeded or Failed"
May 21 10:44:41.392: INFO: Pod "pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.15674ms
May 21 10:44:43.401: INFO: Pod "pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023392249s
May 21 10:44:45.408: INFO: Pod "pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030483392s
STEP: Saw pod success
May 21 10:44:45.408: INFO: Pod "pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b" satisfied condition "Succeeded or Failed"
May 21 10:44:45.414: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b container test-container: <nil>
STEP: delete the pod
May 21 10:44:45.493: INFO: Waiting for pod pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b to disappear
May 21 10:44:45.499: INFO: Pod pod-3bbe8a60-76d3-424a-8659-0b1bcb7a989b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:44:45.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5041" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":168,"skipped":2950,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:44:45.543: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 21 10:44:45.641: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:44:49.531: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:04.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9989" for this suite.

• [SLOW TEST:19.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":303,"completed":169,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:04.980: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 10:45:05.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6460 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 10:45:05.508: INFO: stderr: ""
May 21 10:45:05.509: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 21 10:45:05.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6460 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "mirror.gcr.io/library/busybox:1.29"}]}} --dry-run=server'
May 21 10:45:05.888: INFO: stderr: ""
May 21 10:45:05.888: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 10:45:05.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6460 delete pods e2e-test-httpd-pod'
May 21 10:45:08.721: INFO: stderr: ""
May 21 10:45:08.721: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:08.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6460" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":303,"completed":170,"skipped":2982,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b7a49af5-f97b-4627-94e9-b00f281ca177
STEP: Creating a pod to test consume secrets
May 21 10:45:08.973: INFO: Waiting up to 5m0s for pod "pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05" in namespace "secrets-5869" to be "Succeeded or Failed"
May 21 10:45:08.982: INFO: Pod "pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05": Phase="Pending", Reason="", readiness=false. Elapsed: 9.190249ms
May 21 10:45:10.995: INFO: Pod "pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02185286s
May 21 10:45:13.009: INFO: Pod "pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035834629s
STEP: Saw pod success
May 21 10:45:13.009: INFO: Pod "pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05" satisfied condition "Succeeded or Failed"
May 21 10:45:13.019: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05 container secret-volume-test: <nil>
STEP: delete the pod
May 21 10:45:13.101: INFO: Waiting for pod pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05 to disappear
May 21 10:45:13.109: INFO: Pod pod-secrets-dee1817a-18fe-4261-9345-dc8f84e04e05 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:13.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5869" for this suite.
STEP: Destroying namespace "secret-namespace-687" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":303,"completed":171,"skipped":2994,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:45:13.263: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9776" for this suite.

• [SLOW TEST:7.726 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":303,"completed":172,"skipped":3005,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 21 10:45:21.189: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:45:25.534: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:40.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6776" for this suite.

• [SLOW TEST:20.130 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":303,"completed":173,"skipped":3010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:41.023: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
May 21 10:45:41.138: INFO: Waiting up to 5m0s for pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668" in namespace "var-expansion-7887" to be "Succeeded or Failed"
May 21 10:45:41.150: INFO: Pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668": Phase="Pending", Reason="", readiness=false. Elapsed: 12.442488ms
May 21 10:45:43.158: INFO: Pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020283706s
May 21 10:45:45.173: INFO: Pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035854903s
May 21 10:45:47.183: INFO: Pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045045759s
STEP: Saw pod success
May 21 10:45:47.183: INFO: Pod "var-expansion-058340c3-b5b8-45b6-8825-05229366a668" satisfied condition "Succeeded or Failed"
May 21 10:45:47.190: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod var-expansion-058340c3-b5b8-45b6-8825-05229366a668 container dapi-container: <nil>
STEP: delete the pod
May 21 10:45:47.272: INFO: Waiting for pod var-expansion-058340c3-b5b8-45b6-8825-05229366a668 to disappear
May 21 10:45:47.287: INFO: Pod var-expansion-058340c3-b5b8-45b6-8825-05229366a668 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:45:47.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7887" for this suite.

• [SLOW TEST:6.292 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":303,"completed":174,"skipped":3042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:45:47.324: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 10:45:47.438: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 10:46:47.517: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:46:47.523: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 21 10:46:51.760: INFO: found a healthy node: p1-zq5sznp3cxrb94t9rscobyn6ny
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:47:03.995: INFO: pods created so far: [1 1 1]
May 21 10:47:03.996: INFO: length of pods created so far: 3
May 21 10:47:18.054: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9345" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5827" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:98.070 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":303,"completed":175,"skipped":3082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:25.403: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6030" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":176,"skipped":3130,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:25.709: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:47:25.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29" in namespace "projected-3822" to be "Succeeded or Failed"
May 21 10:47:25.851: INFO: Pod "downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29": Phase="Pending", Reason="", readiness=false. Elapsed: 29.463586ms
May 21 10:47:27.860: INFO: Pod "downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037905414s
May 21 10:47:29.878: INFO: Pod "downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056688478s
STEP: Saw pod success
May 21 10:47:29.878: INFO: Pod "downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29" satisfied condition "Succeeded or Failed"
May 21 10:47:29.884: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29 container client-container: <nil>
STEP: delete the pod
May 21 10:47:29.944: INFO: Waiting for pod downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29 to disappear
May 21 10:47:29.951: INFO: Pod downwardapi-volume-4e8fe1a6-6993-453d-b5ab-fd1ba66eeb29 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:29.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3822" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":177,"skipped":3135,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:29.994: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:47:30.111: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d" in namespace "projected-8269" to be "Succeeded or Failed"
May 21 10:47:30.152: INFO: Pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 41.041908ms
May 21 10:47:32.167: INFO: Pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05618872s
May 21 10:47:34.176: INFO: Pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064789759s
May 21 10:47:36.191: INFO: Pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07948482s
STEP: Saw pod success
May 21 10:47:36.191: INFO: Pod "downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d" satisfied condition "Succeeded or Failed"
May 21 10:47:36.197: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d container client-container: <nil>
STEP: delete the pod
May 21 10:47:36.269: INFO: Waiting for pod downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d to disappear
May 21 10:47:36.275: INFO: Pod downwardapi-volume-6bdb94fb-1ab3-4927-b609-b2b26e61bc0d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8269" for this suite.

• [SLOW TEST:6.325 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":178,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:36.324: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:47:36.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528" in namespace "downward-api-5678" to be "Succeeded or Failed"
May 21 10:47:36.458: INFO: Pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528": Phase="Pending", Reason="", readiness=false. Elapsed: 10.186106ms
May 21 10:47:38.551: INFO: Pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102722207s
May 21 10:47:40.558: INFO: Pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109957136s
May 21 10:47:42.574: INFO: Pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.126135016s
STEP: Saw pod success
May 21 10:47:42.574: INFO: Pod "downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528" satisfied condition "Succeeded or Failed"
May 21 10:47:42.581: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528 container client-container: <nil>
STEP: delete the pod
May 21 10:47:42.674: INFO: Waiting for pod downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528 to disappear
May 21 10:47:42.680: INFO: Pod downwardapi-volume-5ba03677-ca5e-4611-b937-0bb18fff1528 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:42.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5678" for this suite.

• [SLOW TEST:6.382 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":179,"skipped":3180,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 21 10:47:42.834: INFO: Pod name pod-release: Found 0 pods out of 1
May 21 10:47:47.842: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:47:48.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-785" for this suite.

• [SLOW TEST:6.218 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":303,"completed":180,"skipped":3188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:47:48.928: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-277
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-277
STEP: creating replication controller externalsvc in namespace services-277
I0521 10:47:49.224664      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-277, replica count: 2
I0521 10:47:52.275250      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:47:55.275868      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:47:58.277198      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 21 10:47:58.382: INFO: Creating new exec pod
May 21 10:48:04.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-277 exec execpodrqsks -- /bin/sh -x -c nslookup nodeport-service.services-277.svc.cluster.local'
May 21 10:48:04.795: INFO: stderr: "+ nslookup nodeport-service.services-277.svc.cluster.local\n"
May 21 10:48:04.795: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-277.svc.cluster.local\tcanonical name = externalsvc.services-277.svc.cluster.local.\nName:\texternalsvc.services-277.svc.cluster.local\nAddress: 10.105.50.176\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-277, will wait for the garbage collector to delete the pods
May 21 10:48:04.890: INFO: Deleting ReplicationController externalsvc took: 34.975791ms
May 21 10:48:05.391: INFO: Terminating ReplicationController externalsvc pods took: 500.272926ms
May 21 10:48:18.019: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:48:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-277" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.174 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":303,"completed":181,"skipped":3227,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:48:18.104: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4cc4d8e7-89c7-4f71-9a04-286d540a18a1 in namespace container-probe-4108
May 21 10:48:24.256: INFO: Started pod liveness-4cc4d8e7-89c7-4f71-9a04-286d540a18a1 in namespace container-probe-4108
STEP: checking the pod's current state and verifying that restartCount is present
May 21 10:48:24.261: INFO: Initial restart count of pod liveness-4cc4d8e7-89c7-4f71-9a04-286d540a18a1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:52:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4108" for this suite.

• [SLOW TEST:247.745 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":303,"completed":182,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:52:25.855: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-flbj
STEP: Creating a pod to test atomic-volume-subpath
May 21 10:52:26.029: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-flbj" in namespace "subpath-3875" to be "Succeeded or Failed"
May 21 10:52:26.046: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Pending", Reason="", readiness=false. Elapsed: 16.635612ms
May 21 10:52:28.065: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03519891s
May 21 10:52:30.080: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05102543s
May 21 10:52:32.094: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 6.064212538s
May 21 10:52:34.101: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 8.071437618s
May 21 10:52:36.119: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 10.089906832s
May 21 10:52:38.129: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 12.099309651s
May 21 10:52:40.146: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 14.116352546s
May 21 10:52:42.155: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 16.125234184s
May 21 10:52:44.167: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 18.138029949s
May 21 10:52:46.176: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 20.146355491s
May 21 10:52:48.194: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 22.16507006s
May 21 10:52:50.208: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Running", Reason="", readiness=true. Elapsed: 24.17870566s
May 21 10:52:52.217: INFO: Pod "pod-subpath-test-downwardapi-flbj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.187640198s
STEP: Saw pod success
May 21 10:52:52.217: INFO: Pod "pod-subpath-test-downwardapi-flbj" satisfied condition "Succeeded or Failed"
May 21 10:52:52.223: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-subpath-test-downwardapi-flbj container test-container-subpath-downwardapi-flbj: <nil>
STEP: delete the pod
May 21 10:52:52.310: INFO: Waiting for pod pod-subpath-test-downwardapi-flbj to disappear
May 21 10:52:52.321: INFO: Pod pod-subpath-test-downwardapi-flbj no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-flbj
May 21 10:52:52.321: INFO: Deleting pod "pod-subpath-test-downwardapi-flbj" in namespace "subpath-3875"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:52:52.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3875" for this suite.

• [SLOW TEST:26.526 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":303,"completed":183,"skipped":3261,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:52:52.381: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-1b283f66-e006-4df2-955f-884af1123009
STEP: Creating secret with name secret-projected-all-test-volume-63bf1ddf-e44d-4afb-9408-695c19450382
STEP: Creating a pod to test Check all projections for projected volume plugin
May 21 10:52:52.552: INFO: Waiting up to 5m0s for pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3" in namespace "projected-5969" to be "Succeeded or Failed"
May 21 10:52:52.560: INFO: Pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.646959ms
May 21 10:52:54.573: INFO: Pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02082206s
May 21 10:52:56.582: INFO: Pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030119566s
May 21 10:52:58.594: INFO: Pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041399451s
STEP: Saw pod success
May 21 10:52:58.594: INFO: Pod "projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3" satisfied condition "Succeeded or Failed"
May 21 10:52:58.604: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3 container projected-all-volume-test: <nil>
STEP: delete the pod
May 21 10:52:58.669: INFO: Waiting for pod projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3 to disappear
May 21 10:52:58.684: INFO: Pod projected-volume-48162084-9e9d-4eda-a609-21e2c24b28e3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:52:58.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5969" for this suite.

• [SLOW TEST:6.336 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":303,"completed":184,"skipped":3267,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:52:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 10:52:58.829: INFO: Waiting up to 5m0s for pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332" in namespace "downward-api-9085" to be "Succeeded or Failed"
May 21 10:52:58.834: INFO: Pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933267ms
May 21 10:53:00.847: INFO: Pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018182402s
May 21 10:53:02.872: INFO: Pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043494144s
May 21 10:53:04.881: INFO: Pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052091484s
STEP: Saw pod success
May 21 10:53:04.881: INFO: Pod "downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332" satisfied condition "Succeeded or Failed"
May 21 10:53:04.887: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332 container dapi-container: <nil>
STEP: delete the pod
May 21 10:53:04.976: INFO: Waiting for pod downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332 to disappear
May 21 10:53:04.983: INFO: Pod downward-api-2d391f97-6fe2-4476-8e7b-115bc5c59332 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:04.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9085" for this suite.

• [SLOW TEST:6.293 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":303,"completed":185,"skipped":3274,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:53:05.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-3228 version'
May 21 10:53:05.228: INFO: stderr: ""
May 21 10:53:05.228: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.11\", GitCommit:\"c6a2f08fc4378c5381dd948d9ad9d1080e3e6b33\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:27:07Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.11\", GitCommit:\"c6a2f08fc4378c5381dd948d9ad9d1080e3e6b33\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:19:22Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:05.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3228" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":303,"completed":186,"skipped":3276,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:05.265: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 21 10:53:05.332: INFO: namespace kubectl-6965
May 21 10:53:05.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6965 create -f -'
May 21 10:53:05.724: INFO: stderr: ""
May 21 10:53:05.724: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 10:53:06.735: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:06.735: INFO: Found 0 / 1
May 21 10:53:07.733: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:07.733: INFO: Found 0 / 1
May 21 10:53:08.735: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:08.735: INFO: Found 0 / 1
May 21 10:53:09.734: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:09.734: INFO: Found 0 / 1
May 21 10:53:10.733: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:10.733: INFO: Found 0 / 1
May 21 10:53:11.738: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:11.738: INFO: Found 1 / 1
May 21 10:53:11.738: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 10:53:11.746: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 10:53:11.746: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 10:53:11.746: INFO: wait on agnhost-primary startup in kubectl-6965 
May 21 10:53:11.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6965 logs agnhost-primary-fz9qk agnhost-primary'
May 21 10:53:11.897: INFO: stderr: ""
May 21 10:53:11.897: INFO: stdout: "Paused\n"
STEP: exposing RC
May 21 10:53:11.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6965 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 21 10:53:12.043: INFO: stderr: ""
May 21 10:53:12.043: INFO: stdout: "service/rm2 exposed\n"
May 21 10:53:12.063: INFO: Service rm2 in namespace kubectl-6965 found.
STEP: exposing service
May 21 10:53:14.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-6965 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 21 10:53:14.283: INFO: stderr: ""
May 21 10:53:14.283: INFO: stdout: "service/rm3 exposed\n"
May 21 10:53:14.295: INFO: Service rm3 in namespace kubectl-6965 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:16.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6965" for this suite.

• [SLOW TEST:11.079 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":303,"completed":187,"skipped":3279,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:16.345: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-02ff4ce2-2d39-4abf-a141-edce9723d8c6
STEP: Creating a pod to test consume secrets
May 21 10:53:16.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a" in namespace "projected-8999" to be "Succeeded or Failed"
May 21 10:53:16.539: INFO: Pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.255963ms
May 21 10:53:18.548: INFO: Pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041362032s
May 21 10:53:20.563: INFO: Pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056471236s
May 21 10:53:22.575: INFO: Pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067792059s
STEP: Saw pod success
May 21 10:53:22.575: INFO: Pod "pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a" satisfied condition "Succeeded or Failed"
May 21 10:53:22.582: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 10:53:22.659: INFO: Waiting for pod pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a to disappear
May 21 10:53:22.664: INFO: Pod pod-projected-secrets-7767d775-01e6-4db8-aedb-5a3a28bdb60a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:22.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8999" for this suite.

• [SLOW TEST:6.355 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":188,"skipped":3285,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:22.701: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-164
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-164
STEP: Deleting pre-stop pod
May 21 10:53:39.944: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:39.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-164" for this suite.

• [SLOW TEST:17.331 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":303,"completed":189,"skipped":3289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7754
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7754
I0521 10:53:40.267478      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7754, replica count: 2
I0521 10:53:43.318074      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:53:46.318691      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:53:46.318: INFO: Creating new exec pod
May 21 10:53:53.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-7754 exec execpoddzmvv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 10:53:53.731: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 10:53:53.731: INFO: stdout: ""
May 21 10:53:53.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-7754 exec execpoddzmvv -- /bin/sh -x -c nc -zv -t -w 2 10.109.149.44 80'
May 21 10:53:54.041: INFO: stderr: "+ nc -zv -t -w 2 10.109.149.44 80\nConnection to 10.109.149.44 80 port [tcp/http] succeeded!\n"
May 21 10:53:54.041: INFO: stdout: ""
May 21 10:53:54.041: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:53:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7754" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.151 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":303,"completed":190,"skipped":3325,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:53:54.188: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-84c5aa75-44ba-40d0-b2dd-4a1d6e7b8f28
STEP: Creating a pod to test consume secrets
May 21 10:53:54.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567" in namespace "projected-1846" to be "Succeeded or Failed"
May 21 10:53:54.320: INFO: Pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158012ms
May 21 10:53:56.335: INFO: Pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018993753s
May 21 10:53:58.344: INFO: Pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028729973s
May 21 10:54:00.356: INFO: Pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040840029s
STEP: Saw pod success
May 21 10:54:00.358: INFO: Pod "pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567" satisfied condition "Succeeded or Failed"
May 21 10:54:00.386: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567 container secret-volume-test: <nil>
STEP: delete the pod
May 21 10:54:00.496: INFO: Waiting for pod pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567 to disappear
May 21 10:54:00.502: INFO: Pod pod-projected-secrets-953fa3e6-dfdf-48cc-b6e0-5e3b59762567 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:54:00.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1846" for this suite.

• [SLOW TEST:6.341 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":191,"skipped":3332,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:54:00.530: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 10:54:06.722: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:54:06.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1888" for this suite.

• [SLOW TEST:6.274 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":192,"skipped":3337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:54:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 21 10:54:06.907: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 21 10:54:06.946: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 10:54:06.946: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 21 10:54:06.967: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 10:54:06.967: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 21 10:54:07.004: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 21 10:54:07.004: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 21 10:54:14.144: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:54:14.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4460" for this suite.

• [SLOW TEST:7.442 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":303,"completed":193,"skipped":3381,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:54:14.251: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 21 10:54:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
May 21 10:54:15.192: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
May 21 10:54:17.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:54:19.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:54:21.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:54:23.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:54:25.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191255, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 10:54:28.785: INFO: Waited 1.380625122s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:54:29.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-227" for this suite.

• [SLOW TEST:15.233 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":303,"completed":194,"skipped":3384,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:54:29.485: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 10:54:30.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96" in namespace "downward-api-6423" to be "Succeeded or Failed"
May 21 10:54:30.153: INFO: Pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441321ms
May 21 10:54:32.161: INFO: Pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012991175s
May 21 10:54:34.170: INFO: Pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02130809s
May 21 10:54:36.183: INFO: Pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034599456s
STEP: Saw pod success
May 21 10:54:36.183: INFO: Pod "downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96" satisfied condition "Succeeded or Failed"
May 21 10:54:36.197: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96 container client-container: <nil>
STEP: delete the pod
May 21 10:54:36.274: INFO: Waiting for pod downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96 to disappear
May 21 10:54:36.280: INFO: Pod downwardapi-volume-eed8b29b-e051-4206-9d97-393cc6d1fc96 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:54:36.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6423" for this suite.

• [SLOW TEST:6.853 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":195,"skipped":3387,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:54:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2272
STEP: creating service affinity-nodeport in namespace services-2272
STEP: creating replication controller affinity-nodeport in namespace services-2272
I0521 10:54:36.533781      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2272, replica count: 3
I0521 10:54:39.584767      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:54:42.585292      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 10:54:45.586103      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 10:54:45.619: INFO: Creating new exec pod
May 21 10:54:52.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2272 exec execpod-affinitynz87f -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 21 10:54:52.958: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 21 10:54:52.958: INFO: stdout: ""
May 21 10:54:52.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2272 exec execpod-affinitynz87f -- /bin/sh -x -c nc -zv -t -w 2 10.107.178.82 80'
May 21 10:54:53.258: INFO: stderr: "+ nc -zv -t -w 2 10.107.178.82 80\nConnection to 10.107.178.82 80 port [tcp/http] succeeded!\n"
May 21 10:54:53.258: INFO: stdout: ""
May 21 10:54:53.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2272 exec execpod-affinitynz87f -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.36 31169'
May 21 10:54:53.562: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.36 31169\nConnection to 172.30.52.36 31169 port [tcp/31169] succeeded!\n"
May 21 10:54:53.562: INFO: stdout: ""
May 21 10:54:53.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2272 exec execpod-affinitynz87f -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.10 31169'
May 21 10:54:53.864: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.10 31169\nConnection to 172.30.52.10 31169 port [tcp/31169] succeeded!\n"
May 21 10:54:53.864: INFO: stdout: ""
May 21 10:54:53.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-2272 exec execpod-affinitynz87f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.52.36:31169/ ; done'
May 21 10:54:54.214: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.52.36:31169/\n"
May 21 10:54:54.214: INFO: stdout: "\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8\naffinity-nodeport-cfmb8"
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Received response from host: affinity-nodeport-cfmb8
May 21 10:54:54.214: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2272, will wait for the garbage collector to delete the pods
May 21 10:54:54.376: INFO: Deleting ReplicationController affinity-nodeport took: 26.296005ms
May 21 10:54:54.477: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.190464ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:55:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2272" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:31.696 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":196,"skipped":3390,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:55:08.037: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 10:55:08.133: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 10:55:12.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-6489 --namespace=crd-publish-openapi-6489 create -f -'
May 21 10:55:12.852: INFO: stderr: ""
May 21 10:55:12.853: INFO: stdout: "e2e-test-crd-publish-openapi-365-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 10:55:12.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-6489 --namespace=crd-publish-openapi-6489 delete e2e-test-crd-publish-openapi-365-crds test-cr'
May 21 10:55:13.123: INFO: stderr: ""
May 21 10:55:13.123: INFO: stdout: "e2e-test-crd-publish-openapi-365-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 21 10:55:13.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-6489 --namespace=crd-publish-openapi-6489 apply -f -'
May 21 10:55:13.352: INFO: stderr: ""
May 21 10:55:13.352: INFO: stdout: "e2e-test-crd-publish-openapi-365-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 10:55:13.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-6489 --namespace=crd-publish-openapi-6489 delete e2e-test-crd-publish-openapi-365-crds test-cr'
May 21 10:55:13.463: INFO: stderr: ""
May 21 10:55:13.463: INFO: stdout: "e2e-test-crd-publish-openapi-365-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 10:55:13.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-6489 explain e2e-test-crd-publish-openapi-365-crds'
May 21 10:55:13.714: INFO: stderr: ""
May 21 10:55:13.714: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-365-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:55:17.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6489" for this suite.

• [SLOW TEST:9.599 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":303,"completed":197,"skipped":3397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:55:17.637: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 10:55:17.725: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 10:55:17.747: INFO: Waiting for terminating namespaces to be deleted...
May 21 10:55:17.752: INFO: 
Logging pods the apiserver thinks is on node p1-maou9az9wksf7qjxozd15h7j6r before test
May 21 10:55:17.773: INFO: coredns-f9fd979d6-jnqgf from kube-system started at 2021-05-20 22:00:08 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container coredns ready: true, restart count 0
May 21 10:55:17.773: INFO: csi-ridge-node-76q9w from kube-system started at 2021-05-21 10:15:47 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 10:55:17.773: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 10:55:17.773: INFO: fluent-bit-w7b5s from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 10:55:17.773: INFO: kube-proxy-n6n5f from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 10:55:17.773: INFO: meta-bsns4 from kube-system started at 2021-05-21 10:13:03 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container meta ready: true, restart count 0
May 21 10:55:17.773: INFO: weave-net-jq22f from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container weave ready: true, restart count 1
May 21 10:55:17.773: INFO: 	Container weave-npc ready: true, restart count 0
May 21 10:55:17.773: INFO: sonobuoy-e2e-job-d98dbb7be59843f5 from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container e2e ready: true, restart count 0
May 21 10:55:17.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 10:55:17.773: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.773: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 21 10:55:17.773: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 10:55:17.773: INFO: 
Logging pods the apiserver thinks is on node p1-zq5sznp3cxrb94t9rscobyn6ny before test
May 21 10:55:17.789: INFO: csi-ridge-node-8gs7s from kube-system started at 2021-05-21 10:15:07 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 10:55:17.789: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 10:55:17.789: INFO: fluent-bit-v27pz from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 10:55:17.789: INFO: kube-proxy-bf9ms from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 10:55:17.789: INFO: meta-jwhdl from kube-system started at 2021-05-21 10:12:26 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container meta ready: true, restart count 0
May 21 10:55:17.789: INFO: weave-net-4ql9j from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container weave ready: true, restart count 1
May 21 10:55:17.789: INFO: 	Container weave-npc ready: true, restart count 0
May 21 10:55:17.789: INFO: sonobuoy from sonobuoy started at 2021-05-21 09:45:50 +0000 UTC (1 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 10:55:17.789: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 10:55:17.789: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 21 10:55:17.789: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node p1-maou9az9wksf7qjxozd15h7j6r
STEP: verifying the node has the label node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod coredns-f9fd979d6-jnqgf requesting resource cpu=100m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod csi-ridge-node-76q9w requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod csi-ridge-node-8gs7s requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod fluent-bit-v27pz requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod fluent-bit-w7b5s requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod kube-proxy-bf9ms requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod kube-proxy-n6n5f requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod meta-bsns4 requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod meta-jwhdl requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod weave-net-4ql9j requesting resource cpu=100m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod weave-net-jq22f requesting resource cpu=100m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod sonobuoy requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod sonobuoy-e2e-job-d98dbb7be59843f5 requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.912: INFO: Pod sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg requesting resource cpu=0m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
May 21 10:55:17.912: INFO: Pod sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th requesting resource cpu=0m on Node p1-maou9az9wksf7qjxozd15h7j6r
STEP: Starting Pods to consume most of the cluster CPU.
May 21 10:55:17.912: INFO: Creating a pod which consumes cpu=2660m on Node p1-maou9az9wksf7qjxozd15h7j6r
May 21 10:55:17.951: INFO: Creating a pod which consumes cpu=2730m on Node p1-zq5sznp3cxrb94t9rscobyn6ny
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710.16810f9f5d77e81c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2026/filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710 to p1-maou9az9wksf7qjxozd15h7j6r]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710.16810f9f9f54e5a8], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710.16810fa048689fa7], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 2.836619s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710.16810fa04f0728e0], Reason = [Created], Message = [Created container filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710.16810fa05861ae2e], Reason = [Started], Message = [Started container filler-pod-24374ac2-e7d6-46f8-8657-a0b057986710]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d.16810f9f5f19756b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2026/filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d to p1-zq5sznp3cxrb94t9rscobyn6ny]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d.16810f9fa2c7418a], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d.16810fa04f0b4be9], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 2.890102607s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d.16810fa0573c592d], Reason = [Created], Message = [Created container filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d.16810fa062a9765f], Reason = [Started], Message = [Started container filler-pod-79cab70d-60fd-4a5d-8b68-0f47ca69373d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16810fa0c874b60d], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16810fa0ca39d428], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node p1-maou9az9wksf7qjxozd15h7j6r
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node p1-zq5sznp3cxrb94t9rscobyn6ny
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:55:25.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2026" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.544 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":303,"completed":198,"skipped":3426,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:55:25.183: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
May 21 10:55:31.356: INFO: Pod pod-hostip-4bc5e5ca-906d-4b03-9590-875f4d174fb5 has hostIP: 172.30.52.10
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:55:31.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1194" for this suite.

• [SLOW TEST:6.228 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":303,"completed":199,"skipped":3433,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:55:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 10:55:31.559: INFO: Waiting up to 5m0s for pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a" in namespace "downward-api-9688" to be "Succeeded or Failed"
May 21 10:55:31.569: INFO: Pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.35897ms
May 21 10:55:33.582: INFO: Pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02337887s
May 21 10:55:35.593: INFO: Pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034058171s
May 21 10:55:37.602: INFO: Pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042863934s
STEP: Saw pod success
May 21 10:55:37.602: INFO: Pod "downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a" satisfied condition "Succeeded or Failed"
May 21 10:55:37.608: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a container dapi-container: <nil>
STEP: delete the pod
May 21 10:55:37.702: INFO: Waiting for pod downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a to disappear
May 21 10:55:37.711: INFO: Pod downward-api-e9b224dc-252d-47a2-ab56-fe7823df557a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:55:37.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9688" for this suite.

• [SLOW TEST:6.329 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":303,"completed":200,"skipped":3438,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:55:37.742: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9066
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-9066
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9066
May 21 10:55:38.089: INFO: Found 0 stateful pods, waiting for 1
May 21 10:55:48.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 21 10:55:48.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:55:48.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:55:48.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:55:48.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:55:48.473: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 10:55:58.495: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:55:58.495: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:55:58.546: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:55:58.546: INFO: ss-0  p1-zq5sznp3cxrb94t9rscobyn6ny  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  }]
May 21 10:55:58.547: INFO: 
May 21 10:55:58.547: INFO: StatefulSet ss has not reached scale 3, at 1
May 21 10:55:59.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990439495s
May 21 10:56:00.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97527849s
May 21 10:56:01.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.954999891s
May 21 10:56:02.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942196212s
May 21 10:56:03.613: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934221845s
May 21 10:56:04.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.923457907s
May 21 10:56:05.630: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.916075839s
May 21 10:56:06.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.907148706s
May 21 10:56:07.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 893.331637ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9066
May 21 10:56:08.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:56:09.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 10:56:09.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:56:09.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:56:09.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:56:09.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 10:56:09.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:56:09.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:56:09.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 10:56:09.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 10:56:09.622: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 10:56:09.622: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 10:56:09.629: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:56:09.629: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 10:56:09.629: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 21 10:56:09.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:56:09.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:56:09.906: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:56:09.906: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:56:09.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:56:10.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:56:10.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:56:10.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:56:10.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=statefulset-9066 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 10:56:10.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 10:56:10.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 10:56:10.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 10:56:10.447: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:56:10.467: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 21 10:56:20.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:56:20.483: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:56:20.483: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 10:56:20.573: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:20.573: INFO: ss-0  p1-zq5sznp3cxrb94t9rscobyn6ny  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  }]
May 21 10:56:20.574: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:20.574: INFO: ss-2  p1-zq5sznp3cxrb94t9rscobyn6ny  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:20.574: INFO: 
May 21 10:56:20.574: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 10:56:21.584: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:21.584: INFO: ss-0  p1-zq5sznp3cxrb94t9rscobyn6ny  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:38 +0000 UTC  }]
May 21 10:56:21.584: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:21.584: INFO: ss-2  p1-zq5sznp3cxrb94t9rscobyn6ny  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:21.584: INFO: 
May 21 10:56:21.584: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 10:56:22.606: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:22.606: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:22.606: INFO: 
May 21 10:56:22.606: INFO: StatefulSet ss has not reached scale 0, at 1
May 21 10:56:23.618: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:23.618: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:23.618: INFO: 
May 21 10:56:23.618: INFO: StatefulSet ss has not reached scale 0, at 1
May 21 10:56:24.629: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:24.629: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:24.629: INFO: 
May 21 10:56:24.630: INFO: StatefulSet ss has not reached scale 0, at 1
May 21 10:56:25.639: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:25.639: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:25.639: INFO: 
May 21 10:56:25.639: INFO: StatefulSet ss has not reached scale 0, at 1
May 21 10:56:26.652: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 21 10:56:26.652: INFO: ss-1  p1-maou9az9wksf7qjxozd15h7j6r  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:56:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 10:55:58 +0000 UTC  }]
May 21 10:56:26.652: INFO: 
May 21 10:56:26.652: INFO: StatefulSet ss has not reached scale 0, at 1
May 21 10:56:27.661: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.891712457s
May 21 10:56:28.671: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.882747348s
May 21 10:56:29.679: INFO: Verifying statefulset ss doesn't scale past 0 for another 873.159396ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9066
May 21 10:56:30.692: INFO: Scaling statefulset ss to 0
May 21 10:56:30.726: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 10:56:30.731: INFO: Deleting all statefulset in ns statefulset-9066
May 21 10:56:30.738: INFO: Scaling statefulset ss to 0
May 21 10:56:30.758: INFO: Waiting for statefulset status.replicas updated to 0
May 21 10:56:30.764: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:56:30.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9066" for this suite.

• [SLOW TEST:53.091 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":303,"completed":201,"skipped":3452,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:56:30.835: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5616, will wait for the garbage collector to delete the pods
May 21 10:56:39.049: INFO: Deleting Job.batch foo took: 29.578331ms
May 21 10:56:39.149: INFO: Terminating Job.batch foo pods took: 100.470349ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:57:17.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5616" for this suite.

• [SLOW TEST:47.166 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":303,"completed":202,"skipped":3473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:57:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 10:57:28.244: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:28.259: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 10:57:30.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:30.270: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 10:57:32.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:32.268: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 10:57:34.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:34.271: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 10:57:36.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:36.279: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 10:57:38.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 10:57:38.286: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:57:38.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3291" for this suite.

• [SLOW TEST:20.369 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":303,"completed":203,"skipped":3495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:57:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-8w4r
STEP: Creating a pod to test atomic-volume-subpath
May 21 10:57:38.532: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8w4r" in namespace "subpath-4629" to be "Succeeded or Failed"
May 21 10:57:38.560: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Pending", Reason="", readiness=false. Elapsed: 27.708205ms
May 21 10:57:40.570: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037307489s
May 21 10:57:42.580: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 4.047672156s
May 21 10:57:44.586: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 6.053704599s
May 21 10:57:46.597: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 8.064409078s
May 21 10:57:48.605: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 10.072573409s
May 21 10:57:50.612: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 12.079854821s
May 21 10:57:52.623: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 14.0907616s
May 21 10:57:54.648: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 16.115129017s
May 21 10:57:56.659: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 18.126797359s
May 21 10:57:58.671: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 20.138686347s
May 21 10:58:00.682: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Running", Reason="", readiness=true. Elapsed: 22.150007282s
May 21 10:58:02.693: INFO: Pod "pod-subpath-test-projected-8w4r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.160349347s
STEP: Saw pod success
May 21 10:58:02.693: INFO: Pod "pod-subpath-test-projected-8w4r" satisfied condition "Succeeded or Failed"
May 21 10:58:02.701: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-subpath-test-projected-8w4r container test-container-subpath-projected-8w4r: <nil>
STEP: delete the pod
May 21 10:58:02.795: INFO: Waiting for pod pod-subpath-test-projected-8w4r to disappear
May 21 10:58:02.804: INFO: Pod pod-subpath-test-projected-8w4r no longer exists
STEP: Deleting pod pod-subpath-test-projected-8w4r
May 21 10:58:02.804: INFO: Deleting pod "pod-subpath-test-projected-8w4r" in namespace "subpath-4629"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:58:02.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4629" for this suite.

• [SLOW TEST:24.461 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":303,"completed":204,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:58:02.846: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8057
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 10:58:02.927: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 10:58:03.039: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:58:05.050: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:58:07.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:58:09.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:58:11.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:58:13.051: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:58:15.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 10:58:17.050: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 10:58:17.065: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 10:58:19.073: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 10:58:23.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.96.4:8080/dial?request=hostname&protocol=udp&host=172.28.128.4&port=8081&tries=1'] Namespace:pod-network-test-8057 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:58:23.121: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:58:23.302: INFO: Waiting for responses: map[]
May 21 10:58:23.308: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.96.4:8080/dial?request=hostname&protocol=udp&host=172.28.96.3&port=8081&tries=1'] Namespace:pod-network-test-8057 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:58:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:58:23.461: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:58:23.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8057" for this suite.

• [SLOW TEST:20.660 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":303,"completed":205,"skipped":3568,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:58:23.506: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0521 10:58:33.728399      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 10:59:35.796: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:59:35.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-591" for this suite.

• [SLOW TEST:72.338 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":303,"completed":206,"skipped":3569,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:59:35.846: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 10:59:37.039: INFO: starting watch
STEP: patching
STEP: updating
May 21 10:59:37.086: INFO: waiting for watch events with expected annotations
May 21 10:59:37.086: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:59:37.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-703" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":303,"completed":207,"skipped":3581,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:59:37.317: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-43d2ad7f-b6b0-42cf-ab00-41ed244a8de8
STEP: Creating a pod to test consume secrets
May 21 10:59:37.461: INFO: Waiting up to 5m0s for pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016" in namespace "secrets-7420" to be "Succeeded or Failed"
May 21 10:59:37.475: INFO: Pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016": Phase="Pending", Reason="", readiness=false. Elapsed: 13.389268ms
May 21 10:59:39.483: INFO: Pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021755207s
May 21 10:59:41.497: INFO: Pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036137466s
May 21 10:59:43.506: INFO: Pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044958391s
STEP: Saw pod success
May 21 10:59:43.506: INFO: Pod "pod-secrets-e2253549-3057-4315-9ead-0abd5d778016" satisfied condition "Succeeded or Failed"
May 21 10:59:43.514: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-e2253549-3057-4315-9ead-0abd5d778016 container secret-env-test: <nil>
STEP: delete the pod
May 21 10:59:43.603: INFO: Waiting for pod pod-secrets-e2253549-3057-4315-9ead-0abd5d778016 to disappear
May 21 10:59:43.615: INFO: Pod pod-secrets-e2253549-3057-4315-9ead-0abd5d778016 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:59:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7420" for this suite.

• [SLOW TEST:6.329 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":303,"completed":208,"skipped":3587,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:59:43.648: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:59:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1753" for this suite.

• [SLOW TEST:7.228 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":303,"completed":209,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:59:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 21 10:59:51.004: INFO: Created pod &Pod{ObjectMeta:{dns-3638  dns-3638 /api/v1/namespaces/dns-3638/pods/dns-3638 a90272a0-0488-48e5-b787-7b3c09e44b71 184390 0 2021-05-21 10:59:50 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-21 10:59:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7vbwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7vbwl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7vbwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 10:59:51.011: INFO: The status of Pod dns-3638 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:59:53.025: INFO: The status of Pod dns-3638 is Pending, waiting for it to be Running (with Ready = true)
May 21 10:59:55.024: INFO: The status of Pod dns-3638 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 21 10:59:55.025: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3638 PodName:dns-3638 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:59:55.025: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Verifying customized DNS server is configured on pod...
May 21 10:59:55.192: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3638 PodName:dns-3638 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 10:59:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 10:59:55.361: INFO: Deleting pod dns-3638...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 10:59:55.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3638" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":303,"completed":210,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 10:59:55.475: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 10:59:55.596: INFO: Waiting up to 5m0s for pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f" in namespace "downward-api-3662" to be "Succeeded or Failed"
May 21 10:59:55.609: INFO: Pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.345372ms
May 21 10:59:57.617: INFO: Pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021089007s
May 21 10:59:59.628: INFO: Pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031990042s
May 21 11:00:01.638: INFO: Pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041581587s
STEP: Saw pod success
May 21 11:00:01.638: INFO: Pod "downward-api-f95669dd-afac-4acb-9b56-38df2664368f" satisfied condition "Succeeded or Failed"
May 21 11:00:01.642: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downward-api-f95669dd-afac-4acb-9b56-38df2664368f container dapi-container: <nil>
STEP: delete the pod
May 21 11:00:01.699: INFO: Waiting for pod downward-api-f95669dd-afac-4acb-9b56-38df2664368f to disappear
May 21 11:00:01.713: INFO: Pod downward-api-f95669dd-afac-4acb-9b56-38df2664368f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3662" for this suite.

• [SLOW TEST:6.261 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":303,"completed":211,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:15.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-583" for this suite.

• [SLOW TEST:13.377 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":303,"completed":212,"skipped":3699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:15.118: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:00:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:15.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5762" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":303,"completed":213,"skipped":3746,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:16.010: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:00:16.930: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:00:18.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191616, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191616, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191617, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191616, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:00:22.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:22.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4637" for this suite.
STEP: Destroying namespace "webhook-4637-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.639 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":303,"completed":214,"skipped":3747,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
May 21 11:00:29.374: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9631 pod-service-account-cd30a4f1-86f9-4596-a7a3-c9715004656e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 21 11:00:29.678: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9631 pod-service-account-cd30a4f1-86f9-4596-a7a3-c9715004656e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 21 11:00:29.938: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9631 pod-service-account-cd30a4f1-86f9-4596-a7a3-c9715004656e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:30.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9631" for this suite.

• [SLOW TEST:7.584 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":303,"completed":215,"skipped":3757,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:30.235: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:00:34.441: INFO: Waiting up to 5m0s for pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02" in namespace "pods-5980" to be "Succeeded or Failed"
May 21 11:00:34.457: INFO: Pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043794ms
May 21 11:00:36.467: INFO: Pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025553537s
May 21 11:00:38.482: INFO: Pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040983765s
May 21 11:00:40.492: INFO: Pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050312767s
STEP: Saw pod success
May 21 11:00:40.492: INFO: Pod "client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02" satisfied condition "Succeeded or Failed"
May 21 11:00:40.498: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02 container env3cont: <nil>
STEP: delete the pod
May 21 11:00:40.581: INFO: Waiting for pod client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02 to disappear
May 21 11:00:40.596: INFO: Pod client-envvars-ce973a86-3e7f-4131-ab1f-a3e22e89eb02 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5980" for this suite.

• [SLOW TEST:10.397 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":303,"completed":216,"skipped":3766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:40.636: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
May 21 11:00:40.806: INFO: Waiting up to 5m0s for pod "pod-5ec47134-ce27-4735-880e-03de4a9b4179" in namespace "emptydir-5933" to be "Succeeded or Failed"
May 21 11:00:40.829: INFO: Pod "pod-5ec47134-ce27-4735-880e-03de4a9b4179": Phase="Pending", Reason="", readiness=false. Elapsed: 23.000143ms
May 21 11:00:42.851: INFO: Pod "pod-5ec47134-ce27-4735-880e-03de4a9b4179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045208373s
May 21 11:00:44.864: INFO: Pod "pod-5ec47134-ce27-4735-880e-03de4a9b4179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057606905s
STEP: Saw pod success
May 21 11:00:44.864: INFO: Pod "pod-5ec47134-ce27-4735-880e-03de4a9b4179" satisfied condition "Succeeded or Failed"
May 21 11:00:44.869: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-5ec47134-ce27-4735-880e-03de4a9b4179 container test-container: <nil>
STEP: delete the pod
May 21 11:00:44.953: INFO: Waiting for pod pod-5ec47134-ce27-4735-880e-03de4a9b4179 to disappear
May 21 11:00:44.960: INFO: Pod pod-5ec47134-ce27-4735-880e-03de4a9b4179 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:44.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5933" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":217,"skipped":3836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:44.995: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-05101b4d-6a5f-4bcf-81a4-654d7cbc3382
STEP: Creating a pod to test consume configMaps
May 21 11:00:45.129: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184" in namespace "projected-6220" to be "Succeeded or Failed"
May 21 11:00:45.143: INFO: Pod "pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184": Phase="Pending", Reason="", readiness=false. Elapsed: 14.062889ms
May 21 11:00:47.164: INFO: Pod "pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034692121s
May 21 11:00:49.172: INFO: Pod "pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042968317s
STEP: Saw pod success
May 21 11:00:49.172: INFO: Pod "pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184" satisfied condition "Succeeded or Failed"
May 21 11:00:49.178: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 11:00:49.257: INFO: Waiting for pod pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184 to disappear
May 21 11:00:49.263: INFO: Pod pod-projected-configmaps-9dbfbb33-b8e5-4675-969d-c74b2fbfc184 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:49.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6220" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":218,"skipped":3864,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:49.286: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 11:00:55.481: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:00:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9561" for this suite.

• [SLOW TEST:6.287 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":303,"completed":219,"skipped":3869,"failed":0}
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:00:55.575: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:01:00.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3318" for this suite.

• [SLOW TEST:5.432 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":303,"completed":220,"skipped":3869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:01:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 11:01:09.221: INFO: DNS probes using dns-test-71916da1-7ba7-4582-aafe-602e6268c5a7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 11:01:17.425: INFO: File wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:17.432: INFO: File jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:17.432: INFO: Lookups using dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da failed for: [wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local]

May 21 11:01:22.453: INFO: File wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:22.465: INFO: File jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:22.465: INFO: Lookups using dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da failed for: [wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local]

May 21 11:01:27.449: INFO: File wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:27.457: INFO: File jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:27.457: INFO: Lookups using dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da failed for: [wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local]

May 21 11:01:32.444: INFO: File wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:32.453: INFO: File jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local from pod  dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 11:01:32.453: INFO: Lookups using dns-2785/dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da failed for: [wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local]

May 21 11:01:37.459: INFO: DNS probes using dns-test-e99ea46b-32c8-49bc-ae11-28a96f92a6da succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2785.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2785.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 11:01:45.750: INFO: DNS probes using dns-test-e436f8cc-30d4-4ae9-8f86-1577c3a74303 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:01:45.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2785" for this suite.

• [SLOW TEST:44.949 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":303,"completed":221,"skipped":3910,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:01:45.960: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 21 11:01:46.064: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:01:57.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9582" for this suite.

• [SLOW TEST:11.941 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":303,"completed":222,"skipped":3916,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:01:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 11:02:02.659: INFO: Successfully updated pod "labelsupdate202e647f-b42f-41b6-8c1e-1d460cecefc8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:04.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4573" for this suite.

• [SLOW TEST:6.849 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":223,"skipped":3927,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-e516d298-f81a-4411-82bc-e4ce75eaded3
STEP: Creating a pod to test consume secrets
May 21 11:02:04.902: INFO: Waiting up to 5m0s for pod "pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1" in namespace "secrets-2480" to be "Succeeded or Failed"
May 21 11:02:04.908: INFO: Pod "pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.8781ms
May 21 11:02:06.917: INFO: Pod "pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014812118s
May 21 11:02:08.925: INFO: Pod "pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022388863s
STEP: Saw pod success
May 21 11:02:08.925: INFO: Pod "pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1" satisfied condition "Succeeded or Failed"
May 21 11:02:08.930: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1 container secret-volume-test: <nil>
STEP: delete the pod
May 21 11:02:09.004: INFO: Waiting for pod pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1 to disappear
May 21 11:02:09.011: INFO: Pod pod-secrets-22af5f12-5cd4-46d2-bcad-b216a702bff1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:09.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2480" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":224,"skipped":3944,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:09.065: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:02:09.153: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 21 11:02:13.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 create -f -'
May 21 11:02:13.650: INFO: stderr: ""
May 21 11:02:13.650: INFO: stdout: "e2e-test-crd-publish-openapi-6150-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 11:02:13.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 delete e2e-test-crd-publish-openapi-6150-crds test-foo'
May 21 11:02:13.762: INFO: stderr: ""
May 21 11:02:13.762: INFO: stdout: "e2e-test-crd-publish-openapi-6150-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 21 11:02:13.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 apply -f -'
May 21 11:02:14.141: INFO: stderr: ""
May 21 11:02:14.141: INFO: stdout: "e2e-test-crd-publish-openapi-6150-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 11:02:14.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 delete e2e-test-crd-publish-openapi-6150-crds test-foo'
May 21 11:02:14.333: INFO: stderr: ""
May 21 11:02:14.333: INFO: stdout: "e2e-test-crd-publish-openapi-6150-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 21 11:02:14.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 create -f -'
May 21 11:02:14.684: INFO: rc: 1
May 21 11:02:14.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 apply -f -'
May 21 11:02:14.967: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 21 11:02:14.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 create -f -'
May 21 11:02:15.179: INFO: rc: 1
May 21 11:02:15.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 --namespace=crd-publish-openapi-9736 apply -f -'
May 21 11:02:15.512: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 21 11:02:15.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 explain e2e-test-crd-publish-openapi-6150-crds'
May 21 11:02:15.829: INFO: stderr: ""
May 21 11:02:15.830: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6150-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 21 11:02:15.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 explain e2e-test-crd-publish-openapi-6150-crds.metadata'
May 21 11:02:16.124: INFO: stderr: ""
May 21 11:02:16.124: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6150-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 21 11:02:16.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 explain e2e-test-crd-publish-openapi-6150-crds.spec'
May 21 11:02:16.467: INFO: stderr: ""
May 21 11:02:16.467: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6150-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 21 11:02:16.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 explain e2e-test-crd-publish-openapi-6150-crds.spec.bars'
May 21 11:02:16.710: INFO: stderr: ""
May 21 11:02:16.711: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6150-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 21 11:02:16.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-9736 explain e2e-test-crd-publish-openapi-6150-crds.spec.bars2'
May 21 11:02:17.040: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9736" for this suite.

• [SLOW TEST:11.974 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":303,"completed":225,"skipped":3954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:21.040: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:37.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8679" for this suite.

• [SLOW TEST:16.531 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":303,"completed":226,"skipped":3976,"failed":0}
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:37.572: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 21 11:02:37.708: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8628 /api/v1/namespaces/watch-8628/configmaps/e2e-watch-test-watch-closed 7515fa62-0530-4a28-9a5c-c6dafb21a762 185710 0 2021-05-21 11:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 11:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:02:37.709: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8628 /api/v1/namespaces/watch-8628/configmaps/e2e-watch-test-watch-closed 7515fa62-0530-4a28-9a5c-c6dafb21a762 185711 0 2021-05-21 11:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 11:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 21 11:02:37.758: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8628 /api/v1/namespaces/watch-8628/configmaps/e2e-watch-test-watch-closed 7515fa62-0530-4a28-9a5c-c6dafb21a762 185715 0 2021-05-21 11:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 11:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:02:37.758: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8628 /api/v1/namespaces/watch-8628/configmaps/e2e-watch-test-watch-closed 7515fa62-0530-4a28-9a5c-c6dafb21a762 185716 0 2021-05-21 11:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 11:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:37.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8628" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":303,"completed":227,"skipped":3976,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:37.787: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 11:02:50.075: INFO: DNS probes using dns-4684/dns-test-92c6ffe2-5326-4387-a4aa-e34456e6dc7e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:02:50.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4684" for this suite.

• [SLOW TEST:12.389 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":303,"completed":228,"skipped":3976,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:02:50.177: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:02:51.471: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:02:53.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:02:55.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:02:57.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191771, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:03:00.578: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:03:00.587: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8056-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:03:01.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8483" for this suite.
STEP: Destroying namespace "webhook-8483-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.042 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":303,"completed":229,"skipped":3989,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:03:02.220: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:03:44.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8358" for this suite.

• [SLOW TEST:42.368 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":303,"completed":230,"skipped":3997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:03:44.590: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:03:44.719: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37" in namespace "security-context-test-3922" to be "Succeeded or Failed"
May 21 11:03:44.734: INFO: Pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37": Phase="Pending", Reason="", readiness=false. Elapsed: 15.239472ms
May 21 11:03:46.740: INFO: Pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021769057s
May 21 11:03:48.751: INFO: Pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032165236s
May 21 11:03:50.763: INFO: Pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044691058s
May 21 11:03:50.763: INFO: Pod "alpine-nnp-false-09e5c565-d674-43d0-aba0-060c65852e37" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:03:50.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3922" for this suite.

• [SLOW TEST:6.250 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":231,"skipped":4022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:03:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:03:56.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4572" for this suite.

• [SLOW TEST:6.192 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":303,"completed":232,"skipped":4082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:03:57.041: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:04:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-237" for this suite.

• [SLOW TEST:16.641 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":303,"completed":233,"skipped":4109,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:04:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-pwbp
STEP: Creating a pod to test atomic-volume-subpath
May 21 11:04:13.846: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pwbp" in namespace "subpath-81" to be "Succeeded or Failed"
May 21 11:04:13.860: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Pending", Reason="", readiness=false. Elapsed: 13.371596ms
May 21 11:04:15.876: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02959811s
May 21 11:04:17.885: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038904852s
May 21 11:04:19.897: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 6.050954389s
May 21 11:04:21.907: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 8.060400556s
May 21 11:04:23.912: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 10.065938083s
May 21 11:04:25.921: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 12.074309794s
May 21 11:04:27.934: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 14.087830664s
May 21 11:04:29.946: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 16.099230179s
May 21 11:04:31.957: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 18.110624578s
May 21 11:04:33.969: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 20.122543632s
May 21 11:04:35.984: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 22.13793981s
May 21 11:04:37.994: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Running", Reason="", readiness=true. Elapsed: 24.147876819s
May 21 11:04:40.002: INFO: Pod "pod-subpath-test-configmap-pwbp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.155474767s
STEP: Saw pod success
May 21 11:04:40.002: INFO: Pod "pod-subpath-test-configmap-pwbp" satisfied condition "Succeeded or Failed"
May 21 11:04:40.008: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-subpath-test-configmap-pwbp container test-container-subpath-configmap-pwbp: <nil>
STEP: delete the pod
May 21 11:04:40.153: INFO: Waiting for pod pod-subpath-test-configmap-pwbp to disappear
May 21 11:04:40.160: INFO: Pod pod-subpath-test-configmap-pwbp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pwbp
May 21 11:04:40.160: INFO: Deleting pod "pod-subpath-test-configmap-pwbp" in namespace "subpath-81"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:04:40.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-81" for this suite.

• [SLOW TEST:26.518 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":303,"completed":234,"skipped":4109,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:04:40.201: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:04:40.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:04:42.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191881, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:04:44.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191881, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757191880, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:04:47.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:04:48.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2746" for this suite.
STEP: Destroying namespace "webhook-2746-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":303,"completed":235,"skipped":4125,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:04:48.710: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c0c7ab78-1fd1-437c-bddf-9ce103a3bcc6
STEP: Creating a pod to test consume configMaps
May 21 11:04:48.865: INFO: Waiting up to 5m0s for pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb" in namespace "configmap-9434" to be "Succeeded or Failed"
May 21 11:04:48.879: INFO: Pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.226032ms
May 21 11:04:50.889: INFO: Pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023644751s
May 21 11:04:52.899: INFO: Pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034239941s
May 21 11:04:54.906: INFO: Pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040673991s
STEP: Saw pod success
May 21 11:04:54.906: INFO: Pod "pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb" satisfied condition "Succeeded or Failed"
May 21 11:04:54.912: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb container configmap-volume-test: <nil>
STEP: delete the pod
May 21 11:04:54.993: INFO: Waiting for pod pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb to disappear
May 21 11:04:55.000: INFO: Pod pod-configmaps-01453306-e857-4fd0-a509-befaa069c7fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:04:55.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9434" for this suite.

• [SLOW TEST:6.322 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":236,"skipped":4134,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:04:55.037: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:04:55.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5295" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":303,"completed":237,"skipped":4144,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:04:55.331: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-30773866-d818-433b-b9ce-0bf1df8c0eb3
STEP: Creating secret with name s-test-opt-upd-1010560e-8eee-404c-9526-5dc1989ef0dd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-30773866-d818-433b-b9ce-0bf1df8c0eb3
STEP: Updating secret s-test-opt-upd-1010560e-8eee-404c-9526-5dc1989ef0dd
STEP: Creating secret with name s-test-opt-create-9378e60d-4087-4ed6-b7fa-3d0968c313e0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:05:09.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2284" for this suite.

• [SLOW TEST:14.410 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":238,"skipped":4155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:05:09.749: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:05:09.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f" in namespace "downward-api-8173" to be "Succeeded or Failed"
May 21 11:05:09.912: INFO: Pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.89366ms
May 21 11:05:11.935: INFO: Pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042082321s
May 21 11:05:13.948: INFO: Pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054166923s
May 21 11:05:15.964: INFO: Pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07098464s
STEP: Saw pod success
May 21 11:05:15.964: INFO: Pod "downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f" satisfied condition "Succeeded or Failed"
May 21 11:05:15.971: INFO: Trying to get logs from node p1-maou9az9wksf7qjxozd15h7j6r pod downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f container client-container: <nil>
STEP: delete the pod
May 21 11:05:16.065: INFO: Waiting for pod downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f to disappear
May 21 11:05:16.072: INFO: Pod downwardapi-volume-39998304-b6c0-4767-b37a-d3d9bced9a9f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:05:16.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8173" for this suite.

• [SLOW TEST:6.351 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":239,"skipped":4202,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:05:16.102: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-a801fc1b-5942-45bd-a66b-833f64643854
STEP: Creating secret with name s-test-opt-upd-00e0072e-8f50-4c1b-b905-348214a4cedd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a801fc1b-5942-45bd-a66b-833f64643854
STEP: Updating secret s-test-opt-upd-00e0072e-8f50-4c1b-b905-348214a4cedd
STEP: Creating secret with name s-test-opt-create-606b6da3-d328-40a6-8d2c-c3097bc919c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:05:32.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9393" for this suite.

• [SLOW TEST:16.474 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":240,"skipped":4209,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:05:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-36648eee-55e4-4c4b-965e-a0ca63774d03
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:05:32.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4875" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":303,"completed":241,"skipped":4212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:05:32.742: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:07:32.905: INFO: Deleting pod "var-expansion-ba8c2fc6-a0fd-4b0d-8671-140122c9894c" in namespace "var-expansion-6759"
May 21 11:07:32.959: INFO: Wait up to 5m0s for pod "var-expansion-ba8c2fc6-a0fd-4b0d-8671-140122c9894c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:07:34.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6759" for this suite.

• [SLOW TEST:122.292 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":303,"completed":242,"skipped":4262,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:07:35.037: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:07:35.124: INFO: Creating deployment "test-recreate-deployment"
May 21 11:07:35.151: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 21 11:07:35.215: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 21 11:07:37.234: INFO: Waiting deployment "test-recreate-deployment" to complete
May 21 11:07:37.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:07:39.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192055, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:07:41.247: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 21 11:07:41.283: INFO: Updating deployment test-recreate-deployment
May 21 11:07:41.283: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 11:07:41.544: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5668 /apis/apps/v1/namespaces/deployment-5668/deployments/test-recreate-deployment 38137ec9-8cd7-4016-8a5d-b97e099121dc 187476 2 2021-05-21 11:07:35 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025f2f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 11:07:41 +0000 UTC,LastTransitionTime:2021-05-21 11:07:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-05-21 11:07:41 +0000 UTC,LastTransitionTime:2021-05-21 11:07:35 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 21 11:07:41.550: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-5668 /apis/apps/v1/namespaces/deployment-5668/replicasets/test-recreate-deployment-f756bcb56 0ed6d10f-b200-4543-af98-af37f7224577 187469 1 2021-05-21 11:07:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 38137ec9-8cd7-4016-8a5d-b97e099121dc 0xc003a41bd0 0xc003a41bd1}] []  [{kube-controller-manager Update apps/v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38137ec9-8cd7-4016-8a5d-b97e099121dc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a41c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 11:07:41.550: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 21 11:07:41.551: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5668 /apis/apps/v1/namespaces/deployment-5668/replicasets/test-recreate-deployment-c96cf48f 01ce4b4b-a6e8-4d09-8595-a4927a8227c3 187460 2 2021-05-21 11:07:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 38137ec9-8cd7-4016-8a5d-b97e099121dc 0xc003a41adf 0xc003a41af0}] []  [{kube-controller-manager Update apps/v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38137ec9-8cd7-4016-8a5d-b97e099121dc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a41b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 11:07:41.557: INFO: Pod "test-recreate-deployment-f756bcb56-ckmfg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-ckmfg test-recreate-deployment-f756bcb56- deployment-5668 /api/v1/namespaces/deployment-5668/pods/test-recreate-deployment-f756bcb56-ckmfg 6d16bac7-79e6-4467-99a1-23d31fb57e2a 187470 0 2021-05-21 11:07:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 0ed6d10f-b200-4543-af98-af37f7224577 0xc005f00120 0xc005f00121}] []  [{kube-controller-manager Update v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ed6d10f-b200-4543-af98-af37f7224577\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 11:07:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h77p2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h77p2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h77p2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:07:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:07:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:07:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:07:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:,StartTime:2021-05-21 11:07:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:07:41.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5668" for this suite.

• [SLOW TEST:6.548 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":243,"skipped":4263,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:07:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 11:07:42.387: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 11:07:44.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:07:46.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:07:48.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192062, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:07:51.472: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:07:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:07:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2360" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.876 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":303,"completed":244,"skipped":4280,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:07:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:07:54.434: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 11:07:56.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:07:58.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192074, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:08:01.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 21 11:08:01.600: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:01.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-261" for this suite.
STEP: Destroying namespace "webhook-261-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.370 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":303,"completed":245,"skipped":4287,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:01.836: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
May 21 11:08:01.955: INFO: Waiting up to 5m0s for pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29" in namespace "emptydir-9215" to be "Succeeded or Failed"
May 21 11:08:01.971: INFO: Pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29": Phase="Pending", Reason="", readiness=false. Elapsed: 15.516031ms
May 21 11:08:03.982: INFO: Pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026895914s
May 21 11:08:06.004: INFO: Pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04828235s
May 21 11:08:08.014: INFO: Pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058205113s
STEP: Saw pod success
May 21 11:08:08.014: INFO: Pod "pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29" satisfied condition "Succeeded or Failed"
May 21 11:08:08.022: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29 container test-container: <nil>
STEP: delete the pod
May 21 11:08:08.098: INFO: Waiting for pod pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29 to disappear
May 21 11:08:08.104: INFO: Pod pod-69623d8b-aee0-41a0-a3d6-9b16bf2aee29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:08.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9215" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":246,"skipped":4298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 11:08:08.261: INFO: Waiting up to 5m0s for pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13" in namespace "emptydir-3393" to be "Succeeded or Failed"
May 21 11:08:08.266: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13": Phase="Pending", Reason="", readiness=false. Elapsed: 5.456292ms
May 21 11:08:10.276: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015065999s
May 21 11:08:12.291: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030262566s
May 21 11:08:14.305: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043677143s
May 21 11:08:16.322: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061236363s
STEP: Saw pod success
May 21 11:08:16.322: INFO: Pod "pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13" satisfied condition "Succeeded or Failed"
May 21 11:08:16.330: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13 container test-container: <nil>
STEP: delete the pod
May 21 11:08:16.411: INFO: Waiting for pod pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13 to disappear
May 21 11:08:16.418: INFO: Pod pod-1ede016a-4ac4-406c-8d7f-3e27bde15f13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:16.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3393" for this suite.

• [SLOW TEST:8.307 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":247,"skipped":4325,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:16.448: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 21 11:08:16.539: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 21 11:08:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 11:08:35.748: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:51.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9908" for this suite.

• [SLOW TEST:34.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":303,"completed":248,"skipped":4334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:51.309: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:08:51.397: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:52.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9450" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":303,"completed":249,"skipped":4357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:52.479: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-444/configmap-test-576c5622-89d9-4ff4-ba5f-ea89783f7127
STEP: Creating a pod to test consume configMaps
May 21 11:08:52.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7" in namespace "configmap-444" to be "Succeeded or Failed"
May 21 11:08:52.633: INFO: Pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.089799ms
May 21 11:08:54.644: INFO: Pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019002702s
May 21 11:08:56.656: INFO: Pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030068684s
May 21 11:08:58.665: INFO: Pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039588647s
STEP: Saw pod success
May 21 11:08:58.665: INFO: Pod "pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7" satisfied condition "Succeeded or Failed"
May 21 11:08:58.671: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7 container env-test: <nil>
STEP: delete the pod
May 21 11:08:58.750: INFO: Waiting for pod pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7 to disappear
May 21 11:08:58.757: INFO: Pod pod-configmaps-b97f33a9-4e4f-411e-89d4-c4a96ab450f7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:08:58.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-444" for this suite.

• [SLOW TEST:6.305 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":250,"skipped":4385,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:08:58.786: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 11:08:59.840: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 11:09:01.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192140, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:09:03.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192140, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192139, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:09:06.941: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:09:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:09:08.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4078" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.657 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":303,"completed":251,"skipped":4390,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:09:08.446: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-b672b6f7-820f-4a7d-b9a1-58e1c00ff1d8 in namespace container-probe-4207
May 21 11:09:14.903: INFO: Started pod busybox-b672b6f7-820f-4a7d-b9a1-58e1c00ff1d8 in namespace container-probe-4207
STEP: checking the pod's current state and verifying that restartCount is present
May 21 11:09:14.911: INFO: Initial restart count of pod busybox-b672b6f7-820f-4a7d-b9a1-58e1c00ff1d8 is 0
May 21 11:10:09.229: INFO: Restart count of pod container-probe-4207/busybox-b672b6f7-820f-4a7d-b9a1-58e1c00ff1d8 is now 1 (54.318267315s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:10:09.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4207" for this suite.

• [SLOW TEST:60.889 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":252,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:10:09.341: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:10:09.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4506" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":253,"skipped":4435,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:10:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 11:10:09.699: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 11:10:09.722: INFO: Waiting for terminating namespaces to be deleted...
May 21 11:10:09.728: INFO: 
Logging pods the apiserver thinks is on node p1-maou9az9wksf7qjxozd15h7j6r before test
May 21 11:10:09.751: INFO: coredns-f9fd979d6-jnqgf from kube-system started at 2021-05-20 22:00:08 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.751: INFO: 	Container coredns ready: true, restart count 0
May 21 11:10:09.751: INFO: csi-ridge-node-76q9w from kube-system started at 2021-05-21 10:15:47 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.751: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 11:10:09.752: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 11:10:09.752: INFO: fluent-bit-w7b5s from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 11:10:09.752: INFO: kube-proxy-n6n5f from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 11:10:09.752: INFO: meta-bsns4 from kube-system started at 2021-05-21 10:13:03 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container meta ready: true, restart count 0
May 21 11:10:09.752: INFO: weave-net-jq22f from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container weave ready: true, restart count 1
May 21 11:10:09.752: INFO: 	Container weave-npc ready: true, restart count 0
May 21 11:10:09.752: INFO: sonobuoy-e2e-job-d98dbb7be59843f5 from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container e2e ready: true, restart count 0
May 21 11:10:09.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 11:10:09.752: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.752: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 11:10:09.752: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 11:10:09.752: INFO: 
Logging pods the apiserver thinks is on node p1-zq5sznp3cxrb94t9rscobyn6ny before test
May 21 11:10:09.772: INFO: csi-ridge-node-8gs7s from kube-system started at 2021-05-21 10:15:07 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.772: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 11:10:09.772: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 11:10:09.773: INFO: fluent-bit-v27pz from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.773: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 11:10:09.773: INFO: kube-proxy-bf9ms from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.773: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 11:10:09.773: INFO: meta-jwhdl from kube-system started at 2021-05-21 10:12:26 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.773: INFO: 	Container meta ready: true, restart count 0
May 21 11:10:09.773: INFO: weave-net-4ql9j from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.774: INFO: 	Container weave ready: true, restart count 1
May 21 11:10:09.774: INFO: 	Container weave-npc ready: true, restart count 0
May 21 11:10:09.774: INFO: sonobuoy from sonobuoy started at 2021-05-21 09:45:50 +0000 UTC (1 container statuses recorded)
May 21 11:10:09.774: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 11:10:09.774: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:10:09.774: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 21 11:10:09.774: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bb87ff89-661d-4645-a611-5dce7d6aa332 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-bb87ff89-661d-4645-a611-5dce7d6aa332 off the node p1-zq5sznp3cxrb94t9rscobyn6ny
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bb87ff89-661d-4645-a611-5dce7d6aa332
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:10:22.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7940" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:12.467 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":303,"completed":254,"skipped":4437,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:10:22.076: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:10:22.156: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 11:10:26.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-5638 --namespace=crd-publish-openapi-5638 create -f -'
May 21 11:10:26.778: INFO: stderr: ""
May 21 11:10:26.778: INFO: stdout: "e2e-test-crd-publish-openapi-4426-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 11:10:26.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-5638 --namespace=crd-publish-openapi-5638 delete e2e-test-crd-publish-openapi-4426-crds test-cr'
May 21 11:10:26.901: INFO: stderr: ""
May 21 11:10:26.901: INFO: stdout: "e2e-test-crd-publish-openapi-4426-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 21 11:10:26.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-5638 --namespace=crd-publish-openapi-5638 apply -f -'
May 21 11:10:27.281: INFO: stderr: ""
May 21 11:10:27.282: INFO: stdout: "e2e-test-crd-publish-openapi-4426-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 11:10:27.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-5638 --namespace=crd-publish-openapi-5638 delete e2e-test-crd-publish-openapi-4426-crds test-cr'
May 21 11:10:27.469: INFO: stderr: ""
May 21 11:10:27.469: INFO: stdout: "e2e-test-crd-publish-openapi-4426-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 21 11:10:27.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=crd-publish-openapi-5638 explain e2e-test-crd-publish-openapi-4426-crds'
May 21 11:10:27.741: INFO: stderr: ""
May 21 11:10:27.741: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4426-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:10:31.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5638" for this suite.

• [SLOW TEST:9.542 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":303,"completed":255,"skipped":4438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:10:31.620: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:12:31.864: INFO: Deleting pod "var-expansion-0641f778-3950-4985-846f-b0ce1e443c1d" in namespace "var-expansion-7682"
May 21 11:12:31.900: INFO: Wait up to 5m0s for pod "var-expansion-0641f778-3950-4985-846f-b0ce1e443c1d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:12:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7682" for this suite.

• [SLOW TEST:124.360 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":303,"completed":256,"skipped":4466,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:12:35.980: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:12:36.126: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4" in namespace "projected-7017" to be "Succeeded or Failed"
May 21 11:12:36.137: INFO: Pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291789ms
May 21 11:12:38.148: INFO: Pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022383689s
May 21 11:12:40.164: INFO: Pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038308588s
May 21 11:12:42.171: INFO: Pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0451281s
STEP: Saw pod success
May 21 11:12:42.171: INFO: Pod "downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4" satisfied condition "Succeeded or Failed"
May 21 11:12:42.183: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4 container client-container: <nil>
STEP: delete the pod
May 21 11:12:42.256: INFO: Waiting for pod downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4 to disappear
May 21 11:12:42.262: INFO: Pod downwardapi-volume-6c567704-d566-4c21-b3e5-65b090c61ef4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:12:42.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7017" for this suite.

• [SLOW TEST:6.325 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":257,"skipped":4474,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:12:42.306: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:12:42.494: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"012f7dc4-f3f3-4f6d-a9d4-bf62cd360c2c", Controller:(*bool)(0xc0046d5b92), BlockOwnerDeletion:(*bool)(0xc0046d5b93)}}
May 21 11:12:42.538: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e3c156a3-a237-4ff2-8366-a340893d22c3", Controller:(*bool)(0xc004b2095a), BlockOwnerDeletion:(*bool)(0xc004b2095b)}}
May 21 11:12:42.567: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"37b3487b-3119-4b21-84e1-fb1ad18786c8", Controller:(*bool)(0xc004b20bba), BlockOwnerDeletion:(*bool)(0xc004b20bbb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:12:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1847" for this suite.

• [SLOW TEST:5.450 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":303,"completed":258,"skipped":4493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:12:47.758: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:12:47.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 create -f -'
May 21 11:12:48.210: INFO: stderr: ""
May 21 11:12:48.210: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 21 11:12:48.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 create -f -'
May 21 11:12:48.621: INFO: stderr: ""
May 21 11:12:48.621: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 11:12:49.629: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:49.629: INFO: Found 0 / 1
May 21 11:12:50.634: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:50.634: INFO: Found 0 / 1
May 21 11:12:51.632: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:51.632: INFO: Found 0 / 1
May 21 11:12:52.636: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:52.636: INFO: Found 0 / 1
May 21 11:12:53.632: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:53.632: INFO: Found 1 / 1
May 21 11:12:53.632: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 11:12:53.640: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 11:12:53.640: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 11:12:53.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 describe pod agnhost-primary-jjp74'
May 21 11:12:53.780: INFO: stderr: ""
May 21 11:12:53.780: INFO: stdout: "Name:         agnhost-primary-jjp74\nNamespace:    kubectl-9553\nPriority:     0\nNode:         p1-zq5sznp3cxrb94t9rscobyn6ny/172.30.52.10\nStart Time:   Fri, 21 May 2021 11:12:48 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           172.28.96.4\nIPs:\n  IP:           172.28.96.4\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://5cbbe8fe90b3ea35a0ece1def359b8e662a1de01cab9bc44a238ecca3b49960c\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 May 2021 11:12:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qf9bn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qf9bn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qf9bn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned kubectl-9553/agnhost-primary-jjp74 to p1-zq5sznp3cxrb94t9rscobyn6ny\n  Normal  Pulling    4s    kubelet            Pulling image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\"\n  Normal  Pulled     1s    kubelet            Successfully pulled image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" in 2.939380313s\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 21 11:12:53.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 describe rc agnhost-primary'
May 21 11:12:53.921: INFO: stderr: ""
May 21 11:12:53.921: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9553\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-jjp74\n"
May 21 11:12:53.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 describe service agnhost-primary'
May 21 11:12:54.086: INFO: stderr: ""
May 21 11:12:54.086: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9553\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.101.94.252\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.28.96.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 21 11:12:54.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 describe node master-e4gtnjz75xad7zrfkuqrio7iiy'
May 21 11:12:54.295: INFO: stderr: ""
May 21 11:12:54.295: INFO: stdout: "Name:               master-e4gtnjz75xad7zrfkuqrio7iiy\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-e4gtnjz75xad7zrfkuqrio7iiy\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"driver.csi.ridge.com\":\"master-e4gtnjz75xad7zrfkuqrio7iiy\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 20 May 2021 21:58:07 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-e4gtnjz75xad7zrfkuqrio7iiy\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 May 2021 11:12:52 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 20 May 2021 22:00:00 +0000   Thu, 20 May 2021 22:00:00 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Fri, 21 May 2021 11:09:37 +0000   Thu, 20 May 2021 21:58:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 May 2021 11:09:37 +0000   Thu, 20 May 2021 21:58:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 May 2021 11:09:37 +0000   Thu, 20 May 2021 21:58:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 May 2021 11:09:37 +0000   Thu, 20 May 2021 22:00:00 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.30.52.7\n  Hostname:    master-e4gtnjz75xad7zrfkuqrio7iiy\nCapacity:\n  cpu:                2\n  ephemeral-storage:  38216108Ki\n  hugepages-2Mi:      0\n  memory:             8169072Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  35219965075\n  hugepages-2Mi:      0\n  memory:             8066672Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b12eef2a858d46af8fbed1508d311283\n  System UUID:                b12eef2a-858d-46af-8fbe-d1508d311283\n  Boot ID:                    c389259e-5af9-4809-974d-dfb1f29f2f36\n  Kernel Version:             4.19.68-coreos\n  OS Image:                   Container Linux by CoreOS 2191.5.0 (Rhyolite)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.19.11\n  Kube-Proxy Version:         v1.19.11\nPodCIDR:                      172.28.0.0/24\nPodCIDRs:                     172.28.0.0/24\nProviderID:                   ridge://e4gtnjz75xad7zrfkuqrio7iiy\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-jf75x                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         60m\n  kube-system                 coredns-f9fd979d6-fj55r                                      100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     12h\n  kube-system                 csi-ridge-node-4j2tc                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                 etcd-master-e4gtnjz75xad7zrfkuqrio7iiy                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 fluent-bit-98th6                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 kube-apiserver-master-e4gtnjz75xad7zrfkuqrio7iiy             250m (12%)    0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 kube-controller-manager-master-e4gtnjz75xad7zrfkuqrio7iiy    200m (10%)    0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 kube-proxy-2hthn                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 kube-scheduler-master-e4gtnjz75xad7zrfkuqrio7iiy             100m (5%)     0 (0%)      0 (0%)           0 (0%)         13h\n  kube-system                 meta-4j6tx                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 ridge-auth-wt4rz                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 weave-net-8kxv7                                              100m (5%)     0 (0%)      200Mi (2%)       0 (0%)         13h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-9g7q6      0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (37%)  0 (0%)\n  memory             270Mi (3%)  170Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 21 11:12:54.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-9553 describe namespace kubectl-9553'
May 21 11:12:54.412: INFO: stderr: ""
May 21 11:12:54.412: INFO: stdout: "Name:         kubectl-9553\nLabels:       e2e-framework=kubectl\n              e2e-run=cbb716f9-0954-492c-aa58-112d17dc96e2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:12:54.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9553" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":303,"completed":259,"skipped":4515,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:12:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
May 21 11:12:54.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 create -f -'
May 21 11:12:54.888: INFO: stderr: ""
May 21 11:12:54.888: INFO: stdout: "pod/pause created\n"
May 21 11:12:54.888: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 21 11:12:54.888: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-956" to be "running and ready"
May 21 11:12:54.907: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.936763ms
May 21 11:12:56.913: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025109435s
May 21 11:12:58.923: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034663321s
May 21 11:13:00.941: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.053240425s
May 21 11:13:00.941: INFO: Pod "pause" satisfied condition "running and ready"
May 21 11:13:00.941: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
May 21 11:13:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 label pods pause testing-label=testing-label-value'
May 21 11:13:01.070: INFO: stderr: ""
May 21 11:13:01.070: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 21 11:13:01.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 get pod pause -L testing-label'
May 21 11:13:01.177: INFO: stderr: ""
May 21 11:13:01.177: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 21 11:13:01.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 label pods pause testing-label-'
May 21 11:13:01.308: INFO: stderr: ""
May 21 11:13:01.308: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 21 11:13:01.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 get pod pause -L testing-label'
May 21 11:13:01.405: INFO: stderr: ""
May 21 11:13:01.405: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
May 21 11:13:01.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 delete --grace-period=0 --force -f -'
May 21 11:13:01.590: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:13:01.590: INFO: stdout: "pod \"pause\" force deleted\n"
May 21 11:13:01.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 get rc,svc -l name=pause --no-headers'
May 21 11:13:01.705: INFO: stderr: "No resources found in kubectl-956 namespace.\n"
May 21 11:13:01.705: INFO: stdout: ""
May 21 11:13:01.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-956 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 11:13:01.847: INFO: stderr: ""
May 21 11:13:01.847: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:13:01.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-956" for this suite.

• [SLOW TEST:7.436 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1305
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":303,"completed":260,"skipped":4527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:13:01.889: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-zmdk
STEP: Creating a pod to test atomic-volume-subpath
May 21 11:13:02.074: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zmdk" in namespace "subpath-2047" to be "Succeeded or Failed"
May 21 11:13:02.080: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985995ms
May 21 11:13:04.092: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017143588s
May 21 11:13:06.100: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025224705s
May 21 11:13:08.109: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034983235s
May 21 11:13:10.121: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 8.046146477s
May 21 11:13:12.133: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 10.058330982s
May 21 11:13:14.142: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 12.067958888s
May 21 11:13:16.156: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 14.081322486s
May 21 11:13:18.172: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 16.097957284s
May 21 11:13:20.190: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 18.115286458s
May 21 11:13:22.198: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 20.123986404s
May 21 11:13:24.209: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 22.134264389s
May 21 11:13:26.224: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 24.14922316s
May 21 11:13:28.236: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Running", Reason="", readiness=true. Elapsed: 26.1617644s
May 21 11:13:30.251: INFO: Pod "pod-subpath-test-configmap-zmdk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.176677449s
STEP: Saw pod success
May 21 11:13:30.251: INFO: Pod "pod-subpath-test-configmap-zmdk" satisfied condition "Succeeded or Failed"
May 21 11:13:30.261: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-subpath-test-configmap-zmdk container test-container-subpath-configmap-zmdk: <nil>
STEP: delete the pod
May 21 11:13:30.323: INFO: Waiting for pod pod-subpath-test-configmap-zmdk to disappear
May 21 11:13:30.330: INFO: Pod pod-subpath-test-configmap-zmdk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zmdk
May 21 11:13:30.330: INFO: Deleting pod "pod-subpath-test-configmap-zmdk" in namespace "subpath-2047"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:13:30.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2047" for this suite.

• [SLOW TEST:28.505 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":303,"completed":261,"skipped":4565,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:13:30.395: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0521 11:14:10.675904      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 11:15:12.744: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 21 11:15:12.745: INFO: Deleting pod "simpletest.rc-7jwjr" in namespace "gc-8199"
May 21 11:15:12.814: INFO: Deleting pod "simpletest.rc-7lgjn" in namespace "gc-8199"
May 21 11:15:12.859: INFO: Deleting pod "simpletest.rc-cjz2t" in namespace "gc-8199"
May 21 11:15:12.904: INFO: Deleting pod "simpletest.rc-gb6ch" in namespace "gc-8199"
May 21 11:15:12.954: INFO: Deleting pod "simpletest.rc-hfhms" in namespace "gc-8199"
May 21 11:15:13.035: INFO: Deleting pod "simpletest.rc-j9gzg" in namespace "gc-8199"
May 21 11:15:13.079: INFO: Deleting pod "simpletest.rc-jwwjl" in namespace "gc-8199"
May 21 11:15:13.153: INFO: Deleting pod "simpletest.rc-kls72" in namespace "gc-8199"
May 21 11:15:13.198: INFO: Deleting pod "simpletest.rc-pmvjn" in namespace "gc-8199"
May 21 11:15:13.260: INFO: Deleting pod "simpletest.rc-vqv29" in namespace "gc-8199"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:13.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8199" for this suite.

• [SLOW TEST:102.949 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":303,"completed":262,"skipped":4575,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:13.351: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:15:13.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4" in namespace "projected-7314" to be "Succeeded or Failed"
May 21 11:15:13.512: INFO: Pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48796ms
May 21 11:15:15.521: INFO: Pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016028846s
May 21 11:15:17.532: INFO: Pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026675658s
May 21 11:15:19.541: INFO: Pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036141516s
STEP: Saw pod success
May 21 11:15:19.541: INFO: Pod "downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4" satisfied condition "Succeeded or Failed"
May 21 11:15:19.547: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4 container client-container: <nil>
STEP: delete the pod
May 21 11:15:19.641: INFO: Waiting for pod downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4 to disappear
May 21 11:15:19.648: INFO: Pod downwardapi-volume-1fef15cf-42a1-46d2-9a95-d5c4176b80c4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7314" for this suite.

• [SLOW TEST:6.320 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":263,"skipped":4592,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:19.674: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:15:19.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e" in namespace "downward-api-1861" to be "Succeeded or Failed"
May 21 11:15:19.796: INFO: Pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.250458ms
May 21 11:15:21.815: INFO: Pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033710099s
May 21 11:15:23.823: INFO: Pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041548659s
May 21 11:15:25.834: INFO: Pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052693894s
STEP: Saw pod success
May 21 11:15:25.834: INFO: Pod "downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e" satisfied condition "Succeeded or Failed"
May 21 11:15:25.842: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e container client-container: <nil>
STEP: delete the pod
May 21 11:15:25.925: INFO: Waiting for pod downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e to disappear
May 21 11:15:25.937: INFO: Pod downwardapi-volume-93c6ca5f-4b3a-4734-87f9-64665b15167e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:25.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1861" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":264,"skipped":4594,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:25.976: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 11:15:26.109: INFO: Waiting up to 5m0s for pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff" in namespace "emptydir-3725" to be "Succeeded or Failed"
May 21 11:15:26.114: INFO: Pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.949012ms
May 21 11:15:28.125: INFO: Pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01589499s
May 21 11:15:30.135: INFO: Pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025349455s
May 21 11:15:32.147: INFO: Pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037152726s
STEP: Saw pod success
May 21 11:15:32.147: INFO: Pod "pod-77c3e3e4-dae1-4987-88b4-b54a48791aff" satisfied condition "Succeeded or Failed"
May 21 11:15:32.152: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-77c3e3e4-dae1-4987-88b4-b54a48791aff container test-container: <nil>
STEP: delete the pod
May 21 11:15:32.228: INFO: Waiting for pod pod-77c3e3e4-dae1-4987-88b4-b54a48791aff to disappear
May 21 11:15:32.233: INFO: Pod pod-77c3e3e4-dae1-4987-88b4-b54a48791aff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:32.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3725" for this suite.

• [SLOW TEST:6.308 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":265,"skipped":4606,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:32.287: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-37b628b3-f20c-4dbb-8fc0-78f6e00260b4
STEP: Creating a pod to test consume secrets
May 21 11:15:32.439: INFO: Waiting up to 5m0s for pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072" in namespace "secrets-4697" to be "Succeeded or Failed"
May 21 11:15:32.494: INFO: Pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072": Phase="Pending", Reason="", readiness=false. Elapsed: 55.324744ms
May 21 11:15:34.510: INFO: Pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071669965s
May 21 11:15:36.523: INFO: Pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084779454s
May 21 11:15:38.538: INFO: Pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.099191442s
STEP: Saw pod success
May 21 11:15:38.538: INFO: Pod "pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072" satisfied condition "Succeeded or Failed"
May 21 11:15:38.547: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072 container secret-volume-test: <nil>
STEP: delete the pod
May 21 11:15:38.639: INFO: Waiting for pod pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072 to disappear
May 21 11:15:38.646: INFO: Pod pod-secrets-7c23c16c-5dcf-4f14-ae61-e1ea2e400072 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4697" for this suite.

• [SLOW TEST:6.382 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":266,"skipped":4627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:38.672: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 11:15:38.764: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 11:15:38.782: INFO: Waiting for terminating namespaces to be deleted...
May 21 11:15:38.788: INFO: 
Logging pods the apiserver thinks is on node p1-maou9az9wksf7qjxozd15h7j6r before test
May 21 11:15:38.825: INFO: coredns-f9fd979d6-jnqgf from kube-system started at 2021-05-20 22:00:08 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.825: INFO: 	Container coredns ready: true, restart count 0
May 21 11:15:38.825: INFO: csi-ridge-node-76q9w from kube-system started at 2021-05-21 10:15:47 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 11:15:38.826: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 11:15:38.826: INFO: fluent-bit-w7b5s from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 11:15:38.826: INFO: kube-proxy-n6n5f from kube-system started at 2021-05-20 21:58:37 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 11:15:38.826: INFO: meta-bsns4 from kube-system started at 2021-05-21 10:13:03 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container meta ready: true, restart count 0
May 21 11:15:38.826: INFO: weave-net-jq22f from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container weave ready: true, restart count 1
May 21 11:15:38.826: INFO: 	Container weave-npc ready: true, restart count 0
May 21 11:15:38.826: INFO: sonobuoy-e2e-job-d98dbb7be59843f5 from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container e2e ready: true, restart count 0
May 21 11:15:38.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 11:15:38.826: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hv6th from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.826: INFO: 	Container sonobuoy-worker ready: false, restart count 10
May 21 11:15:38.826: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 11:15:38.826: INFO: 
Logging pods the apiserver thinks is on node p1-zq5sznp3cxrb94t9rscobyn6ny before test
May 21 11:15:38.851: INFO: csi-ridge-node-8gs7s from kube-system started at 2021-05-21 10:15:07 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.851: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 21 11:15:38.851: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
May 21 11:15:38.852: INFO: fluent-bit-v27pz from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container fluent-bit ready: true, restart count 0
May 21 11:15:38.852: INFO: kube-proxy-bf9ms from kube-system started at 2021-05-20 21:58:38 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container kube-proxy ready: true, restart count 0
May 21 11:15:38.852: INFO: meta-jwhdl from kube-system started at 2021-05-21 10:12:26 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container meta ready: true, restart count 0
May 21 11:15:38.852: INFO: weave-net-4ql9j from kube-system started at 2021-05-20 21:59:41 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container weave ready: true, restart count 1
May 21 11:15:38.852: INFO: 	Container weave-npc ready: true, restart count 0
May 21 11:15:38.852: INFO: sonobuoy from sonobuoy started at 2021-05-21 09:45:50 +0000 UTC (1 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 11:15:38.852: INFO: sonobuoy-systemd-logs-daemon-set-28a5af7fd50142be-hgwqg from sonobuoy started at 2021-05-21 09:45:53 +0000 UTC (2 container statuses recorded)
May 21 11:15:38.852: INFO: 	Container sonobuoy-worker ready: false, restart count 10
May 21 11:15:38.852: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168110bba6e82147], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168110bba87f27b5], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4657" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":303,"completed":267,"skipped":4670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:40.028: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
May 21 11:15:40.163: INFO: Waiting up to 5m0s for pod "var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3" in namespace "var-expansion-9094" to be "Succeeded or Failed"
May 21 11:15:40.179: INFO: Pod "var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.813569ms
May 21 11:15:42.192: INFO: Pod "var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029326578s
May 21 11:15:44.200: INFO: Pod "var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037011771s
STEP: Saw pod success
May 21 11:15:44.200: INFO: Pod "var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3" satisfied condition "Succeeded or Failed"
May 21 11:15:44.216: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3 container dapi-container: <nil>
STEP: delete the pod
May 21 11:15:44.277: INFO: Waiting for pod var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3 to disappear
May 21 11:15:44.295: INFO: Pod var-expansion-1ffe32e1-c1ad-4cbb-ac3a-4098fc1926c3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:44.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9094" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":303,"completed":268,"skipped":4697,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:44.331: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-8779
STEP: creating replication controller nodeport-test in namespace services-8779
I0521 11:15:44.548968      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8779, replica count: 2
I0521 11:15:47.600444      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 11:15:50.601134      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 11:15:50.601: INFO: Creating new exec pod
May 21 11:15:57.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-8779 exec execpodbfd8f -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 21 11:15:58.081: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 21 11:15:58.081: INFO: stdout: ""
May 21 11:15:58.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-8779 exec execpodbfd8f -- /bin/sh -x -c nc -zv -t -w 2 10.106.93.119 80'
May 21 11:15:58.370: INFO: stderr: "+ nc -zv -t -w 2 10.106.93.119 80\nConnection to 10.106.93.119 80 port [tcp/http] succeeded!\n"
May 21 11:15:58.370: INFO: stdout: ""
May 21 11:15:58.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-8779 exec execpodbfd8f -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.36 31829'
May 21 11:15:58.645: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.36 31829\nConnection to 172.30.52.36 31829 port [tcp/31829] succeeded!\n"
May 21 11:15:58.646: INFO: stdout: ""
May 21 11:15:58.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-8779 exec execpodbfd8f -- /bin/sh -x -c nc -zv -t -w 2 172.30.52.10 31829'
May 21 11:15:58.930: INFO: stderr: "+ nc -zv -t -w 2 172.30.52.10 31829\nConnection to 172.30.52.10 31829 port [tcp/31829] succeeded!\n"
May 21 11:15:58.930: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:15:58.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8779" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.647 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":303,"completed":269,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:15:58.979: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:15:59.996: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 11:16:02.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:16:04.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:16:06.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192560, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:16:09.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:16:09.118: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3674-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:16:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7361" for this suite.
STEP: Destroying namespace "webhook-7361-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.212 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":303,"completed":270,"skipped":4716,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:16:11.195: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-6bc1d4d7-c184-4597-b10d-48d316c876dd
STEP: Creating a pod to test consume secrets
May 21 11:16:11.370: INFO: Waiting up to 5m0s for pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12" in namespace "secrets-3236" to be "Succeeded or Failed"
May 21 11:16:11.389: INFO: Pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12": Phase="Pending", Reason="", readiness=false. Elapsed: 18.6311ms
May 21 11:16:13.396: INFO: Pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025526625s
May 21 11:16:15.403: INFO: Pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032095595s
May 21 11:16:17.413: INFO: Pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042665719s
STEP: Saw pod success
May 21 11:16:17.413: INFO: Pod "pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12" satisfied condition "Succeeded or Failed"
May 21 11:16:17.419: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12 container secret-volume-test: <nil>
STEP: delete the pod
May 21 11:16:17.481: INFO: Waiting for pod pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12 to disappear
May 21 11:16:17.493: INFO: Pod pod-secrets-a236463c-bece-43c9-9637-4a686e2faa12 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:16:17.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3236" for this suite.

• [SLOW TEST:6.328 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":271,"skipped":4722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:16:17.526: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:17:17.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7188" for this suite.

• [SLOW TEST:60.159 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":303,"completed":272,"skipped":4745,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:17:17.686: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:17:17.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a" in namespace "downward-api-6214" to be "Succeeded or Failed"
May 21 11:17:17.868: INFO: Pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.396697ms
May 21 11:17:19.881: INFO: Pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03190566s
May 21 11:17:21.893: INFO: Pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044283498s
May 21 11:17:23.905: INFO: Pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056311821s
STEP: Saw pod success
May 21 11:17:23.905: INFO: Pod "downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a" satisfied condition "Succeeded or Failed"
May 21 11:17:23.919: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a container client-container: <nil>
STEP: delete the pod
May 21 11:17:24.036: INFO: Waiting for pod downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a to disappear
May 21 11:17:24.047: INFO: Pod downwardapi-volume-5a0aeeb3-aad2-4648-8bd2-5d5c77d6d90a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:17:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6214" for this suite.

• [SLOW TEST:6.394 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":273,"skipped":4748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:17:24.084: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 11:17:24.167: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:17:33.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6763" for this suite.

• [SLOW TEST:9.374 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":303,"completed":274,"skipped":4779,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:17:33.460: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 21 11:17:33.612: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190873 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:17:33.612: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190874 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:17:33.613: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190875 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 21 11:17:43.722: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190936 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:17:43.723: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190937 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 11:17:43.723: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8561 /api/v1/namespaces/watch-8561/configmaps/e2e-watch-test-label-changed 8616bdf9-05fc-4a41-953e-047fd75114a5 190938 0 2021-05-21 11:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 11:17:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:17:43.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8561" for this suite.

• [SLOW TEST:10.305 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":303,"completed":275,"skipped":4794,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:17:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:17:43.925: INFO: Create a RollingUpdate DaemonSet
May 21 11:17:43.958: INFO: Check that daemon pods launch on every node of the cluster
May 21 11:17:43.967: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:43.967: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:43.968: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:43.977: INFO: Number of nodes with available pods: 0
May 21 11:17:43.977: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 11:17:44.994: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:44.994: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:44.994: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:45.003: INFO: Number of nodes with available pods: 0
May 21 11:17:45.003: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 11:17:45.996: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:45.996: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:45.996: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:46.004: INFO: Number of nodes with available pods: 0
May 21 11:17:46.004: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 11:17:46.994: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:46.994: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:46.994: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:47.004: INFO: Number of nodes with available pods: 0
May 21 11:17:47.004: INFO: Node p1-maou9az9wksf7qjxozd15h7j6r is running more than one daemon pod
May 21 11:17:47.993: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:47.993: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:47.993: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:48.000: INFO: Number of nodes with available pods: 2
May 21 11:17:48.000: INFO: Number of running nodes: 2, number of available pods: 2
May 21 11:17:48.000: INFO: Update the DaemonSet to trigger a rollout
May 21 11:17:48.058: INFO: Updating DaemonSet daemon-set
May 21 11:17:58.115: INFO: Roll back the DaemonSet before rollout is complete
May 21 11:17:58.144: INFO: Updating DaemonSet daemon-set
May 21 11:17:58.144: INFO: Make sure DaemonSet rollback is complete
May 21 11:17:58.155: INFO: Wrong image for pod: daemon-set-2msrw. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 11:17:58.155: INFO: Pod daemon-set-2msrw is not available
May 21 11:17:58.183: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:58.183: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:58.183: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:59.198: INFO: Wrong image for pod: daemon-set-2msrw. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 11:17:59.198: INFO: Pod daemon-set-2msrw is not available
May 21 11:17:59.222: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:59.222: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:17:59.222: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:18:00.201: INFO: Pod daemon-set-ssnjn is not available
May 21 11:18:00.212: INFO: DaemonSet pods can't tolerate node master-e4gtnjz75xad7zrfkuqrio7iiy with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:18:00.212: INFO: DaemonSet pods can't tolerate node master-qcbxs7j5yy57ux5hzemkfp73pr with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 21 11:18:00.212: INFO: DaemonSet pods can't tolerate node master-xyggfoxndyrx77igf6boa815sh with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8133, will wait for the garbage collector to delete the pods
May 21 11:18:00.311: INFO: Deleting DaemonSet.extensions daemon-set took: 24.810883ms
May 21 11:18:00.812: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.32443ms
May 21 11:18:02.721: INFO: Number of nodes with available pods: 0
May 21 11:18:02.721: INFO: Number of running nodes: 0, number of available pods: 0
May 21 11:18:02.728: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8133/daemonsets","resourceVersion":"191086"},"items":null}

May 21 11:18:02.734: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8133/pods","resourceVersion":"191086"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:18:02.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8133" for this suite.

• [SLOW TEST:19.027 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":303,"completed":276,"skipped":4805,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:18:02.797: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6785
STEP: creating service affinity-clusterip-transition in namespace services-6785
STEP: creating replication controller affinity-clusterip-transition in namespace services-6785
I0521 11:18:02.995678      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6785, replica count: 3
I0521 11:18:06.046779      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 11:18:09.047525      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 11:18:12.048253      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 11:18:12.069: INFO: Creating new exec pod
May 21 11:18:19.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6785 exec execpod-affinityw57k7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 21 11:18:19.411: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 21 11:18:19.411: INFO: stdout: ""
May 21 11:18:19.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6785 exec execpod-affinityw57k7 -- /bin/sh -x -c nc -zv -t -w 2 10.105.120.241 80'
May 21 11:18:19.683: INFO: stderr: "+ nc -zv -t -w 2 10.105.120.241 80\nConnection to 10.105.120.241 80 port [tcp/http] succeeded!\n"
May 21 11:18:19.683: INFO: stdout: ""
May 21 11:18:19.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6785 exec execpod-affinityw57k7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.120.241:80/ ; done'
May 21 11:18:20.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n"
May 21 11:18:20.073: INFO: stdout: "\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-sfk8p\naffinity-clusterip-transition-sfk8p\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-sfk8p\naffinity-clusterip-transition-sfk8p\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-6kpmc\naffinity-clusterip-transition-qk778"
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-sfk8p
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-sfk8p
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.073: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-sfk8p
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-sfk8p
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-6kpmc
May 21 11:18:20.074: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=services-6785 exec execpod-affinityw57k7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.120.241:80/ ; done'
May 21 11:18:20.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.120.241:80/\n"
May 21 11:18:20.460: INFO: stdout: "\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778\naffinity-clusterip-transition-qk778"
May 21 11:18:20.460: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Received response from host: affinity-clusterip-transition-qk778
May 21 11:18:20.461: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6785, will wait for the garbage collector to delete the pods
May 21 11:18:20.607: INFO: Deleting ReplicationController affinity-clusterip-transition took: 25.213707ms
May 21 11:18:20.707: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.466513ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:18:28.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6785" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:25.269 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":277,"skipped":4809,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:18:28.068: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-e13c350b-91a5-4cd4-bbc7-29df2618a240
STEP: Creating a pod to test consume configMaps
May 21 11:18:28.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc" in namespace "projected-2880" to be "Succeeded or Failed"
May 21 11:18:28.257: INFO: Pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.515316ms
May 21 11:18:30.264: INFO: Pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020909383s
May 21 11:18:32.272: INFO: Pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028533012s
May 21 11:18:34.293: INFO: Pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049918096s
STEP: Saw pod success
May 21 11:18:34.293: INFO: Pod "pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc" satisfied condition "Succeeded or Failed"
May 21 11:18:34.301: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 11:18:34.387: INFO: Waiting for pod pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc to disappear
May 21 11:18:34.410: INFO: Pod pod-projected-configmaps-eb1a9825-b7f7-48a5-8dad-371e8dd9e3fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:18:34.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2880" for this suite.

• [SLOW TEST:6.373 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":278,"skipped":4825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:18:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-7244eed8-8f32-4a11-b4a9-0af9d95d49b0
STEP: Creating a pod to test consume secrets
May 21 11:18:34.571: INFO: Waiting up to 5m0s for pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b" in namespace "secrets-4444" to be "Succeeded or Failed"
May 21 11:18:34.576: INFO: Pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321338ms
May 21 11:18:36.585: INFO: Pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014521258s
May 21 11:18:38.601: INFO: Pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030359003s
May 21 11:18:40.613: INFO: Pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042482573s
STEP: Saw pod success
May 21 11:18:40.613: INFO: Pod "pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b" satisfied condition "Succeeded or Failed"
May 21 11:18:40.620: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b container secret-volume-test: <nil>
STEP: delete the pod
May 21 11:18:40.707: INFO: Waiting for pod pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b to disappear
May 21 11:18:40.714: INFO: Pod pod-secrets-8baa68ae-15df-48d1-b2af-60e652fe4a6b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:18:40.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4444" for this suite.

• [SLOW TEST:6.296 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":279,"skipped":4854,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:18:40.744: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:18:40.874: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 21 11:18:40.898: INFO: Number of nodes with available pods: 0
May 21 11:18:40.898: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 21 11:18:40.976: INFO: Number of nodes with available pods: 0
May 21 11:18:40.976: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:41.983: INFO: Number of nodes with available pods: 0
May 21 11:18:41.983: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:42.985: INFO: Number of nodes with available pods: 0
May 21 11:18:42.985: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:43.992: INFO: Number of nodes with available pods: 0
May 21 11:18:43.993: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:44.984: INFO: Number of nodes with available pods: 1
May 21 11:18:44.984: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 21 11:18:45.047: INFO: Number of nodes with available pods: 1
May 21 11:18:45.047: INFO: Number of running nodes: 0, number of available pods: 1
May 21 11:18:46.057: INFO: Number of nodes with available pods: 0
May 21 11:18:46.057: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 21 11:18:46.094: INFO: Number of nodes with available pods: 0
May 21 11:18:46.094: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:47.102: INFO: Number of nodes with available pods: 0
May 21 11:18:47.102: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:48.105: INFO: Number of nodes with available pods: 0
May 21 11:18:48.105: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:49.104: INFO: Number of nodes with available pods: 0
May 21 11:18:49.104: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:50.106: INFO: Number of nodes with available pods: 0
May 21 11:18:50.106: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:51.104: INFO: Number of nodes with available pods: 0
May 21 11:18:51.104: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:52.102: INFO: Number of nodes with available pods: 0
May 21 11:18:52.102: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:53.128: INFO: Number of nodes with available pods: 0
May 21 11:18:53.128: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:54.102: INFO: Number of nodes with available pods: 0
May 21 11:18:54.102: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:55.109: INFO: Number of nodes with available pods: 0
May 21 11:18:55.109: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:56.102: INFO: Number of nodes with available pods: 0
May 21 11:18:56.102: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:57.106: INFO: Number of nodes with available pods: 0
May 21 11:18:57.106: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:58.106: INFO: Number of nodes with available pods: 0
May 21 11:18:58.107: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:18:59.102: INFO: Number of nodes with available pods: 0
May 21 11:18:59.103: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:19:00.117: INFO: Number of nodes with available pods: 0
May 21 11:19:00.117: INFO: Node p1-zq5sznp3cxrb94t9rscobyn6ny is running more than one daemon pod
May 21 11:19:01.104: INFO: Number of nodes with available pods: 1
May 21 11:19:01.104: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2055, will wait for the garbage collector to delete the pods
May 21 11:19:01.214: INFO: Deleting DaemonSet.extensions daemon-set took: 30.226458ms
May 21 11:19:01.614: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.360146ms
May 21 11:19:04.629: INFO: Number of nodes with available pods: 0
May 21 11:19:04.629: INFO: Number of running nodes: 0, number of available pods: 0
May 21 11:19:04.636: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2055/daemonsets","resourceVersion":"191562"},"items":null}

May 21 11:19:04.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2055/pods","resourceVersion":"191562"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:04.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2055" for this suite.

• [SLOW TEST:24.003 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":303,"completed":280,"skipped":4860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:04.753: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:05.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5654" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":303,"completed":281,"skipped":4902,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:05.054: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
May 21 11:19:05.139: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-794 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:05.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-794" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":303,"completed":282,"skipped":4917,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:05.299: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:19:06.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:19:08.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:19:10.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192746, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:19:13.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:13.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8259" for this suite.
STEP: Destroying namespace "webhook-8259-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.409 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":303,"completed":283,"skipped":4918,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:13.711: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 21 11:19:19.865: INFO: &Pod{ObjectMeta:{send-events-ac331dda-bdbf-45df-83b9-2bbc6009db7c  events-9378 /api/v1/namespaces/events-9378/pods/send-events-ac331dda-bdbf-45df-83b9-2bbc6009db7c 0fe990c1-64ef-4ec0-9dbd-8ea2aeda08f2 191737 0 2021-05-21 11:19:13 +0000 UTC <nil> <nil> map[name:foo time:805233561] map[] [] []  [{e2e.test Update v1 2021-05-21 11:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 11:19:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9q77l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9q77l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9q77l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:19:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:19:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.3,StartTime:2021-05-21 11:19:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 11:19:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://9fa123f245810ae5b2b3dfc8e1e4fd740e969fa437d06c844dbab22b6e3d7a79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 21 11:19:21.882: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 21 11:19:23.899: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:23.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9378" for this suite.

• [SLOW TEST:10.283 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":303,"completed":284,"skipped":4929,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:23.996: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
May 21 11:19:24.659: INFO: created pod pod-service-account-defaultsa
May 21 11:19:24.659: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 21 11:19:24.680: INFO: created pod pod-service-account-mountsa
May 21 11:19:24.680: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 21 11:19:24.712: INFO: created pod pod-service-account-nomountsa
May 21 11:19:24.712: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 21 11:19:24.736: INFO: created pod pod-service-account-defaultsa-mountspec
May 21 11:19:24.736: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 21 11:19:24.771: INFO: created pod pod-service-account-mountsa-mountspec
May 21 11:19:24.771: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 21 11:19:24.823: INFO: created pod pod-service-account-nomountsa-mountspec
May 21 11:19:24.823: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 21 11:19:24.897: INFO: created pod pod-service-account-defaultsa-nomountspec
May 21 11:19:24.897: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 21 11:19:24.948: INFO: created pod pod-service-account-mountsa-nomountspec
May 21 11:19:24.948: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 21 11:19:24.987: INFO: created pod pod-service-account-nomountsa-nomountspec
May 21 11:19:24.987: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:19:24.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-37" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":303,"completed":285,"skipped":4930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:19:25.045: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 21 11:19:25.899: INFO: Pod name wrapped-volume-race-9c2e9d6e-0aa9-47e6-97fb-58acda1482bc: Found 0 pods out of 5
May 21 11:19:30.932: INFO: Pod name wrapped-volume-race-9c2e9d6e-0aa9-47e6-97fb-58acda1482bc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9c2e9d6e-0aa9-47e6-97fb-58acda1482bc in namespace emptydir-wrapper-1558, will wait for the garbage collector to delete the pods
May 21 11:19:55.128: INFO: Deleting ReplicationController wrapped-volume-race-9c2e9d6e-0aa9-47e6-97fb-58acda1482bc took: 47.344299ms
May 21 11:19:55.628: INFO: Terminating ReplicationController wrapped-volume-race-9c2e9d6e-0aa9-47e6-97fb-58acda1482bc pods took: 500.373887ms
STEP: Creating RC which spawns configmap-volume pods
May 21 11:20:08.401: INFO: Pod name wrapped-volume-race-4e541ffa-cec4-4e39-aeb1-54a2af218896: Found 0 pods out of 5
May 21 11:20:13.561: INFO: Pod name wrapped-volume-race-4e541ffa-cec4-4e39-aeb1-54a2af218896: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4e541ffa-cec4-4e39-aeb1-54a2af218896 in namespace emptydir-wrapper-1558, will wait for the garbage collector to delete the pods
May 21 11:20:37.747: INFO: Deleting ReplicationController wrapped-volume-race-4e541ffa-cec4-4e39-aeb1-54a2af218896 took: 40.777835ms
May 21 11:20:38.347: INFO: Terminating ReplicationController wrapped-volume-race-4e541ffa-cec4-4e39-aeb1-54a2af218896 pods took: 600.248768ms
STEP: Creating RC which spawns configmap-volume pods
May 21 11:20:42.211: INFO: Pod name wrapped-volume-race-b6ed87fd-dd94-42a2-a7c6-076759affab2: Found 0 pods out of 5
May 21 11:20:47.233: INFO: Pod name wrapped-volume-race-b6ed87fd-dd94-42a2-a7c6-076759affab2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b6ed87fd-dd94-42a2-a7c6-076759affab2 in namespace emptydir-wrapper-1558, will wait for the garbage collector to delete the pods
May 21 11:21:13.430: INFO: Deleting ReplicationController wrapped-volume-race-b6ed87fd-dd94-42a2-a7c6-076759affab2 took: 36.567039ms
May 21 11:21:14.031: INFO: Terminating ReplicationController wrapped-volume-race-b6ed87fd-dd94-42a2-a7c6-076759affab2 pods took: 600.279345ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:21:28.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1558" for this suite.

• [SLOW TEST:123.658 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":303,"completed":286,"skipped":4954,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:21:28.704: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:21:30.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:21:32.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:21:34.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:21:36.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757192890, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:21:39.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:21:39.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3513" for this suite.
STEP: Destroying namespace "webhook-3513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.076 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":303,"completed":287,"skipped":4965,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:21:39.780: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1938
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 11:21:39.880: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 11:21:40.018: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 11:21:42.034: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 11:21:44.027: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 11:21:46.029: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:48.033: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:50.028: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:52.027: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:54.028: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:56.027: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:21:58.033: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 11:22:00.034: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 11:22:00.052: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 11:22:02.062: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 11:22:04.061: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 21 11:22:10.127: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.96.5:8080/dial?request=hostname&protocol=http&host=172.28.128.4&port=8080&tries=1'] Namespace:pod-network-test-1938 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 11:22:10.127: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 11:22:10.325: INFO: Waiting for responses: map[]
May 21 11:22:10.336: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.96.5:8080/dial?request=hostname&protocol=http&host=172.28.96.4&port=8080&tries=1'] Namespace:pod-network-test-1938 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 11:22:10.336: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
May 21 11:22:10.491: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:22:10.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1938" for this suite.

• [SLOW TEST:30.752 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":303,"completed":288,"skipped":4968,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:22:10.534: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 21 11:22:15.221: INFO: Successfully updated pod "adopt-release-bwhvl"
STEP: Checking that the Job readopts the Pod
May 21 11:22:15.221: INFO: Waiting up to 15m0s for pod "adopt-release-bwhvl" in namespace "job-2639" to be "adopted"
May 21 11:22:15.230: INFO: Pod "adopt-release-bwhvl": Phase="Running", Reason="", readiness=true. Elapsed: 8.91244ms
May 21 11:22:17.244: INFO: Pod "adopt-release-bwhvl": Phase="Running", Reason="", readiness=true. Elapsed: 2.022815883s
May 21 11:22:17.244: INFO: Pod "adopt-release-bwhvl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 21 11:22:17.800: INFO: Successfully updated pod "adopt-release-bwhvl"
STEP: Checking that the Job releases the Pod
May 21 11:22:17.800: INFO: Waiting up to 15m0s for pod "adopt-release-bwhvl" in namespace "job-2639" to be "released"
May 21 11:22:17.809: INFO: Pod "adopt-release-bwhvl": Phase="Running", Reason="", readiness=true. Elapsed: 9.315489ms
May 21 11:22:19.817: INFO: Pod "adopt-release-bwhvl": Phase="Running", Reason="", readiness=true. Elapsed: 2.017085588s
May 21 11:22:19.817: INFO: Pod "adopt-release-bwhvl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:22:19.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2639" for this suite.

• [SLOW TEST:9.356 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":303,"completed":289,"skipped":4976,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:22:19.892: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:22:20.062: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0b75133d-218e-4f04-9a02-7809fe8f6366" in namespace "security-context-test-5380" to be "Succeeded or Failed"
May 21 11:22:20.082: INFO: Pod "busybox-readonly-false-0b75133d-218e-4f04-9a02-7809fe8f6366": Phase="Pending", Reason="", readiness=false. Elapsed: 20.591532ms
May 21 11:22:22.098: INFO: Pod "busybox-readonly-false-0b75133d-218e-4f04-9a02-7809fe8f6366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035758725s
May 21 11:22:24.107: INFO: Pod "busybox-readonly-false-0b75133d-218e-4f04-9a02-7809fe8f6366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045117413s
May 21 11:22:24.107: INFO: Pod "busybox-readonly-false-0b75133d-218e-4f04-9a02-7809fe8f6366" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:22:24.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5380" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":303,"completed":290,"skipped":4984,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:22:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:22:24.249: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765" in namespace "security-context-test-6330" to be "Succeeded or Failed"
May 21 11:22:24.264: INFO: Pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765": Phase="Pending", Reason="", readiness=false. Elapsed: 14.130643ms
May 21 11:22:26.274: INFO: Pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024795809s
May 21 11:22:28.286: INFO: Pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03625114s
May 21 11:22:28.286: INFO: Pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765" satisfied condition "Succeeded or Failed"
May 21 11:22:28.305: INFO: Got logs for pod "busybox-privileged-false-99c19bca-e3ec-4585-bada-c5e4be9a0765": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:22:28.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6330" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":291,"skipped":4987,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:22:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:22:28.443: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:22:32.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6063" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":303,"completed":292,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:22:32.695: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-17af6935-613a-4edf-8106-ebc9b5804488 in namespace container-probe-3915
May 21 11:22:36.859: INFO: Started pod busybox-17af6935-613a-4edf-8106-ebc9b5804488 in namespace container-probe-3915
STEP: checking the pod's current state and verifying that restartCount is present
May 21 11:22:36.864: INFO: Initial restart count of pod busybox-17af6935-613a-4edf-8106-ebc9b5804488 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:26:38.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3915" for this suite.

• [SLOW TEST:245.865 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":293,"skipped":5020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:26:38.571: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 11:26:39.134: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 11:26:41.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:26:43.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193199, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 11:26:46.258: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:26:46.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2451" for this suite.
STEP: Destroying namespace "webhook-2451-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.008 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":303,"completed":294,"skipped":5077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:26:46.585: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-1aad5a25-d598-4b56-8b9e-915f2e68b33e
STEP: Creating configMap with name cm-test-opt-upd-540624f8-0922-40a5-87b2-fb9b68eaf938
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1aad5a25-d598-4b56-8b9e-915f2e68b33e
STEP: Updating configmap cm-test-opt-upd-540624f8-0922-40a5-87b2-fb9b68eaf938
STEP: Creating configMap with name cm-test-opt-create-e822e419-0408-41df-bd6e-4a04f227c2ac
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:28:16.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9765" for this suite.

• [SLOW TEST:89.525 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":295,"skipped":5103,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:28:16.110: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
May 21 11:28:16.229: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 21 11:28:16.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:16.917: INFO: stderr: ""
May 21 11:28:16.917: INFO: stdout: "service/agnhost-replica created\n"
May 21 11:28:16.918: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 21 11:28:16.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:17.276: INFO: stderr: ""
May 21 11:28:17.276: INFO: stdout: "service/agnhost-primary created\n"
May 21 11:28:17.276: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 21 11:28:17.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:17.661: INFO: stderr: ""
May 21 11:28:17.661: INFO: stdout: "service/frontend created\n"
May 21 11:28:17.662: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 21 11:28:17.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:17.908: INFO: stderr: ""
May 21 11:28:17.908: INFO: stdout: "deployment.apps/frontend created\n"
May 21 11:28:17.908: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 11:28:17.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:18.191: INFO: stderr: ""
May 21 11:28:18.191: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 21 11:28:18.192: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 11:28:18.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 create -f -'
May 21 11:28:18.645: INFO: stderr: ""
May 21 11:28:18.645: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 21 11:28:18.645: INFO: Waiting for all frontend pods to be Running.
May 21 11:28:23.696: INFO: Waiting for frontend to serve content.
May 21 11:28:23.723: INFO: Trying to add a new entry to the guestbook.
May 21 11:28:23.741: INFO: Verifying that added entry can be retrieved.
May 21 11:28:23.768: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
May 21 11:28:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:28.999: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:28.999: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 21 11:28:29.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:29.200: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:29.200: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 11:28:29.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:29.411: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:29.411: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 11:28:29.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:29.551: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:29.551: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 11:28:29.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:29.686: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:29.686: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 11:28:29.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-5623 delete --grace-period=0 --force -f -'
May 21 11:28:29.875: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 11:28:29.875: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:28:29.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5623" for this suite.

• [SLOW TEST:13.793 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":303,"completed":296,"skipped":5118,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:28:29.904: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 11:28:30.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846" in namespace "projected-3929" to be "Succeeded or Failed"
May 21 11:28:30.049: INFO: Pod "downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846": Phase="Pending", Reason="", readiness=false. Elapsed: 9.776717ms
May 21 11:28:32.056: INFO: Pod "downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016188695s
May 21 11:28:34.062: INFO: Pod "downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022504419s
STEP: Saw pod success
May 21 11:28:34.062: INFO: Pod "downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846" satisfied condition "Succeeded or Failed"
May 21 11:28:34.069: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846 container client-container: <nil>
STEP: delete the pod
May 21 11:28:34.140: INFO: Waiting for pod downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846 to disappear
May 21 11:28:34.147: INFO: Pod downwardapi-volume-120278fe-ba7d-4aee-b430-36c2fce19846 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:28:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3929" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":297,"skipped":5118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:28:34.173: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 11:28:40.858: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7ea153db-6659-42b5-8574-defad009bf49"
May 21 11:28:40.859: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7ea153db-6659-42b5-8574-defad009bf49" in namespace "pods-3057" to be "terminated due to deadline exceeded"
May 21 11:28:40.868: INFO: Pod "pod-update-activedeadlineseconds-7ea153db-6659-42b5-8574-defad009bf49": Phase="Running", Reason="", readiness=true. Elapsed: 9.460494ms
May 21 11:28:42.877: INFO: Pod "pod-update-activedeadlineseconds-7ea153db-6659-42b5-8574-defad009bf49": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.018670224s
May 21 11:28:42.878: INFO: Pod "pod-update-activedeadlineseconds-7ea153db-6659-42b5-8574-defad009bf49" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:28:42.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3057" for this suite.

• [SLOW TEST:8.740 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":303,"completed":298,"skipped":5141,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:28:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 11:28:42.994: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 21 11:28:43.035: INFO: Pod name sample-pod: Found 0 pods out of 1
May 21 11:28:48.046: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 11:28:50.073: INFO: Creating deployment "test-rolling-update-deployment"
May 21 11:28:50.097: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 21 11:28:50.119: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 21 11:28:52.141: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 21 11:28:52.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193330, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193330, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193330, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757193330, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 11:28:54.169: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 11:28:54.193: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1413 /apis/apps/v1/namespaces/deployment-1413/deployments/test-rolling-update-deployment de788cfa-2928-4db5-bc94-cc653f6096d8 195421 1 2021-05-21 11:28:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-21 11:28:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 11:28:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e44f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 11:28:50 +0000 UTC,LastTransitionTime:2021-05-21 11:28:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-05-21 11:28:52 +0000 UTC,LastTransitionTime:2021-05-21 11:28:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 11:28:54.202: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-1413 /apis/apps/v1/namespaces/deployment-1413/replicasets/test-rolling-update-deployment-c4cb8d6d9 02088192-14ab-47dc-b579-3468579354e2 195410 1 2021-05-21 11:28:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment de788cfa-2928-4db5-bc94-cc653f6096d8 0xc004e45510 0xc004e45511}] []  [{kube-controller-manager Update apps/v1 2021-05-21 11:28:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de788cfa-2928-4db5-bc94-cc653f6096d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e45588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 11:28:54.202: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 21 11:28:54.202: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1413 /apis/apps/v1/namespaces/deployment-1413/replicasets/test-rolling-update-controller f592d7f1-da07-4d1d-a84b-12a4a79c6679 195420 2 2021-05-21 11:28:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment de788cfa-2928-4db5-bc94-cc653f6096d8 0xc004e45407 0xc004e45408}] []  [{e2e.test Update apps/v1 2021-05-21 11:28:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 11:28:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de788cfa-2928-4db5-bc94-cc653f6096d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e454a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 11:28:54.213: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-w9p29" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-w9p29 test-rolling-update-deployment-c4cb8d6d9- deployment-1413 /api/v1/namespaces/deployment-1413/pods/test-rolling-update-deployment-c4cb8d6d9-w9p29 57f15553-6978-44ab-a450-c3d6bdd50ea8 195409 0 2021-05-21 11:28:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 02088192-14ab-47dc-b579-3468579354e2 0xc005f69020 0xc005f69021}] []  [{kube-controller-manager Update v1 2021-05-21 11:28:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02088192-14ab-47dc-b579-3468579354e2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 11:28:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z55z9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z55z9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z55z9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-zq5sznp3cxrb94t9rscobyn6ny,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:28:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:28:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:28:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 11:28:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.30.52.10,PodIP:172.28.96.4,StartTime:2021-05-21 11:28:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 11:28:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://d38c73afc85086f8a774be9fbd49492860e018a090e9523e46785ca0da746e6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:28:54.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1413" for this suite.

• [SLOW TEST:11.331 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":299,"skipped":5156,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:28:54.247: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 11:29:02.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 11:29:02.525: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 11:29:04.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 11:29:04.535: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 11:29:06.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 11:29:06.535: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 11:29:08.525: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 11:29:08.535: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:29:08.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6384" for this suite.

• [SLOW TEST:14.330 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":303,"completed":300,"skipped":5156,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:29:08.580: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
May 21 11:29:08.689: INFO: created test-pod-1
May 21 11:29:08.718: INFO: created test-pod-2
May 21 11:29:08.752: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:29:08.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5383" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":303,"completed":301,"skipped":5173,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:29:08.919: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-3e128c72-ccc8-4622-a4d6-bdd2e3248b82
STEP: Creating a pod to test consume configMaps
May 21 11:29:09.034: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c" in namespace "projected-5741" to be "Succeeded or Failed"
May 21 11:29:09.057: INFO: Pod "pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.333662ms
May 21 11:29:11.067: INFO: Pod "pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032498151s
May 21 11:29:13.080: INFO: Pod "pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045488923s
STEP: Saw pod success
May 21 11:29:13.080: INFO: Pod "pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c" satisfied condition "Succeeded or Failed"
May 21 11:29:13.090: INFO: Trying to get logs from node p1-zq5sznp3cxrb94t9rscobyn6ny pod pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 11:29:13.183: INFO: Waiting for pod pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c to disappear
May 21 11:29:13.196: INFO: Pod pod-projected-configmaps-960d902b-cc22-40b2-ab31-168a5050c23c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:29:13.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5741" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":302,"skipped":5178,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 11:29:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-234519559
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 11:29:13.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7013 run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine'
May 21 11:29:13.508: INFO: stderr: ""
May 21 11:29:13.508: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
May 21 11:29:13.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-234519559 --namespace=kubectl-7013 delete pods e2e-test-httpd-pod'
May 21 11:29:16.601: INFO: stderr: ""
May 21 11:29:16.601: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 11:29:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7013" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":303,"completed":303,"skipped":5180,"failed":0}
SMay 21 11:29:16.639: INFO: Running AfterSuite actions on all nodes
May 21 11:29:16.639: INFO: Running AfterSuite actions on node 1
May 21 11:29:16.639: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":303,"completed":303,"skipped":5181,"failed":0}

Ran 303 of 5484 Specs in 6197.871 seconds
SUCCESS! -- 303 Passed | 0 Failed | 0 Pending | 5181 Skipped
PASS

Ginkgo ran 1 suite in 1h43m19.940525162s
Test Suite Passed
