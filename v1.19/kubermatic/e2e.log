I0212 14:03:45.339801      21 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-249037184
I0212 14:03:45.340084      21 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0212 14:03:45.340294      21 e2e.go:129] Starting e2e run "09567d32-68f5-452f-9ff0-5bc8afcf3d4c" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1613138623 - Will randomize all specs
Will run 305 of 5233 specs

Feb 12 14:03:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:03:45.352: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 12 14:03:45.380: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 12 14:03:45.432: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 12 14:03:45.432: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Feb 12 14:03:45.432: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 12 14:03:45.449: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 12 14:03:45.449: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 12 14:03:45.449: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'logrotate' (0 seconds elapsed)
Feb 12 14:03:45.449: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 12 14:03:45.449: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Feb 12 14:03:45.450: INFO: e2e test version: v1.19.3
Feb 12 14:03:45.452: INFO: kube-apiserver version: v1.19.3
Feb 12 14:03:45.452: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:03:45.462: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:03:45.464: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename namespaces
Feb 12 14:03:45.523: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Feb 12 14:03:45.542: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:03:51.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8626" for this suite.
STEP: Destroying namespace "nsdeletetest-631" for this suite.
Feb 12 14:03:51.751: INFO: Namespace nsdeletetest-631 was already deleted
STEP: Destroying namespace "nsdeletetest-9286" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:03:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-9eda48ed-6531-4eb9-bf97-14d850f4e7b9
STEP: Creating a pod to test consume secrets
Feb 12 14:03:51.870: INFO: Waiting up to 5m0s for pod "pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f" in namespace "secrets-6862" to be "Succeeded or Failed"
Feb 12 14:03:51.879: INFO: Pod "pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14459ms
Feb 12 14:03:53.889: INFO: Pod "pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018974203s
Feb 12 14:03:55.901: INFO: Pod "pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030927756s
STEP: Saw pod success
Feb 12 14:03:55.901: INFO: Pod "pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f" satisfied condition "Succeeded or Failed"
Feb 12 14:03:55.907: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:03:55.981: INFO: Waiting for pod pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f to disappear
Feb 12 14:03:55.987: INFO: Pod pod-secrets-b22b806a-1321-46e3-a537-d28c23ec3f6f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:03:55.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6862" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":16,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:03:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1953
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Feb 12 14:03:56.112: INFO: Found 0 stateful pods, waiting for 3
Feb 12 14:04:06.123: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:04:06.123: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:04:06.123: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:04:06.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-1953 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:04:06.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:04:06.879: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:04:06.879: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 12 14:04:16.938: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 12 14:04:26.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-1953 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:04:27.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:04:27.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:04:27.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:04:47.649: INFO: Waiting for StatefulSet statefulset-1953/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 12 14:04:57.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-1953 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:04:58.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:04:58.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:04:58.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:05:08.321: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 12 14:05:18.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-1953 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:05:18.921: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:05:18.921: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:05:18.921: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:05:28.969: INFO: Waiting for StatefulSet statefulset-1953/ss2 to complete update
Feb 12 14:05:28.969: INFO: Waiting for Pod statefulset-1953/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 12 14:05:28.969: INFO: Waiting for Pod statefulset-1953/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 12 14:05:28.969: INFO: Waiting for Pod statefulset-1953/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 12 14:05:38.986: INFO: Waiting for StatefulSet statefulset-1953/ss2 to complete update
Feb 12 14:05:38.987: INFO: Waiting for Pod statefulset-1953/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 12 14:05:48.984: INFO: Waiting for StatefulSet statefulset-1953/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 14:05:58.987: INFO: Deleting all statefulset in ns statefulset-1953
Feb 12 14:05:58.992: INFO: Scaling statefulset ss2 to 0
Feb 12 14:06:19.022: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:06:19.028: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:19.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1953" for this suite.

• [SLOW TEST:143.081 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":3,"skipped":16,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-mkgh
STEP: Creating a pod to test atomic-volume-subpath
Feb 12 14:06:19.200: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mkgh" in namespace "subpath-4176" to be "Succeeded or Failed"
Feb 12 14:06:19.205: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989092ms
Feb 12 14:06:21.213: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 2.012685016s
Feb 12 14:06:23.220: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 4.020236722s
Feb 12 14:06:25.226: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 6.026145885s
Feb 12 14:06:27.236: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 8.035955575s
Feb 12 14:06:29.248: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 10.04833075s
Feb 12 14:06:31.258: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 12.057946338s
Feb 12 14:06:33.264: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 14.064034844s
Feb 12 14:06:35.275: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 16.075121199s
Feb 12 14:06:37.282: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 18.081700586s
Feb 12 14:06:39.292: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 20.092535943s
Feb 12 14:06:41.300: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Running", Reason="", readiness=true. Elapsed: 22.100226699s
Feb 12 14:06:43.308: INFO: Pod "pod-subpath-test-configmap-mkgh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.108248561s
STEP: Saw pod success
Feb 12 14:06:43.308: INFO: Pod "pod-subpath-test-configmap-mkgh" satisfied condition "Succeeded or Failed"
Feb 12 14:06:43.314: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-subpath-test-configmap-mkgh container test-container-subpath-configmap-mkgh: <nil>
STEP: delete the pod
Feb 12 14:06:43.361: INFO: Waiting for pod pod-subpath-test-configmap-mkgh to disappear
Feb 12 14:06:43.366: INFO: Pod pod-subpath-test-configmap-mkgh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mkgh
Feb 12 14:06:43.366: INFO: Deleting pod "pod-subpath-test-configmap-mkgh" in namespace "subpath-4176"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:43.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4176" for this suite.

• [SLOW TEST:24.316 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":4,"skipped":29,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:06:44.120: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:06:46.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735604, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735604, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735604, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735604, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:06:49.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:49.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-286" for this suite.
STEP: Destroying namespace "webhook-286-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.299 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":5,"skipped":34,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:49.706: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-007f45c3-ea51-4b5f-9987-3be58443f50f
STEP: Creating a pod to test consume secrets
Feb 12 14:06:49.804: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82" in namespace "projected-4944" to be "Succeeded or Failed"
Feb 12 14:06:49.822: INFO: Pod "pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82": Phase="Pending", Reason="", readiness=false. Elapsed: 18.37575ms
Feb 12 14:06:51.836: INFO: Pod "pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031570024s
STEP: Saw pod success
Feb 12 14:06:51.836: INFO: Pod "pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82" satisfied condition "Succeeded or Failed"
Feb 12 14:06:51.843: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:06:51.923: INFO: Waiting for pod pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82 to disappear
Feb 12 14:06:51.928: INFO: Pod pod-projected-secrets-1825f354-b2f6-4944-aa5e-7e5ce6440b82 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:51.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4944" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":34,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:51.954: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:06:52.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647" in namespace "projected-4540" to be "Succeeded or Failed"
Feb 12 14:06:52.041: INFO: Pod "downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211633ms
Feb 12 14:06:54.049: INFO: Pod "downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017304419s
STEP: Saw pod success
Feb 12 14:06:54.049: INFO: Pod "downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647" satisfied condition "Succeeded or Failed"
Feb 12 14:06:54.055: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647 container client-container: <nil>
STEP: delete the pod
Feb 12 14:06:54.105: INFO: Waiting for pod downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647 to disappear
Feb 12 14:06:54.112: INFO: Pod downwardapi-volume-a22abf84-3155-4b9b-9b45-9ae2c3d33647 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:54.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4540" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":46,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:54.130: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:06:54.217: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3d6587ac-8445-442d-bf7e-c4bf8b4be63f" in namespace "security-context-test-7947" to be "Succeeded or Failed"
Feb 12 14:06:54.226: INFO: Pod "busybox-user-65534-3d6587ac-8445-442d-bf7e-c4bf8b4be63f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.68806ms
Feb 12 14:06:56.233: INFO: Pod "busybox-user-65534-3d6587ac-8445-442d-bf7e-c4bf8b4be63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015953236s
Feb 12 14:06:58.242: INFO: Pod "busybox-user-65534-3d6587ac-8445-442d-bf7e-c4bf8b4be63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025242499s
Feb 12 14:06:58.242: INFO: Pod "busybox-user-65534-3d6587ac-8445-442d-bf7e-c4bf8b4be63f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:06:58.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7947" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":74,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:06:58.269: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-454
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 12 14:06:58.328: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 14:06:58.400: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 14:07:00.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:07:02.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:07:04.408: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:07:06.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:07:08.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:07:10.414: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 12 14:07:10.425: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 12 14:07:10.438: INFO: The status of Pod netserver-2 is Running (Ready = false)
Feb 12 14:07:12.449: INFO: The status of Pod netserver-2 is Running (Ready = false)
Feb 12 14:07:14.446: INFO: The status of Pod netserver-2 is Running (Ready = false)
Feb 12 14:07:16.444: INFO: The status of Pod netserver-2 is Running (Ready = false)
Feb 12 14:07:18.445: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 12 14:07:22.500: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.67:8080/dial?request=hostname&protocol=udp&host=172.25.0.66&port=8081&tries=1'] Namespace:pod-network-test-454 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:07:22.500: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:07:22.984: INFO: Waiting for responses: map[]
Feb 12 14:07:22.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.67:8080/dial?request=hostname&protocol=udp&host=172.25.2.65&port=8081&tries=1'] Namespace:pod-network-test-454 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:07:22.990: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:07:23.548: INFO: Waiting for responses: map[]
Feb 12 14:07:23.554: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.67:8080/dial?request=hostname&protocol=udp&host=172.25.1.117&port=8081&tries=1'] Namespace:pod-network-test-454 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:07:23.554: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:07:24.113: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:07:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-454" for this suite.

• [SLOW TEST:25.870 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:07:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Feb 12 14:09:24.773: INFO: Successfully updated pod "var-expansion-c74912e9-9d05-4ab8-acb0-0e3ef56b14c7"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb 12 14:09:26.788: INFO: Deleting pod "var-expansion-c74912e9-9d05-4ab8-acb0-0e3ef56b14c7" in namespace "var-expansion-3160"
Feb 12 14:09:26.806: INFO: Wait up to 5m0s for pod "var-expansion-c74912e9-9d05-4ab8-acb0-0e3ef56b14c7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:09:58.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3160" for this suite.

• [SLOW TEST:154.701 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":10,"skipped":124,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:09:58.843: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-387
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-387
STEP: Deleting pre-stop pod
Feb 12 14:10:10.081: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:10:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-387" for this suite.

• [SLOW TEST:11.275 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":11,"skipped":126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:10:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:10:10.194: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:10:12.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4469" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":199,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:10:12.288: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 12 14:10:12.367: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:10:18.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9321" for this suite.

• [SLOW TEST:6.154 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":200,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:10:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:10:18.502: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Creating first CR 
Feb 12 14:10:19.206: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:19Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:19Z]] name:name1 resourceVersion:53510 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1917a7a4-dfe3-4f6a-a641-8b0e381fccc7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 12 14:10:29.218: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:29Z]] name:name2 resourceVersion:53570 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:582203d3-1862-4a86-86bf-45ea565d85bc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 12 14:10:39.232: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:39Z]] name:name1 resourceVersion:53619 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1917a7a4-dfe3-4f6a-a641-8b0e381fccc7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 12 14:10:49.248: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:49Z]] name:name2 resourceVersion:53668 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:582203d3-1862-4a86-86bf-45ea565d85bc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 12 14:10:59.265: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:39Z]] name:name1 resourceVersion:53718 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1917a7a4-dfe3-4f6a-a641-8b0e381fccc7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 12 14:11:09.282: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-12T14:10:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-12T14:10:49Z]] name:name2 resourceVersion:53766 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:582203d3-1862-4a86-86bf-45ea565d85bc] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:11:19.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8119" for this suite.

• [SLOW TEST:61.390 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":14,"skipped":214,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:11:19.838: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 12 14:11:19.950: INFO: Waiting up to 5m0s for pod "pod-3163ae77-4c0f-4163-a38a-a1e3841c600e" in namespace "emptydir-2156" to be "Succeeded or Failed"
Feb 12 14:11:19.957: INFO: Pod "pod-3163ae77-4c0f-4163-a38a-a1e3841c600e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.632443ms
Feb 12 14:11:21.965: INFO: Pod "pod-3163ae77-4c0f-4163-a38a-a1e3841c600e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014985978s
STEP: Saw pod success
Feb 12 14:11:21.965: INFO: Pod "pod-3163ae77-4c0f-4163-a38a-a1e3841c600e" satisfied condition "Succeeded or Failed"
Feb 12 14:11:21.970: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-3163ae77-4c0f-4163-a38a-a1e3841c600e container test-container: <nil>
STEP: delete the pod
Feb 12 14:11:22.054: INFO: Waiting for pod pod-3163ae77-4c0f-4163-a38a-a1e3841c600e to disappear
Feb 12 14:11:22.064: INFO: Pod pod-3163ae77-4c0f-4163-a38a-a1e3841c600e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:11:22.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2156" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":222,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:11:22.089: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6562
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6562
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6562
Feb 12 14:11:22.197: INFO: Found 0 stateful pods, waiting for 1
Feb 12 14:11:32.208: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 12 14:11:32.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:11:32.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:11:32.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:11:32.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:11:32.827: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 12 14:11:42.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:11:42.835: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:11:42.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999737s
Feb 12 14:11:43.879: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993390006s
Feb 12 14:11:44.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983036426s
Feb 12 14:11:45.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.975438142s
Feb 12 14:11:46.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.968718734s
Feb 12 14:11:47.907: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961021659s
Feb 12 14:11:48.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.95441031s
Feb 12 14:11:49.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946798848s
Feb 12 14:11:50.936: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.939012348s
Feb 12 14:11:51.953: INFO: Verifying statefulset ss doesn't scale past 1 for another 925.934365ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6562
Feb 12 14:11:52.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:11:53.573: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:11:53.573: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:11:53.573: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:11:53.581: INFO: Found 1 stateful pods, waiting for 3
Feb 12 14:12:03.590: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:12:03.590: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:12:03.590: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 12 14:12:03.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:12:04.211: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:12:04.211: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:12:04.211: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:12:04.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:12:04.883: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:12:04.883: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:12:04.883: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:12:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:12:05.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:12:05.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:12:05.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:12:05.512: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:12:05.517: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 12 14:12:15.536: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:12:15.536: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:12:15.536: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:12:15.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999676s
Feb 12 14:12:16.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976505963s
Feb 12 14:12:17.685: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969497657s
Feb 12 14:12:18.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961028022s
Feb 12 14:12:19.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954344962s
Feb 12 14:12:20.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947158281s
Feb 12 14:12:21.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939674937s
Feb 12 14:12:22.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930873373s
Feb 12 14:12:23.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.924072402s
Feb 12 14:12:24.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.666707ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6562
Feb 12 14:12:25.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:12:26.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:12:26.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:12:26.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:12:26.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:12:26.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:12:26.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:12:26.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:12:26.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-6562 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:12:27.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:12:27.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:12:27.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:12:27.603: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 14:12:47.633: INFO: Deleting all statefulset in ns statefulset-6562
Feb 12 14:12:47.639: INFO: Scaling statefulset ss to 0
Feb 12 14:12:47.663: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:12:47.672: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:12:47.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6562" for this suite.

• [SLOW TEST:85.627 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":16,"skipped":242,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:12:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0212 14:12:54.093030      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 14:12:54.093053      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 14:12:54.093058      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 12 14:12:54.093: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:12:54.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2295" for this suite.

• [SLOW TEST:6.389 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":17,"skipped":256,"failed":0}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:12:54.110: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:12:54.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-5779'
Feb 12 14:12:54.555: INFO: stderr: ""
Feb 12 14:12:54.555: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 12 14:12:54.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-5779'
Feb 12 14:12:54.864: INFO: stderr: ""
Feb 12 14:12:54.864: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 12 14:12:55.873: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:12:55.873: INFO: Found 0 / 1
Feb 12 14:12:56.872: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:12:56.872: INFO: Found 0 / 1
Feb 12 14:12:57.872: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:12:57.873: INFO: Found 1 / 1
Feb 12 14:12:57.873: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 12 14:12:57.880: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:12:57.880: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 14:12:57.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 describe pod agnhost-primary-rcfgp --namespace=kubectl-5779'
Feb 12 14:12:57.982: INFO: stderr: ""
Feb 12 14:12:57.982: INFO: stdout: "Name:         agnhost-primary-rcfgp\nNamespace:    kubectl-5779\nPriority:     0\nNode:         modest-nightingale-745f6d5bd4-h2tct/104.248.253.154\nStart Time:   Fri, 12 Feb 2021 14:12:54 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.25.0.76/32\nStatus:       Running\nIP:           172.25.0.76\nIPs:\n  IP:           172.25.0.76\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://3393b70f6cdd611a94f7e57722d90ace5857a1729da129b4bfd6110bc8c1e4c0\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 12 Feb 2021 14:12:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-d7gsg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-d7gsg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-d7gsg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5779/agnhost-primary-rcfgp to modest-nightingale-745f6d5bd4-h2tct\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Feb 12 14:12:57.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 describe rc agnhost-primary --namespace=kubectl-5779'
Feb 12 14:12:58.102: INFO: stderr: ""
Feb 12 14:12:58.102: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5779\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-rcfgp\n"
Feb 12 14:12:58.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 describe service agnhost-primary --namespace=kubectl-5779'
Feb 12 14:12:58.199: INFO: stderr: ""
Feb 12 14:12:58.199: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5779\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.240.31.229\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.0.76:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 12 14:12:58.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 describe node modest-nightingale-745f6d5bd4-h2tct'
Feb 12 14:12:58.337: INFO: stderr: ""
Feb 12 14:12:58.337: INFO: stdout: "Name:               modest-nightingale-745f6d5bd4-h2tct\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=modest-nightingale-745f6d5bd4-h2tct\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=e01e39e0-9375-494e-8d5e-a039d20dd624\n                    system/cluster=k9t6zzgwn2\n                    system/project=kr6n49b5rm\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        cluster.k8s.io/machine: kube-system/modest-nightingale-745f6d5bd4-h2tct\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"9a:74:cf:85:21:90\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 104.248.253.154\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 12 Feb 2021 12:16:37 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  modest-nightingale-745f6d5bd4-h2tct\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 12 Feb 2021 14:12:56 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 12 Feb 2021 14:09:18 +0000   Fri, 12 Feb 2021 12:16:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 12 Feb 2021 14:09:18 +0000   Fri, 12 Feb 2021 12:16:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 12 Feb 2021 14:09:18 +0000   Fri, 12 Feb 2021 12:16:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 12 Feb 2021 14:09:18 +0000   Fri, 12 Feb 2021 12:16:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  104.248.253.154\n  Hostname:    modest-nightingale-745f6d5bd4-h2tct\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    25226960Ki\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               4039256Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1600m\n  ephemeral-storage:    21101682650\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               3527256Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 b41c68f6ce884300aeaa9322dffd8e1d\n  System UUID:                B41C68F6-CE88-4300-AEAA-9322DFFD8E1D\n  Boot ID:                    4939f452-a338-495a-be85-8fedb3aaa3b8\n  Kernel Version:             4.15.0-121-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.12\n  Kubelet Version:            v1.19.3\n  Kube-Proxy Version:         v1.19.3\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-zprb7                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         116m\n  kube-system                 kube-proxy-wl68v                                           75m (4%)      250m (15%)  50Mi (1%)        250Mi (7%)     116m\n  kube-system                 logrotate-gqknn                                            75m (4%)      250m (15%)  50Mi (1%)        250Mi (7%)     36m\n  kube-system                 node-local-dns-s4mdg                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m\n  kube-system                 user-ssh-keys-agent-tm2rj                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  kubectl-5779                agnhost-primary-rcfgp                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m19s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m18s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  400m (25%)  500m (31%)\n  memory               100Mi (2%)  500Mi (14%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:                <none>\n"
Feb 12 14:12:58.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 describe namespace kubectl-5779'
Feb 12 14:12:58.434: INFO: stderr: ""
Feb 12 14:12:58.434: INFO: stdout: "Name:         kubectl-5779\nLabels:       e2e-framework=kubectl\n              e2e-run=09567d32-68f5-452f-9ff0-5bc8afcf3d4c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:12:58.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5779" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":18,"skipped":256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:12:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0212 14:12:59.651011      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 14:12:59.651109      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 14:12:59.651140      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 12 14:12:59.651: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:12:59.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2846" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":19,"skipped":281,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:12:59.677: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 12 14:13:01.815: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:01.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2310" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":295,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:01.877: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7544.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7544.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7544.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7544.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:13:08.723: INFO: DNS probes using dns-7544/dns-test-c4750c11-d0c0-400e-bbc3-398cb3026d21 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:08.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7544" for this suite.

• [SLOW TEST:6.898 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":21,"skipped":299,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:08.779: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:08.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3318" for this suite.
STEP: Destroying namespace "nspatchtest-4ce84872-825e-4838-bd11-259c4ac1da11-6615" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":22,"skipped":314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:08.971: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:13:09.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:13:11.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735989, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735989, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735989, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748735989, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:13:14.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 12 14:13:16.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 attach --namespace=webhook-2216 to-be-attached-pod -i -c=container1'
Feb 12 14:13:17.060: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:17.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2216" for this suite.
STEP: Destroying namespace "webhook-2216-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":23,"skipped":346,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:17.211: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-78mb
STEP: Creating a pod to test atomic-volume-subpath
Feb 12 14:13:17.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-78mb" in namespace "subpath-8030" to be "Succeeded or Failed"
Feb 12 14:13:17.386: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.51312ms
Feb 12 14:13:19.394: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021088402s
Feb 12 14:13:21.402: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 4.029087147s
Feb 12 14:13:23.410: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 6.036593937s
Feb 12 14:13:25.417: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 8.044553177s
Feb 12 14:13:27.429: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 10.056146348s
Feb 12 14:13:29.443: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 12.069890365s
Feb 12 14:13:31.452: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 14.079102968s
Feb 12 14:13:33.460: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 16.086892641s
Feb 12 14:13:35.468: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 18.09521867s
Feb 12 14:13:37.477: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 20.103784965s
Feb 12 14:13:39.485: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Running", Reason="", readiness=true. Elapsed: 22.111981297s
Feb 12 14:13:41.491: INFO: Pod "pod-subpath-test-downwardapi-78mb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117652998s
STEP: Saw pod success
Feb 12 14:13:41.491: INFO: Pod "pod-subpath-test-downwardapi-78mb" satisfied condition "Succeeded or Failed"
Feb 12 14:13:41.497: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-subpath-test-downwardapi-78mb container test-container-subpath-downwardapi-78mb: <nil>
STEP: delete the pod
Feb 12 14:13:41.547: INFO: Waiting for pod pod-subpath-test-downwardapi-78mb to disappear
Feb 12 14:13:41.554: INFO: Pod pod-subpath-test-downwardapi-78mb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-78mb
Feb 12 14:13:41.554: INFO: Deleting pod "pod-subpath-test-downwardapi-78mb" in namespace "subpath-8030"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:41.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8030" for this suite.

• [SLOW TEST:24.373 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":24,"skipped":353,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:41.585: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:13:41.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99" in namespace "downward-api-8001" to be "Succeeded or Failed"
Feb 12 14:13:41.678: INFO: Pod "downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670273ms
Feb 12 14:13:43.685: INFO: Pod "downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012727439s
STEP: Saw pod success
Feb 12 14:13:43.685: INFO: Pod "downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99" satisfied condition "Succeeded or Failed"
Feb 12 14:13:43.691: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99 container client-container: <nil>
STEP: delete the pod
Feb 12 14:13:43.758: INFO: Waiting for pod downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99 to disappear
Feb 12 14:13:43.763: INFO: Pod downwardapi-volume-0d548258-bdfe-4bd5-8942-5270b380ee99 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:13:43.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8001" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":25,"skipped":370,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:13:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-685b5d7b-025b-48e0-a36a-26db146bd303 in namespace container-probe-173
Feb 12 14:13:47.875: INFO: Started pod busybox-685b5d7b-025b-48e0-a36a-26db146bd303 in namespace container-probe-173
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:13:47.880: INFO: Initial restart count of pod busybox-685b5d7b-025b-48e0-a36a-26db146bd303 is 0
Feb 12 14:14:36.094: INFO: Restart count of pod container-probe-173/busybox-685b5d7b-025b-48e0-a36a-26db146bd303 is now 1 (48.213901187s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:14:36.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-173" for this suite.

• [SLOW TEST:52.352 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":372,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:14:36.139: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Feb 12 14:14:36.208: INFO: Waiting up to 5m0s for pod "client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3" in namespace "containers-6154" to be "Succeeded or Failed"
Feb 12 14:14:36.213: INFO: Pod "client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.557753ms
Feb 12 14:14:38.219: INFO: Pod "client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011437993s
STEP: Saw pod success
Feb 12 14:14:38.219: INFO: Pod "client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3" satisfied condition "Succeeded or Failed"
Feb 12 14:14:38.228: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3 container test-container: <nil>
STEP: delete the pod
Feb 12 14:14:38.396: INFO: Waiting for pod client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3 to disappear
Feb 12 14:14:38.406: INFO: Pod client-containers-171cf990-6680-45bd-a8c1-de22b3ae74c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:14:38.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6154" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:14:38.428: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 12 14:14:38.506: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:14:42.011: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:14:55.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2093" for this suite.

• [SLOW TEST:17.330 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":28,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:14:55.767: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:14:58.071: INFO: DNS probes using dns-test-14268cdd-2867-475e-b01a-719ff68cf6de succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:15:00.295: INFO: File wheezy_udp@dns-test-service-3.dns-6449.svc.cluster.local from pod  dns-6449/dns-test-1a79813b-d8f2-4a80-823b-07b0396c1f76 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 12 14:15:00.382: INFO: File jessie_udp@dns-test-service-3.dns-6449.svc.cluster.local from pod  dns-6449/dns-test-1a79813b-d8f2-4a80-823b-07b0396c1f76 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 12 14:15:00.382: INFO: Lookups using dns-6449/dns-test-1a79813b-d8f2-4a80-823b-07b0396c1f76 failed for: [wheezy_udp@dns-test-service-3.dns-6449.svc.cluster.local jessie_udp@dns-test-service-3.dns-6449.svc.cluster.local]

Feb 12 14:15:05.552: INFO: DNS probes using dns-test-1a79813b-d8f2-4a80-823b-07b0396c1f76 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6449.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6449.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:15:07.961: INFO: DNS probes using dns-test-1b009612-fc6f-4f29-a7ca-a0ac04865350 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:15:08.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6449" for this suite.

• [SLOW TEST:12.266 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":29,"skipped":480,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:15:08.033: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Feb 12 14:15:08.131: INFO: Waiting up to 5m0s for pod "var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76" in namespace "var-expansion-7302" to be "Succeeded or Failed"
Feb 12 14:15:08.137: INFO: Pod "var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626965ms
Feb 12 14:15:10.144: INFO: Pod "var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012593402s
Feb 12 14:15:12.156: INFO: Pod "var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025019938s
STEP: Saw pod success
Feb 12 14:15:12.156: INFO: Pod "var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76" satisfied condition "Succeeded or Failed"
Feb 12 14:15:12.167: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76 container dapi-container: <nil>
STEP: delete the pod
Feb 12 14:15:12.264: INFO: Waiting for pod var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76 to disappear
Feb 12 14:15:12.270: INFO: Pod var-expansion-aeba4355-aed6-44da-b46b-e57eca953d76 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:15:12.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7302" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":484,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:15:12.295: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Feb 12 14:15:12.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 api-versions'
Feb 12 14:15:12.468: INFO: stderr: ""
Feb 12 14:15:12.468: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:15:12.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2592" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":31,"skipped":489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:15:12.495: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-sfcp
STEP: Creating a pod to test atomic-volume-subpath
Feb 12 14:15:12.658: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sfcp" in namespace "subpath-1778" to be "Succeeded or Failed"
Feb 12 14:15:12.688: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Pending", Reason="", readiness=false. Elapsed: 29.856036ms
Feb 12 14:15:14.695: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.036880021s
Feb 12 14:15:16.705: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 4.047090959s
Feb 12 14:15:18.713: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 6.055221391s
Feb 12 14:15:20.719: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.061334417s
Feb 12 14:15:22.737: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 10.079001326s
Feb 12 14:15:24.751: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.093051723s
Feb 12 14:15:26.770: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 14.112407069s
Feb 12 14:15:28.781: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 16.123150577s
Feb 12 14:15:30.788: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 18.129890513s
Feb 12 14:15:32.794: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.13649166s
Feb 12 14:15:34.801: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Running", Reason="", readiness=true. Elapsed: 22.143602977s
Feb 12 14:15:36.812: INFO: Pod "pod-subpath-test-configmap-sfcp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.154106235s
STEP: Saw pod success
Feb 12 14:15:36.812: INFO: Pod "pod-subpath-test-configmap-sfcp" satisfied condition "Succeeded or Failed"
Feb 12 14:15:36.819: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-subpath-test-configmap-sfcp container test-container-subpath-configmap-sfcp: <nil>
STEP: delete the pod
Feb 12 14:15:36.869: INFO: Waiting for pod pod-subpath-test-configmap-sfcp to disappear
Feb 12 14:15:36.876: INFO: Pod pod-subpath-test-configmap-sfcp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sfcp
Feb 12 14:15:36.876: INFO: Deleting pod "pod-subpath-test-configmap-sfcp" in namespace "subpath-1778"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:15:36.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1778" for this suite.

• [SLOW TEST:24.407 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":32,"skipped":521,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:15:36.905: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Feb 12 14:15:36.997: INFO: Waiting up to 5m0s for pod "var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9" in namespace "var-expansion-244" to be "Succeeded or Failed"
Feb 12 14:15:37.003: INFO: Pod "var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.926028ms
Feb 12 14:15:39.012: INFO: Pod "var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014913773s
Feb 12 14:15:41.018: INFO: Pod "var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021190737s
STEP: Saw pod success
Feb 12 14:15:41.018: INFO: Pod "var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9" satisfied condition "Succeeded or Failed"
Feb 12 14:15:41.028: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9 container dapi-container: <nil>
STEP: delete the pod
Feb 12 14:15:41.107: INFO: Waiting for pod var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9 to disappear
Feb 12 14:15:41.112: INFO: Pod var-expansion-caa43c85-bfaa-4124-a38c-479abc835bd9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:15:41.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-244" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":33,"skipped":527,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:15:41.133: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2011
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 12 14:15:41.186: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 14:15:41.263: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 14:15:43.269: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:45.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:47.270: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:49.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:51.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:53.271: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:55.269: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:57.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:15:59.270: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:16:01.271: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 12 14:16:01.282: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 12 14:16:01.295: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 12 14:16:05.395: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.88:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2011 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:16:05.395: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:16:05.956: INFO: Found all expected endpoints: [netserver-0]
Feb 12 14:16:05.964: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.70:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2011 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:16:05.964: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:16:06.534: INFO: Found all expected endpoints: [netserver-1]
Feb 12 14:16:06.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.128:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2011 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:16:06.544: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:16:07.102: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2011" for this suite.

• [SLOW TEST:25.992 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:07.126: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:11.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7425" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":35,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 12 14:16:17.433: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:17.457: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:19.457: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:19.466: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:21.458: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:21.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:23.457: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:23.474: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:25.457: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:25.467: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:27.457: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:27.467: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 14:16:29.457: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 14:16:29.470: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:29.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9296" for this suite.

• [SLOW TEST:18.295 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:29.575: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Feb 12 14:16:34.252: INFO: Successfully updated pod "annotationupdatef86eb2ad-b25b-4ee0-a278-9f8e7b4fda71"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2873" for this suite.

• [SLOW TEST:6.740 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:36.321: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Feb 12 14:16:36.384: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:39.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4717" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":38,"skipped":712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:39.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2475" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":39,"skipped":741,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 12 14:16:41.786: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:41.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1204" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:41.838: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:16:41.905: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 12 14:16:41.918: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 12 14:16:46.958: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 12 14:16:46.959: INFO: Creating deployment "test-rolling-update-deployment"
Feb 12 14:16:46.968: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 12 14:16:46.983: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 12 14:16:49.017: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 12 14:16:49.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748736207, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748736207, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748736208, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748736206, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:16:51.036: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Feb 12 14:16:51.057: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5965 /apis/apps/v1/namespaces/deployment-5965/deployments/test-rolling-update-deployment f92a131e-85f4-4156-81c6-6484864908fd 56859 1 2021-02-12 14:16:46 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-02-12 14:16:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0010f8c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-12 14:16:47 +0000 UTC,LastTransitionTime:2021-02-12 14:16:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-02-12 14:16:49 +0000 UTC,LastTransitionTime:2021-02-12 14:16:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 14:16:51.063: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-5965 /apis/apps/v1/namespaces/deployment-5965/replicasets/test-rolling-update-deployment-c4cb8d6d9 a1ecc538-715f-4452-ad5f-8c49ed71f806 56848 1 2021-02-12 14:16:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f92a131e-85f4-4156-81c6-6484864908fd 0xc0010f91b0 0xc0010f91b1}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f92a131e-85f4-4156-81c6-6484864908fd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0010f9228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:16:51.063: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 12 14:16:51.063: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5965 /apis/apps/v1/namespaces/deployment-5965/replicasets/test-rolling-update-controller 02b0dea4-106f-4051-a077-f1000ffee565 56857 2 2021-02-12 14:16:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f92a131e-85f4-4156-81c6-6484864908fd 0xc0010f90a7 0xc0010f90a8}] []  [{e2e.test Update apps/v1 2021-02-12 14:16:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f92a131e-85f4-4156-81c6-6484864908fd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0010f9148 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:16:51.070: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-7zct5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-7zct5 test-rolling-update-deployment-c4cb8d6d9- deployment-5965 /api/v1/namespaces/deployment-5965/pods/test-rolling-update-deployment-c4cb8d6d9-7zct5 be77e56e-7b11-45a4-bccb-633102c14a63 56847 0 2021-02-12 14:16:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:172.25.0.95/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 a1ecc538-715f-4452-ad5f-8c49ed71f806 0xc0010f9720 0xc0010f9721}] []  [{kube-controller-manager Update v1 2021-02-12 14:16:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1ecc538-715f-4452-ad5f-8c49ed71f806\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zlst8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zlst8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zlst8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:172.25.0.95,StartTime:2021-02-12 14:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:16:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://312426ac7da9a9ba366146079d43311390b5f9d035b25b355f39819818b45273,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:51.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5965" for this suite.

• [SLOW TEST:9.254 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":41,"skipped":777,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:51.092: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Feb 12 14:16:51.191: INFO: Waiting up to 5m0s for pod "var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc" in namespace "var-expansion-1089" to be "Succeeded or Failed"
Feb 12 14:16:51.199: INFO: Pod "var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.178127ms
Feb 12 14:16:53.205: INFO: Pod "var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013951125s
STEP: Saw pod success
Feb 12 14:16:53.205: INFO: Pod "var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc" satisfied condition "Succeeded or Failed"
Feb 12 14:16:53.211: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc container dapi-container: <nil>
STEP: delete the pod
Feb 12 14:16:53.287: INFO: Waiting for pod var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc to disappear
Feb 12 14:16:53.292: INFO: Pod var-expansion-ec4ccc91-030b-4b65-b05e-0cadc3a5fddc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:53.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1089" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":796,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:53.312: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 12 14:16:54.020: INFO: starting watch
STEP: patching
STEP: updating
Feb 12 14:16:54.064: INFO: waiting for watch events with expected annotations
Feb 12 14:16:54.064: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:54.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-216" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":43,"skipped":797,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:54.280: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:16:54.344: INFO: Creating deployment "test-recreate-deployment"
Feb 12 14:16:54.355: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 12 14:16:54.384: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 12 14:16:56.396: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 12 14:16:56.403: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 12 14:16:56.418: INFO: Updating deployment test-recreate-deployment
Feb 12 14:16:56.418: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Feb 12 14:16:56.586: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2030 /apis/apps/v1/namespaces/deployment-2030/deployments/test-recreate-deployment 4c3a64c0-47d0-4bc2-ab71-72224638c59b 57005 2 2021-02-12 14:16:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002142558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-12 14:16:56 +0000 UTC,LastTransitionTime:2021-02-12 14:16:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-02-12 14:16:56 +0000 UTC,LastTransitionTime:2021-02-12 14:16:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 12 14:16:56.592: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-2030 /apis/apps/v1/namespaces/deployment-2030/replicasets/test-recreate-deployment-f79dd4667 f90d71dc-f4fe-4c22-9b2f-95d4c2779792 57002 1 2021-02-12 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4c3a64c0-47d0-4bc2-ab71-72224638c59b 0xc001f1b550 0xc001f1b551}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3a64c0-47d0-4bc2-ab71-72224638c59b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f1b5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:16:56.592: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 12 14:16:56.592: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-2030 /apis/apps/v1/namespaces/deployment-2030/replicasets/test-recreate-deployment-c96cf48f c22b7bb5-0659-4dea-86aa-1687aae389e3 56992 2 2021-02-12 14:16:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4c3a64c0-47d0-4bc2-ab71-72224638c59b 0xc001f1b45f 0xc001f1b470}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3a64c0-47d0-4bc2-ab71-72224638c59b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f1b4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:16:56.598: INFO: Pod "test-recreate-deployment-f79dd4667-djblg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-djblg test-recreate-deployment-f79dd4667- deployment-2030 /api/v1/namespaces/deployment-2030/pods/test-recreate-deployment-f79dd4667-djblg da44aae5-fc58-469a-9765-38146b9315b0 57004 0 2021-02-12 14:16:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 f90d71dc-f4fe-4c22-9b2f-95d4c2779792 0xc002142960 0xc002142961}] []  [{kube-controller-manager Update v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f90d71dc-f4fe-4c22-9b2f-95d4c2779792\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:16:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hj49z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hj49z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hj49z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:16:56.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2030" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":44,"skipped":804,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:16:56.619: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 12 14:16:56.801: INFO: Number of nodes with available pods: 0
Feb 12 14:16:56.801: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:16:57.814: INFO: Number of nodes with available pods: 0
Feb 12 14:16:57.814: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:16:58.822: INFO: Number of nodes with available pods: 3
Feb 12 14:16:58.822: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 12 14:16:58.888: INFO: Number of nodes with available pods: 2
Feb 12 14:16:58.888: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 14:16:59.901: INFO: Number of nodes with available pods: 2
Feb 12 14:16:59.901: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 14:17:00.902: INFO: Number of nodes with available pods: 2
Feb 12 14:17:00.902: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 14:17:01.902: INFO: Number of nodes with available pods: 3
Feb 12 14:17:01.902: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2142, will wait for the garbage collector to delete the pods
Feb 12 14:17:01.991: INFO: Deleting DaemonSet.extensions daemon-set took: 21.873732ms
Feb 12 14:17:02.092: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.373495ms
Feb 12 14:17:14.098: INFO: Number of nodes with available pods: 0
Feb 12 14:17:14.098: INFO: Number of running nodes: 0, number of available pods: 0
Feb 12 14:17:14.106: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2142/daemonsets","resourceVersion":"57215"},"items":null}

Feb 12 14:17:14.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2142/pods","resourceVersion":"57215"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:17:14.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2142" for this suite.

• [SLOW TEST:17.571 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":45,"skipped":824,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:17:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Feb 12 14:17:14.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-5134'
Feb 12 14:17:14.700: INFO: stderr: ""
Feb 12 14:17:14.700: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 12 14:17:14.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:14.780: INFO: stderr: ""
Feb 12 14:17:14.780: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-m47kg "
Feb 12 14:17:14.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:14.862: INFO: stderr: ""
Feb 12 14:17:14.862: INFO: stdout: ""
Feb 12 14:17:14.862: INFO: update-demo-nautilus-l5kn5 is created but not running
Feb 12 14:17:19.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:19.963: INFO: stderr: ""
Feb 12 14:17:19.963: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-m47kg "
Feb 12 14:17:19.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:20.069: INFO: stderr: ""
Feb 12 14:17:20.069: INFO: stdout: "true"
Feb 12 14:17:20.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:20.159: INFO: stderr: ""
Feb 12 14:17:20.159: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:20.159: INFO: validating pod update-demo-nautilus-l5kn5
Feb 12 14:17:20.262: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:20.262: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:20.262: INFO: update-demo-nautilus-l5kn5 is verified up and running
Feb 12 14:17:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-m47kg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:20.339: INFO: stderr: ""
Feb 12 14:17:20.339: INFO: stdout: "true"
Feb 12 14:17:20.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-m47kg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:20.421: INFO: stderr: ""
Feb 12 14:17:20.421: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:20.421: INFO: validating pod update-demo-nautilus-m47kg
Feb 12 14:17:20.574: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:20.574: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:20.574: INFO: update-demo-nautilus-m47kg is verified up and running
STEP: scaling down the replication controller
Feb 12 14:17:20.576: INFO: scanned /root for discovery docs: <nil>
Feb 12 14:17:20.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5134'
Feb 12 14:17:21.708: INFO: stderr: ""
Feb 12 14:17:21.708: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 12 14:17:21.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:21.860: INFO: stderr: ""
Feb 12 14:17:21.860: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-m47kg "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 12 14:17:26.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:26.944: INFO: stderr: ""
Feb 12 14:17:26.944: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-m47kg "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 12 14:17:31.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:32.031: INFO: stderr: ""
Feb 12 14:17:32.031: INFO: stdout: "update-demo-nautilus-l5kn5 "
Feb 12 14:17:32.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:32.124: INFO: stderr: ""
Feb 12 14:17:32.124: INFO: stdout: "true"
Feb 12 14:17:32.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:32.198: INFO: stderr: ""
Feb 12 14:17:32.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:32.198: INFO: validating pod update-demo-nautilus-l5kn5
Feb 12 14:17:32.217: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:32.217: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:32.217: INFO: update-demo-nautilus-l5kn5 is verified up and running
STEP: scaling up the replication controller
Feb 12 14:17:32.219: INFO: scanned /root for discovery docs: <nil>
Feb 12 14:17:32.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5134'
Feb 12 14:17:33.325: INFO: stderr: ""
Feb 12 14:17:33.325: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 12 14:17:33.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:33.402: INFO: stderr: ""
Feb 12 14:17:33.402: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-xjh4g "
Feb 12 14:17:33.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:33.489: INFO: stderr: ""
Feb 12 14:17:33.489: INFO: stdout: "true"
Feb 12 14:17:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:33.563: INFO: stderr: ""
Feb 12 14:17:33.563: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:33.563: INFO: validating pod update-demo-nautilus-l5kn5
Feb 12 14:17:33.575: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:33.575: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:33.575: INFO: update-demo-nautilus-l5kn5 is verified up and running
Feb 12 14:17:33.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-xjh4g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:33.660: INFO: stderr: ""
Feb 12 14:17:33.660: INFO: stdout: ""
Feb 12 14:17:33.660: INFO: update-demo-nautilus-xjh4g is created but not running
Feb 12 14:17:38.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5134'
Feb 12 14:17:38.766: INFO: stderr: ""
Feb 12 14:17:38.766: INFO: stdout: "update-demo-nautilus-l5kn5 update-demo-nautilus-xjh4g "
Feb 12 14:17:38.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:38.849: INFO: stderr: ""
Feb 12 14:17:38.849: INFO: stdout: "true"
Feb 12 14:17:38.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-l5kn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:38.954: INFO: stderr: ""
Feb 12 14:17:38.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:38.954: INFO: validating pod update-demo-nautilus-l5kn5
Feb 12 14:17:38.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:38.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:38.968: INFO: update-demo-nautilus-l5kn5 is verified up and running
Feb 12 14:17:38.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-xjh4g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:39.067: INFO: stderr: ""
Feb 12 14:17:39.067: INFO: stdout: "true"
Feb 12 14:17:39.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-xjh4g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5134'
Feb 12 14:17:39.161: INFO: stderr: ""
Feb 12 14:17:39.161: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 14:17:39.161: INFO: validating pod update-demo-nautilus-xjh4g
Feb 12 14:17:39.258: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 14:17:39.258: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 14:17:39.258: INFO: update-demo-nautilus-xjh4g is verified up and running
STEP: using delete to clean up resources
Feb 12 14:17:39.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-5134'
Feb 12 14:17:39.365: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 14:17:39.365: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 12 14:17:39.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5134'
Feb 12 14:17:39.461: INFO: stderr: "No resources found in kubectl-5134 namespace.\n"
Feb 12 14:17:39.461: INFO: stdout: ""
Feb 12 14:17:39.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -l name=update-demo --namespace=kubectl-5134 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 14:17:39.543: INFO: stderr: ""
Feb 12 14:17:39.543: INFO: stdout: "update-demo-nautilus-l5kn5\nupdate-demo-nautilus-xjh4g\n"
Feb 12 14:17:40.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5134'
Feb 12 14:17:40.127: INFO: stderr: "No resources found in kubectl-5134 namespace.\n"
Feb 12 14:17:40.127: INFO: stdout: ""
Feb 12 14:17:40.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -l name=update-demo --namespace=kubectl-5134 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 14:17:40.215: INFO: stderr: ""
Feb 12 14:17:40.215: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:17:40.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5134" for this suite.

• [SLOW TEST:26.060 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":46,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:17:40.250: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:17:40.298: INFO: Creating deployment "webserver-deployment"
Feb 12 14:17:40.306: INFO: Waiting for observed generation 1
Feb 12 14:17:42.317: INFO: Waiting for all required pods to come up
Feb 12 14:17:42.337: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 12 14:17:44.367: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 12 14:17:44.377: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 12 14:17:44.394: INFO: Updating deployment webserver-deployment
Feb 12 14:17:44.394: INFO: Waiting for observed generation 2
Feb 12 14:17:46.405: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 12 14:17:46.410: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 12 14:17:46.415: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 12 14:17:46.467: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 12 14:17:46.467: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 12 14:17:46.473: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 12 14:17:46.482: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 12 14:17:46.482: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 12 14:17:46.498: INFO: Updating deployment webserver-deployment
Feb 12 14:17:46.498: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 12 14:17:46.511: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 12 14:17:46.519: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Feb 12 14:17:46.637: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5839 /apis/apps/v1/namespaces/deployment-5839/deployments/webserver-deployment b009eea0-00e8-4538-92d6-83ac9e1ec387 57700 3 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d16238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-02-12 14:17:44 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-12 14:17:46 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 12 14:17:46.677: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5839 /apis/apps/v1/namespaces/deployment-5839/replicasets/webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 57688 3 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b009eea0-00e8-4538-92d6-83ac9e1ec387 0xc006c5c2e7 0xc006c5c2e8}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b009eea0-00e8-4538-92d6-83ac9e1ec387\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c5c368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:17:46.677: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 12 14:17:46.677: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5839 /apis/apps/v1/namespaces/deployment-5839/replicasets/webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 57745 3 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b009eea0-00e8-4538-92d6-83ac9e1ec387 0xc006c5c3c7 0xc006c5c3c8}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b009eea0-00e8-4538-92d6-83ac9e1ec387\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c5c448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:17:46.760: INFO: Pod "webserver-deployment-795d758f88-7fwrl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7fwrl webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-7fwrl 094a200f-fb38-4aa7-816e-01bee0e74760 57677 0 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.140/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d16947 0xc006d16948}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-12 14:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:,StartTime:2021-02-12 14:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.760: INFO: Pod "webserver-deployment-795d758f88-7s6w4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7s6w4 webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-7s6w4 9cd2b0c8-b1af-4cc8-a8f3-ccf7f6d773a2 57761 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d16af7 0xc006d16af8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.760: INFO: Pod "webserver-deployment-795d758f88-bcqwg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bcqwg webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-bcqwg f42cd1cc-5014-4c6e-b1e0-03d97b6c9e12 57734 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d16c97 0xc006d16c98}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.761: INFO: Pod "webserver-deployment-795d758f88-djrdn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-djrdn webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-djrdn 0dac9bfc-760c-4d57-bea4-8a23009233e9 57756 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d16dc0 0xc006d16dc1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.761: INFO: Pod "webserver-deployment-795d758f88-g485l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g485l webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-g485l 2b524caa-dce5-4e43-800a-0d6017585379 57675 0 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.105/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d16f00 0xc006d16f01}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-12 14:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.761: INFO: Pod "webserver-deployment-795d758f88-hdgfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hdgfs webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-hdgfs a3d16b5e-58f3-4df9-a6e7-41bf126a737d 57666 0 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.104/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d170c7 0xc006d170c8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-12 14:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.761: INFO: Pod "webserver-deployment-795d758f88-hlwvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hlwvl webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-hlwvl 5175e54b-71fa-4124-b58d-db6fe4d4c766 57733 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d17277 0xc006d17278}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.761: INFO: Pod "webserver-deployment-795d758f88-nt5km" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nt5km webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-nt5km 7cbfccad-2330-45e5-83d5-763443c00441 57720 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d173a0 0xc006d173a1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.762: INFO: Pod "webserver-deployment-795d758f88-rcwfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rcwfp webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-rcwfp 27ffc7f4-2598-48f6-9ebb-e489e820280d 57672 0 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.139/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d174e0 0xc006d174e1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-12 14:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:,StartTime:2021-02-12 14:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.762: INFO: Pod "webserver-deployment-795d758f88-rrbqd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rrbqd webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-rrbqd 272c2567-5b15-4e0a-9811-ffd51add8cb6 57667 0 2021-02-12 14:17:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.2.75/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d176a7 0xc006d176a8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-12 14:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:,StartTime:2021-02-12 14:17:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.762: INFO: Pod "webserver-deployment-795d758f88-x28q6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x28q6 webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-x28q6 0ca1c73f-fdf7-461e-aece-ece6a9046d79 57741 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d17857 0xc006d17858}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.762: INFO: Pod "webserver-deployment-795d758f88-xrxtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xrxtk webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-xrxtk 8963f28f-6c79-4c14-861c-6dbc172fbf61 57751 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d179f7 0xc006d179f8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-795d758f88-zppm7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zppm7 webserver-deployment-795d758f88- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-795d758f88-zppm7 3d37ab7f-0dbe-4871-b8ec-d1bd9c768525 57731 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 0ab668df-fee0-4273-8ba6-b061157f15b0 0xc006d17b97 0xc006d17b98}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ab668df-fee0-4273-8ba6-b061157f15b0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-dd94f59b7-24qkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-24qkb webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-24qkb 86743e54-b741-421d-93f9-f7586017c9a6 57735 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006d17cc0 0xc006d17cc1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-dd94f59b7-2h6r2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2h6r2 webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-2h6r2 c096986d-619a-4670-85b1-18c32b4b40da 57767 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006d17de0 0xc006d17de1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-dd94f59b7-5vqrb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5vqrb webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-5vqrb 4bcdead0-9002-4ec5-b6c8-29a384d1be3a 57738 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006d17f57 0xc006d17f58}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-dd94f59b7-6ncmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6ncmv webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-6ncmv 31db68ac-c02a-4166-a1f1-f5818464fff9 57754 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78070 0xc006f78071}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.763: INFO: Pod "webserver-deployment-dd94f59b7-79w22" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-79w22 webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-79w22 8f0cb37d-3435-4094-b1b6-6bc8785ecb72 57753 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f781e7 0xc006f781e8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.764: INFO: Pod "webserver-deployment-dd94f59b7-8g7ff" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8g7ff webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-8g7ff 654fb53b-d4ec-4d29-aa7a-ca39750fd14a 57744 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78300 0xc006f78301}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.764: INFO: Pod "webserver-deployment-dd94f59b7-8l92j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8l92j webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-8l92j 482c4c92-e872-4a31-9be3-6834854dc487 57576 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78430 0xc006f78431}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:172.25.0.102,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5a90dc3a5f23a3cf589995a9c58515e2687625171d94ffd63cb497d19de03082,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.764: INFO: Pod "webserver-deployment-dd94f59b7-8mzsz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8mzsz webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-8mzsz e8a85e91-a604-450e-aa1c-1aea6523074c 57557 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.1.138/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f785f7 0xc006f785f8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:172.25.1.138,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c322d564e35d40bed2ac75d34c969392a49810a6ce9343c76460a743d3aafe91,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.764: INFO: Pod "webserver-deployment-dd94f59b7-8vkjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8vkjq webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-8vkjq 4047f972-d855-4e8f-aead-0a6a57abe4f0 57703 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f787a7 0xc006f787a8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-9z758" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9z758 webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-9z758 ce60b3a4-d571-40a6-9301-7c0fc09a714c 57746 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78927 0xc006f78928}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-bzxmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bzxmt webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-bzxmt 12a88b8f-bb03-404a-98db-250a32b5d1f8 57721 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78a40 0xc006f78a41}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-jg6j8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jg6j8 webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-jg6j8 dbe62bb9-44f3-4226-9af9-24c8221d284f 57564 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.1.136/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78b70 0xc006f78b71}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:172.25.1.136,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://81ea2c2dc48d12743a938e0e5d40e5d94e68340eb9c945d58877002a88bcadd6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-lpcpd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lpcpd webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-lpcpd 22ada362-5e34-46e6-9488-389db4f8088b 57579 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78d37 0xc006f78d38}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:172.25.0.100,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://52a05d159484815571c263a8550ae84a1292a5c3a9d0186ec4ccc58e0f2979ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-p8djv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p8djv webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-p8djv fe03a455-36bd-434c-b67a-f48283e7c5b0 57559 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.1.137/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f78ef7 0xc006f78ef8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:172.25.1.137,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7db241d71ce8111d93dfdf620fa49dcc70d0c983c8bd9cfcc23eb0ebdf3f2890,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.766: INFO: Pod "webserver-deployment-dd94f59b7-qwrrs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qwrrs webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-qwrrs 692feb18-799a-47bb-9937-fcbece4c19b8 57752 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f790a7 0xc006f790a8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.767: INFO: Pod "webserver-deployment-dd94f59b7-rxcqj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rxcqj webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-rxcqj 4399d8a7-8fd9-4d42-b811-05587ebd0b08 57568 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.73/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f791d0 0xc006f791d1}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:172.25.2.73,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://66c734d24c0879b84b228c918bfcbee4371be6dfcb57facdc7ce358a807a9ae8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.767: INFO: Pod "webserver-deployment-dd94f59b7-twfft" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-twfft webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-twfft dde71bfe-7d01-4fac-a42c-0f82cf7daabe 57571 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.74/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f79390 0xc006f79391}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:172.25.2.74,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f66f7733f6045bcb556e3f4b7be9c9f81120f7870ad6b9483725ba1935330b3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.767: INFO: Pod "webserver-deployment-dd94f59b7-vb5vp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vb5vp webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-vb5vp f1094e0e-65c7-4918-b43e-590f374da308 57722 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f79540 0xc006f79541}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.767: INFO: Pod "webserver-deployment-dd94f59b7-wjkr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wjkr8 webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-wjkr8 fd787c56-5b6b-46e6-893a-78fc20b80dbd 57717 0 2021-02-12 14:17:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f796b7 0xc006f796b8}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-12 14:17:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:,StartTime:2021-02-12 14:17:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:17:46.767: INFO: Pod "webserver-deployment-dd94f59b7-wk87h" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wk87h webserver-deployment-dd94f59b7- deployment-5839 /api/v1/namespaces/deployment-5839/pods/webserver-deployment-dd94f59b7-wk87h daa92c06-b91c-4ef7-b214-8a468440db81 57526 0 2021-02-12 14:17:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.72/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 3c0a9495-70ca-4bc4-b038-c35344a34e69 0xc006f79847 0xc006f79848}] []  [{kube-controller-manager Update v1 2021-02-12 14:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0a9495-70ca-4bc4-b038-c35344a34e69\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:17:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kgjpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kgjpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kgjpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-qftnt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:17:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:68.183.68.155,PodIP:172.25.2.72,StartTime:2021-02-12 14:17:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:17:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2ff5ebaf13c0a615e4413e4b017bde37161e3b43feb1272db05f303eabe651c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:17:46.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5839" for this suite.

• [SLOW TEST:6.555 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":47,"skipped":842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:17:46.807: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-6d18259a-539c-4c58-bfd0-e37b50426353
STEP: Creating a pod to test consume secrets
Feb 12 14:17:46.901: INFO: Waiting up to 5m0s for pod "pod-secrets-a4d387e0-113a-499b-8892-a749094299c2" in namespace "secrets-1879" to be "Succeeded or Failed"
Feb 12 14:17:46.909: INFO: Pod "pod-secrets-a4d387e0-113a-499b-8892-a749094299c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.911437ms
Feb 12 14:17:48.934: INFO: Pod "pod-secrets-a4d387e0-113a-499b-8892-a749094299c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033356193s
Feb 12 14:17:50.955: INFO: Pod "pod-secrets-a4d387e0-113a-499b-8892-a749094299c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05399816s
STEP: Saw pod success
Feb 12 14:17:50.955: INFO: Pod "pod-secrets-a4d387e0-113a-499b-8892-a749094299c2" satisfied condition "Succeeded or Failed"
Feb 12 14:17:50.963: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-qftnt pod pod-secrets-a4d387e0-113a-499b-8892-a749094299c2 container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:17:51.379: INFO: Waiting for pod pod-secrets-a4d387e0-113a-499b-8892-a749094299c2 to disappear
Feb 12 14:17:51.384: INFO: Pod pod-secrets-a4d387e0-113a-499b-8892-a749094299c2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:17:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1879" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":875,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:17:51.401: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-3cd081e1-7027-4e1a-a128-d573d269a800 in namespace container-probe-6014
Feb 12 14:17:55.502: INFO: Started pod liveness-3cd081e1-7027-4e1a-a128-d573d269a800 in namespace container-probe-6014
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:17:55.507: INFO: Initial restart count of pod liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is 0
Feb 12 14:18:11.609: INFO: Restart count of pod container-probe-6014/liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is now 1 (16.101835602s elapsed)
Feb 12 14:18:31.777: INFO: Restart count of pod container-probe-6014/liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is now 2 (36.269545387s elapsed)
Feb 12 14:18:51.878: INFO: Restart count of pod container-probe-6014/liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is now 3 (56.37080494s elapsed)
Feb 12 14:19:09.981: INFO: Restart count of pod container-probe-6014/liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is now 4 (1m14.473540751s elapsed)
Feb 12 14:20:22.369: INFO: Restart count of pod container-probe-6014/liveness-3cd081e1-7027-4e1a-a128-d573d269a800 is now 5 (2m26.861533553s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:20:22.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6014" for this suite.

• [SLOW TEST:151.021 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:20:22.424: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Feb 12 14:20:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:20:39.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5529" for this suite.

• [SLOW TEST:17.543 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":50,"skipped":917,"failed":0}
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:20:39.967: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb 12 14:20:40.022: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 14:21:40.082: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:21:40.088: INFO: Starting informer...
STEP: Starting pod...
Feb 12 14:21:40.315: INFO: Pod is running on modest-nightingale-745f6d5bd4-h2tct. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 12 14:21:40.346: INFO: Pod wasn't evicted. Proceeding
Feb 12 14:21:40.346: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 12 14:22:55.432: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:22:55.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1472" for this suite.

• [SLOW TEST:135.487 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":51,"skipped":917,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:22:55.454: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-513a4fd1-584c-42df-b3cf-8faee1b3a59c
STEP: Creating secret with name secret-projected-all-test-volume-2f7a94a0-2578-404d-bab4-a776224d8385
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 12 14:22:55.551: INFO: Waiting up to 5m0s for pod "projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538" in namespace "projected-5639" to be "Succeeded or Failed"
Feb 12 14:22:55.563: INFO: Pod "projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538": Phase="Pending", Reason="", readiness=false. Elapsed: 11.284924ms
Feb 12 14:22:57.572: INFO: Pod "projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021023737s
STEP: Saw pod success
Feb 12 14:22:57.572: INFO: Pod "projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538" satisfied condition "Succeeded or Failed"
Feb 12 14:22:57.590: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 12 14:22:57.665: INFO: Waiting for pod projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538 to disappear
Feb 12 14:22:57.670: INFO: Pod projected-volume-6dc35c2a-fe0c-4082-a663-ffc15d728538 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:22:57.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5639" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":52,"skipped":930,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:22:57.704: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 12 14:22:57.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5807'
Feb 12 14:22:57.856: INFO: stderr: ""
Feb 12 14:22:57.856: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Feb 12 14:22:57.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete pods e2e-test-httpd-pod --namespace=kubectl-5807'
Feb 12 14:23:02.546: INFO: stderr: ""
Feb 12 14:23:02.546: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:23:02.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5807" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":53,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:23:02.569: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-5f8f03d5-ccf7-4daf-b504-4c49758158ce in namespace container-probe-8435
Feb 12 14:23:04.669: INFO: Started pod liveness-5f8f03d5-ccf7-4daf-b504-4c49758158ce in namespace container-probe-8435
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:23:04.676: INFO: Initial restart count of pod liveness-5f8f03d5-ccf7-4daf-b504-4c49758158ce is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:27:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8435" for this suite.

• [SLOW TEST:243.190 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:27:05.760: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0212 14:27:45.923140      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 14:27:45.923160      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 14:27:45.923166      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 12 14:27:45.923: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 12 14:27:45.923: INFO: Deleting pod "simpletest.rc-2lxnq" in namespace "gc-5184"
Feb 12 14:27:45.947: INFO: Deleting pod "simpletest.rc-blhf2" in namespace "gc-5184"
Feb 12 14:27:45.967: INFO: Deleting pod "simpletest.rc-crl62" in namespace "gc-5184"
Feb 12 14:27:45.994: INFO: Deleting pod "simpletest.rc-kmhwc" in namespace "gc-5184"
Feb 12 14:27:46.025: INFO: Deleting pod "simpletest.rc-lqhbt" in namespace "gc-5184"
Feb 12 14:27:46.055: INFO: Deleting pod "simpletest.rc-ql9l7" in namespace "gc-5184"
Feb 12 14:27:46.094: INFO: Deleting pod "simpletest.rc-sb7zt" in namespace "gc-5184"
Feb 12 14:27:46.138: INFO: Deleting pod "simpletest.rc-w82kg" in namespace "gc-5184"
Feb 12 14:27:46.162: INFO: Deleting pod "simpletest.rc-wkmdn" in namespace "gc-5184"
Feb 12 14:27:46.210: INFO: Deleting pod "simpletest.rc-z7mpn" in namespace "gc-5184"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:27:46.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5184" for this suite.

• [SLOW TEST:40.511 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":55,"skipped":998,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:27:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 12 14:27:50.951: INFO: Successfully updated pod "pod-update-c4f682c7-d911-441b-a9a5-713c7c13013b"
STEP: verifying the updated pod is in kubernetes
Feb 12 14:27:50.964: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:27:50.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9210" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":1009,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:27:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-7334/configmap-test-6f936f4e-8404-4cf8-8a17-eeba3d119b7d
STEP: Creating a pod to test consume configMaps
Feb 12 14:27:51.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22" in namespace "configmap-7334" to be "Succeeded or Failed"
Feb 12 14:27:51.080: INFO: Pod "pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22": Phase="Pending", Reason="", readiness=false. Elapsed: 7.794849ms
Feb 12 14:27:53.087: INFO: Pod "pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015170786s
STEP: Saw pod success
Feb 12 14:27:53.087: INFO: Pod "pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22" satisfied condition "Succeeded or Failed"
Feb 12 14:27:53.102: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22 container env-test: <nil>
STEP: delete the pod
Feb 12 14:27:53.160: INFO: Waiting for pod pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22 to disappear
Feb 12 14:27:53.166: INFO: Pod pod-configmaps-a49516c7-055b-4afc-8d6c-574b13884f22 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:27:53.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7334" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":1023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:27:53.191: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7
Feb 12 14:27:53.291: INFO: Pod name my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7: Found 0 pods out of 1
Feb 12 14:27:58.298: INFO: Pod name my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7: Found 1 pods out of 1
Feb 12 14:27:58.298: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7" are running
Feb 12 14:27:58.303: INFO: Pod "my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7-tqgpt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 14:27:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 14:27:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 14:27:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 14:27:53 +0000 UTC Reason: Message:}])
Feb 12 14:27:58.303: INFO: Trying to dial the pod
Feb 12 14:28:03.413: INFO: Controller my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7: Got expected result from replica 1 [my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7-tqgpt]: "my-hostname-basic-e96ee690-746e-41cc-a7b7-fae8056171f7-tqgpt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:03.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9159" for this suite.

• [SLOW TEST:10.241 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":58,"skipped":1053,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 12 14:28:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6165'
Feb 12 14:28:03.741: INFO: stderr: ""
Feb 12 14:28:03.741: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb 12 14:28:03.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pod e2e-test-httpd-pod -o json --namespace=kubectl-6165'
Feb 12 14:28:03.822: INFO: stderr: ""
Feb 12 14:28:03.822: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-02-12T14:28:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-12T14:28:03Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-12T14:28:03Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6165\",\n        \"resourceVersion\": \"61502\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6165/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1b1a5f47-1a81-4e5d-92ee-4193230f7fc6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5dx42\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"modest-nightingale-745f6d5bd4-h2tct\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5dx42\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5dx42\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T14:28:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T14:28:03Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T14:28:03Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T14:28:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"104.248.253.154\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-02-12T14:28:03Z\"\n    }\n}\n"
Feb 12 14:28:03.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 replace -f - --dry-run server --namespace=kubectl-6165'
Feb 12 14:28:04.092: INFO: stderr: "W0212 14:28:03.873689     311 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Feb 12 14:28:04.092: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Feb 12 14:28:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete pods e2e-test-httpd-pod --namespace=kubectl-6165'
Feb 12 14:28:05.709: INFO: stderr: ""
Feb 12 14:28:05.709: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:05.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6165" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":59,"skipped":1060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:05.733: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 12 14:28:08.373: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2441 pod-service-account-056b8331-c4b6-4b8d-96e6-a88481c3557d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 12 14:28:09.022: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2441 pod-service-account-056b8331-c4b6-4b8d-96e6-a88481c3557d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 12 14:28:09.659: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2441 pod-service-account-056b8331-c4b6-4b8d-96e6-a88481c3557d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:10.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2441" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":60,"skipped":1085,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:10.300: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Feb 12 14:28:14.422: INFO: Pod pod-hostip-9d739c02-e4e5-4480-9b7a-86900bee5fdb has hostIP: 165.227.175.251
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:14.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-629" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":1095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:14.441: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 12 14:28:14.523: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 12 14:28:27.509: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:28:30.976: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:43.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-272" for this suite.

• [SLOW TEST:29.273 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":62,"skipped":1124,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:43.717: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:28:43.781: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:44.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7896" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":63,"skipped":1137,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:44.377: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-ab852f2e-42ea-4552-9658-a69954a1c32f
STEP: Creating a pod to test consume configMaps
Feb 12 14:28:44.524: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3" in namespace "configmap-8870" to be "Succeeded or Failed"
Feb 12 14:28:44.546: INFO: Pod "pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02201ms
Feb 12 14:28:46.555: INFO: Pod "pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031162188s
STEP: Saw pod success
Feb 12 14:28:46.555: INFO: Pod "pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3" satisfied condition "Succeeded or Failed"
Feb 12 14:28:46.562: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:28:46.608: INFO: Waiting for pod pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3 to disappear
Feb 12 14:28:46.614: INFO: Pod pod-configmaps-e8752ca0-2d17-4db2-a037-1a0d7e2796d3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:46.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8870" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1153,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:46.638: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:28:46.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10" in namespace "projected-5867" to be "Succeeded or Failed"
Feb 12 14:28:46.727: INFO: Pod "downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10": Phase="Pending", Reason="", readiness=false. Elapsed: 7.062367ms
Feb 12 14:28:48.743: INFO: Pod "downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022769946s
STEP: Saw pod success
Feb 12 14:28:48.743: INFO: Pod "downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10" satisfied condition "Succeeded or Failed"
Feb 12 14:28:48.750: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10 container client-container: <nil>
STEP: delete the pod
Feb 12 14:28:48.797: INFO: Waiting for pod downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10 to disappear
Feb 12 14:28:48.803: INFO: Pod downwardapi-volume-8bffcb22-8031-45b5-8a61-e035336a6b10 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5867" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":1161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:48.825: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-95e0ba80-ea0c-44d9-a1f6-a4b4595a6d80
STEP: Creating a pod to test consume configMaps
Feb 12 14:28:48.902: INFO: Waiting up to 5m0s for pod "pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab" in namespace "configmap-4184" to be "Succeeded or Failed"
Feb 12 14:28:48.910: INFO: Pod "pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668885ms
Feb 12 14:28:50.916: INFO: Pod "pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014096515s
STEP: Saw pod success
Feb 12 14:28:50.916: INFO: Pod "pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab" satisfied condition "Succeeded or Failed"
Feb 12 14:28:50.923: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:28:50.963: INFO: Waiting for pod pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab to disappear
Feb 12 14:28:50.970: INFO: Pod pod-configmaps-750dbe05-fcbb-4042-a3cc-b6d53ef7e8ab no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:28:50.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4184" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1204,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:28:50.989: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2044
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 12 14:28:51.048: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 14:28:51.102: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 14:28:53.108: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:28:55.109: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:28:57.109: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:28:59.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:29:01.109: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:29:03.112: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:29:05.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 14:29:07.107: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 12 14:29:07.119: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 12 14:29:09.125: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 12 14:29:09.137: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 12 14:29:13.180: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.127:8080/dial?request=hostname&protocol=http&host=172.25.0.126&port=8080&tries=1'] Namespace:pod-network-test-2044 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:29:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:29:13.712: INFO: Waiting for responses: map[]
Feb 12 14:29:13.719: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.127:8080/dial?request=hostname&protocol=http&host=172.25.2.88&port=8080&tries=1'] Namespace:pod-network-test-2044 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:29:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:29:14.269: INFO: Waiting for responses: map[]
Feb 12 14:29:14.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.127:8080/dial?request=hostname&protocol=http&host=172.25.1.156&port=8080&tries=1'] Namespace:pod-network-test-2044 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:29:14.276: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:29:14.892: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:29:14.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2044" for this suite.

• [SLOW TEST:23.929 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":1210,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:29:14.920: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-620.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-620.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 202.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.202_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-620.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-620.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-620.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 202.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.19.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.19.202_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:29:25.146: INFO: Unable to read wheezy_udp@dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.203: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.212: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.746: INFO: Unable to read jessie_udp@dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.754: INFO: Unable to read jessie_tcp@dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.766: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:25.776: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local from pod dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73: the server could not find the requested resource (get pods dns-test-66ab5199-42b8-4511-bdc1-46358be47c73)
Feb 12 14:29:26.257: INFO: Lookups using dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73 failed for: [wheezy_udp@dns-test-service.dns-620.svc.cluster.local wheezy_tcp@dns-test-service.dns-620.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local jessie_udp@dns-test-service.dns-620.svc.cluster.local jessie_tcp@dns-test-service.dns-620.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-620.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-620.svc.cluster.local]

Feb 12 14:29:32.878: INFO: DNS probes using dns-620/dns-test-66ab5199-42b8-4511-bdc1-46358be47c73 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:29:33.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-620" for this suite.

• [SLOW TEST:18.209 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":68,"skipped":1220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:29:33.133: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-92df6f61-ad10-4ae4-975b-3bae768552ba in namespace container-probe-1283
Feb 12 14:29:37.213: INFO: Started pod liveness-92df6f61-ad10-4ae4-975b-3bae768552ba in namespace container-probe-1283
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:29:37.219: INFO: Initial restart count of pod liveness-92df6f61-ad10-4ae4-975b-3bae768552ba is 0
Feb 12 14:30:01.317: INFO: Restart count of pod container-probe-1283/liveness-92df6f61-ad10-4ae4-975b-3bae768552ba is now 1 (24.097875963s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:30:01.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1283" for this suite.

• [SLOW TEST:28.231 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1260,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:30:01.367: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:30:01.496: INFO: Create a RollingUpdate DaemonSet
Feb 12 14:30:01.505: INFO: Check that daemon pods launch on every node of the cluster
Feb 12 14:30:01.521: INFO: Number of nodes with available pods: 0
Feb 12 14:30:01.521: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:30:02.540: INFO: Number of nodes with available pods: 0
Feb 12 14:30:02.540: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:30:03.541: INFO: Number of nodes with available pods: 3
Feb 12 14:30:03.541: INFO: Number of running nodes: 3, number of available pods: 3
Feb 12 14:30:03.541: INFO: Update the DaemonSet to trigger a rollout
Feb 12 14:30:03.559: INFO: Updating DaemonSet daemon-set
Feb 12 14:30:07.608: INFO: Roll back the DaemonSet before rollout is complete
Feb 12 14:30:07.627: INFO: Updating DaemonSet daemon-set
Feb 12 14:30:07.627: INFO: Make sure DaemonSet rollback is complete
Feb 12 14:30:07.641: INFO: Wrong image for pod: daemon-set-tww9r. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 12 14:30:07.641: INFO: Pod daemon-set-tww9r is not available
Feb 12 14:30:08.660: INFO: Wrong image for pod: daemon-set-tww9r. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 12 14:30:08.660: INFO: Pod daemon-set-tww9r is not available
Feb 12 14:30:09.661: INFO: Pod daemon-set-8h4zz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9914, will wait for the garbage collector to delete the pods
Feb 12 14:30:09.764: INFO: Deleting DaemonSet.extensions daemon-set took: 18.98918ms
Feb 12 14:30:10.364: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.420745ms
Feb 12 14:31:40.272: INFO: Number of nodes with available pods: 0
Feb 12 14:31:40.272: INFO: Number of running nodes: 0, number of available pods: 0
Feb 12 14:31:40.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9914/daemonsets","resourceVersion":"63044"},"items":null}

Feb 12 14:31:40.299: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9914/pods","resourceVersion":"63044"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:31:40.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9914" for this suite.

• [SLOW TEST:98.990 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":70,"skipped":1277,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:31:40.358: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:31:40.426: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 12 14:31:45.437: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 12 14:31:45.437: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Feb 12 14:31:47.517: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4993 /apis/apps/v1/namespaces/deployment-4993/deployments/test-cleanup-deployment 23744b12-8ca0-49b3-8a5e-06e02f37e376 63164 1 2021-02-12 14:31:45 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-02-12 14:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:31:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00326cf58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-12 14:31:45 +0000 UTC,LastTransitionTime:2021-02-12 14:31:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2021-02-12 14:31:46 +0000 UTC,LastTransitionTime:2021-02-12 14:31:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 14:31:47.523: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-4993 /apis/apps/v1/namespaces/deployment-4993/replicasets/test-cleanup-deployment-5d446bdd47 db07fc4d-4d27-4698-aa5a-f408a8cb04ea 63153 1 2021-02-12 14:31:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 23744b12-8ca0-49b3-8a5e-06e02f37e376 0xc00326d3b7 0xc00326d3b8}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:31:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23744b12-8ca0-49b3-8a5e-06e02f37e376\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00326d448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:31:47.528: INFO: Pod "test-cleanup-deployment-5d446bdd47-bkjc9" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-bkjc9 test-cleanup-deployment-5d446bdd47- deployment-4993 /api/v1/namespaces/deployment-4993/pods/test-cleanup-deployment-5d446bdd47-bkjc9 b175fe5c-e5f2-47ae-9e29-f67ee14f0bf3 63152 0 2021-02-12 14:31:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[cni.projectcalico.org/podIP:172.25.1.161/32] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 db07fc4d-4d27-4698-aa5a-f408a8cb04ea 0xc00326d887 0xc00326d888}] []  [{kube-controller-manager Update v1 2021-02-12 14:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db07fc4d-4d27-4698-aa5a-f408a8cb04ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:31:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:31:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5zdlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5zdlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5zdlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:31:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:31:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:172.25.1.161,StartTime:2021-02-12 14:31:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:31:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://c771e3ae0b1f99f93ae369342a2f774582e04ffcd7ec30dbfcbf0587d1c8ff55,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:31:47.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4993" for this suite.

• [SLOW TEST:7.189 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":71,"skipped":1289,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:31:47.548: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 12 14:31:47.597: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 14:31:47.610: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 14:31:47.617: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-h2tct before test
Feb 12 14:31:47.629: INFO: canal-zprb7 from kube-system started at 2021-02-12 12:16:38 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:31:47.629: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:31:47.629: INFO: kube-proxy-wl68v from kube-system started at 2021-02-12 12:16:38 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:31:47.629: INFO: logrotate-6jb84 from kube-system started at 2021-02-12 14:21:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:31:47.629: INFO: node-local-dns-xsl7b from kube-system started at 2021-02-12 14:21:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:31:47.629: INFO: user-ssh-keys-agent-tm2rj from kube-system started at 2021-02-12 13:11:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:31:47.629: INFO: sonobuoy from sonobuoy started at 2021-02-12 14:03:39 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 14:31:47.629: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:31:47.629: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:31:47.629: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-qftnt before test
Feb 12 14:31:47.643: INFO: canal-j2gzw from kube-system started at 2021-02-12 12:16:44 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:31:47.643: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:31:47.643: INFO: coredns-856777c9f5-tzdw7 from kube-system started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:31:47.643: INFO: kube-proxy-rfp6j from kube-system started at 2021-02-12 12:16:44 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:31:47.643: INFO: logrotate-zkvll from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:31:47.643: INFO: node-local-dns-2lwvq from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:31:47.643: INFO: user-ssh-keys-agent-vdvks from kube-system started at 2021-02-12 13:12:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:31:47.643: INFO: dashboard-metrics-scraper-975c84c89-7drg5 from kubernetes-dashboard started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:31:47.643: INFO: sonobuoy-e2e-job-7631d608de454e9c from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container e2e ready: true, restart count 0
Feb 12 14:31:47.643: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:31:47.643: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.643: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:31:47.643: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:31:47.643: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-w42vq before test
Feb 12 14:31:47.655: INFO: test-cleanup-deployment-5d446bdd47-bkjc9 from deployment-4993 started at 2021-02-12 14:31:45 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container agnhost ready: true, restart count 0
Feb 12 14:31:47.655: INFO: canal-mx9gb from kube-system started at 2021-02-12 12:16:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:31:47.655: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:31:47.655: INFO: coredns-856777c9f5-b27nr from kube-system started at 2021-02-12 12:16:59 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:31:47.655: INFO: kube-proxy-s2hw4 from kube-system started at 2021-02-12 12:16:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:31:47.655: INFO: logrotate-54qnh from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:31:47.655: INFO: node-local-dns-4dbnq from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:31:47.655: INFO: openvpn-client-6c9dd998bc-lr8jh from kube-system started at 2021-02-12 12:16:59 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container dnat-controller ready: true, restart count 1
Feb 12 14:31:47.655: INFO: 	Container openvpn-client ready: true, restart count 0
Feb 12 14:31:47.655: INFO: user-ssh-keys-agent-2llsr from kube-system started at 2021-02-12 13:12:10 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:31:47.655: INFO: dashboard-metrics-scraper-975c84c89-xfxv8 from kubernetes-dashboard started at 2021-02-12 12:16:57 +0000 UTC (1 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:31:47.655: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:31:47.655: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:31:47.655: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cf2362e4-2069-47ed-b64f-3426b68562c6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cf2362e4-2069-47ed-b64f-3426b68562c6 off the node modest-nightingale-745f6d5bd4-h2tct
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cf2362e4-2069-47ed-b64f-3426b68562c6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:31:55.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4929" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":72,"skipped":1301,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:31:55.863: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 12 14:31:55.961: INFO: Waiting up to 5m0s for pod "pod-28d261b6-cec5-4c60-a986-9e68cfd931fc" in namespace "emptydir-1241" to be "Succeeded or Failed"
Feb 12 14:31:55.966: INFO: Pod "pod-28d261b6-cec5-4c60-a986-9e68cfd931fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377221ms
Feb 12 14:31:57.974: INFO: Pod "pod-28d261b6-cec5-4c60-a986-9e68cfd931fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012832014s
Feb 12 14:31:59.980: INFO: Pod "pod-28d261b6-cec5-4c60-a986-9e68cfd931fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019157411s
STEP: Saw pod success
Feb 12 14:31:59.980: INFO: Pod "pod-28d261b6-cec5-4c60-a986-9e68cfd931fc" satisfied condition "Succeeded or Failed"
Feb 12 14:31:59.986: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-28d261b6-cec5-4c60-a986-9e68cfd931fc container test-container: <nil>
STEP: delete the pod
Feb 12 14:32:00.028: INFO: Waiting for pod pod-28d261b6-cec5-4c60-a986-9e68cfd931fc to disappear
Feb 12 14:32:00.041: INFO: Pod pod-28d261b6-cec5-4c60-a986-9e68cfd931fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1241" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":1310,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:00.072: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4594" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":74,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:00.154: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:11.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2109" for this suite.

• [SLOW TEST:11.192 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":75,"skipped":1390,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:11.346: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-3d3d320f-70c0-4d27-8903-d0ca51a97e60
STEP: Creating a pod to test consume configMaps
Feb 12 14:32:11.423: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba" in namespace "projected-7456" to be "Succeeded or Failed"
Feb 12 14:32:11.430: INFO: Pod "pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38682ms
Feb 12 14:32:13.437: INFO: Pod "pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014819109s
STEP: Saw pod success
Feb 12 14:32:13.438: INFO: Pod "pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba" satisfied condition "Succeeded or Failed"
Feb 12 14:32:13.445: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:32:13.484: INFO: Waiting for pod pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba to disappear
Feb 12 14:32:13.491: INFO: Pod pod-projected-configmaps-858b0114-0b9b-4add-b272-eaf5080af7ba no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:13.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7456" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1393,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:13.523: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 12 14:32:13.590: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 14:32:13.602: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 14:32:13.609: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-h2tct before test
Feb 12 14:32:13.618: INFO: canal-zprb7 from kube-system started at 2021-02-12 12:16:38 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.618: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:32:13.618: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:32:13.618: INFO: kube-proxy-wl68v from kube-system started at 2021-02-12 12:16:38 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.618: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:32:13.618: INFO: logrotate-6jb84 from kube-system started at 2021-02-12 14:21:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.618: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:32:13.618: INFO: node-local-dns-xsl7b from kube-system started at 2021-02-12 14:21:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.618: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:32:13.618: INFO: user-ssh-keys-agent-tm2rj from kube-system started at 2021-02-12 13:11:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.618: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:32:13.619: INFO: sonobuoy from sonobuoy started at 2021-02-12 14:03:39 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.619: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 14:32:13.619: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:32:13.619: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:32:13.619: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-qftnt before test
Feb 12 14:32:13.628: INFO: canal-j2gzw from kube-system started at 2021-02-12 12:16:44 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.628: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:32:13.628: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:32:13.628: INFO: coredns-856777c9f5-tzdw7 from kube-system started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.628: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:32:13.629: INFO: kube-proxy-rfp6j from kube-system started at 2021-02-12 12:16:44 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:32:13.629: INFO: logrotate-zkvll from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:32:13.629: INFO: node-local-dns-2lwvq from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:32:13.629: INFO: user-ssh-keys-agent-vdvks from kube-system started at 2021-02-12 13:12:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:32:13.629: INFO: dashboard-metrics-scraper-975c84c89-7drg5 from kubernetes-dashboard started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:32:13.629: INFO: sonobuoy-e2e-job-7631d608de454e9c from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container e2e ready: true, restart count 0
Feb 12 14:32:13.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:32:13.629: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:32:13.629: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:32:13.629: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-w42vq before test
Feb 12 14:32:13.644: INFO: canal-mx9gb from kube-system started at 2021-02-12 12:16:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.644: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:32:13.644: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:32:13.644: INFO: coredns-856777c9f5-b27nr from kube-system started at 2021-02-12 12:16:59 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.644: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:32:13.644: INFO: kube-proxy-s2hw4 from kube-system started at 2021-02-12 12:16:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.644: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:32:13.644: INFO: logrotate-54qnh from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.644: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:32:13.644: INFO: node-local-dns-4dbnq from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.645: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:32:13.645: INFO: openvpn-client-6c9dd998bc-lr8jh from kube-system started at 2021-02-12 12:16:59 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.645: INFO: 	Container dnat-controller ready: true, restart count 1
Feb 12 14:32:13.645: INFO: 	Container openvpn-client ready: true, restart count 0
Feb 12 14:32:13.645: INFO: user-ssh-keys-agent-2llsr from kube-system started at 2021-02-12 13:12:10 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.645: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:32:13.645: INFO: dashboard-metrics-scraper-975c84c89-xfxv8 from kubernetes-dashboard started at 2021-02-12 12:16:57 +0000 UTC (1 container statuses recorded)
Feb 12 14:32:13.645: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:32:13.645: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:32:13.645: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:32:13.645: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node modest-nightingale-745f6d5bd4-h2tct
STEP: verifying the node has the label node modest-nightingale-745f6d5bd4-qftnt
STEP: verifying the node has the label node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod canal-j2gzw requesting resource cpu=250m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.745: INFO: Pod canal-mx9gb requesting resource cpu=250m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod canal-zprb7 requesting resource cpu=250m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.745: INFO: Pod coredns-856777c9f5-b27nr requesting resource cpu=50m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod coredns-856777c9f5-tzdw7 requesting resource cpu=50m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.745: INFO: Pod kube-proxy-rfp6j requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.745: INFO: Pod kube-proxy-s2hw4 requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod kube-proxy-wl68v requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.745: INFO: Pod logrotate-54qnh requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod logrotate-6jb84 requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.745: INFO: Pod logrotate-zkvll requesting resource cpu=75m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.745: INFO: Pod node-local-dns-2lwvq requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.745: INFO: Pod node-local-dns-4dbnq requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod node-local-dns-xsl7b requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.745: INFO: Pod openvpn-client-6c9dd998bc-lr8jh requesting resource cpu=30m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod user-ssh-keys-agent-2llsr requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.745: INFO: Pod user-ssh-keys-agent-tm2rj requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.746: INFO: Pod user-ssh-keys-agent-vdvks requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.746: INFO: Pod dashboard-metrics-scraper-975c84c89-7drg5 requesting resource cpu=50m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.746: INFO: Pod dashboard-metrics-scraper-975c84c89-xfxv8 requesting resource cpu=50m on Node modest-nightingale-745f6d5bd4-w42vq
Feb 12 14:32:13.746: INFO: Pod sonobuoy requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.746: INFO: Pod sonobuoy-e2e-job-7631d608de454e9c requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.746: INFO: Pod sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.746: INFO: Pod sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.746: INFO: Pod sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb requesting resource cpu=0m on Node modest-nightingale-745f6d5bd4-w42vq
STEP: Starting Pods to consume most of the cluster CPU.
Feb 12 14:32:13.746: INFO: Creating a pod which consumes cpu=840m on Node modest-nightingale-745f6d5bd4-h2tct
Feb 12 14:32:13.761: INFO: Creating a pod which consumes cpu=770m on Node modest-nightingale-745f6d5bd4-qftnt
Feb 12 14:32:13.775: INFO: Creating a pod which consumes cpu=749m on Node modest-nightingale-745f6d5bd4-w42vq
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce.16630696371f814b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-798/filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce to modest-nightingale-745f6d5bd4-w42vq]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407.16630696791afe41], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407.16630696369e6303], Reason = [Scheduled], Message = [Successfully assigned sched-pred-798/filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407 to modest-nightingale-745f6d5bd4-qftnt]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce.1663069683ad6484], Reason = [Started], Message = [Started container filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407.1663069687e91440], Reason = [Started], Message = [Started container filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce.166306967a6713b4], Reason = [Created], Message = [Created container filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50f2a380-6ed0-4b0c-9bc8-3d3a30dfafce.16630696770fbfda], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247.1663069675974e89], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247.1663069684273a5b], Reason = [Started], Message = [Started container filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247.166306963583216c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-798/filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247 to modest-nightingale-745f6d5bd4-h2tct]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247.1663069678bc8f90], Reason = [Created], Message = [Created container filler-pod-0a15b0c0-31a3-4517-9c02-7d27dc505247]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407.166306967e02391e], Reason = [Created], Message = [Created container filler-pod-b19fbd7b-0218-4db9-accd-ade9ee7c7407]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16630696b19ef106], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16630696b2ef335c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node modest-nightingale-745f6d5bd4-h2tct
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node modest-nightingale-745f6d5bd4-qftnt
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node modest-nightingale-745f6d5bd4-w42vq
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:16.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-798" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":77,"skipped":1399,"failed":0}

------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:16.978: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:17.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9303" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":78,"skipped":1399,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:17.123: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Feb 12 14:32:21.793: INFO: Successfully updated pod "annotationupdated70ee53a-fca4-48aa-947b-c2d5c4cdeb74"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:23.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2326" for this suite.

• [SLOW TEST:6.739 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:23.863: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 12 14:32:24.009: INFO: Number of nodes with available pods: 0
Feb 12 14:32:24.009: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:32:25.026: INFO: Number of nodes with available pods: 0
Feb 12 14:32:25.026: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:32:26.027: INFO: Number of nodes with available pods: 2
Feb 12 14:32:26.027: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 14:32:27.031: INFO: Number of nodes with available pods: 3
Feb 12 14:32:27.031: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 12 14:32:27.112: INFO: Number of nodes with available pods: 2
Feb 12 14:32:27.112: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:28.133: INFO: Number of nodes with available pods: 2
Feb 12 14:32:28.133: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:29.127: INFO: Number of nodes with available pods: 2
Feb 12 14:32:29.127: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:30.130: INFO: Number of nodes with available pods: 2
Feb 12 14:32:30.130: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:31.130: INFO: Number of nodes with available pods: 2
Feb 12 14:32:31.130: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:32.130: INFO: Number of nodes with available pods: 2
Feb 12 14:32:32.130: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:33.126: INFO: Number of nodes with available pods: 2
Feb 12 14:32:33.126: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:34.127: INFO: Number of nodes with available pods: 2
Feb 12 14:32:34.127: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:35.128: INFO: Number of nodes with available pods: 2
Feb 12 14:32:35.128: INFO: Node modest-nightingale-745f6d5bd4-qftnt is running more than one daemon pod
Feb 12 14:32:36.127: INFO: Number of nodes with available pods: 3
Feb 12 14:32:36.127: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-527, will wait for the garbage collector to delete the pods
Feb 12 14:32:36.203: INFO: Deleting DaemonSet.extensions daemon-set took: 13.5577ms
Feb 12 14:32:36.803: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.16376ms
Feb 12 14:32:47.517: INFO: Number of nodes with available pods: 0
Feb 12 14:32:47.517: INFO: Number of running nodes: 0, number of available pods: 0
Feb 12 14:32:47.523: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-527/daemonsets","resourceVersion":"63833"},"items":null}

Feb 12 14:32:47.528: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-527/pods","resourceVersion":"63833"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:47.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-527" for this suite.

• [SLOW TEST:23.720 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":80,"skipped":1442,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:47.583: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-7b597009-f736-412c-b0d7-1abfe1cd5549
STEP: Creating a pod to test consume configMaps
Feb 12 14:32:47.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b" in namespace "projected-4309" to be "Succeeded or Failed"
Feb 12 14:32:47.672: INFO: Pod "pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.170297ms
Feb 12 14:32:49.685: INFO: Pod "pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022253505s
STEP: Saw pod success
Feb 12 14:32:49.686: INFO: Pod "pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b" satisfied condition "Succeeded or Failed"
Feb 12 14:32:49.707: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:32:49.768: INFO: Waiting for pod pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b to disappear
Feb 12 14:32:49.774: INFO: Pod pod-projected-configmaps-6fff01b7-aa78-4cae-8918-7df4e105392b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:32:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4309" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1442,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:32:49.796: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:32:49.850: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5933
I0212 14:32:49.865437      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5933, replica count: 1
I0212 14:32:50.915904      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 14:32:51.916158      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:32:52.035: INFO: Created: latency-svc-jlqdq
Feb 12 14:32:52.046: INFO: Got endpoints: latency-svc-jlqdq [30.20323ms]
Feb 12 14:32:52.078: INFO: Created: latency-svc-t9r4q
Feb 12 14:32:52.088: INFO: Created: latency-svc-2m7xv
Feb 12 14:32:52.088: INFO: Got endpoints: latency-svc-t9r4q [41.141932ms]
Feb 12 14:32:52.104: INFO: Got endpoints: latency-svc-2m7xv [56.624982ms]
Feb 12 14:32:52.131: INFO: Created: latency-svc-j88sr
Feb 12 14:32:52.131: INFO: Created: latency-svc-mwpdr
Feb 12 14:32:52.132: INFO: Got endpoints: latency-svc-mwpdr [84.607459ms]
Feb 12 14:32:52.135: INFO: Got endpoints: latency-svc-j88sr [47.192129ms]
Feb 12 14:32:52.141: INFO: Created: latency-svc-bnplm
Feb 12 14:32:52.154: INFO: Got endpoints: latency-svc-bnplm [106.693451ms]
Feb 12 14:32:52.159: INFO: Created: latency-svc-bdl5j
Feb 12 14:32:52.166: INFO: Got endpoints: latency-svc-bdl5j [117.986941ms]
Feb 12 14:32:52.168: INFO: Created: latency-svc-2x2nx
Feb 12 14:32:52.172: INFO: Got endpoints: latency-svc-2x2nx [123.552023ms]
Feb 12 14:32:52.172: INFO: Created: latency-svc-2g4z2
Feb 12 14:32:52.185: INFO: Got endpoints: latency-svc-2g4z2 [136.82111ms]
Feb 12 14:32:52.194: INFO: Created: latency-svc-sg7r8
Feb 12 14:32:52.197: INFO: Got endpoints: latency-svc-sg7r8 [149.177115ms]
Feb 12 14:32:52.201: INFO: Created: latency-svc-wtmc6
Feb 12 14:32:52.213: INFO: Created: latency-svc-t67ph
Feb 12 14:32:52.214: INFO: Got endpoints: latency-svc-wtmc6 [165.285721ms]
Feb 12 14:32:52.221: INFO: Got endpoints: latency-svc-t67ph [173.200837ms]
Feb 12 14:32:52.226: INFO: Created: latency-svc-psrcz
Feb 12 14:32:52.236: INFO: Got endpoints: latency-svc-psrcz [187.502831ms]
Feb 12 14:32:52.240: INFO: Created: latency-svc-4mj8n
Feb 12 14:32:52.255: INFO: Got endpoints: latency-svc-4mj8n [207.402855ms]
Feb 12 14:32:52.256: INFO: Created: latency-svc-wjgwv
Feb 12 14:32:52.265: INFO: Got endpoints: latency-svc-wjgwv [216.345727ms]
Feb 12 14:32:52.273: INFO: Created: latency-svc-g7jn5
Feb 12 14:32:52.281: INFO: Got endpoints: latency-svc-g7jn5 [233.151791ms]
Feb 12 14:32:52.291: INFO: Created: latency-svc-bwcdq
Feb 12 14:32:52.297: INFO: Created: latency-svc-lnfgp
Feb 12 14:32:52.303: INFO: Got endpoints: latency-svc-bwcdq [254.94853ms]
Feb 12 14:32:52.304: INFO: Got endpoints: latency-svc-lnfgp [200.674711ms]
Feb 12 14:32:52.319: INFO: Created: latency-svc-pxvgf
Feb 12 14:32:52.325: INFO: Got endpoints: latency-svc-pxvgf [189.885481ms]
Feb 12 14:32:52.330: INFO: Created: latency-svc-z6shr
Feb 12 14:32:52.341: INFO: Created: latency-svc-rjxt9
Feb 12 14:32:52.347: INFO: Got endpoints: latency-svc-z6shr [215.219087ms]
Feb 12 14:32:52.360: INFO: Got endpoints: latency-svc-rjxt9 [205.388604ms]
Feb 12 14:32:52.369: INFO: Created: latency-svc-hvvkt
Feb 12 14:32:52.379: INFO: Got endpoints: latency-svc-hvvkt [212.681498ms]
Feb 12 14:32:52.379: INFO: Created: latency-svc-j4mjw
Feb 12 14:32:52.403: INFO: Created: latency-svc-nntx6
Feb 12 14:32:52.403: INFO: Got endpoints: latency-svc-j4mjw [230.801502ms]
Feb 12 14:32:52.407: INFO: Got endpoints: latency-svc-nntx6 [222.573316ms]
Feb 12 14:32:52.414: INFO: Created: latency-svc-phgqd
Feb 12 14:32:52.425: INFO: Created: latency-svc-jhbdj
Feb 12 14:32:52.427: INFO: Got endpoints: latency-svc-phgqd [229.31343ms]
Feb 12 14:32:52.430: INFO: Got endpoints: latency-svc-jhbdj [216.651244ms]
Feb 12 14:32:52.433: INFO: Created: latency-svc-hst2n
Feb 12 14:32:52.438: INFO: Got endpoints: latency-svc-hst2n [216.09574ms]
Feb 12 14:32:52.454: INFO: Created: latency-svc-rvg9m
Feb 12 14:32:52.459: INFO: Created: latency-svc-jn2pj
Feb 12 14:32:52.466: INFO: Got endpoints: latency-svc-rvg9m [229.772211ms]
Feb 12 14:32:52.475: INFO: Got endpoints: latency-svc-jn2pj [219.394447ms]
Feb 12 14:32:52.477: INFO: Created: latency-svc-z58h6
Feb 12 14:32:52.487: INFO: Got endpoints: latency-svc-z58h6 [222.910824ms]
Feb 12 14:32:52.498: INFO: Created: latency-svc-c4f89
Feb 12 14:32:52.509: INFO: Got endpoints: latency-svc-c4f89 [228.052041ms]
Feb 12 14:32:52.510: INFO: Created: latency-svc-wrfpd
Feb 12 14:32:52.510: INFO: Created: latency-svc-mz9jj
Feb 12 14:32:52.513: INFO: Got endpoints: latency-svc-wrfpd [210.028222ms]
Feb 12 14:32:52.520: INFO: Got endpoints: latency-svc-mz9jj [215.152442ms]
Feb 12 14:32:52.526: INFO: Created: latency-svc-8bhxc
Feb 12 14:32:52.534: INFO: Got endpoints: latency-svc-8bhxc [208.559045ms]
Feb 12 14:32:52.542: INFO: Created: latency-svc-gcrqw
Feb 12 14:32:52.552: INFO: Created: latency-svc-5pwht
Feb 12 14:32:52.553: INFO: Got endpoints: latency-svc-gcrqw [206.244956ms]
Feb 12 14:32:52.566: INFO: Got endpoints: latency-svc-5pwht [206.249886ms]
Feb 12 14:32:52.580: INFO: Created: latency-svc-cbmx9
Feb 12 14:32:52.588: INFO: Created: latency-svc-77w4r
Feb 12 14:32:52.589: INFO: Got endpoints: latency-svc-cbmx9 [210.107618ms]
Feb 12 14:32:52.595: INFO: Created: latency-svc-zwhnp
Feb 12 14:32:52.608: INFO: Got endpoints: latency-svc-zwhnp [200.391723ms]
Feb 12 14:32:52.609: INFO: Got endpoints: latency-svc-77w4r [205.667842ms]
Feb 12 14:32:52.618: INFO: Created: latency-svc-z54zx
Feb 12 14:32:52.621: INFO: Created: latency-svc-dglvr
Feb 12 14:32:52.624: INFO: Got endpoints: latency-svc-z54zx [196.578639ms]
Feb 12 14:32:52.628: INFO: Created: latency-svc-4ncsn
Feb 12 14:32:52.631: INFO: Got endpoints: latency-svc-dglvr [200.22235ms]
Feb 12 14:32:52.645: INFO: Got endpoints: latency-svc-4ncsn [207.494274ms]
Feb 12 14:32:52.651: INFO: Created: latency-svc-pzrb2
Feb 12 14:32:52.657: INFO: Created: latency-svc-7kwz5
Feb 12 14:32:52.667: INFO: Created: latency-svc-d6tt5
Feb 12 14:32:52.699: INFO: Created: latency-svc-t774n
Feb 12 14:32:52.699: INFO: Got endpoints: latency-svc-pzrb2 [233.121128ms]
Feb 12 14:32:52.704: INFO: Created: latency-svc-qmvnj
Feb 12 14:32:52.704: INFO: Created: latency-svc-59x29
Feb 12 14:32:52.712: INFO: Created: latency-svc-wdmr6
Feb 12 14:32:52.721: INFO: Created: latency-svc-9mchm
Feb 12 14:32:52.728: INFO: Created: latency-svc-vjdlv
Feb 12 14:32:52.750: INFO: Created: latency-svc-x68d7
Feb 12 14:32:52.750: INFO: Got endpoints: latency-svc-7kwz5 [274.882342ms]
Feb 12 14:32:52.758: INFO: Created: latency-svc-q76r5
Feb 12 14:32:52.774: INFO: Created: latency-svc-ljpw4
Feb 12 14:32:52.793: INFO: Created: latency-svc-7s4n5
Feb 12 14:32:52.794: INFO: Got endpoints: latency-svc-d6tt5 [306.209095ms]
Feb 12 14:32:52.797: INFO: Created: latency-svc-wt5lg
Feb 12 14:32:52.805: INFO: Created: latency-svc-jg4ql
Feb 12 14:32:52.814: INFO: Created: latency-svc-jkvfk
Feb 12 14:32:52.823: INFO: Created: latency-svc-76jq9
Feb 12 14:32:52.829: INFO: Created: latency-svc-dgm7m
Feb 12 14:32:52.846: INFO: Got endpoints: latency-svc-t774n [337.100067ms]
Feb 12 14:32:52.872: INFO: Created: latency-svc-2cdx5
Feb 12 14:32:52.891: INFO: Got endpoints: latency-svc-59x29 [377.896819ms]
Feb 12 14:32:52.912: INFO: Created: latency-svc-bft2c
Feb 12 14:32:52.943: INFO: Got endpoints: latency-svc-qmvnj [423.587955ms]
Feb 12 14:32:52.962: INFO: Created: latency-svc-gs28f
Feb 12 14:32:53.013: INFO: Got endpoints: latency-svc-wdmr6 [479.380309ms]
Feb 12 14:32:53.034: INFO: Created: latency-svc-wbx9x
Feb 12 14:32:53.046: INFO: Got endpoints: latency-svc-9mchm [492.910759ms]
Feb 12 14:32:53.074: INFO: Created: latency-svc-5dpvt
Feb 12 14:32:53.111: INFO: Got endpoints: latency-svc-vjdlv [544.743363ms]
Feb 12 14:32:53.186: INFO: Got endpoints: latency-svc-x68d7 [596.775188ms]
Feb 12 14:32:53.209: INFO: Created: latency-svc-rrzlh
Feb 12 14:32:53.209: INFO: Got endpoints: latency-svc-q76r5 [601.460902ms]
Feb 12 14:32:53.229: INFO: Created: latency-svc-jwj79
Feb 12 14:32:53.232: INFO: Created: latency-svc-7ljb8
Feb 12 14:32:53.243: INFO: Got endpoints: latency-svc-ljpw4 [634.495444ms]
Feb 12 14:32:53.271: INFO: Created: latency-svc-2wm7s
Feb 12 14:32:53.294: INFO: Got endpoints: latency-svc-7s4n5 [669.118088ms]
Feb 12 14:32:53.326: INFO: Created: latency-svc-t9d6p
Feb 12 14:32:53.348: INFO: Got endpoints: latency-svc-wt5lg [717.347385ms]
Feb 12 14:32:53.370: INFO: Created: latency-svc-d9dx7
Feb 12 14:32:53.393: INFO: Got endpoints: latency-svc-jg4ql [747.77894ms]
Feb 12 14:32:53.411: INFO: Created: latency-svc-fqslh
Feb 12 14:32:53.446: INFO: Got endpoints: latency-svc-jkvfk [747.420411ms]
Feb 12 14:32:53.465: INFO: Created: latency-svc-sfvsb
Feb 12 14:32:53.500: INFO: Got endpoints: latency-svc-76jq9 [749.290405ms]
Feb 12 14:32:53.519: INFO: Created: latency-svc-496gc
Feb 12 14:32:53.542: INFO: Got endpoints: latency-svc-dgm7m [748.052978ms]
Feb 12 14:32:53.566: INFO: Created: latency-svc-hsgpz
Feb 12 14:32:53.596: INFO: Got endpoints: latency-svc-2cdx5 [749.812493ms]
Feb 12 14:32:53.616: INFO: Created: latency-svc-scvxs
Feb 12 14:32:53.646: INFO: Got endpoints: latency-svc-bft2c [754.378175ms]
Feb 12 14:32:53.680: INFO: Created: latency-svc-ggbwm
Feb 12 14:32:53.707: INFO: Got endpoints: latency-svc-gs28f [762.891207ms]
Feb 12 14:32:53.725: INFO: Created: latency-svc-wl7vx
Feb 12 14:32:53.745: INFO: Got endpoints: latency-svc-wbx9x [731.383155ms]
Feb 12 14:32:53.765: INFO: Created: latency-svc-8ptbh
Feb 12 14:32:53.798: INFO: Got endpoints: latency-svc-5dpvt [751.472829ms]
Feb 12 14:32:53.816: INFO: Created: latency-svc-qm62z
Feb 12 14:32:53.849: INFO: Got endpoints: latency-svc-rrzlh [737.861295ms]
Feb 12 14:32:53.868: INFO: Created: latency-svc-zg2gq
Feb 12 14:32:53.896: INFO: Got endpoints: latency-svc-jwj79 [710.314542ms]
Feb 12 14:32:53.922: INFO: Created: latency-svc-fm4zl
Feb 12 14:32:53.944: INFO: Got endpoints: latency-svc-7ljb8 [735.219551ms]
Feb 12 14:32:53.962: INFO: Created: latency-svc-k2457
Feb 12 14:32:53.998: INFO: Got endpoints: latency-svc-2wm7s [754.57745ms]
Feb 12 14:32:54.022: INFO: Created: latency-svc-qwwd9
Feb 12 14:32:54.045: INFO: Got endpoints: latency-svc-t9d6p [750.991283ms]
Feb 12 14:32:54.072: INFO: Created: latency-svc-txd9q
Feb 12 14:32:54.104: INFO: Got endpoints: latency-svc-d9dx7 [755.437611ms]
Feb 12 14:32:54.126: INFO: Created: latency-svc-th8kq
Feb 12 14:32:54.146: INFO: Got endpoints: latency-svc-fqslh [752.511078ms]
Feb 12 14:32:54.176: INFO: Created: latency-svc-gvt25
Feb 12 14:32:54.193: INFO: Got endpoints: latency-svc-sfvsb [746.894133ms]
Feb 12 14:32:54.215: INFO: Created: latency-svc-2sv92
Feb 12 14:32:54.248: INFO: Got endpoints: latency-svc-496gc [748.494905ms]
Feb 12 14:32:54.269: INFO: Created: latency-svc-nvrkl
Feb 12 14:32:54.297: INFO: Got endpoints: latency-svc-hsgpz [755.337068ms]
Feb 12 14:32:54.322: INFO: Created: latency-svc-kbwrp
Feb 12 14:32:54.345: INFO: Got endpoints: latency-svc-scvxs [749.285087ms]
Feb 12 14:32:54.371: INFO: Created: latency-svc-gkgpf
Feb 12 14:32:54.405: INFO: Got endpoints: latency-svc-ggbwm [759.082159ms]
Feb 12 14:32:54.424: INFO: Created: latency-svc-zqlrv
Feb 12 14:32:54.446: INFO: Got endpoints: latency-svc-wl7vx [739.211905ms]
Feb 12 14:32:54.479: INFO: Created: latency-svc-mf5p7
Feb 12 14:32:54.514: INFO: Got endpoints: latency-svc-8ptbh [768.934798ms]
Feb 12 14:32:54.529: INFO: Created: latency-svc-442xj
Feb 12 14:32:54.544: INFO: Got endpoints: latency-svc-qm62z [745.997529ms]
Feb 12 14:32:54.561: INFO: Created: latency-svc-xcbqj
Feb 12 14:32:54.600: INFO: Got endpoints: latency-svc-zg2gq [751.223387ms]
Feb 12 14:32:54.619: INFO: Created: latency-svc-wzmsk
Feb 12 14:32:54.642: INFO: Got endpoints: latency-svc-fm4zl [745.76687ms]
Feb 12 14:32:54.662: INFO: Created: latency-svc-nw2kv
Feb 12 14:32:54.707: INFO: Got endpoints: latency-svc-k2457 [762.338612ms]
Feb 12 14:32:54.728: INFO: Created: latency-svc-zjkrh
Feb 12 14:32:54.746: INFO: Got endpoints: latency-svc-qwwd9 [748.223216ms]
Feb 12 14:32:54.768: INFO: Created: latency-svc-5n662
Feb 12 14:32:54.796: INFO: Got endpoints: latency-svc-txd9q [751.271165ms]
Feb 12 14:32:54.815: INFO: Created: latency-svc-hf4ms
Feb 12 14:32:54.845: INFO: Got endpoints: latency-svc-th8kq [740.522881ms]
Feb 12 14:32:54.861: INFO: Created: latency-svc-w2rzh
Feb 12 14:32:54.893: INFO: Got endpoints: latency-svc-gvt25 [746.732588ms]
Feb 12 14:32:54.914: INFO: Created: latency-svc-v96gc
Feb 12 14:32:54.947: INFO: Got endpoints: latency-svc-2sv92 [753.525333ms]
Feb 12 14:32:54.972: INFO: Created: latency-svc-wb6fz
Feb 12 14:32:54.990: INFO: Got endpoints: latency-svc-nvrkl [741.935708ms]
Feb 12 14:32:55.015: INFO: Created: latency-svc-rmxrz
Feb 12 14:32:55.049: INFO: Got endpoints: latency-svc-kbwrp [751.386288ms]
Feb 12 14:32:55.066: INFO: Created: latency-svc-7c6xh
Feb 12 14:32:55.119: INFO: Got endpoints: latency-svc-gkgpf [773.302182ms]
Feb 12 14:32:55.134: INFO: Created: latency-svc-fkjp8
Feb 12 14:32:55.144: INFO: Got endpoints: latency-svc-zqlrv [739.36858ms]
Feb 12 14:32:55.160: INFO: Created: latency-svc-6lllq
Feb 12 14:32:55.193: INFO: Got endpoints: latency-svc-mf5p7 [747.614448ms]
Feb 12 14:32:55.209: INFO: Created: latency-svc-w5wp6
Feb 12 14:32:55.242: INFO: Got endpoints: latency-svc-442xj [727.914061ms]
Feb 12 14:32:55.260: INFO: Created: latency-svc-9xhwj
Feb 12 14:32:55.298: INFO: Got endpoints: latency-svc-xcbqj [753.431825ms]
Feb 12 14:32:55.324: INFO: Created: latency-svc-ll58f
Feb 12 14:32:55.344: INFO: Got endpoints: latency-svc-wzmsk [744.481877ms]
Feb 12 14:32:55.366: INFO: Created: latency-svc-sk5rk
Feb 12 14:32:55.398: INFO: Got endpoints: latency-svc-nw2kv [756.004176ms]
Feb 12 14:32:55.427: INFO: Created: latency-svc-z67xm
Feb 12 14:32:55.442: INFO: Got endpoints: latency-svc-zjkrh [734.924023ms]
Feb 12 14:32:55.464: INFO: Created: latency-svc-ms6kj
Feb 12 14:32:55.504: INFO: Got endpoints: latency-svc-5n662 [758.1843ms]
Feb 12 14:32:55.522: INFO: Created: latency-svc-cwkmt
Feb 12 14:32:55.545: INFO: Got endpoints: latency-svc-hf4ms [748.736889ms]
Feb 12 14:32:55.569: INFO: Created: latency-svc-vrh7p
Feb 12 14:32:55.593: INFO: Got endpoints: latency-svc-w2rzh [748.463589ms]
Feb 12 14:32:55.619: INFO: Created: latency-svc-6mq56
Feb 12 14:32:55.645: INFO: Got endpoints: latency-svc-v96gc [752.235268ms]
Feb 12 14:32:55.669: INFO: Created: latency-svc-mxqnk
Feb 12 14:32:55.695: INFO: Got endpoints: latency-svc-wb6fz [748.317193ms]
Feb 12 14:32:55.714: INFO: Created: latency-svc-gwkxk
Feb 12 14:32:55.747: INFO: Got endpoints: latency-svc-rmxrz [756.254012ms]
Feb 12 14:32:55.766: INFO: Created: latency-svc-zqlmt
Feb 12 14:32:55.796: INFO: Got endpoints: latency-svc-7c6xh [747.657208ms]
Feb 12 14:32:55.816: INFO: Created: latency-svc-ss5fl
Feb 12 14:32:55.846: INFO: Got endpoints: latency-svc-fkjp8 [726.82823ms]
Feb 12 14:32:55.863: INFO: Created: latency-svc-z8kbv
Feb 12 14:32:55.897: INFO: Got endpoints: latency-svc-6lllq [752.570639ms]
Feb 12 14:32:55.918: INFO: Created: latency-svc-wkf42
Feb 12 14:32:55.943: INFO: Got endpoints: latency-svc-w5wp6 [749.402949ms]
Feb 12 14:32:55.960: INFO: Created: latency-svc-mftvw
Feb 12 14:32:55.992: INFO: Got endpoints: latency-svc-9xhwj [750.215354ms]
Feb 12 14:32:56.007: INFO: Created: latency-svc-4fpgx
Feb 12 14:32:56.047: INFO: Got endpoints: latency-svc-ll58f [749.76954ms]
Feb 12 14:32:56.072: INFO: Created: latency-svc-md57w
Feb 12 14:32:56.095: INFO: Got endpoints: latency-svc-sk5rk [750.423832ms]
Feb 12 14:32:56.126: INFO: Created: latency-svc-mtncj
Feb 12 14:32:56.147: INFO: Got endpoints: latency-svc-z67xm [748.557251ms]
Feb 12 14:32:56.166: INFO: Created: latency-svc-zq9fl
Feb 12 14:32:56.192: INFO: Got endpoints: latency-svc-ms6kj [749.489707ms]
Feb 12 14:32:56.211: INFO: Created: latency-svc-kjg7z
Feb 12 14:32:56.243: INFO: Got endpoints: latency-svc-cwkmt [738.024087ms]
Feb 12 14:32:56.261: INFO: Created: latency-svc-gxhrx
Feb 12 14:32:56.294: INFO: Got endpoints: latency-svc-vrh7p [749.257655ms]
Feb 12 14:32:56.309: INFO: Created: latency-svc-6p2kj
Feb 12 14:32:56.347: INFO: Got endpoints: latency-svc-6mq56 [753.394786ms]
Feb 12 14:32:56.367: INFO: Created: latency-svc-9dgx4
Feb 12 14:32:56.392: INFO: Got endpoints: latency-svc-mxqnk [747.027133ms]
Feb 12 14:32:56.407: INFO: Created: latency-svc-czhn5
Feb 12 14:32:56.452: INFO: Got endpoints: latency-svc-gwkxk [756.469425ms]
Feb 12 14:32:56.483: INFO: Created: latency-svc-4lk6d
Feb 12 14:32:56.491: INFO: Got endpoints: latency-svc-zqlmt [744.536526ms]
Feb 12 14:32:56.509: INFO: Created: latency-svc-5dwfq
Feb 12 14:32:56.550: INFO: Got endpoints: latency-svc-ss5fl [753.279305ms]
Feb 12 14:32:56.571: INFO: Created: latency-svc-sh8d6
Feb 12 14:32:56.598: INFO: Got endpoints: latency-svc-z8kbv [752.580599ms]
Feb 12 14:32:56.616: INFO: Created: latency-svc-pb544
Feb 12 14:32:56.645: INFO: Got endpoints: latency-svc-wkf42 [748.019727ms]
Feb 12 14:32:56.663: INFO: Created: latency-svc-q9j9m
Feb 12 14:32:56.698: INFO: Got endpoints: latency-svc-mftvw [755.308655ms]
Feb 12 14:32:56.715: INFO: Created: latency-svc-74gpp
Feb 12 14:32:56.747: INFO: Got endpoints: latency-svc-4fpgx [754.494848ms]
Feb 12 14:32:56.771: INFO: Created: latency-svc-p6njs
Feb 12 14:32:56.796: INFO: Got endpoints: latency-svc-md57w [748.968443ms]
Feb 12 14:32:56.814: INFO: Created: latency-svc-76q4l
Feb 12 14:32:56.847: INFO: Got endpoints: latency-svc-mtncj [751.882455ms]
Feb 12 14:32:56.863: INFO: Created: latency-svc-krk2d
Feb 12 14:32:56.897: INFO: Got endpoints: latency-svc-zq9fl [750.143894ms]
Feb 12 14:32:56.914: INFO: Created: latency-svc-gdrlx
Feb 12 14:32:56.942: INFO: Got endpoints: latency-svc-kjg7z [749.708661ms]
Feb 12 14:32:56.963: INFO: Created: latency-svc-l6rrc
Feb 12 14:32:57.008: INFO: Got endpoints: latency-svc-gxhrx [764.932606ms]
Feb 12 14:32:57.025: INFO: Created: latency-svc-2mftq
Feb 12 14:32:57.049: INFO: Got endpoints: latency-svc-6p2kj [754.42864ms]
Feb 12 14:32:57.070: INFO: Created: latency-svc-xbxzq
Feb 12 14:32:57.098: INFO: Got endpoints: latency-svc-9dgx4 [750.788474ms]
Feb 12 14:32:57.115: INFO: Created: latency-svc-zxlhz
Feb 12 14:32:57.147: INFO: Got endpoints: latency-svc-czhn5 [754.293058ms]
Feb 12 14:32:57.168: INFO: Created: latency-svc-m6dgk
Feb 12 14:32:57.194: INFO: Got endpoints: latency-svc-4lk6d [742.042617ms]
Feb 12 14:32:57.209: INFO: Created: latency-svc-fq995
Feb 12 14:32:57.254: INFO: Got endpoints: latency-svc-5dwfq [762.573216ms]
Feb 12 14:32:57.276: INFO: Created: latency-svc-h869t
Feb 12 14:32:57.295: INFO: Got endpoints: latency-svc-sh8d6 [745.421967ms]
Feb 12 14:32:57.312: INFO: Created: latency-svc-d8zqz
Feb 12 14:32:57.348: INFO: Got endpoints: latency-svc-pb544 [749.112684ms]
Feb 12 14:32:57.365: INFO: Created: latency-svc-lgggv
Feb 12 14:32:57.397: INFO: Got endpoints: latency-svc-q9j9m [751.224611ms]
Feb 12 14:32:57.414: INFO: Created: latency-svc-ppwhg
Feb 12 14:32:57.446: INFO: Got endpoints: latency-svc-74gpp [747.534152ms]
Feb 12 14:32:57.463: INFO: Created: latency-svc-66jzt
Feb 12 14:32:57.492: INFO: Got endpoints: latency-svc-p6njs [744.827658ms]
Feb 12 14:32:57.512: INFO: Created: latency-svc-t25br
Feb 12 14:32:57.544: INFO: Got endpoints: latency-svc-76q4l [747.256351ms]
Feb 12 14:32:57.568: INFO: Created: latency-svc-x9kwz
Feb 12 14:32:57.594: INFO: Got endpoints: latency-svc-krk2d [746.959564ms]
Feb 12 14:32:57.608: INFO: Created: latency-svc-64kb9
Feb 12 14:32:57.644: INFO: Got endpoints: latency-svc-gdrlx [746.58126ms]
Feb 12 14:32:57.667: INFO: Created: latency-svc-6dlwv
Feb 12 14:32:57.692: INFO: Got endpoints: latency-svc-l6rrc [750.297703ms]
Feb 12 14:32:57.716: INFO: Created: latency-svc-wxd8c
Feb 12 14:32:57.748: INFO: Got endpoints: latency-svc-2mftq [740.287932ms]
Feb 12 14:32:57.767: INFO: Created: latency-svc-tngp2
Feb 12 14:32:57.792: INFO: Got endpoints: latency-svc-xbxzq [743.491162ms]
Feb 12 14:32:57.817: INFO: Created: latency-svc-9tsvs
Feb 12 14:32:57.844: INFO: Got endpoints: latency-svc-zxlhz [746.588057ms]
Feb 12 14:32:57.872: INFO: Created: latency-svc-ckd65
Feb 12 14:32:57.901: INFO: Got endpoints: latency-svc-m6dgk [753.974369ms]
Feb 12 14:32:57.938: INFO: Created: latency-svc-8b75s
Feb 12 14:32:57.946: INFO: Got endpoints: latency-svc-fq995 [752.048755ms]
Feb 12 14:32:57.974: INFO: Created: latency-svc-k5ck8
Feb 12 14:32:57.999: INFO: Got endpoints: latency-svc-h869t [744.435073ms]
Feb 12 14:32:58.016: INFO: Created: latency-svc-6z778
Feb 12 14:32:58.046: INFO: Got endpoints: latency-svc-d8zqz [750.040092ms]
Feb 12 14:32:58.065: INFO: Created: latency-svc-swkrb
Feb 12 14:32:58.093: INFO: Got endpoints: latency-svc-lgggv [744.999967ms]
Feb 12 14:32:58.111: INFO: Created: latency-svc-dl4zb
Feb 12 14:32:58.150: INFO: Got endpoints: latency-svc-ppwhg [752.892653ms]
Feb 12 14:32:58.172: INFO: Created: latency-svc-jsvq2
Feb 12 14:32:58.194: INFO: Got endpoints: latency-svc-66jzt [748.303979ms]
Feb 12 14:32:58.209: INFO: Created: latency-svc-ktm92
Feb 12 14:32:58.246: INFO: Got endpoints: latency-svc-t25br [754.02763ms]
Feb 12 14:32:58.275: INFO: Created: latency-svc-8jbwm
Feb 12 14:32:58.292: INFO: Got endpoints: latency-svc-x9kwz [748.336589ms]
Feb 12 14:32:58.312: INFO: Created: latency-svc-qwd9z
Feb 12 14:32:58.346: INFO: Got endpoints: latency-svc-64kb9 [752.10999ms]
Feb 12 14:32:58.362: INFO: Created: latency-svc-9c8cq
Feb 12 14:32:58.394: INFO: Got endpoints: latency-svc-6dlwv [749.463449ms]
Feb 12 14:32:58.415: INFO: Created: latency-svc-vxtst
Feb 12 14:32:58.448: INFO: Got endpoints: latency-svc-wxd8c [756.51293ms]
Feb 12 14:32:58.467: INFO: Created: latency-svc-f55sf
Feb 12 14:32:58.493: INFO: Got endpoints: latency-svc-tngp2 [745.429048ms]
Feb 12 14:32:58.512: INFO: Created: latency-svc-7fftf
Feb 12 14:32:58.543: INFO: Got endpoints: latency-svc-9tsvs [750.940091ms]
Feb 12 14:32:58.562: INFO: Created: latency-svc-rb2l4
Feb 12 14:32:58.592: INFO: Got endpoints: latency-svc-ckd65 [747.52412ms]
Feb 12 14:32:58.611: INFO: Created: latency-svc-st5r4
Feb 12 14:32:58.646: INFO: Got endpoints: latency-svc-8b75s [745.103292ms]
Feb 12 14:32:58.672: INFO: Created: latency-svc-7v6rk
Feb 12 14:32:58.691: INFO: Got endpoints: latency-svc-k5ck8 [744.68092ms]
Feb 12 14:32:58.709: INFO: Created: latency-svc-vww7k
Feb 12 14:32:58.744: INFO: Got endpoints: latency-svc-6z778 [745.399249ms]
Feb 12 14:32:58.771: INFO: Created: latency-svc-6gscz
Feb 12 14:32:58.792: INFO: Got endpoints: latency-svc-swkrb [746.674432ms]
Feb 12 14:32:58.819: INFO: Created: latency-svc-bdrcd
Feb 12 14:32:58.847: INFO: Got endpoints: latency-svc-dl4zb [754.236831ms]
Feb 12 14:32:58.869: INFO: Created: latency-svc-qrl4g
Feb 12 14:32:58.897: INFO: Got endpoints: latency-svc-jsvq2 [746.786087ms]
Feb 12 14:32:58.915: INFO: Created: latency-svc-5vrxr
Feb 12 14:32:58.946: INFO: Got endpoints: latency-svc-ktm92 [751.305976ms]
Feb 12 14:32:58.960: INFO: Created: latency-svc-bqh9q
Feb 12 14:32:58.995: INFO: Got endpoints: latency-svc-8jbwm [748.981122ms]
Feb 12 14:32:59.094: INFO: Got endpoints: latency-svc-9c8cq [747.703215ms]
Feb 12 14:32:59.099: INFO: Got endpoints: latency-svc-qwd9z [806.321459ms]
Feb 12 14:32:59.143: INFO: Got endpoints: latency-svc-vxtst [749.858754ms]
Feb 12 14:32:59.161: INFO: Created: latency-svc-46r52
Feb 12 14:32:59.165: INFO: Created: latency-svc-h4rk6
Feb 12 14:32:59.174: INFO: Created: latency-svc-ctkt2
Feb 12 14:32:59.244: INFO: Got endpoints: latency-svc-7fftf [750.965592ms]
Feb 12 14:32:59.296: INFO: Got endpoints: latency-svc-rb2l4 [753.331072ms]
Feb 12 14:32:59.397: INFO: Got endpoints: latency-svc-7v6rk [751.303837ms]
Feb 12 14:32:59.444: INFO: Got endpoints: latency-svc-vww7k [753.343495ms]
Feb 12 14:32:59.500: INFO: Got endpoints: latency-svc-6gscz [755.505843ms]
Feb 12 14:32:59.592: INFO: Got endpoints: latency-svc-qrl4g [745.357244ms]
Feb 12 14:32:59.613: INFO: Got endpoints: latency-svc-st5r4 [1.021125986s]
Feb 12 14:32:59.614: INFO: Got endpoints: latency-svc-f55sf [1.165181545s]
Feb 12 14:32:59.696: INFO: Got endpoints: latency-svc-bqh9q [749.971418ms]
Feb 12 14:32:59.744: INFO: Got endpoints: latency-svc-46r52 [749.272626ms]
Feb 12 14:32:59.782: INFO: Got endpoints: latency-svc-bdrcd [989.706515ms]
Feb 12 14:32:59.790: INFO: Got endpoints: latency-svc-5vrxr [893.385582ms]
Feb 12 14:32:59.795: INFO: Got endpoints: latency-svc-h4rk6 [700.446571ms]
Feb 12 14:32:59.806: INFO: Created: latency-svc-vjqkt
Feb 12 14:32:59.810: INFO: Created: latency-svc-66wx7
Feb 12 14:32:59.821: INFO: Created: latency-svc-82v6h
Feb 12 14:32:59.849: INFO: Got endpoints: latency-svc-ctkt2 [749.895857ms]
Feb 12 14:32:59.853: INFO: Created: latency-svc-pwjbl
Feb 12 14:32:59.868: INFO: Created: latency-svc-9f6ct
Feb 12 14:32:59.878: INFO: Created: latency-svc-kh6t9
Feb 12 14:32:59.890: INFO: Created: latency-svc-rzqrx
Feb 12 14:32:59.893: INFO: Got endpoints: latency-svc-vjqkt [749.715926ms]
Feb 12 14:32:59.903: INFO: Created: latency-svc-b5k8h
Feb 12 14:32:59.915: INFO: Created: latency-svc-tlrfd
Feb 12 14:32:59.933: INFO: Created: latency-svc-v9s8p
Feb 12 14:32:59.947: INFO: Got endpoints: latency-svc-66wx7 [702.200318ms]
Feb 12 14:32:59.951: INFO: Created: latency-svc-q2szv
Feb 12 14:32:59.963: INFO: Created: latency-svc-46tbh
Feb 12 14:32:59.985: INFO: Created: latency-svc-8x4xs
Feb 12 14:32:59.989: INFO: Created: latency-svc-xqpdq
Feb 12 14:32:59.997: INFO: Got endpoints: latency-svc-82v6h [700.021676ms]
Feb 12 14:33:00.000: INFO: Created: latency-svc-c7cw7
Feb 12 14:33:00.050: INFO: Got endpoints: latency-svc-pwjbl [652.154152ms]
Feb 12 14:33:00.103: INFO: Got endpoints: latency-svc-9f6ct [658.11833ms]
Feb 12 14:33:00.170: INFO: Got endpoints: latency-svc-kh6t9 [670.471601ms]
Feb 12 14:33:00.208: INFO: Got endpoints: latency-svc-rzqrx [615.230651ms]
Feb 12 14:33:00.243: INFO: Got endpoints: latency-svc-b5k8h [629.691901ms]
Feb 12 14:33:00.293: INFO: Got endpoints: latency-svc-tlrfd [678.419693ms]
Feb 12 14:33:00.346: INFO: Got endpoints: latency-svc-v9s8p [650.440158ms]
Feb 12 14:33:00.404: INFO: Got endpoints: latency-svc-q2szv [660.025614ms]
Feb 12 14:33:00.447: INFO: Got endpoints: latency-svc-46tbh [664.979999ms]
Feb 12 14:33:00.495: INFO: Got endpoints: latency-svc-8x4xs [704.439197ms]
Feb 12 14:33:00.544: INFO: Got endpoints: latency-svc-xqpdq [748.721832ms]
Feb 12 14:33:00.596: INFO: Got endpoints: latency-svc-c7cw7 [746.645389ms]
Feb 12 14:33:00.596: INFO: Latencies: [41.141932ms 47.192129ms 56.624982ms 84.607459ms 106.693451ms 117.986941ms 123.552023ms 136.82111ms 149.177115ms 165.285721ms 173.200837ms 187.502831ms 189.885481ms 196.578639ms 200.22235ms 200.391723ms 200.674711ms 205.388604ms 205.667842ms 206.244956ms 206.249886ms 207.402855ms 207.494274ms 208.559045ms 210.028222ms 210.107618ms 212.681498ms 215.152442ms 215.219087ms 216.09574ms 216.345727ms 216.651244ms 219.394447ms 222.573316ms 222.910824ms 228.052041ms 229.31343ms 229.772211ms 230.801502ms 233.121128ms 233.151791ms 254.94853ms 274.882342ms 306.209095ms 337.100067ms 377.896819ms 423.587955ms 479.380309ms 492.910759ms 544.743363ms 596.775188ms 601.460902ms 615.230651ms 629.691901ms 634.495444ms 650.440158ms 652.154152ms 658.11833ms 660.025614ms 664.979999ms 669.118088ms 670.471601ms 678.419693ms 700.021676ms 700.446571ms 702.200318ms 704.439197ms 710.314542ms 717.347385ms 726.82823ms 727.914061ms 731.383155ms 734.924023ms 735.219551ms 737.861295ms 738.024087ms 739.211905ms 739.36858ms 740.287932ms 740.522881ms 741.935708ms 742.042617ms 743.491162ms 744.435073ms 744.481877ms 744.536526ms 744.68092ms 744.827658ms 744.999967ms 745.103292ms 745.357244ms 745.399249ms 745.421967ms 745.429048ms 745.76687ms 745.997529ms 746.58126ms 746.588057ms 746.645389ms 746.674432ms 746.732588ms 746.786087ms 746.894133ms 746.959564ms 747.027133ms 747.256351ms 747.420411ms 747.52412ms 747.534152ms 747.614448ms 747.657208ms 747.703215ms 747.77894ms 748.019727ms 748.052978ms 748.223216ms 748.303979ms 748.317193ms 748.336589ms 748.463589ms 748.494905ms 748.557251ms 748.721832ms 748.736889ms 748.968443ms 748.981122ms 749.112684ms 749.257655ms 749.272626ms 749.285087ms 749.290405ms 749.402949ms 749.463449ms 749.489707ms 749.708661ms 749.715926ms 749.76954ms 749.812493ms 749.858754ms 749.895857ms 749.971418ms 750.040092ms 750.143894ms 750.215354ms 750.297703ms 750.423832ms 750.788474ms 750.940091ms 750.965592ms 750.991283ms 751.223387ms 751.224611ms 751.271165ms 751.303837ms 751.305976ms 751.386288ms 751.472829ms 751.882455ms 752.048755ms 752.10999ms 752.235268ms 752.511078ms 752.570639ms 752.580599ms 752.892653ms 753.279305ms 753.331072ms 753.343495ms 753.394786ms 753.431825ms 753.525333ms 753.974369ms 754.02763ms 754.236831ms 754.293058ms 754.378175ms 754.42864ms 754.494848ms 754.57745ms 755.308655ms 755.337068ms 755.437611ms 755.505843ms 756.004176ms 756.254012ms 756.469425ms 756.51293ms 758.1843ms 759.082159ms 762.338612ms 762.573216ms 762.891207ms 764.932606ms 768.934798ms 773.302182ms 806.321459ms 893.385582ms 989.706515ms 1.021125986s 1.165181545s]
Feb 12 14:33:00.596: INFO: 50 %ile: 746.732588ms
Feb 12 14:33:00.596: INFO: 90 %ile: 755.337068ms
Feb 12 14:33:00.596: INFO: 99 %ile: 1.021125986s
Feb 12 14:33:00.596: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:33:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5933" for this suite.

• [SLOW TEST:10.826 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":82,"skipped":1443,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:33:00.634: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 12 14:33:00.729: INFO: Waiting up to 5m0s for pod "pod-df979cc9-982c-4eab-92af-5e2d9b549468" in namespace "emptydir-374" to be "Succeeded or Failed"
Feb 12 14:33:00.743: INFO: Pod "pod-df979cc9-982c-4eab-92af-5e2d9b549468": Phase="Pending", Reason="", readiness=false. Elapsed: 13.006966ms
Feb 12 14:33:02.749: INFO: Pod "pod-df979cc9-982c-4eab-92af-5e2d9b549468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019286661s
Feb 12 14:33:04.757: INFO: Pod "pod-df979cc9-982c-4eab-92af-5e2d9b549468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027539809s
STEP: Saw pod success
Feb 12 14:33:04.757: INFO: Pod "pod-df979cc9-982c-4eab-92af-5e2d9b549468" satisfied condition "Succeeded or Failed"
Feb 12 14:33:04.765: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-df979cc9-982c-4eab-92af-5e2d9b549468 container test-container: <nil>
STEP: delete the pod
Feb 12 14:33:04.845: INFO: Waiting for pod pod-df979cc9-982c-4eab-92af-5e2d9b549468 to disappear
Feb 12 14:33:04.851: INFO: Pod pod-df979cc9-982c-4eab-92af-5e2d9b549468 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:33:04.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-374" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:33:04.874: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-8d42a611-f5e0-4e61-bf6c-260d12267cf2
STEP: Creating a pod to test consume configMaps
Feb 12 14:33:04.954: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279" in namespace "projected-6550" to be "Succeeded or Failed"
Feb 12 14:33:04.963: INFO: Pod "pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279": Phase="Pending", Reason="", readiness=false. Elapsed: 9.613931ms
Feb 12 14:33:06.969: INFO: Pod "pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015657883s
STEP: Saw pod success
Feb 12 14:33:06.969: INFO: Pod "pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279" satisfied condition "Succeeded or Failed"
Feb 12 14:33:06.991: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:33:07.043: INFO: Waiting for pod pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279 to disappear
Feb 12 14:33:07.060: INFO: Pod pod-projected-configmaps-7d14006f-6e37-4bdf-9624-778d68a5a279 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:33:07.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6550" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":84,"skipped":1518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:33:07.096: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:33:07.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29" in namespace "projected-4030" to be "Succeeded or Failed"
Feb 12 14:33:07.195: INFO: Pod "downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.778135ms
Feb 12 14:33:09.205: INFO: Pod "downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019152826s
Feb 12 14:33:11.218: INFO: Pod "downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033058146s
STEP: Saw pod success
Feb 12 14:33:11.219: INFO: Pod "downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29" satisfied condition "Succeeded or Failed"
Feb 12 14:33:11.228: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29 container client-container: <nil>
STEP: delete the pod
Feb 12 14:33:11.315: INFO: Waiting for pod downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29 to disappear
Feb 12 14:33:11.320: INFO: Pod downwardapi-volume-e98749c5-a468-4175-98b6-e93e9db5ea29 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:33:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4030" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:33:11.349: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8758
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8758
I0212 14:33:11.490392      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8758, replica count: 2
I0212 14:33:14.540740      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:33:14.541: INFO: Creating new exec pod
Feb 12 14:33:17.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-8758 execpod7bfgp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 12 14:33:18.192: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 12 14:33:18.192: INFO: stdout: ""
Feb 12 14:33:18.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-8758 execpod7bfgp -- /bin/sh -x -c nc -zv -t -w 2 10.240.29.234 80'
Feb 12 14:33:18.855: INFO: stderr: "+ nc -zv -t -w 2 10.240.29.234 80\nConnection to 10.240.29.234 80 port [tcp/http] succeeded!\n"
Feb 12 14:33:18.855: INFO: stdout: ""
Feb 12 14:33:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-8758 execpod7bfgp -- /bin/sh -x -c nc -zv -t -w 2 68.183.68.155 30138'
Feb 12 14:33:19.444: INFO: stderr: "+ nc -zv -t -w 2 68.183.68.155 30138\nConnection to 68.183.68.155 30138 port [tcp/30138] succeeded!\n"
Feb 12 14:33:19.444: INFO: stdout: ""
Feb 12 14:33:19.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-8758 execpod7bfgp -- /bin/sh -x -c nc -zv -t -w 2 104.248.253.154 30138'
Feb 12 14:33:20.046: INFO: stderr: "+ nc -zv -t -w 2 104.248.253.154 30138\nConnection to 104.248.253.154 30138 port [tcp/30138] succeeded!\n"
Feb 12 14:33:20.046: INFO: stdout: ""
Feb 12 14:33:20.046: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:33:20.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8758" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.781 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":86,"skipped":1581,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:33:20.130: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:35:20.304: INFO: Deleting pod "var-expansion-0b0c7d0e-037c-4687-b234-ade4f157f31e" in namespace "var-expansion-9164"
Feb 12 14:35:20.325: INFO: Wait up to 5m0s for pod "var-expansion-0b0c7d0e-037c-4687-b234-ade4f157f31e" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:35:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9164" for this suite.

• [SLOW TEST:122.232 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":87,"skipped":1585,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:35:22.362: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:35:22.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2431" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":88,"skipped":1592,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:35:22.593: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:35:22.654: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:35:24.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3330" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:35:25.000: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-3104dd35-00f0-4517-b73d-2bd5b976b5b8 in namespace container-probe-2996
Feb 12 14:35:27.100: INFO: Started pod busybox-3104dd35-00f0-4517-b73d-2bd5b976b5b8 in namespace container-probe-2996
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:35:27.109: INFO: Initial restart count of pod busybox-3104dd35-00f0-4517-b73d-2bd5b976b5b8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:39:28.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2996" for this suite.

• [SLOW TEST:243.187 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:39:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:39:28.778: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:39:30.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737568, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737568, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737568, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737568, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:39:33.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:39:33.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4340" for this suite.
STEP: Destroying namespace "webhook-4340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.785 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":91,"skipped":1647,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:39:33.975: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-9eaf33e4-47bb-4a02-97ea-1e9ac0f46440
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9eaf33e4-47bb-4a02-97ea-1e9ac0f46440
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:40:49.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3533" for this suite.

• [SLOW TEST:75.210 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":92,"skipped":1648,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:40:49.185: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 12 14:40:49.265: INFO: Waiting up to 5m0s for pod "pod-c7f03088-f203-440d-a5a5-c53dc2a54850" in namespace "emptydir-6553" to be "Succeeded or Failed"
Feb 12 14:40:49.281: INFO: Pod "pod-c7f03088-f203-440d-a5a5-c53dc2a54850": Phase="Pending", Reason="", readiness=false. Elapsed: 15.915799ms
Feb 12 14:40:51.288: INFO: Pod "pod-c7f03088-f203-440d-a5a5-c53dc2a54850": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023298601s
Feb 12 14:40:53.295: INFO: Pod "pod-c7f03088-f203-440d-a5a5-c53dc2a54850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03039711s
STEP: Saw pod success
Feb 12 14:40:53.295: INFO: Pod "pod-c7f03088-f203-440d-a5a5-c53dc2a54850" satisfied condition "Succeeded or Failed"
Feb 12 14:40:53.303: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-c7f03088-f203-440d-a5a5-c53dc2a54850 container test-container: <nil>
STEP: delete the pod
Feb 12 14:40:53.385: INFO: Waiting for pod pod-c7f03088-f203-440d-a5a5-c53dc2a54850 to disappear
Feb 12 14:40:53.391: INFO: Pod pod-c7f03088-f203-440d-a5a5-c53dc2a54850 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:40:53.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6553" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1656,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:40:53.418: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3807
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-3807
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3807
Feb 12 14:40:53.532: INFO: Found 0 stateful pods, waiting for 1
Feb 12 14:41:03.542: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 12 14:41:03.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:41:04.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:41:04.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:41:04.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:41:04.311: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 12 14:41:14.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:41:14.321: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:41:14.350: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 12 14:41:14.350: INFO: ss-0  modest-nightingale-745f6d5bd4-h2tct  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  }]
Feb 12 14:41:14.350: INFO: 
Feb 12 14:41:14.350: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 12 14:41:15.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992935218s
Feb 12 14:41:16.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98442054s
Feb 12 14:41:17.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9502214s
Feb 12 14:41:18.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931551298s
Feb 12 14:41:19.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.923501485s
Feb 12 14:41:20.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911026501s
Feb 12 14:41:21.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.904814377s
Feb 12 14:41:22.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.897856138s
Feb 12 14:41:23.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.158806ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3807
Feb 12 14:41:24.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:41:25.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 14:41:25.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:41:25.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:41:25.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:41:25.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 12 14:41:25.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:41:25.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:41:25.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 14:41:26.332: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 12 14:41:26.332: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 14:41:26.332: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 14:41:26.343: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:41:26.343: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 14:41:26.343: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 12 14:41:26.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:41:26.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:41:26.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:41:26.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:41:26.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:41:27.620: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:41:27.620: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:41:27.620: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:41:27.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=statefulset-3807 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 14:41:28.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 14:41:28.228: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 14:41:28.228: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 14:41:28.228: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:41:28.237: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 12 14:41:38.253: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:41:38.253: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:41:38.253: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 14:41:38.278: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 12 14:41:38.278: INFO: ss-0  modest-nightingale-745f6d5bd4-h2tct  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  }]
Feb 12 14:41:38.278: INFO: ss-1  modest-nightingale-745f6d5bd4-w42vq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:38.279: INFO: ss-2  modest-nightingale-745f6d5bd4-qftnt  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:38.279: INFO: 
Feb 12 14:41:38.279: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 12 14:41:39.285: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 12 14:41:39.285: INFO: ss-0  modest-nightingale-745f6d5bd4-h2tct  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:40:53 +0000 UTC  }]
Feb 12 14:41:39.285: INFO: ss-1  modest-nightingale-745f6d5bd4-w42vq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:39.285: INFO: ss-2  modest-nightingale-745f6d5bd4-qftnt  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:39.285: INFO: 
Feb 12 14:41:39.285: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 12 14:41:40.293: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 12 14:41:40.293: INFO: ss-1  modest-nightingale-745f6d5bd4-w42vq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:40.293: INFO: ss-2  modest-nightingale-745f6d5bd4-qftnt  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-12 14:41:14 +0000 UTC  }]
Feb 12 14:41:40.293: INFO: 
Feb 12 14:41:40.293: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 12 14:41:41.301: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.978687712s
Feb 12 14:41:42.327: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.970489891s
Feb 12 14:41:43.334: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.943948831s
Feb 12 14:41:44.345: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.937359556s
Feb 12 14:41:45.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.926164474s
Feb 12 14:41:46.360: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.918755419s
Feb 12 14:41:47.372: INFO: Verifying statefulset ss doesn't scale past 0 for another 911.351428ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3807
Feb 12 14:41:48.380: INFO: Scaling statefulset ss to 0
Feb 12 14:41:48.401: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 14:41:48.406: INFO: Deleting all statefulset in ns statefulset-3807
Feb 12 14:41:48.411: INFO: Scaling statefulset ss to 0
Feb 12 14:41:48.428: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 14:41:48.433: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:41:48.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3807" for this suite.

• [SLOW TEST:55.064 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":94,"skipped":1656,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:41:48.485: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:41:48.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74" in namespace "downward-api-4790" to be "Succeeded or Failed"
Feb 12 14:41:48.551: INFO: Pod "downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74": Phase="Pending", Reason="", readiness=false. Elapsed: 7.009993ms
Feb 12 14:41:50.561: INFO: Pod "downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017016672s
STEP: Saw pod success
Feb 12 14:41:50.561: INFO: Pod "downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74" satisfied condition "Succeeded or Failed"
Feb 12 14:41:50.567: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74 container client-container: <nil>
STEP: delete the pod
Feb 12 14:41:50.624: INFO: Waiting for pod downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74 to disappear
Feb 12 14:41:50.637: INFO: Pod downwardapi-volume-c3acbe38-4a69-43bd-997d-6856f9b38e74 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:41:50.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4790" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:41:50.664: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:41:51.222: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:41:53.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737711, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737711, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737711, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748737711, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:41:56.285: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:41:56.292: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:41:58.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2377" for this suite.
STEP: Destroying namespace "webhook-2377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.687 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":96,"skipped":1705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:41:58.351: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:41:58.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6526" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":97,"skipped":1741,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:41:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 12 14:42:02.702: INFO: &Pod{ObjectMeta:{send-events-eacc98f6-b1ee-44e5-84ad-6fa959c0397a  events-7995 /api/v1/namespaces/events-7995/pods/send-events-eacc98f6-b1ee-44e5-84ad-6fa959c0397a 5f730fb8-183d-46fe-8a68-63a5b1552fd9 68798 0 2021-02-12 14:41:58 +0000 UTC <nil> <nil> map[name:foo time:631206703] map[cni.projectcalico.org/podIP:172.25.0.146/32] [] []  [{e2e.test Update v1 2021-02-12 14:41:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:42:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kmzss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kmzss,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kmzss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-h2tct,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:42:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:42:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:41:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.253.154,PodIP:172.25.0.146,StartTime:2021-02-12 14:41:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:41:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://d09ed182bc3e33ae121b91eb5daa4cead08c689b014a477cc6563342d0513d95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 12 14:42:04.723: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 12 14:42:06.730: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7995" for this suite.

• [SLOW TEST:8.233 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":98,"skipped":1742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:06.787: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:06.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-183" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":99,"skipped":1791,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:06.932: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 12 14:42:07.019: INFO: Waiting up to 5m0s for pod "pod-c585468c-4c46-43ca-b8ca-b93be196ce1a" in namespace "emptydir-4461" to be "Succeeded or Failed"
Feb 12 14:42:07.027: INFO: Pod "pod-c585468c-4c46-43ca-b8ca-b93be196ce1a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.208209ms
Feb 12 14:42:09.034: INFO: Pod "pod-c585468c-4c46-43ca-b8ca-b93be196ce1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015340665s
STEP: Saw pod success
Feb 12 14:42:09.034: INFO: Pod "pod-c585468c-4c46-43ca-b8ca-b93be196ce1a" satisfied condition "Succeeded or Failed"
Feb 12 14:42:09.040: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-c585468c-4c46-43ca-b8ca-b93be196ce1a container test-container: <nil>
STEP: delete the pod
Feb 12 14:42:09.114: INFO: Waiting for pod pod-c585468c-4c46-43ca-b8ca-b93be196ce1a to disappear
Feb 12 14:42:09.119: INFO: Pod pod-c585468c-4c46-43ca-b8ca-b93be196ce1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:09.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4461" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1805,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:09.141: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-f8387146-7256-480c-b760-746fd9ee8fd6
STEP: Creating a pod to test consume secrets
Feb 12 14:42:09.273: INFO: Waiting up to 5m0s for pod "pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670" in namespace "secrets-7043" to be "Succeeded or Failed"
Feb 12 14:42:09.286: INFO: Pod "pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670": Phase="Pending", Reason="", readiness=false. Elapsed: 13.341133ms
Feb 12 14:42:11.292: INFO: Pod "pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019623614s
STEP: Saw pod success
Feb 12 14:42:11.293: INFO: Pod "pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670" satisfied condition "Succeeded or Failed"
Feb 12 14:42:11.298: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670 container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:42:11.336: INFO: Waiting for pod pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670 to disappear
Feb 12 14:42:11.344: INFO: Pod pod-secrets-0e00d1b9-3090-4fb3-a6b4-1bcae9dda670 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:11.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7043" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1820,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:11.375: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:42:11.489: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 12 14:42:11.522: INFO: Number of nodes with available pods: 0
Feb 12 14:42:11.522: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:42:12.536: INFO: Number of nodes with available pods: 0
Feb 12 14:42:12.536: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:42:13.543: INFO: Number of nodes with available pods: 3
Feb 12 14:42:13.543: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 12 14:42:13.619: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:13.620: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:13.620: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:14.637: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:14.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:14.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:15.637: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:15.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:15.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:16.639: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:16.639: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:16.639: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:16.639: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:17.636: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:17.636: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:17.636: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:17.636: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:18.637: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:18.637: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:18.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:18.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:19.640: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:19.640: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:19.640: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:19.640: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:20.637: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:20.637: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:20.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:20.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:21.639: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:21.639: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:21.639: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:21.639: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:22.637: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:22.637: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:22.638: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:22.638: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:23.638: INFO: Wrong image for pod: daemon-set-flgv6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:23.638: INFO: Pod daemon-set-flgv6 is not available
Feb 12 14:42:23.638: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:23.638: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:24.638: INFO: Pod daemon-set-dvdk2 is not available
Feb 12 14:42:24.638: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:24.638: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:25.636: INFO: Pod daemon-set-dvdk2 is not available
Feb 12 14:42:25.636: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:25.636: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:26.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:26.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:27.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:27.637: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:27.637: INFO: Pod daemon-set-jq9vq is not available
Feb 12 14:42:28.643: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:28.643: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:28.643: INFO: Pod daemon-set-jq9vq is not available
Feb 12 14:42:29.638: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:29.638: INFO: Wrong image for pod: daemon-set-jq9vq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:29.638: INFO: Pod daemon-set-jq9vq is not available
Feb 12 14:42:30.638: INFO: Pod daemon-set-2cgs9 is not available
Feb 12 14:42:30.638: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:31.637: INFO: Pod daemon-set-2cgs9 is not available
Feb 12 14:42:31.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:32.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:33.637: INFO: Wrong image for pod: daemon-set-hbrcb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Feb 12 14:42:33.637: INFO: Pod daemon-set-hbrcb is not available
Feb 12 14:42:34.639: INFO: Pod daemon-set-4m7kq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 12 14:42:34.662: INFO: Number of nodes with available pods: 2
Feb 12 14:42:34.662: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:42:35.679: INFO: Number of nodes with available pods: 2
Feb 12 14:42:35.679: INFO: Node modest-nightingale-745f6d5bd4-h2tct is running more than one daemon pod
Feb 12 14:42:36.684: INFO: Number of nodes with available pods: 3
Feb 12 14:42:36.684: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5545, will wait for the garbage collector to delete the pods
Feb 12 14:42:36.806: INFO: Deleting DaemonSet.extensions daemon-set took: 18.912444ms
Feb 12 14:42:37.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.167764ms
Feb 12 14:42:50.331: INFO: Number of nodes with available pods: 0
Feb 12 14:42:50.332: INFO: Number of running nodes: 0, number of available pods: 0
Feb 12 14:42:50.337: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5545/daemonsets","resourceVersion":"69276"},"items":null}

Feb 12 14:42:50.342: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5545/pods","resourceVersion":"69276"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:50.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5545" for this suite.

• [SLOW TEST:39.036 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":102,"skipped":1832,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 12 14:42:50.532: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4387 /api/v1/namespaces/watch-4387/configmaps/e2e-watch-test-watch-closed c992820a-7fd1-41d4-8bf7-218ad042cdd6 69287 0 2021-02-12 14:42:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-12 14:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:42:50.532: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4387 /api/v1/namespaces/watch-4387/configmaps/e2e-watch-test-watch-closed c992820a-7fd1-41d4-8bf7-218ad042cdd6 69288 0 2021-02-12 14:42:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-12 14:42:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 12 14:42:50.563: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4387 /api/v1/namespaces/watch-4387/configmaps/e2e-watch-test-watch-closed c992820a-7fd1-41d4-8bf7-218ad042cdd6 69289 0 2021-02-12 14:42:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-12 14:42:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:42:50.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4387 /api/v1/namespaces/watch-4387/configmaps/e2e-watch-test-watch-closed c992820a-7fd1-41d4-8bf7-218ad042cdd6 69290 0 2021-02-12 14:42:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-12 14:42:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:50.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4387" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":103,"skipped":1844,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 12 14:42:50.647: INFO: Waiting up to 5m0s for pod "pod-85183dfe-5721-4070-be1a-94560f3a4962" in namespace "emptydir-8488" to be "Succeeded or Failed"
Feb 12 14:42:50.652: INFO: Pod "pod-85183dfe-5721-4070-be1a-94560f3a4962": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121037ms
Feb 12 14:42:52.662: INFO: Pod "pod-85183dfe-5721-4070-be1a-94560f3a4962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014550447s
Feb 12 14:42:54.687: INFO: Pod "pod-85183dfe-5721-4070-be1a-94560f3a4962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039565965s
STEP: Saw pod success
Feb 12 14:42:54.687: INFO: Pod "pod-85183dfe-5721-4070-be1a-94560f3a4962" satisfied condition "Succeeded or Failed"
Feb 12 14:42:54.702: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-85183dfe-5721-4070-be1a-94560f3a4962 container test-container: <nil>
STEP: delete the pod
Feb 12 14:42:54.799: INFO: Waiting for pod pod-85183dfe-5721-4070-be1a-94560f3a4962 to disappear
Feb 12 14:42:54.824: INFO: Pod pod-85183dfe-5721-4070-be1a-94560f3a4962 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:42:54.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8488" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1854,"failed":0}
S
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:42:54.857: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 12 14:42:54.938: INFO: Created pod &Pod{ObjectMeta:{dns-7532  dns-7532 /api/v1/namespaces/dns-7532/pods/dns-7532 af011b10-7d19-4e2a-8167-ada393c83dde 69340 0 2021-02-12 14:42:54 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-02-12 14:42:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj2bs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj2bs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj2bs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 14:42:54.944: INFO: The status of Pod dns-7532 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 14:42:56.951: INFO: The status of Pod dns-7532 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 14:42:58.952: INFO: The status of Pod dns-7532 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 12 14:42:58.952: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7532 PodName:dns-7532 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:42:58.952: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Verifying customized DNS server is configured on pod...
Feb 12 14:42:59.516: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7532 PodName:dns-7532 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:42:59.516: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 14:43:00.075: INFO: Deleting pod dns-7532...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:43:00.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7532" for this suite.

• [SLOW TEST:5.286 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":105,"skipped":1855,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:43:00.144: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 12 14:43:00.239: INFO: Waiting up to 5m0s for pod "pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a" in namespace "emptydir-9130" to be "Succeeded or Failed"
Feb 12 14:43:00.252: INFO: Pod "pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.429584ms
Feb 12 14:43:02.261: INFO: Pod "pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a": Phase="Running", Reason="", readiness=true. Elapsed: 2.02161886s
Feb 12 14:43:04.269: INFO: Pod "pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030299083s
STEP: Saw pod success
Feb 12 14:43:04.271: INFO: Pod "pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a" satisfied condition "Succeeded or Failed"
Feb 12 14:43:04.276: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a container test-container: <nil>
STEP: delete the pod
Feb 12 14:43:04.316: INFO: Waiting for pod pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a to disappear
Feb 12 14:43:04.331: INFO: Pod pod-b5b09ed4-2a3e-4f4d-be67-7bc5aa036e1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:43:04.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9130" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":106,"skipped":1861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:43:04.352: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 12 14:43:08.959: INFO: Successfully updated pod "adopt-release-dcr8j"
STEP: Checking that the Job readopts the Pod
Feb 12 14:43:08.959: INFO: Waiting up to 15m0s for pod "adopt-release-dcr8j" in namespace "job-7910" to be "adopted"
Feb 12 14:43:08.972: INFO: Pod "adopt-release-dcr8j": Phase="Running", Reason="", readiness=true. Elapsed: 13.105905ms
Feb 12 14:43:10.979: INFO: Pod "adopt-release-dcr8j": Phase="Running", Reason="", readiness=true. Elapsed: 2.019859345s
Feb 12 14:43:10.979: INFO: Pod "adopt-release-dcr8j" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 12 14:43:11.500: INFO: Successfully updated pod "adopt-release-dcr8j"
STEP: Checking that the Job releases the Pod
Feb 12 14:43:11.500: INFO: Waiting up to 15m0s for pod "adopt-release-dcr8j" in namespace "job-7910" to be "released"
Feb 12 14:43:11.520: INFO: Pod "adopt-release-dcr8j": Phase="Running", Reason="", readiness=true. Elapsed: 20.135037ms
Feb 12 14:43:13.526: INFO: Pod "adopt-release-dcr8j": Phase="Running", Reason="", readiness=true. Elapsed: 2.026217429s
Feb 12 14:43:13.527: INFO: Pod "adopt-release-dcr8j" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:43:13.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7910" for this suite.

• [SLOW TEST:9.192 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":107,"skipped":1902,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:43:13.545: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:43:13.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1642" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":108,"skipped":1903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:43:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:43:13.762: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-94d72092-b728-479d-b614-73e4e7100266" in namespace "security-context-test-8133" to be "Succeeded or Failed"
Feb 12 14:43:13.769: INFO: Pod "busybox-privileged-false-94d72092-b728-479d-b614-73e4e7100266": Phase="Pending", Reason="", readiness=false. Elapsed: 6.956109ms
Feb 12 14:43:15.777: INFO: Pod "busybox-privileged-false-94d72092-b728-479d-b614-73e4e7100266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014668697s
Feb 12 14:43:15.777: INFO: Pod "busybox-privileged-false-94d72092-b728-479d-b614-73e4e7100266" satisfied condition "Succeeded or Failed"
Feb 12 14:43:15.799: INFO: Got logs for pod "busybox-privileged-false-94d72092-b728-479d-b614-73e4e7100266": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:43:15.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8133" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1925,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:43:15.831: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1172
Feb 12 14:43:17.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 12 14:43:18.517: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 12 14:43:18.517: INFO: stdout: "ipvs"
Feb 12 14:43:18.517: INFO: proxyMode: ipvs
Feb 12 14:43:18.530: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:18.536: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:20.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:20.543: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:22.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:22.542: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:24.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:24.546: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:26.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:26.542: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:28.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:28.549: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 14:43:30.536: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 14:43:30.551: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1172
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1172
I0212 14:43:30.593195      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1172, replica count: 3
I0212 14:43:33.643759      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:43:33.693: INFO: Creating new exec pod
Feb 12 14:43:36.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Feb 12 14:43:37.361: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 12 14:43:37.361: INFO: stdout: ""
Feb 12 14:43:37.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c nc -zv -t -w 2 10.240.24.84 80'
Feb 12 14:43:37.965: INFO: stderr: "+ nc -zv -t -w 2 10.240.24.84 80\nConnection to 10.240.24.84 80 port [tcp/http] succeeded!\n"
Feb 12 14:43:37.965: INFO: stdout: ""
Feb 12 14:43:37.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.24.84:80/ ; done'
Feb 12 14:43:38.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n"
Feb 12 14:43:38.629: INFO: stdout: "\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl\naffinity-clusterip-timeout-tkvhl"
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Received response from host: affinity-clusterip-timeout-tkvhl
Feb 12 14:43:38.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.24.84:80/'
Feb 12 14:43:39.242: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n"
Feb 12 14:43:39.242: INFO: stdout: "affinity-clusterip-timeout-tkvhl"
Feb 12 14:45:44.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.24.84:80/'
Feb 12 14:45:44.898: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n"
Feb 12 14:45:44.898: INFO: stdout: "affinity-clusterip-timeout-tkvhl"
Feb 12 14:47:49.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-1172 execpod-affinitycp6ps -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.24.84:80/'
Feb 12 14:47:50.532: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.24.84:80/\n"
Feb 12 14:47:50.532: INFO: stdout: "affinity-clusterip-timeout-2m94l"
Feb 12 14:47:50.532: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1172, will wait for the garbage collector to delete the pods
Feb 12 14:47:50.628: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 17.149507ms
Feb 12 14:47:51.228: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.636965ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:48:00.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1172" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:284.560 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":110,"skipped":1938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:48:00.393: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-5e875cf7-ddab-4029-a4f8-e8d5b4a8de55
STEP: Creating a pod to test consume secrets
Feb 12 14:48:00.546: INFO: Waiting up to 5m0s for pod "pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe" in namespace "secrets-3021" to be "Succeeded or Failed"
Feb 12 14:48:00.561: INFO: Pod "pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.829549ms
Feb 12 14:48:02.570: INFO: Pod "pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023760506s
Feb 12 14:48:04.577: INFO: Pod "pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031343011s
STEP: Saw pod success
Feb 12 14:48:04.578: INFO: Pod "pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe" satisfied condition "Succeeded or Failed"
Feb 12 14:48:04.583: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:48:04.645: INFO: Waiting for pod pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe to disappear
Feb 12 14:48:04.651: INFO: Pod pod-secrets-9651ae21-e12b-4224-8f04-a13b9c3ebafe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:48:04.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3021" for this suite.
STEP: Destroying namespace "secret-namespace-1646" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1961,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:48:04.698: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hq2gz in namespace proxy-4367
I0212 14:48:04.810459      21 runners.go:190] Created replication controller with name: proxy-service-hq2gz, namespace: proxy-4367, replica count: 1
I0212 14:48:05.860794      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 14:48:06.861205      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 14:48:07.861466      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:08.861608      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:09.861760      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:10.861905      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:11.862047      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:12.863643      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:13.863824      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:14.864100      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 14:48:15.864243      21 runners.go:190] proxy-service-hq2gz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:48:15.871: INFO: setup took 11.106828135s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 12 14:48:15.895: INFO: (0) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 23.812148ms)
Feb 12 14:48:15.899: INFO: (0) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 27.381049ms)
Feb 12 14:48:15.899: INFO: (0) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 28.405119ms)
Feb 12 14:48:15.899: INFO: (0) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 28.186832ms)
Feb 12 14:48:15.900: INFO: (0) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 28.413832ms)
Feb 12 14:48:15.903: INFO: (0) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 31.755631ms)
Feb 12 14:48:15.903: INFO: (0) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 31.985198ms)
Feb 12 14:48:15.903: INFO: (0) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 32.273756ms)
Feb 12 14:48:15.903: INFO: (0) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 32.043901ms)
Feb 12 14:48:15.903: INFO: (0) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 31.749497ms)
Feb 12 14:48:15.911: INFO: (0) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 39.122885ms)
Feb 12 14:48:15.911: INFO: (0) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 39.531259ms)
Feb 12 14:48:15.911: INFO: (0) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 39.6179ms)
Feb 12 14:48:15.912: INFO: (0) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 40.08631ms)
Feb 12 14:48:15.912: INFO: (0) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 40.100011ms)
Feb 12 14:48:15.964: INFO: (0) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 92.04581ms)
Feb 12 14:48:15.979: INFO: (1) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 15.340573ms)
Feb 12 14:48:15.979: INFO: (1) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 15.522445ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 15.643302ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 15.92954ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 15.910471ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 16.237019ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 15.650049ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 16.23169ms)
Feb 12 14:48:15.980: INFO: (1) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 16.189122ms)
Feb 12 14:48:15.982: INFO: (1) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 18.036661ms)
Feb 12 14:48:15.987: INFO: (1) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 23.671796ms)
Feb 12 14:48:15.987: INFO: (1) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 23.470279ms)
Feb 12 14:48:15.988: INFO: (1) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 23.677762ms)
Feb 12 14:48:16.028: INFO: (1) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 63.935625ms)
Feb 12 14:48:16.028: INFO: (1) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 63.666203ms)
Feb 12 14:48:16.029: INFO: (1) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 65.43729ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 14.534595ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 14.040503ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 14.39768ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 14.223311ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 12.692891ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 15.261051ms)
Feb 12 14:48:16.045: INFO: (2) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 14.759563ms)
Feb 12 14:48:16.047: INFO: (2) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 16.846283ms)
Feb 12 14:48:16.047: INFO: (2) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 15.78885ms)
Feb 12 14:48:16.047: INFO: (2) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 16.592255ms)
Feb 12 14:48:16.047: INFO: (2) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 16.122097ms)
Feb 12 14:48:16.047: INFO: (2) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 14.624858ms)
Feb 12 14:48:16.049: INFO: (2) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 18.497725ms)
Feb 12 14:48:16.050: INFO: (2) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 18.979118ms)
Feb 12 14:48:16.050: INFO: (2) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.85698ms)
Feb 12 14:48:16.050: INFO: (2) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 20.512106ms)
Feb 12 14:48:16.072: INFO: (3) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 21.078404ms)
Feb 12 14:48:16.072: INFO: (3) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 21.61337ms)
Feb 12 14:48:16.072: INFO: (3) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 21.786217ms)
Feb 12 14:48:16.072: INFO: (3) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 21.775778ms)
Feb 12 14:48:16.073: INFO: (3) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 22.249639ms)
Feb 12 14:48:16.074: INFO: (3) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 22.819514ms)
Feb 12 14:48:16.074: INFO: (3) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 23.162782ms)
Feb 12 14:48:16.074: INFO: (3) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 23.728414ms)
Feb 12 14:48:16.076: INFO: (3) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 25.064953ms)
Feb 12 14:48:16.076: INFO: (3) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 25.04251ms)
Feb 12 14:48:16.076: INFO: (3) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 25.486123ms)
Feb 12 14:48:16.076: INFO: (3) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 25.196521ms)
Feb 12 14:48:16.095: INFO: (3) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 44.024523ms)
Feb 12 14:48:16.095: INFO: (3) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 44.249941ms)
Feb 12 14:48:16.095: INFO: (3) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 44.237978ms)
Feb 12 14:48:16.114: INFO: (3) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 63.407965ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 19.076076ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.686827ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 19.422167ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 20.063989ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.375694ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 19.158993ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 18.811454ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 19.831112ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.400931ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 20.063617ms)
Feb 12 14:48:16.134: INFO: (4) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 19.701431ms)
Feb 12 14:48:16.135: INFO: (4) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 20.068592ms)
Feb 12 14:48:16.135: INFO: (4) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 19.507619ms)
Feb 12 14:48:16.135: INFO: (4) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 20.253905ms)
Feb 12 14:48:16.135: INFO: (4) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 20.984771ms)
Feb 12 14:48:16.135: INFO: (4) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 20.01888ms)
Feb 12 14:48:16.152: INFO: (5) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 15.778801ms)
Feb 12 14:48:16.152: INFO: (5) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 16.745572ms)
Feb 12 14:48:16.152: INFO: (5) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 16.467885ms)
Feb 12 14:48:16.152: INFO: (5) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 16.501084ms)
Feb 12 14:48:16.152: INFO: (5) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 16.716119ms)
Feb 12 14:48:16.156: INFO: (5) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 20.174934ms)
Feb 12 14:48:16.156: INFO: (5) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 20.127603ms)
Feb 12 14:48:16.156: INFO: (5) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 20.442056ms)
Feb 12 14:48:16.157: INFO: (5) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 20.970737ms)
Feb 12 14:48:16.200: INFO: (5) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 64.931548ms)
Feb 12 14:48:16.200: INFO: (5) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 64.760769ms)
Feb 12 14:48:16.201: INFO: (5) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 64.74364ms)
Feb 12 14:48:16.200: INFO: (5) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 65.042917ms)
Feb 12 14:48:16.201: INFO: (5) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 64.941801ms)
Feb 12 14:48:16.241: INFO: (5) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 105.303707ms)
Feb 12 14:48:16.284: INFO: (5) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 148.744567ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 20.657018ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 20.534385ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 20.437211ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 20.317981ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 20.315973ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 20.36446ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 20.725006ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 20.814422ms)
Feb 12 14:48:16.305: INFO: (6) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 20.660316ms)
Feb 12 14:48:16.306: INFO: (6) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 20.92166ms)
Feb 12 14:48:16.307: INFO: (6) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 23.027866ms)
Feb 12 14:48:16.307: INFO: (6) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 22.728193ms)
Feb 12 14:48:16.307: INFO: (6) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 22.691645ms)
Feb 12 14:48:16.308: INFO: (6) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 23.058982ms)
Feb 12 14:48:16.308: INFO: (6) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 22.995784ms)
Feb 12 14:48:16.309: INFO: (6) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 24.69057ms)
Feb 12 14:48:16.362: INFO: (7) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 52.846297ms)
Feb 12 14:48:16.363: INFO: (7) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 53.234895ms)
Feb 12 14:48:16.363: INFO: (7) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 53.713592ms)
Feb 12 14:48:16.363: INFO: (7) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 53.991826ms)
Feb 12 14:48:16.365: INFO: (7) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 55.468209ms)
Feb 12 14:48:16.366: INFO: (7) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 56.122802ms)
Feb 12 14:48:16.366: INFO: (7) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 56.371615ms)
Feb 12 14:48:16.366: INFO: (7) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 56.700365ms)
Feb 12 14:48:16.366: INFO: (7) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 56.736152ms)
Feb 12 14:48:16.367: INFO: (7) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 57.244626ms)
Feb 12 14:48:16.368: INFO: (7) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 58.938422ms)
Feb 12 14:48:16.369: INFO: (7) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 59.141497ms)
Feb 12 14:48:16.369: INFO: (7) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 59.871531ms)
Feb 12 14:48:16.369: INFO: (7) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 60.130479ms)
Feb 12 14:48:16.372: INFO: (7) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 62.724865ms)
Feb 12 14:48:16.373: INFO: (7) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 64.080836ms)
Feb 12 14:48:16.389: INFO: (8) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 15.498274ms)
Feb 12 14:48:16.396: INFO: (8) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 22.20252ms)
Feb 12 14:48:16.398: INFO: (8) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 24.571947ms)
Feb 12 14:48:16.400: INFO: (8) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 26.819515ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 33.239257ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 32.916386ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 32.969338ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 32.719097ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 33.164983ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 32.963444ms)
Feb 12 14:48:16.407: INFO: (8) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 33.104232ms)
Feb 12 14:48:16.409: INFO: (8) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 35.776031ms)
Feb 12 14:48:16.409: INFO: (8) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 35.325187ms)
Feb 12 14:48:16.410: INFO: (8) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 36.369351ms)
Feb 12 14:48:16.411: INFO: (8) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 36.744868ms)
Feb 12 14:48:16.437: INFO: (8) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 62.955894ms)
Feb 12 14:48:16.450: INFO: (9) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 13.111526ms)
Feb 12 14:48:16.450: INFO: (9) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 12.988457ms)
Feb 12 14:48:16.450: INFO: (9) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 12.810935ms)
Feb 12 14:48:16.451: INFO: (9) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 12.996382ms)
Feb 12 14:48:16.451: INFO: (9) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 13.401065ms)
Feb 12 14:48:16.451: INFO: (9) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 13.75828ms)
Feb 12 14:48:16.451: INFO: (9) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 13.893045ms)
Feb 12 14:48:16.452: INFO: (9) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 14.387248ms)
Feb 12 14:48:16.452: INFO: (9) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 15.073784ms)
Feb 12 14:48:16.453: INFO: (9) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 15.398448ms)
Feb 12 14:48:16.453: INFO: (9) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 16.082871ms)
Feb 12 14:48:16.457: INFO: (9) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 19.513649ms)
Feb 12 14:48:16.461: INFO: (9) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 23.093989ms)
Feb 12 14:48:16.461: INFO: (9) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 23.769106ms)
Feb 12 14:48:16.461: INFO: (9) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 23.512202ms)
Feb 12 14:48:16.461: INFO: (9) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 23.77672ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 47.279196ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 47.204627ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 47.001196ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 47.21044ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 47.540146ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 47.841443ms)
Feb 12 14:48:16.509: INFO: (10) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 47.822983ms)
Feb 12 14:48:16.510: INFO: (10) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 47.962731ms)
Feb 12 14:48:16.510: INFO: (10) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 47.875344ms)
Feb 12 14:48:16.510: INFO: (10) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 48.330881ms)
Feb 12 14:48:16.510: INFO: (10) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 48.352679ms)
Feb 12 14:48:16.510: INFO: (10) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 48.464252ms)
Feb 12 14:48:16.554: INFO: (10) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 91.834974ms)
Feb 12 14:48:16.554: INFO: (10) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 92.160294ms)
Feb 12 14:48:16.554: INFO: (10) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 91.978386ms)
Feb 12 14:48:16.554: INFO: (10) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 92.186271ms)
Feb 12 14:48:16.572: INFO: (11) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.14981ms)
Feb 12 14:48:16.573: INFO: (11) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 18.251703ms)
Feb 12 14:48:16.573: INFO: (11) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.323885ms)
Feb 12 14:48:16.573: INFO: (11) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 18.437408ms)
Feb 12 14:48:16.573: INFO: (11) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 18.492074ms)
Feb 12 14:48:16.573: INFO: (11) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 18.588357ms)
Feb 12 14:48:16.581: INFO: (11) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 26.890407ms)
Feb 12 14:48:16.583: INFO: (11) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 28.7984ms)
Feb 12 14:48:16.583: INFO: (11) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 28.907772ms)
Feb 12 14:48:16.584: INFO: (11) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 29.846869ms)
Feb 12 14:48:16.584: INFO: (11) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 30.318279ms)
Feb 12 14:48:16.584: INFO: (11) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 29.947078ms)
Feb 12 14:48:16.586: INFO: (11) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 31.864486ms)
Feb 12 14:48:16.587: INFO: (11) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 32.519594ms)
Feb 12 14:48:16.587: INFO: (11) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 32.685974ms)
Feb 12 14:48:16.592: INFO: (11) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 37.825154ms)
Feb 12 14:48:16.612: INFO: (12) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.692237ms)
Feb 12 14:48:16.612: INFO: (12) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.225369ms)
Feb 12 14:48:16.612: INFO: (12) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 19.29077ms)
Feb 12 14:48:16.614: INFO: (12) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 21.670544ms)
Feb 12 14:48:16.614: INFO: (12) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 21.798758ms)
Feb 12 14:48:16.616: INFO: (12) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 23.535241ms)
Feb 12 14:48:16.617: INFO: (12) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 24.766323ms)
Feb 12 14:48:16.654: INFO: (12) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 61.599823ms)
Feb 12 14:48:16.654: INFO: (12) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 61.452985ms)
Feb 12 14:48:16.654: INFO: (12) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 61.680036ms)
Feb 12 14:48:16.658: INFO: (12) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 65.853104ms)
Feb 12 14:48:16.658: INFO: (12) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 65.490849ms)
Feb 12 14:48:16.658: INFO: (12) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 65.004108ms)
Feb 12 14:48:16.658: INFO: (12) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 65.175693ms)
Feb 12 14:48:16.658: INFO: (12) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 65.05369ms)
Feb 12 14:48:16.659: INFO: (12) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 66.209326ms)
Feb 12 14:48:16.680: INFO: (13) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 21.082356ms)
Feb 12 14:48:16.680: INFO: (13) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 21.418302ms)
Feb 12 14:48:16.681: INFO: (13) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 22.006635ms)
Feb 12 14:48:16.682: INFO: (13) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 22.503252ms)
Feb 12 14:48:16.683: INFO: (13) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 24.098023ms)
Feb 12 14:48:16.686: INFO: (13) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 27.614142ms)
Feb 12 14:48:16.686: INFO: (13) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 27.632446ms)
Feb 12 14:48:16.687: INFO: (13) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 27.642495ms)
Feb 12 14:48:16.687: INFO: (13) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 28.091332ms)
Feb 12 14:48:16.687: INFO: (13) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 28.280471ms)
Feb 12 14:48:16.687: INFO: (13) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 28.521735ms)
Feb 12 14:48:16.691: INFO: (13) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 32.460478ms)
Feb 12 14:48:16.692: INFO: (13) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 33.365532ms)
Feb 12 14:48:16.692: INFO: (13) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 33.423459ms)
Feb 12 14:48:16.693: INFO: (13) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 34.101868ms)
Feb 12 14:48:16.695: INFO: (13) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 36.212393ms)
Feb 12 14:48:16.712: INFO: (14) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 16.39055ms)
Feb 12 14:48:16.713: INFO: (14) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 17.207182ms)
Feb 12 14:48:16.713: INFO: (14) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 17.443494ms)
Feb 12 14:48:16.713: INFO: (14) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 17.504065ms)
Feb 12 14:48:16.716: INFO: (14) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 19.876256ms)
Feb 12 14:48:16.716: INFO: (14) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 20.16355ms)
Feb 12 14:48:16.716: INFO: (14) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 20.005777ms)
Feb 12 14:48:16.716: INFO: (14) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 20.381115ms)
Feb 12 14:48:16.717: INFO: (14) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 20.758869ms)
Feb 12 14:48:16.717: INFO: (14) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 21.159464ms)
Feb 12 14:48:16.717: INFO: (14) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 21.623223ms)
Feb 12 14:48:16.717: INFO: (14) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 21.682452ms)
Feb 12 14:48:16.718: INFO: (14) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 22.355586ms)
Feb 12 14:48:16.720: INFO: (14) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 23.715135ms)
Feb 12 14:48:16.756: INFO: (14) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 59.604178ms)
Feb 12 14:48:16.756: INFO: (14) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 60.227985ms)
Feb 12 14:48:16.770: INFO: (15) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 13.523893ms)
Feb 12 14:48:16.771: INFO: (15) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 14.855257ms)
Feb 12 14:48:16.772: INFO: (15) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 15.360753ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 18.185532ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 18.301369ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.507115ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 18.453843ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 18.737311ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 18.520678ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 18.608098ms)
Feb 12 14:48:16.775: INFO: (15) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 18.838781ms)
Feb 12 14:48:16.776: INFO: (15) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 19.853744ms)
Feb 12 14:48:16.813: INFO: (15) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 56.984215ms)
Feb 12 14:48:16.814: INFO: (15) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 57.451271ms)
Feb 12 14:48:16.814: INFO: (15) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 57.745797ms)
Feb 12 14:48:16.814: INFO: (15) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 57.792933ms)
Feb 12 14:48:16.826: INFO: (16) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 12.141473ms)
Feb 12 14:48:16.830: INFO: (16) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 15.086939ms)
Feb 12 14:48:16.830: INFO: (16) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 15.520226ms)
Feb 12 14:48:16.830: INFO: (16) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 15.634468ms)
Feb 12 14:48:16.830: INFO: (16) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 15.925583ms)
Feb 12 14:48:16.830: INFO: (16) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 16.138378ms)
Feb 12 14:48:16.832: INFO: (16) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 17.45985ms)
Feb 12 14:48:16.832: INFO: (16) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 17.924444ms)
Feb 12 14:48:16.832: INFO: (16) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 17.400036ms)
Feb 12 14:48:16.835: INFO: (16) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 20.532158ms)
Feb 12 14:48:16.835: INFO: (16) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 20.854228ms)
Feb 12 14:48:16.836: INFO: (16) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 21.036669ms)
Feb 12 14:48:16.836: INFO: (16) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 21.833156ms)
Feb 12 14:48:16.873: INFO: (16) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 58.535286ms)
Feb 12 14:48:16.873: INFO: (16) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 58.566082ms)
Feb 12 14:48:16.874: INFO: (16) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 59.474182ms)
Feb 12 14:48:16.891: INFO: (17) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 16.309554ms)
Feb 12 14:48:16.891: INFO: (17) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 16.746402ms)
Feb 12 14:48:16.891: INFO: (17) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 16.612366ms)
Feb 12 14:48:16.891: INFO: (17) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 16.562633ms)
Feb 12 14:48:16.891: INFO: (17) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 16.272684ms)
Feb 12 14:48:16.892: INFO: (17) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 17.476712ms)
Feb 12 14:48:16.902: INFO: (17) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 27.75258ms)
Feb 12 14:48:16.932: INFO: (17) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 57.297514ms)
Feb 12 14:48:16.932: INFO: (17) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 57.568963ms)
Feb 12 14:48:16.932: INFO: (17) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 57.293018ms)
Feb 12 14:48:16.932: INFO: (17) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 57.563515ms)
Feb 12 14:48:16.933: INFO: (17) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 58.169277ms)
Feb 12 14:48:16.933: INFO: (17) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 58.097502ms)
Feb 12 14:48:16.933: INFO: (17) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 59.041294ms)
Feb 12 14:48:16.933: INFO: (17) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 59.243398ms)
Feb 12 14:48:16.942: INFO: (17) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 67.685914ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.271926ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 17.842057ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 18.170128ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 18.464533ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 18.532276ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 18.459912ms)
Feb 12 14:48:16.961: INFO: (18) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 18.805965ms)
Feb 12 14:48:16.963: INFO: (18) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 20.16833ms)
Feb 12 14:48:16.969: INFO: (18) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 26.336899ms)
Feb 12 14:48:16.969: INFO: (18) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 26.205903ms)
Feb 12 14:48:16.971: INFO: (18) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 28.226289ms)
Feb 12 14:48:16.971: INFO: (18) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 27.972713ms)
Feb 12 14:48:16.971: INFO: (18) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 27.95761ms)
Feb 12 14:48:16.971: INFO: (18) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 27.956355ms)
Feb 12 14:48:16.971: INFO: (18) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 28.186172ms)
Feb 12 14:48:16.972: INFO: (18) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 29.003566ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:460/proxy/: tls baz (200; 19.572427ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz/proxy/rewriteme">test</a> (200; 19.338327ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 19.47764ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:462/proxy/: tls qux (200; 19.73045ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/https:proxy-service-hq2gz-ppjwz:443/proxy/tlsrewritem... (200; 19.82171ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">test<... (200; 19.906672ms)
Feb 12 14:48:16.992: INFO: (19) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname1/proxy/: foo (200; 19.859373ms)
Feb 12 14:48:16.993: INFO: (19) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname2/proxy/: bar (200; 20.096234ms)
Feb 12 14:48:16.993: INFO: (19) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 20.026847ms)
Feb 12 14:48:16.994: INFO: (19) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname1/proxy/: tls baz (200; 21.700065ms)
Feb 12 14:48:16.995: INFO: (19) /api/v1/namespaces/proxy-4367/services/https:proxy-service-hq2gz:tlsportname2/proxy/: tls qux (200; 22.506451ms)
Feb 12 14:48:16.995: INFO: (19) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:1080/proxy/rewriteme">... (200; 22.639105ms)
Feb 12 14:48:16.998: INFO: (19) /api/v1/namespaces/proxy-4367/services/proxy-service-hq2gz:portname1/proxy/: foo (200; 25.742813ms)
Feb 12 14:48:16.998: INFO: (19) /api/v1/namespaces/proxy-4367/pods/proxy-service-hq2gz-ppjwz:162/proxy/: bar (200; 25.943827ms)
Feb 12 14:48:16.998: INFO: (19) /api/v1/namespaces/proxy-4367/pods/http:proxy-service-hq2gz-ppjwz:160/proxy/: foo (200; 25.653387ms)
Feb 12 14:48:16.999: INFO: (19) /api/v1/namespaces/proxy-4367/services/http:proxy-service-hq2gz:portname2/proxy/: bar (200; 27.019575ms)
STEP: deleting ReplicationController proxy-service-hq2gz in namespace proxy-4367, will wait for the garbage collector to delete the pods
Feb 12 14:48:17.072: INFO: Deleting ReplicationController proxy-service-hq2gz took: 14.081989ms
Feb 12 14:48:17.172: INFO: Terminating ReplicationController proxy-service-hq2gz pods took: 100.119563ms
[AfterEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:48:27.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4367" for this suite.

• [SLOW TEST:22.899 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":112,"skipped":1973,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:48:27.597: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:48:27.680: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 12 14:48:32.689: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 12 14:48:32.689: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 12 14:48:34.696: INFO: Creating deployment "test-rollover-deployment"
Feb 12 14:48:34.713: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 12 14:48:36.726: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 12 14:48:36.738: INFO: Ensure that both replica sets have 1 created replica
Feb 12 14:48:36.750: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 12 14:48:36.774: INFO: Updating deployment test-rollover-deployment
Feb 12 14:48:36.774: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 12 14:48:38.789: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 12 14:48:38.801: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 12 14:48:38.812: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:38.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738117, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:40.827: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:40.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738119, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:42.840: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:42.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738119, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:44.824: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:44.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738119, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:46.828: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:46.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738119, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:48.827: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 14:48:48.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738119, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738114, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 14:48:50.830: INFO: 
Feb 12 14:48:50.830: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Feb 12 14:48:50.848: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-381 /apis/apps/v1/namespaces/deployment-381/deployments/test-rollover-deployment 4ea0c203-4cbd-48ac-b401-c484e55585d2 71518 2 2021-02-12 14:48:34 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-12 14:48:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000164cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-12 14:48:34 +0000 UTC,LastTransitionTime:2021-02-12 14:48:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-02-12 14:48:49 +0000 UTC,LastTransitionTime:2021-02-12 14:48:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 14:48:50.854: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-381 /apis/apps/v1/namespaces/deployment-381/replicasets/test-rollover-deployment-5797c7764 bcc1926f-ebf3-4fc2-8396-ce2d6af72ad2 71507 2 2021-02-12 14:48:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ea0c203-4cbd-48ac-b401-c484e55585d2 0xc0009c7280 0xc0009c7281}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ea0c203-4cbd-48ac-b401-c484e55585d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009c7308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:48:50.854: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 12 14:48:50.854: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-381 /apis/apps/v1/namespaces/deployment-381/replicasets/test-rollover-controller be38b3bc-c6f9-4787-a6f3-138d813e900b 71517 2 2021-02-12 14:48:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ea0c203-4cbd-48ac-b401-c484e55585d2 0xc0009c7147 0xc0009c7148}] []  [{e2e.test Update apps/v1 2021-02-12 14:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-12 14:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ea0c203-4cbd-48ac-b401-c484e55585d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0009c71f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:48:50.854: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-381 /apis/apps/v1/namespaces/deployment-381/replicasets/test-rollover-deployment-78bc8b888c 44eee12a-0cc0-4445-aaff-dd8719343b1c 71443 2 2021-02-12 14:48:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ea0c203-4cbd-48ac-b401-c484e55585d2 0xc0009c7377 0xc0009c7378}] []  [{kube-controller-manager Update apps/v1 2021-02-12 14:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ea0c203-4cbd-48ac-b401-c484e55585d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009c7428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 14:48:50.862: INFO: Pod "test-rollover-deployment-5797c7764-s86n2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-s86n2 test-rollover-deployment-5797c7764- deployment-381 /api/v1/namespaces/deployment-381/pods/test-rollover-deployment-5797c7764-s86n2 b8f08825-81ea-43eb-b88a-bddaf6004a61 71459 0 2021-02-12 14:48:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:172.25.1.182/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 bcc1926f-ebf3-4fc2-8396-ce2d6af72ad2 0xc00336a7f0 0xc00336a7f1}] []  [{kube-controller-manager Update v1 2021-02-12 14:48:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bcc1926f-ebf3-4fc2-8396-ce2d6af72ad2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-12 14:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-02-12 14:48:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-clrkr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-clrkr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-clrkr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:modest-nightingale-745f6d5bd4-w42vq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-12 14:48:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:165.227.175.251,PodIP:172.25.1.182,StartTime:2021-02-12 14:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-12 14:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://559adb4cfe4adeafff87d752167cdd422fd7231bcddc764ac9f34e5091afd95c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:48:50.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-381" for this suite.

• [SLOW TEST:23.289 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":113,"skipped":1981,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:48:50.887: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:48:50.944: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:48:57.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-24" for this suite.

• [SLOW TEST:6.583 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":114,"skipped":1988,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:48:57.472: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:48:58.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:49:00.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738138, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738138, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738138, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738138, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:49:03.555: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:49:03.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7351" for this suite.
STEP: Destroying namespace "webhook-7351-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":115,"skipped":1988,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:49:03.947: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-eea42574-aebf-4bfe-a93f-4cdfc34f6ac9
STEP: Creating a pod to test consume secrets
Feb 12 14:49:04.047: INFO: Waiting up to 5m0s for pod "pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f" in namespace "secrets-6584" to be "Succeeded or Failed"
Feb 12 14:49:04.056: INFO: Pod "pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.444807ms
Feb 12 14:49:06.064: INFO: Pod "pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016494359s
Feb 12 14:49:08.071: INFO: Pod "pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023634984s
STEP: Saw pod success
Feb 12 14:49:08.071: INFO: Pod "pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f" satisfied condition "Succeeded or Failed"
Feb 12 14:49:08.079: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f container secret-env-test: <nil>
STEP: delete the pod
Feb 12 14:49:08.145: INFO: Waiting for pod pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f to disappear
Feb 12 14:49:08.151: INFO: Pod pod-secrets-62f51b45-aaca-4468-a613-b1374ad8877f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:49:08.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6584" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":116,"skipped":2006,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:49:08.173: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 12 14:49:08.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71790 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:08.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71790 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 12 14:49:18.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71870 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:18.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71870 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 12 14:49:28.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71916 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:28.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71916 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 12 14:49:38.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71963 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:38.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-a 1be7902e-baa8-4049-804f-08652889b5aa 71963 0 2021-02-12 14:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 12 14:49:48.322: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-b 1c5aacce-3cbd-48f3-9721-5bb1eb1c9995 72010 0 2021-02-12 14:49:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:48.322: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-b 1c5aacce-3cbd-48f3-9721-5bb1eb1c9995 72010 0 2021-02-12 14:49:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 12 14:49:58.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-b 1c5aacce-3cbd-48f3-9721-5bb1eb1c9995 72054 0 2021-02-12 14:49:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:49:58.341: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6202 /api/v1/namespaces/watch-6202/configmaps/e2e-watch-test-configmap-b 1c5aacce-3cbd-48f3-9721-5bb1eb1c9995 72054 0 2021-02-12 14:49:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-12 14:49:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:08.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6202" for this suite.

• [SLOW TEST:60.190 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":117,"skipped":2008,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:08.363: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-4004b1e0-bdf9-40f3-897f-8fb2bc5750c8
STEP: Creating a pod to test consume configMaps
Feb 12 14:50:08.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e" in namespace "projected-6573" to be "Succeeded or Failed"
Feb 12 14:50:08.456: INFO: Pod "pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.551496ms
Feb 12 14:50:10.464: INFO: Pod "pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012469803s
STEP: Saw pod success
Feb 12 14:50:10.464: INFO: Pod "pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e" satisfied condition "Succeeded or Failed"
Feb 12 14:50:10.470: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:50:10.538: INFO: Waiting for pod pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e to disappear
Feb 12 14:50:10.547: INFO: Pod pod-projected-configmaps-dc5a467d-6b85-427f-ab96-a456c5b1ee5e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:10.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6573" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":2024,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:10.568: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-80bfa17c-5d9a-496c-8d41-6dc75d8fb36e
STEP: Creating a pod to test consume secrets
Feb 12 14:50:10.658: INFO: Waiting up to 5m0s for pod "pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30" in namespace "secrets-9197" to be "Succeeded or Failed"
Feb 12 14:50:10.668: INFO: Pod "pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30": Phase="Pending", Reason="", readiness=false. Elapsed: 10.384063ms
Feb 12 14:50:12.675: INFO: Pod "pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01658025s
STEP: Saw pod success
Feb 12 14:50:12.675: INFO: Pod "pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30" satisfied condition "Succeeded or Failed"
Feb 12 14:50:12.679: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30 container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:50:12.721: INFO: Waiting for pod pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30 to disappear
Feb 12 14:50:12.729: INFO: Pod pod-secrets-54711a61-fff9-47f3-aadf-02d60db03b30 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:12.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9197" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":2030,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:12.752: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:50:12.810: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 12 14:50:16.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-5629 create -f -'
Feb 12 14:50:16.670: INFO: stderr: ""
Feb 12 14:50:16.670: INFO: stdout: "e2e-test-crd-publish-openapi-6506-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 12 14:50:16.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-5629 delete e2e-test-crd-publish-openapi-6506-crds test-cr'
Feb 12 14:50:16.836: INFO: stderr: ""
Feb 12 14:50:16.836: INFO: stdout: "e2e-test-crd-publish-openapi-6506-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 12 14:50:16.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-5629 apply -f -'
Feb 12 14:50:17.125: INFO: stderr: ""
Feb 12 14:50:17.125: INFO: stdout: "e2e-test-crd-publish-openapi-6506-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 12 14:50:17.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-5629 delete e2e-test-crd-publish-openapi-6506-crds test-cr'
Feb 12 14:50:17.219: INFO: stderr: ""
Feb 12 14:50:17.219: INFO: stdout: "e2e-test-crd-publish-openapi-6506-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 12 14:50:17.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-6506-crds'
Feb 12 14:50:17.513: INFO: stderr: ""
Feb 12 14:50:17.513: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6506-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:21.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5629" for this suite.

• [SLOW TEST:8.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":120,"skipped":2033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:21.601: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2077.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2077.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2077.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2077.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2077.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2077.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:50:24.418: INFO: DNS probes using dns-2077/dns-test-bd8b7bd5-dac0-419f-9dba-b6412ff207c2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:24.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2077" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":121,"skipped":2056,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:24.500: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 12 14:50:25.654: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0212 14:50:25.654781      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 14:50:25.654809      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 14:50:25.654815      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:25.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5168" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":122,"skipped":2060,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:25.674: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:50:25.738: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169" in namespace "downward-api-2489" to be "Succeeded or Failed"
Feb 12 14:50:25.746: INFO: Pod "downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169": Phase="Pending", Reason="", readiness=false. Elapsed: 7.132696ms
Feb 12 14:50:27.753: INFO: Pod "downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01415985s
Feb 12 14:50:29.761: INFO: Pod "downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022411424s
STEP: Saw pod success
Feb 12 14:50:29.761: INFO: Pod "downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169" satisfied condition "Succeeded or Failed"
Feb 12 14:50:29.767: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169 container client-container: <nil>
STEP: delete the pod
Feb 12 14:50:29.811: INFO: Waiting for pod downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169 to disappear
Feb 12 14:50:29.816: INFO: Pod downwardapi-volume-d319a11a-0245-4ab2-a6a9-1f595f49f169 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:29.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2489" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":2066,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Feb 12 14:50:29.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-3440 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 12 14:50:30.011: INFO: stderr: ""
Feb 12 14:50:30.011: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Feb 12 14:50:30.011: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 12 14:50:30.011: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3440" to be "running and ready, or succeeded"
Feb 12 14:50:30.016: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.920076ms
Feb 12 14:50:32.023: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418165s
Feb 12 14:50:34.030: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.019212034s
Feb 12 14:50:34.030: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 12 14:50:34.030: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 12 14:50:34.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440'
Feb 12 14:50:34.244: INFO: stderr: ""
Feb 12 14:50:34.244: INFO: stdout: "I0212 14:50:31.476221       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/xw8k 291\nI0212 14:50:31.676347       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qv2w 257\nI0212 14:50:31.876328       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/hhb 371\nI0212 14:50:32.076309       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/g46 417\nI0212 14:50:32.276296       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/rt5q 468\nI0212 14:50:32.476329       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kfz4 288\nI0212 14:50:32.676293       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/tcd 501\nI0212 14:50:32.876351       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xdh 563\nI0212 14:50:33.076343       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/r2v 518\nI0212 14:50:33.276306       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/6mg 231\nI0212 14:50:33.476356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/k64 412\nI0212 14:50:33.676340       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/dgs 333\nI0212 14:50:33.876308       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8zcr 334\nI0212 14:50:34.076320       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/tbnk 336\n"
STEP: limiting log lines
Feb 12 14:50:34.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440 --tail=1'
Feb 12 14:50:34.444: INFO: stderr: ""
Feb 12 14:50:34.444: INFO: stdout: "I0212 14:50:34.276310       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tfl 401\n"
Feb 12 14:50:34.444: INFO: got output "I0212 14:50:34.276310       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tfl 401\n"
STEP: limiting log bytes
Feb 12 14:50:34.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440 --limit-bytes=1'
Feb 12 14:50:34.540: INFO: stderr: ""
Feb 12 14:50:34.540: INFO: stdout: "I"
Feb 12 14:50:34.540: INFO: got output "I"
STEP: exposing timestamps
Feb 12 14:50:34.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440 --tail=1 --timestamps'
Feb 12 14:50:34.634: INFO: stderr: ""
Feb 12 14:50:34.634: INFO: stdout: "2021-02-12T14:50:34.476536970Z I0212 14:50:34.476380       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pj4c 561\n"
Feb 12 14:50:34.634: INFO: got output "2021-02-12T14:50:34.476536970Z I0212 14:50:34.476380       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pj4c 561\n"
STEP: restricting to a time range
Feb 12 14:50:37.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440 --since=1s'
Feb 12 14:50:37.377: INFO: stderr: ""
Feb 12 14:50:37.377: INFO: stdout: "I0212 14:50:36.476320       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ksvn 415\nI0212 14:50:36.676328       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/h9k 218\nI0212 14:50:36.876354       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/v994 341\nI0212 14:50:37.076338       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/js4h 243\nI0212 14:50:37.276333       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/8hg 455\n"
Feb 12 14:50:37.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs logs-generator logs-generator --namespace=kubectl-3440 --since=24h'
Feb 12 14:50:37.515: INFO: stderr: ""
Feb 12 14:50:37.516: INFO: stdout: "I0212 14:50:31.476221       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/xw8k 291\nI0212 14:50:31.676347       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qv2w 257\nI0212 14:50:31.876328       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/hhb 371\nI0212 14:50:32.076309       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/g46 417\nI0212 14:50:32.276296       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/rt5q 468\nI0212 14:50:32.476329       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kfz4 288\nI0212 14:50:32.676293       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/tcd 501\nI0212 14:50:32.876351       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xdh 563\nI0212 14:50:33.076343       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/r2v 518\nI0212 14:50:33.276306       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/6mg 231\nI0212 14:50:33.476356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/k64 412\nI0212 14:50:33.676340       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/dgs 333\nI0212 14:50:33.876308       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8zcr 334\nI0212 14:50:34.076320       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/tbnk 336\nI0212 14:50:34.276310       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tfl 401\nI0212 14:50:34.476380       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pj4c 561\nI0212 14:50:34.676294       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bnnv 568\nI0212 14:50:34.876333       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8wmq 255\nI0212 14:50:35.076336       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/cd5 523\nI0212 14:50:35.276411       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hm25 582\nI0212 14:50:35.476326       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/g4zb 356\nI0212 14:50:35.676317       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/wkcn 351\nI0212 14:50:35.876353       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/mrs 236\nI0212 14:50:36.076315       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/8gw5 414\nI0212 14:50:36.276320       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/7dp 509\nI0212 14:50:36.476320       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ksvn 415\nI0212 14:50:36.676328       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/h9k 218\nI0212 14:50:36.876354       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/v994 341\nI0212 14:50:37.076338       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/js4h 243\nI0212 14:50:37.276333       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/8hg 455\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Feb 12 14:50:37.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete pod logs-generator --namespace=kubectl-3440'
Feb 12 14:50:39.667: INFO: stderr: ""
Feb 12 14:50:39.667: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:39.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3440" for this suite.

• [SLOW TEST:9.861 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":124,"skipped":2073,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:39.694: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5733
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5733
I0212 14:50:39.804018      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5733, replica count: 2
Feb 12 14:50:42.854: INFO: Creating new exec pod
I0212 14:50:42.854431      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:50:45.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-5733 execpod8rfck -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 12 14:50:46.427: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 12 14:50:46.427: INFO: stdout: ""
Feb 12 14:50:46.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-5733 execpod8rfck -- /bin/sh -x -c nc -zv -t -w 2 10.240.21.101 80'
Feb 12 14:50:47.087: INFO: stderr: "+ nc -zv -t -w 2 10.240.21.101 80\nConnection to 10.240.21.101 80 port [tcp/http] succeeded!\n"
Feb 12 14:50:47.087: INFO: stdout: ""
Feb 12 14:50:47.087: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:47.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5733" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.452 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":125,"skipped":2074,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:47.146: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Feb 12 14:50:47.205: INFO: namespace kubectl-6048
Feb 12 14:50:47.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-6048'
Feb 12 14:50:47.519: INFO: stderr: ""
Feb 12 14:50:47.520: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 12 14:50:48.526: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:50:48.526: INFO: Found 0 / 1
Feb 12 14:50:49.527: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:50:49.527: INFO: Found 1 / 1
Feb 12 14:50:49.527: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 12 14:50:49.531: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 14:50:49.531: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 14:50:49.531: INFO: wait on agnhost-primary startup in kubectl-6048 
Feb 12 14:50:49.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 logs agnhost-primary-dj8dl agnhost-primary --namespace=kubectl-6048'
Feb 12 14:50:49.675: INFO: stderr: ""
Feb 12 14:50:49.675: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 12 14:50:49.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6048'
Feb 12 14:50:49.787: INFO: stderr: ""
Feb 12 14:50:49.787: INFO: stdout: "service/rm2 exposed\n"
Feb 12 14:50:49.799: INFO: Service rm2 in namespace kubectl-6048 found.
STEP: exposing service
Feb 12 14:50:51.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6048'
Feb 12 14:50:51.911: INFO: stderr: ""
Feb 12 14:50:51.911: INFO: stdout: "service/rm3 exposed\n"
Feb 12 14:50:51.922: INFO: Service rm3 in namespace kubectl-6048 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:53.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6048" for this suite.

• [SLOW TEST:6.810 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":126,"skipped":2075,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:53.956: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:50:54.168: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933" in namespace "downward-api-1610" to be "Succeeded or Failed"
Feb 12 14:50:54.177: INFO: Pod "downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933": Phase="Pending", Reason="", readiness=false. Elapsed: 9.015568ms
Feb 12 14:50:56.184: INFO: Pod "downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015789003s
STEP: Saw pod success
Feb 12 14:50:56.184: INFO: Pod "downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933" satisfied condition "Succeeded or Failed"
Feb 12 14:50:56.189: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933 container client-container: <nil>
STEP: delete the pod
Feb 12 14:50:56.268: INFO: Waiting for pod downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933 to disappear
Feb 12 14:50:56.274: INFO: Pod downwardapi-volume-6bb1ba6a-095d-421a-9703-63fe6ecce933 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:50:56.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1610" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:50:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2297
STEP: creating service affinity-nodeport-transition in namespace services-2297
STEP: creating replication controller affinity-nodeport-transition in namespace services-2297
I0212 14:50:56.388569      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2297, replica count: 3
I0212 14:50:59.438958      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:50:59.457: INFO: Creating new exec pod
Feb 12 14:51:04.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Feb 12 14:51:05.092: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 12 14:51:05.092: INFO: stdout: ""
Feb 12 14:51:05.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c nc -zv -t -w 2 10.240.17.123 80'
Feb 12 14:51:05.696: INFO: stderr: "+ nc -zv -t -w 2 10.240.17.123 80\nConnection to 10.240.17.123 80 port [tcp/http] succeeded!\n"
Feb 12 14:51:05.696: INFO: stdout: ""
Feb 12 14:51:05.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c nc -zv -t -w 2 68.183.68.155 30905'
Feb 12 14:51:06.276: INFO: stderr: "+ nc -zv -t -w 2 68.183.68.155 30905\nConnection to 68.183.68.155 30905 port [tcp/30905] succeeded!\n"
Feb 12 14:51:06.276: INFO: stdout: ""
Feb 12 14:51:06.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c nc -zv -t -w 2 165.227.175.251 30905'
Feb 12 14:51:06.908: INFO: stderr: "+ nc -zv -t -w 2 165.227.175.251 30905\nConnection to 165.227.175.251 30905 port [tcp/30905] succeeded!\n"
Feb 12 14:51:06.908: INFO: stdout: ""
Feb 12 14:51:06.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://104.248.253.154:30905/ ; done'
Feb 12 14:51:07.659: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n"
Feb 12 14:51:07.659: INFO: stdout: "\naffinity-nodeport-transition-t4fc2\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-lns4z\naffinity-nodeport-transition-t4fc2\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-lns4z\naffinity-nodeport-transition-t4fc2\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-lns4z\naffinity-nodeport-transition-t4fc2\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-lns4z\naffinity-nodeport-transition-t4fc2\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-lns4z\naffinity-nodeport-transition-t4fc2"
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-lns4z
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-lns4z
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-lns4z
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-lns4z
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-lns4z
Feb 12 14:51:07.659: INFO: Received response from host: affinity-nodeport-transition-t4fc2
Feb 12 14:51:07.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-2297 execpod-affinity6gp22 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://104.248.253.154:30905/ ; done'
Feb 12 14:51:08.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30905/\n"
Feb 12 14:51:08.383: INFO: stdout: "\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn\naffinity-nodeport-transition-j76bn"
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Received response from host: affinity-nodeport-transition-j76bn
Feb 12 14:51:08.383: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2297, will wait for the garbage collector to delete the pods
Feb 12 14:51:08.483: INFO: Deleting ReplicationController affinity-nodeport-transition took: 18.538513ms
Feb 12 14:51:08.583: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.142899ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:51:17.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2297" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.350 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":128,"skipped":2114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:51:17.653: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-7886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7886 to expose endpoints map[]
Feb 12 14:51:17.762: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 12 14:51:18.779: INFO: successfully validated that service endpoint-test2 in namespace services-7886 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7886 to expose endpoints map[pod1:[80]]
Feb 12 14:51:20.860: INFO: successfully validated that service endpoint-test2 in namespace services-7886 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7886 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 12 14:51:23.912: INFO: successfully validated that service endpoint-test2 in namespace services-7886 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7886 to expose endpoints map[pod2:[80]]
Feb 12 14:51:23.955: INFO: successfully validated that service endpoint-test2 in namespace services-7886 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7886 to expose endpoints map[]
Feb 12 14:51:25.015: INFO: successfully validated that service endpoint-test2 in namespace services-7886 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:51:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7886" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.430 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":129,"skipped":2234,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:51:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:51:25.466: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:51:28.522: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:51:28.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8002" for this suite.
STEP: Destroying namespace "webhook-8002-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":130,"skipped":2235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:51:28.910: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:51:31.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3158" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2258,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:51:31.093: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9794
STEP: creating service affinity-nodeport in namespace services-9794
STEP: creating replication controller affinity-nodeport in namespace services-9794
I0212 14:51:31.196822      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9794, replica count: 3
I0212 14:51:34.247852      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:51:34.281: INFO: Creating new exec pod
Feb 12 14:51:39.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9794 execpod-affinityl5mtv -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Feb 12 14:51:39.969: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 12 14:51:39.969: INFO: stdout: ""
Feb 12 14:51:39.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9794 execpod-affinityl5mtv -- /bin/sh -x -c nc -zv -t -w 2 10.240.24.156 80'
Feb 12 14:51:40.585: INFO: stderr: "+ nc -zv -t -w 2 10.240.24.156 80\nConnection to 10.240.24.156 80 port [tcp/http] succeeded!\n"
Feb 12 14:51:40.585: INFO: stdout: ""
Feb 12 14:51:40.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9794 execpod-affinityl5mtv -- /bin/sh -x -c nc -zv -t -w 2 104.248.253.154 30253'
Feb 12 14:51:41.161: INFO: stderr: "+ nc -zv -t -w 2 104.248.253.154 30253\nConnection to 104.248.253.154 30253 port [tcp/30253] succeeded!\n"
Feb 12 14:51:41.161: INFO: stdout: ""
Feb 12 14:51:41.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9794 execpod-affinityl5mtv -- /bin/sh -x -c nc -zv -t -w 2 68.183.68.155 30253'
Feb 12 14:51:41.777: INFO: stderr: "+ nc -zv -t -w 2 68.183.68.155 30253\nConnection to 68.183.68.155 30253 port [tcp/30253] succeeded!\n"
Feb 12 14:51:41.777: INFO: stdout: ""
Feb 12 14:51:41.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9794 execpod-affinityl5mtv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://104.248.253.154:30253/ ; done'
Feb 12 14:51:42.480: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:30253/\n"
Feb 12 14:51:42.480: INFO: stdout: "\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9\naffinity-nodeport-cc5x9"
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Received response from host: affinity-nodeport-cc5x9
Feb 12 14:51:42.480: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9794, will wait for the garbage collector to delete the pods
Feb 12 14:51:42.581: INFO: Deleting ReplicationController affinity-nodeport took: 14.723225ms
Feb 12 14:51:42.681: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.132211ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:51:57.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9794" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.561 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":132,"skipped":2261,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:51:57.654: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6581
STEP: creating service affinity-clusterip-transition in namespace services-6581
STEP: creating replication controller affinity-clusterip-transition in namespace services-6581
I0212 14:51:57.752554      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6581, replica count: 3
I0212 14:52:00.802900      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 14:52:00.816: INFO: Creating new exec pod
Feb 12 14:52:05.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6581 execpod-affinityflrmx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Feb 12 14:52:06.499: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 12 14:52:06.499: INFO: stdout: ""
Feb 12 14:52:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6581 execpod-affinityflrmx -- /bin/sh -x -c nc -zv -t -w 2 10.240.29.114 80'
Feb 12 14:52:07.100: INFO: stderr: "+ nc -zv -t -w 2 10.240.29.114 80\nConnection to 10.240.29.114 80 port [tcp/http] succeeded!\n"
Feb 12 14:52:07.100: INFO: stdout: ""
Feb 12 14:52:07.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6581 execpod-affinityflrmx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.29.114:80/ ; done'
Feb 12 14:52:07.818: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n"
Feb 12 14:52:07.818: INFO: stdout: "\naffinity-clusterip-transition-txjz4\naffinity-clusterip-transition-wrcj8\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-txjz4\naffinity-clusterip-transition-wrcj8\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-txjz4\naffinity-clusterip-transition-wrcj8\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-txjz4\naffinity-clusterip-transition-wrcj8\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-txjz4\naffinity-clusterip-transition-wrcj8\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-txjz4"
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-wrcj8
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-wrcj8
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-wrcj8
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-wrcj8
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-wrcj8
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:07.818: INFO: Received response from host: affinity-clusterip-transition-txjz4
Feb 12 14:52:07.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6581 execpod-affinityflrmx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.29.114:80/ ; done'
Feb 12 14:52:08.512: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.114:80/\n"
Feb 12 14:52:08.512: INFO: stdout: "\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f\naffinity-clusterip-transition-gcp5f"
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Received response from host: affinity-clusterip-transition-gcp5f
Feb 12 14:52:08.512: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6581, will wait for the garbage collector to delete the pods
Feb 12 14:52:08.602: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.276929ms
Feb 12 14:52:08.702: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.142804ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:52:24.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6581" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.519 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":133,"skipped":2274,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:52:24.175: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-20796b29-c224-40b8-8f9c-1ca9efb795fb in namespace container-probe-5772
Feb 12 14:52:26.255: INFO: Started pod test-webserver-20796b29-c224-40b8-8f9c-1ca9efb795fb in namespace container-probe-5772
STEP: checking the pod's current state and verifying that restartCount is present
Feb 12 14:52:26.261: INFO: Initial restart count of pod test-webserver-20796b29-c224-40b8-8f9c-1ca9efb795fb is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:56:27.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5772" for this suite.

• [SLOW TEST:243.400 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2285,"failed":0}
SSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:56:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 12 14:56:27.681: INFO: starting watch
STEP: patching
STEP: updating
Feb 12 14:56:27.701: INFO: waiting for watch events with expected annotations
Feb 12 14:56:27.701: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:56:27.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5309" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":135,"skipped":2290,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:56:27.772: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:56:34.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8970" for this suite.

• [SLOW TEST:7.109 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":136,"skipped":2293,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:56:34.881: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:56:34.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2403" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":137,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:56:35.018: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb 12 14:56:39.121: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5713 PodName:var-expansion-81921760-1780-4220-b638-76b0bf7236d1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:56:39.121: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: test for file in mounted path
Feb 12 14:56:39.668: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5713 PodName:var-expansion-81921760-1780-4220-b638-76b0bf7236d1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 14:56:39.669: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: updating the annotation value
Feb 12 14:56:40.751: INFO: Successfully updated pod "var-expansion-81921760-1780-4220-b638-76b0bf7236d1"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb 12 14:56:40.761: INFO: Deleting pod "var-expansion-81921760-1780-4220-b638-76b0bf7236d1" in namespace "var-expansion-5713"
Feb 12 14:56:40.775: INFO: Wait up to 5m0s for pod "var-expansion-81921760-1780-4220-b638-76b0bf7236d1" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:57:14.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5713" for this suite.

• [SLOW TEST:39.795 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":138,"skipped":2320,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:57:14.813: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:57:14.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704" in namespace "projected-7636" to be "Succeeded or Failed"
Feb 12 14:57:14.936: INFO: Pod "downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704": Phase="Pending", Reason="", readiness=false. Elapsed: 15.594362ms
Feb 12 14:57:16.944: INFO: Pod "downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02409348s
Feb 12 14:57:18.953: INFO: Pod "downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032507607s
STEP: Saw pod success
Feb 12 14:57:18.953: INFO: Pod "downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704" satisfied condition "Succeeded or Failed"
Feb 12 14:57:18.960: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704 container client-container: <nil>
STEP: delete the pod
Feb 12 14:57:19.023: INFO: Waiting for pod downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704 to disappear
Feb 12 14:57:19.040: INFO: Pod downwardapi-volume-949ebc82-c6c3-4af8-8462-2d67ffa7e704 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:57:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7636" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2321,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:57:19.068: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 14:57:19.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422" in namespace "projected-1238" to be "Succeeded or Failed"
Feb 12 14:57:19.152: INFO: Pod "downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422": Phase="Pending", Reason="", readiness=false. Elapsed: 7.931982ms
Feb 12 14:57:21.160: INFO: Pod "downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0156718s
Feb 12 14:57:23.171: INFO: Pod "downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027137302s
STEP: Saw pod success
Feb 12 14:57:23.171: INFO: Pod "downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422" satisfied condition "Succeeded or Failed"
Feb 12 14:57:23.221: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422 container client-container: <nil>
STEP: delete the pod
Feb 12 14:57:23.328: INFO: Waiting for pod downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422 to disappear
Feb 12 14:57:23.335: INFO: Pod downwardapi-volume-2e2ca993-8674-459f-87a6-1f8f43857422 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:57:23.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1238" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2324,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:57:23.357: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-88d2f9fa-cf9c-415a-9192-2dc94d853011
STEP: Creating a pod to test consume secrets
Feb 12 14:57:23.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294" in namespace "projected-9540" to be "Succeeded or Failed"
Feb 12 14:57:23.454: INFO: Pod "pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294": Phase="Pending", Reason="", readiness=false. Elapsed: 8.694838ms
Feb 12 14:57:25.469: INFO: Pod "pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02411297s
Feb 12 14:57:27.482: INFO: Pod "pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037160632s
STEP: Saw pod success
Feb 12 14:57:27.482: INFO: Pod "pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294" satisfied condition "Succeeded or Failed"
Feb 12 14:57:27.489: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 12 14:57:27.537: INFO: Waiting for pod pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294 to disappear
Feb 12 14:57:27.556: INFO: Pod pod-projected-secrets-f72e8c66-61db-4aab-955a-924d0f955294 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:57:27.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9540" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:57:27.589: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 12 14:58:04.723: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:05.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8507" for this suite.

• [SLOW TEST:38.201 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":142,"skipped":2365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:05.796: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:58:06.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 14:58:08.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738686, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738686, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738686, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738686, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:58:11.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:11.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2979" for this suite.
STEP: Destroying namespace "webhook-2979-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":143,"skipped":2419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:11.857: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 12 14:58:12.473: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 12 14:58:14.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738692, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738692, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738692, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738692, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 14:58:17.516: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:58:17.522: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:19.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2370" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.463 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":144,"skipped":2475,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:19.326: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:58:23.555: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.598: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.611: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.621: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.842: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.859: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.874: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:23.888: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6: the server could not find the requested resource (get pods dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6)
Feb 12 14:58:24.027: INFO: Lookups using dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local]

Feb 12 14:58:30.058: INFO: DNS probes using dns-7024/dns-test-056eff01-41b2-4eef-9ecc-84c25a6bb6a6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:30.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7024" for this suite.

• [SLOW TEST:10.839 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":145,"skipped":2477,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:30.165: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 12 14:58:30.246: INFO: Waiting up to 5m0s for pod "pod-bb26b1e5-6efc-4997-a607-e0443c5d8416" in namespace "emptydir-3263" to be "Succeeded or Failed"
Feb 12 14:58:30.253: INFO: Pod "pod-bb26b1e5-6efc-4997-a607-e0443c5d8416": Phase="Pending", Reason="", readiness=false. Elapsed: 7.004214ms
Feb 12 14:58:32.260: INFO: Pod "pod-bb26b1e5-6efc-4997-a607-e0443c5d8416": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013603332s
Feb 12 14:58:34.268: INFO: Pod "pod-bb26b1e5-6efc-4997-a607-e0443c5d8416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022411123s
STEP: Saw pod success
Feb 12 14:58:34.268: INFO: Pod "pod-bb26b1e5-6efc-4997-a607-e0443c5d8416" satisfied condition "Succeeded or Failed"
Feb 12 14:58:34.277: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-bb26b1e5-6efc-4997-a607-e0443c5d8416 container test-container: <nil>
STEP: delete the pod
Feb 12 14:58:34.317: INFO: Waiting for pod pod-bb26b1e5-6efc-4997-a607-e0443c5d8416 to disappear
Feb 12 14:58:34.323: INFO: Pod pod-bb26b1e5-6efc-4997-a607-e0443c5d8416 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:34.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3263" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:34.347: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 12 14:58:34.405: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 14:58:34.418: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 14:58:34.426: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-h2tct before test
Feb 12 14:58:34.441: INFO: canal-zprb7 from kube-system started at 2021-02-12 12:16:38 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.441: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:58:34.441: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:58:34.441: INFO: kube-proxy-wl68v from kube-system started at 2021-02-12 12:16:38 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.441: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:58:34.441: INFO: logrotate-6jb84 from kube-system started at 2021-02-12 14:21:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.442: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:58:34.442: INFO: node-local-dns-xsl7b from kube-system started at 2021-02-12 14:21:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.442: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:58:34.442: INFO: user-ssh-keys-agent-tm2rj from kube-system started at 2021-02-12 13:11:47 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.442: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:58:34.442: INFO: sonobuoy from sonobuoy started at 2021-02-12 14:03:39 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.442: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 14:58:34.442: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:58:34.442: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:58:34.442: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-qftnt before test
Feb 12 14:58:34.454: INFO: canal-j2gzw from kube-system started at 2021-02-12 12:16:44 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:58:34.454: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:58:34.454: INFO: coredns-856777c9f5-tzdw7 from kube-system started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:58:34.454: INFO: kube-proxy-rfp6j from kube-system started at 2021-02-12 12:16:44 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:58:34.454: INFO: logrotate-zkvll from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:58:34.454: INFO: node-local-dns-2lwvq from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:58:34.454: INFO: user-ssh-keys-agent-vdvks from kube-system started at 2021-02-12 13:12:04 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:58:34.454: INFO: dashboard-metrics-scraper-975c84c89-7drg5 from kubernetes-dashboard started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:58:34.454: INFO: sonobuoy-e2e-job-7631d608de454e9c from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container e2e ready: true, restart count 0
Feb 12 14:58:34.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:58:34.454: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:58:34.454: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 14:58:34.454: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-w42vq before test
Feb 12 14:58:34.466: INFO: canal-mx9gb from kube-system started at 2021-02-12 12:16:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 14:58:34.466: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 14:58:34.466: INFO: coredns-856777c9f5-b27nr from kube-system started at 2021-02-12 12:16:59 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container coredns ready: true, restart count 0
Feb 12 14:58:34.466: INFO: kube-proxy-s2hw4 from kube-system started at 2021-02-12 12:16:40 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 14:58:34.466: INFO: logrotate-54qnh from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 14:58:34.466: INFO: node-local-dns-4dbnq from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 14:58:34.466: INFO: openvpn-client-6c9dd998bc-lr8jh from kube-system started at 2021-02-12 12:16:59 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container dnat-controller ready: true, restart count 1
Feb 12 14:58:34.466: INFO: 	Container openvpn-client ready: true, restart count 0
Feb 12 14:58:34.466: INFO: user-ssh-keys-agent-2llsr from kube-system started at 2021-02-12 13:12:10 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 14:58:34.466: INFO: dashboard-metrics-scraper-975c84c89-xfxv8 from kubernetes-dashboard started at 2021-02-12 12:16:57 +0000 UTC (1 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 14:58:34.466: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 14:58:34.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 14:58:34.466: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2018ab2f-089b-4e0e-823a-ea43f22eba2f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-2018ab2f-089b-4e0e-823a-ea43f22eba2f off the node modest-nightingale-745f6d5bd4-h2tct
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2018ab2f-089b-4e0e-823a-ea43f22eba2f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:48.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9583" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:14.371 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":147,"skipped":2499,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:48.718: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-45aa6381-0f8d-495f-9163-76876ea57c32
STEP: Creating a pod to test consume configMaps
Feb 12 14:58:48.871: INFO: Waiting up to 5m0s for pod "pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f" in namespace "configmap-4327" to be "Succeeded or Failed"
Feb 12 14:58:48.881: INFO: Pod "pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.795284ms
Feb 12 14:58:50.894: INFO: Pod "pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023230795s
Feb 12 14:58:52.901: INFO: Pod "pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02957324s
STEP: Saw pod success
Feb 12 14:58:52.901: INFO: Pod "pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f" satisfied condition "Succeeded or Failed"
Feb 12 14:58:52.906: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 14:58:52.989: INFO: Waiting for pod pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f to disappear
Feb 12 14:58:52.999: INFO: Pod pod-configmaps-2891b065-aa96-4376-bfd4-9597b555e62f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:52.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4327" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2499,"failed":0}
SSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 12 14:58:53.231: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 12 14:58:53.242: INFO: starting watch
STEP: patching
STEP: updating
Feb 12 14:58:53.278: INFO: waiting for watch events with expected annotations
Feb 12 14:58:53.278: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:53.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3994" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":149,"skipped":2502,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:53.408: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b678d9e-0b62-4a8a-a211-235222e3beba
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6b678d9e-0b62-4a8a-a211-235222e3beba
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:57.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-856" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:57.701: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 12 14:58:57.825: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7153 /api/v1/namespaces/watch-7153/configmaps/e2e-watch-test-resource-version 375a0cbc-cd89-4b89-86b9-d425f1304da8 76205 0 2021-02-12 14:58:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-12 14:58:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 14:58:57.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7153 /api/v1/namespaces/watch-7153/configmaps/e2e-watch-test-resource-version 375a0cbc-cd89-4b89-86b9-d425f1304da8 76206 0 2021-02-12 14:58:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-12 14:58:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:57.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7153" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":151,"skipped":2533,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:57.843: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Feb 12 14:58:58.463: INFO: created pod pod-service-account-defaultsa
Feb 12 14:58:58.463: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 12 14:58:58.488: INFO: created pod pod-service-account-mountsa
Feb 12 14:58:58.489: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 12 14:58:58.525: INFO: created pod pod-service-account-nomountsa
Feb 12 14:58:58.525: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 12 14:58:58.539: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 12 14:58:58.539: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 12 14:58:58.552: INFO: created pod pod-service-account-mountsa-mountspec
Feb 12 14:58:58.552: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 12 14:58:58.562: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 12 14:58:58.562: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 12 14:58:58.579: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 12 14:58:58.579: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 12 14:58:58.615: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 12 14:58:58.615: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 12 14:58:58.631: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 12 14:58:58.631: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:58:58.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4002" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":152,"skipped":2551,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:58:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:58:58.751: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 12 14:59:00.838: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:00.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-868" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":153,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:00.883: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-2026
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2026 to expose endpoints map[]
Feb 12 14:59:01.001: INFO: successfully validated that service multi-endpoint-test in namespace services-2026 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2026
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2026 to expose endpoints map[pod1:[100]]
Feb 12 14:59:04.073: INFO: successfully validated that service multi-endpoint-test in namespace services-2026 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2026
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2026 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 12 14:59:08.123: INFO: successfully validated that service multi-endpoint-test in namespace services-2026 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2026
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2026 to expose endpoints map[pod2:[101]]
Feb 12 14:59:08.171: INFO: successfully validated that service multi-endpoint-test in namespace services-2026 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2026
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2026 to expose endpoints map[]
Feb 12 14:59:08.197: INFO: successfully validated that service multi-endpoint-test in namespace services-2026 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:08.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2026" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.421 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":154,"skipped":2593,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-hj9v
STEP: Creating a pod to test atomic-volume-subpath
Feb 12 14:59:08.443: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hj9v" in namespace "subpath-7198" to be "Succeeded or Failed"
Feb 12 14:59:08.449: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221525ms
Feb 12 14:59:10.456: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013417632s
Feb 12 14:59:12.470: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 4.027260779s
Feb 12 14:59:14.480: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 6.036611126s
Feb 12 14:59:16.498: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 8.054608799s
Feb 12 14:59:18.505: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 10.061575293s
Feb 12 14:59:20.511: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.068344116s
Feb 12 14:59:22.519: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.075652703s
Feb 12 14:59:24.527: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.084215457s
Feb 12 14:59:26.534: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.091111712s
Feb 12 14:59:28.542: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.099416697s
Feb 12 14:59:30.550: INFO: Pod "pod-subpath-test-secret-hj9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.106907925s
STEP: Saw pod success
Feb 12 14:59:30.550: INFO: Pod "pod-subpath-test-secret-hj9v" satisfied condition "Succeeded or Failed"
Feb 12 14:59:30.558: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-subpath-test-secret-hj9v container test-container-subpath-secret-hj9v: <nil>
STEP: delete the pod
Feb 12 14:59:30.598: INFO: Waiting for pod pod-subpath-test-secret-hj9v to disappear
Feb 12 14:59:30.603: INFO: Pod pod-subpath-test-secret-hj9v no longer exists
STEP: Deleting pod pod-subpath-test-secret-hj9v
Feb 12 14:59:30.604: INFO: Deleting pod "pod-subpath-test-secret-hj9v" in namespace "subpath-7198"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:30.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7198" for this suite.

• [SLOW TEST:22.328 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":155,"skipped":2601,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:30.633: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0212 14:59:40.879101      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 14:59:40.879119      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 14:59:40.879137      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 12 14:59:40.879: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 12 14:59:40.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-45vdw" in namespace "gc-7878"
Feb 12 14:59:40.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-4flkz" in namespace "gc-7878"
Feb 12 14:59:40.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d55x" in namespace "gc-7878"
Feb 12 14:59:40.951: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xh44" in namespace "gc-7878"
Feb 12 14:59:40.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5pm4" in namespace "gc-7878"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:41.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7878" for this suite.

• [SLOW TEST:10.410 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":156,"skipped":2604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:41.047: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:45.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3815" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2661,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:45.210: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 14:59:45.572: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 12 14:59:45.576: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 12 14:59:45.577: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.577: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 12 14:59:45.577: INFO: Checking APIGroup: extensions
Feb 12 14:59:45.580: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Feb 12 14:59:45.580: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Feb 12 14:59:45.580: INFO: extensions/v1beta1 matches extensions/v1beta1
Feb 12 14:59:45.580: INFO: Checking APIGroup: apps
Feb 12 14:59:45.583: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 12 14:59:45.583: INFO: Versions found [{apps/v1 v1}]
Feb 12 14:59:45.583: INFO: apps/v1 matches apps/v1
Feb 12 14:59:45.583: INFO: Checking APIGroup: events.k8s.io
Feb 12 14:59:45.586: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 12 14:59:45.586: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.586: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 12 14:59:45.586: INFO: Checking APIGroup: authentication.k8s.io
Feb 12 14:59:45.588: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 12 14:59:45.588: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.588: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 12 14:59:45.589: INFO: Checking APIGroup: authorization.k8s.io
Feb 12 14:59:45.591: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 12 14:59:45.591: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.591: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 12 14:59:45.591: INFO: Checking APIGroup: autoscaling
Feb 12 14:59:45.594: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Feb 12 14:59:45.594: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb 12 14:59:45.594: INFO: autoscaling/v1 matches autoscaling/v1
Feb 12 14:59:45.594: INFO: Checking APIGroup: batch
Feb 12 14:59:45.597: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 12 14:59:45.597: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Feb 12 14:59:45.597: INFO: batch/v1 matches batch/v1
Feb 12 14:59:45.597: INFO: Checking APIGroup: certificates.k8s.io
Feb 12 14:59:45.599: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 12 14:59:45.599: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.599: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 12 14:59:45.599: INFO: Checking APIGroup: networking.k8s.io
Feb 12 14:59:45.602: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 12 14:59:45.602: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.602: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 12 14:59:45.602: INFO: Checking APIGroup: policy
Feb 12 14:59:45.605: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Feb 12 14:59:45.605: INFO: Versions found [{policy/v1beta1 v1beta1}]
Feb 12 14:59:45.605: INFO: policy/v1beta1 matches policy/v1beta1
Feb 12 14:59:45.605: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 12 14:59:45.607: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 12 14:59:45.607: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.608: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 12 14:59:45.608: INFO: Checking APIGroup: storage.k8s.io
Feb 12 14:59:45.610: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 12 14:59:45.610: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.610: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 12 14:59:45.610: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 12 14:59:45.613: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 12 14:59:45.613: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.613: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 12 14:59:45.613: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 12 14:59:45.615: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 12 14:59:45.615: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.615: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 12 14:59:45.615: INFO: Checking APIGroup: scheduling.k8s.io
Feb 12 14:59:45.624: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 12 14:59:45.624: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.624: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 12 14:59:45.624: INFO: Checking APIGroup: coordination.k8s.io
Feb 12 14:59:45.628: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 12 14:59:45.628: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.628: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 12 14:59:45.628: INFO: Checking APIGroup: node.k8s.io
Feb 12 14:59:45.630: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Feb 12 14:59:45.631: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.631: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Feb 12 14:59:45.631: INFO: Checking APIGroup: discovery.k8s.io
Feb 12 14:59:45.633: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Feb 12 14:59:45.633: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.633: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Feb 12 14:59:45.633: INFO: Checking APIGroup: crd.projectcalico.org
Feb 12 14:59:45.638: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 12 14:59:45.638: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 12 14:59:45.638: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 12 14:59:45.638: INFO: Checking APIGroup: cluster.k8s.io
Feb 12 14:59:45.640: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Feb 12 14:59:45.640: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Feb 12 14:59:45.640: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Feb 12 14:59:45.640: INFO: Checking APIGroup: metrics.k8s.io
Feb 12 14:59:45.642: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 12 14:59:45.643: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 12 14:59:45.643: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:45.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8855" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":158,"skipped":2674,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:45.681: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 12 14:59:48.310: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1"
Feb 12 14:59:48.310: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1" in namespace "pods-9952" to be "terminated due to deadline exceeded"
Feb 12 14:59:48.316: INFO: Pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1": Phase="Running", Reason="", readiness=true. Elapsed: 5.473828ms
Feb 12 14:59:50.323: INFO: Pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013097911s
Feb 12 14:59:52.330: INFO: Pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.020359465s
Feb 12 14:59:52.331: INFO: Pod "pod-update-activedeadlineseconds-6b1907c9-1678-4622-8f2f-27c74c07ccb1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:52.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9952" for this suite.

• [SLOW TEST:6.673 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2678,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:52.355: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Feb 12 14:59:52.437: INFO: created test-pod-1
Feb 12 14:59:52.454: INFO: created test-pod-2
Feb 12 14:59:52.466: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:52.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1503" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":160,"skipped":2694,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-be97b2df-cf41-4cd7-883d-3a953e12ebac
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-29" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":161,"skipped":2705,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:52.704: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6295.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6295.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 14:59:55.508: INFO: DNS probes using dns-6295/dns-test-e4c6807d-d0c4-4e5e-8567-723611d48520 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:55.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6295" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":162,"skipped":2718,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:55.558: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb 12 14:59:55.665: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 14:59:55.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2731" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":163,"skipped":2718,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 14:59:55.728: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:12.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6468" for this suite.

• [SLOW TEST:17.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":164,"skipped":2729,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:12.912: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:13.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8867" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:13.080: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 12 15:00:15.183: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:15.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7304" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2766,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:15.233: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-03b8124a-1642-4a6d-aa73-5ccc5b26583b
STEP: Creating a pod to test consume configMaps
Feb 12 15:00:15.341: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108" in namespace "projected-4241" to be "Succeeded or Failed"
Feb 12 15:00:15.350: INFO: Pod "pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051304ms
Feb 12 15:00:17.365: INFO: Pod "pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023762474s
STEP: Saw pod success
Feb 12 15:00:17.365: INFO: Pod "pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108" satisfied condition "Succeeded or Failed"
Feb 12 15:00:17.393: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:00:17.438: INFO: Waiting for pod pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108 to disappear
Feb 12 15:00:17.444: INFO: Pod pod-projected-configmaps-357929b8-fb40-4c16-948a-5c17cb37e108 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4241" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2766,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:17.465: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6998
STEP: creating service affinity-clusterip in namespace services-6998
STEP: creating replication controller affinity-clusterip in namespace services-6998
I0212 15:00:17.550842      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6998, replica count: 3
I0212 15:00:20.602821      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 15:00:20.612: INFO: Creating new exec pod
Feb 12 15:00:25.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6998 execpod-affinityh762m -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Feb 12 15:00:26.353: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 12 15:00:26.353: INFO: stdout: ""
Feb 12 15:00:26.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6998 execpod-affinityh762m -- /bin/sh -x -c nc -zv -t -w 2 10.240.27.224 80'
Feb 12 15:00:26.919: INFO: stderr: "+ nc -zv -t -w 2 10.240.27.224 80\nConnection to 10.240.27.224 80 port [tcp/http] succeeded!\n"
Feb 12 15:00:26.919: INFO: stdout: ""
Feb 12 15:00:26.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-6998 execpod-affinityh762m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.27.224:80/ ; done'
Feb 12 15:00:27.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.224:80/\n"
Feb 12 15:00:27.633: INFO: stdout: "\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz\naffinity-clusterip-z4sdz"
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Received response from host: affinity-clusterip-z4sdz
Feb 12 15:00:27.633: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6998, will wait for the garbage collector to delete the pods
Feb 12 15:00:27.746: INFO: Deleting ReplicationController affinity-clusterip took: 31.382446ms
Feb 12 15:00:28.346: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.144793ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:37.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6998" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.180 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":168,"skipped":2780,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Feb 12 15:00:37.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-5683'
Feb 12 15:00:38.039: INFO: stderr: ""
Feb 12 15:00:38.039: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 12 15:00:39.245: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 15:00:39.245: INFO: Found 0 / 1
Feb 12 15:00:40.048: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 15:00:40.048: INFO: Found 0 / 1
Feb 12 15:00:41.052: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 15:00:41.052: INFO: Found 1 / 1
Feb 12 15:00:41.052: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 12 15:00:41.060: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 15:00:41.060: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 15:00:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 patch pod agnhost-primary-6fm8l --namespace=kubectl-5683 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 12 15:00:41.162: INFO: stderr: ""
Feb 12 15:00:41.162: INFO: stdout: "pod/agnhost-primary-6fm8l patched\n"
STEP: checking annotations
Feb 12 15:00:41.172: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 15:00:41.172: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:41.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5683" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":169,"skipped":2796,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:41.273: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 12 15:00:41.405: INFO: Waiting up to 5m0s for pod "pod-fa973c28-a91e-46c6-9e08-30658beb79f2" in namespace "emptydir-3170" to be "Succeeded or Failed"
Feb 12 15:00:41.421: INFO: Pod "pod-fa973c28-a91e-46c6-9e08-30658beb79f2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.54656ms
Feb 12 15:00:43.429: INFO: Pod "pod-fa973c28-a91e-46c6-9e08-30658beb79f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023950842s
Feb 12 15:00:45.434: INFO: Pod "pod-fa973c28-a91e-46c6-9e08-30658beb79f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029205381s
STEP: Saw pod success
Feb 12 15:00:45.434: INFO: Pod "pod-fa973c28-a91e-46c6-9e08-30658beb79f2" satisfied condition "Succeeded or Failed"
Feb 12 15:00:45.440: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-fa973c28-a91e-46c6-9e08-30658beb79f2 container test-container: <nil>
STEP: delete the pod
Feb 12 15:00:45.644: INFO: Waiting for pod pod-fa973c28-a91e-46c6-9e08-30658beb79f2 to disappear
Feb 12 15:00:45.649: INFO: Pod pod-fa973c28-a91e-46c6-9e08-30658beb79f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:45.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3170" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2800,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:45.676: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Feb 12 15:00:45.738: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:49.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1752" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":171,"skipped":2800,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:49.913: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:00:50.384: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:00:52.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738850, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738850, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738850, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748738850, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:00:55.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:00:55.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4293" for this suite.
STEP: Destroying namespace "webhook-4293-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.120 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":172,"skipped":2820,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:00:56.042: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Feb 12 15:00:56.134: INFO: Waiting up to 5m0s for pod "downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0" in namespace "downward-api-9375" to be "Succeeded or Failed"
Feb 12 15:00:56.142: INFO: Pod "downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427178ms
Feb 12 15:00:58.150: INFO: Pod "downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015462886s
Feb 12 15:01:00.156: INFO: Pod "downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021471839s
STEP: Saw pod success
Feb 12 15:01:00.156: INFO: Pod "downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0" satisfied condition "Succeeded or Failed"
Feb 12 15:01:00.161: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0 container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:01:00.207: INFO: Waiting for pod downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0 to disappear
Feb 12 15:01:00.213: INFO: Pod downward-api-823ff43d-9cd6-4395-8115-19036b89a2e0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:01:00.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9375" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:01:00.235: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-642
STEP: creating replication controller nodeport-test in namespace services-642
I0212 15:01:00.338222      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-642, replica count: 2
I0212 15:01:03.388474      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 15:01:06.388787      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 15:01:09.389118      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 15:01:12.389422      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 15:01:15.389739      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 15:01:15.389: INFO: Creating new exec pod
Feb 12 15:01:18.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-642 execpod9c5vq -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 12 15:01:18.951: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 12 15:01:18.951: INFO: stdout: ""
Feb 12 15:01:18.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-642 execpod9c5vq -- /bin/sh -x -c nc -zv -t -w 2 10.240.22.78 80'
Feb 12 15:01:19.545: INFO: stderr: "+ nc -zv -t -w 2 10.240.22.78 80\nConnection to 10.240.22.78 80 port [tcp/http] succeeded!\n"
Feb 12 15:01:19.545: INFO: stdout: ""
Feb 12 15:01:19.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-642 execpod9c5vq -- /bin/sh -x -c nc -zv -t -w 2 68.183.68.155 31401'
Feb 12 15:01:20.124: INFO: stderr: "+ nc -zv -t -w 2 68.183.68.155 31401\nConnection to 68.183.68.155 31401 port [tcp/31401] succeeded!\n"
Feb 12 15:01:20.124: INFO: stdout: ""
Feb 12 15:01:20.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-642 execpod9c5vq -- /bin/sh -x -c nc -zv -t -w 2 165.227.175.251 31401'
Feb 12 15:01:20.727: INFO: stderr: "+ nc -zv -t -w 2 165.227.175.251 31401\nConnection to 165.227.175.251 31401 port [tcp/31401] succeeded!\n"
Feb 12 15:01:20.727: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:01:20.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-642" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.518 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":174,"skipped":2845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:01:20.753: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9071
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9071
STEP: creating replication controller externalsvc in namespace services-9071
I0212 15:01:20.903345      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9071, replica count: 2
I0212 15:01:23.953584      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 12 15:01:24.018: INFO: Creating new exec pod
Feb 12 15:01:26.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-9071 execpod74df9 -- /bin/sh -x -c nslookup nodeport-service.services-9071.svc.cluster.local'
Feb 12 15:01:26.606: INFO: stderr: "+ nslookup nodeport-service.services-9071.svc.cluster.local\n"
Feb 12 15:01:26.606: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nnodeport-service.services-9071.svc.cluster.local\tcanonical name = externalsvc.services-9071.svc.cluster.local.\nName:\texternalsvc.services-9071.svc.cluster.local\nAddress: 10.240.16.185\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9071, will wait for the garbage collector to delete the pods
Feb 12 15:01:26.683: INFO: Deleting ReplicationController externalsvc took: 20.201179ms
Feb 12 15:01:26.783: INFO: Terminating ReplicationController externalsvc pods took: 100.158975ms
Feb 12 15:01:40.324: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:01:40.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9071" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.618 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":175,"skipped":2872,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:01:40.371: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:01:48.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2290" for this suite.

• [SLOW TEST:8.126 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":176,"skipped":2882,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:01:48.500: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-ea60af9c-d2a9-4dc5-b05d-322c80b79006
STEP: Creating configMap with name cm-test-opt-upd-8e825264-bd4a-486d-a2cb-40a152bbf029
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ea60af9c-d2a9-4dc5-b05d-322c80b79006
STEP: Updating configmap cm-test-opt-upd-8e825264-bd4a-486d-a2cb-40a152bbf029
STEP: Creating configMap with name cm-test-opt-create-fea2998a-2850-4eb3-a0a0-17cf6929dfd9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:10.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-783" for this suite.

• [SLOW TEST:81.944 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2894,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:10.447: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-c9a4eca7-7a87-4c85-a88d-9c6b7ee742ca
STEP: Creating secret with name s-test-opt-upd-7c3c6b25-7378-4573-bca3-bfb04ba6fce4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c9a4eca7-7a87-4c85-a88d-9c6b7ee742ca
STEP: Updating secret s-test-opt-upd-7c3c6b25-7378-4573-bca3-bfb04ba6fce4
STEP: Creating secret with name s-test-opt-create-4659a3b7-c222-4235-891f-60e6d7dc0bb1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:19.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8685" for this suite.

• [SLOW TEST:8.669 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":178,"skipped":2913,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:19.117: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Feb 12 15:03:19.192: INFO: Waiting up to 5m0s for pod "downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149" in namespace "downward-api-5057" to be "Succeeded or Failed"
Feb 12 15:03:19.201: INFO: Pod "downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.780372ms
Feb 12 15:03:21.210: INFO: Pod "downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017892959s
STEP: Saw pod success
Feb 12 15:03:21.211: INFO: Pod "downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149" satisfied condition "Succeeded or Failed"
Feb 12 15:03:21.221: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149 container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:03:21.314: INFO: Waiting for pod downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149 to disappear
Feb 12 15:03:21.326: INFO: Pod downward-api-353ed2f7-2b9f-43f5-b1b1-7345d72a1149 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:21.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5057" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2914,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:03:21.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:03:23.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739001, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739001, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739001, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739001, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:03:27.004: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4326" for this suite.
STEP: Destroying namespace "webhook-4326-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":180,"skipped":2914,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:37.942: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:38.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5357" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":181,"skipped":2926,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:38.094: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:03:38.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f" in namespace "projected-3994" to be "Succeeded or Failed"
Feb 12 15:03:38.185: INFO: Pod "downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.559057ms
Feb 12 15:03:40.195: INFO: Pod "downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017370312s
STEP: Saw pod success
Feb 12 15:03:40.195: INFO: Pod "downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f" satisfied condition "Succeeded or Failed"
Feb 12 15:03:40.201: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f container client-container: <nil>
STEP: delete the pod
Feb 12 15:03:40.253: INFO: Waiting for pod downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f to disappear
Feb 12 15:03:40.260: INFO: Pod downwardapi-volume-68a13713-b6ea-4ef9-8a6e-ff38332b848f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:40.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3994" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":2926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:40.291: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:03:40.373: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022" in namespace "projected-3194" to be "Succeeded or Failed"
Feb 12 15:03:40.382: INFO: Pod "downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65693ms
Feb 12 15:03:42.393: INFO: Pod "downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019618593s
Feb 12 15:03:44.401: INFO: Pod "downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027277274s
STEP: Saw pod success
Feb 12 15:03:44.401: INFO: Pod "downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022" satisfied condition "Succeeded or Failed"
Feb 12 15:03:44.406: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022 container client-container: <nil>
STEP: delete the pod
Feb 12 15:03:44.484: INFO: Waiting for pod downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022 to disappear
Feb 12 15:03:44.489: INFO: Pod downwardapi-volume-bea33f5e-a57f-4298-9efc-537a53191022 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:03:44.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3194" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:03:44.530: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 12 15:03:44.637: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 15:03:44.659: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 15:03:44.668: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-h2tct before test
Feb 12 15:03:44.690: INFO: canal-zprb7 from kube-system started at 2021-02-12 12:16:38 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:03:44.691: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:03:44.691: INFO: kube-proxy-wl68v from kube-system started at 2021-02-12 12:16:38 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:03:44.691: INFO: logrotate-6jb84 from kube-system started at 2021-02-12 14:21:47 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:03:44.691: INFO: node-local-dns-xsl7b from kube-system started at 2021-02-12 14:21:40 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:03:44.691: INFO: user-ssh-keys-agent-tm2rj from kube-system started at 2021-02-12 13:11:47 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:03:44.691: INFO: sonobuoy from sonobuoy started at 2021-02-12 14:03:39 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.691: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 15:03:44.692: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 15:03:44.692: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 15:03:44.692: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-qftnt before test
Feb 12 15:03:44.716: INFO: canal-j2gzw from kube-system started at 2021-02-12 12:16:44 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.716: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:03:44.716: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:03:44.716: INFO: coredns-856777c9f5-tzdw7 from kube-system started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.716: INFO: 	Container coredns ready: true, restart count 0
Feb 12 15:03:44.716: INFO: kube-proxy-rfp6j from kube-system started at 2021-02-12 12:16:44 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.716: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:03:44.716: INFO: logrotate-zkvll from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.716: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:03:44.717: INFO: node-local-dns-2lwvq from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.717: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:03:44.717: INFO: user-ssh-keys-agent-vdvks from kube-system started at 2021-02-12 13:12:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.717: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:03:44.717: INFO: dashboard-metrics-scraper-975c84c89-7drg5 from kubernetes-dashboard started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.717: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 15:03:44.717: INFO: sonobuoy-e2e-job-7631d608de454e9c from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.717: INFO: 	Container e2e ready: true, restart count 0
Feb 12 15:03:44.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 15:03:44.717: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 15:03:44.717: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 15:03:44.717: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-w42vq before test
Feb 12 15:03:44.809: INFO: canal-mx9gb from kube-system started at 2021-02-12 12:16:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:03:44.809: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:03:44.809: INFO: coredns-856777c9f5-b27nr from kube-system started at 2021-02-12 12:16:59 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container coredns ready: true, restart count 0
Feb 12 15:03:44.809: INFO: kube-proxy-s2hw4 from kube-system started at 2021-02-12 12:16:40 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:03:44.809: INFO: logrotate-54qnh from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:03:44.809: INFO: node-local-dns-4dbnq from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:03:44.809: INFO: openvpn-client-6c9dd998bc-lr8jh from kube-system started at 2021-02-12 12:16:59 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container dnat-controller ready: true, restart count 1
Feb 12 15:03:44.809: INFO: 	Container openvpn-client ready: true, restart count 0
Feb 12 15:03:44.809: INFO: user-ssh-keys-agent-2llsr from kube-system started at 2021-02-12 13:12:10 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:03:44.809: INFO: dashboard-metrics-scraper-975c84c89-xfxv8 from kubernetes-dashboard started at 2021-02-12 12:16:57 +0000 UTC (1 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 15:03:44.809: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:03:44.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 15:03:44.809: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4156b12d-5e41-4c84-9662-29ed85eb9ef9 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4156b12d-5e41-4c84-9662-29ed85eb9ef9 off the node modest-nightingale-745f6d5bd4-h2tct
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4156b12d-5e41-4c84-9662-29ed85eb9ef9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:08:53.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7458" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.527 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":184,"skipped":2968,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:08:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Feb 12 15:08:53.119: INFO: Major version: 1
STEP: Confirm minor version
Feb 12 15:08:53.119: INFO: cleanMinorVersion: 19
Feb 12 15:08:53.120: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:08:53.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5432" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":185,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:08:53.140: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Feb 12 15:08:53.220: INFO: Waiting up to 5m0s for pod "downward-api-3f70c2db-d80f-4093-937f-8990aadaded2" in namespace "downward-api-6152" to be "Succeeded or Failed"
Feb 12 15:08:53.229: INFO: Pod "downward-api-3f70c2db-d80f-4093-937f-8990aadaded2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336654ms
Feb 12 15:08:55.235: INFO: Pod "downward-api-3f70c2db-d80f-4093-937f-8990aadaded2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014900958s
STEP: Saw pod success
Feb 12 15:08:55.235: INFO: Pod "downward-api-3f70c2db-d80f-4093-937f-8990aadaded2" satisfied condition "Succeeded or Failed"
Feb 12 15:08:55.240: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downward-api-3f70c2db-d80f-4093-937f-8990aadaded2 container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:08:55.290: INFO: Waiting for pod downward-api-3f70c2db-d80f-4093-937f-8990aadaded2 to disappear
Feb 12 15:08:55.296: INFO: Pod downward-api-3f70c2db-d80f-4093-937f-8990aadaded2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:08:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6152" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":2995,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:08:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:08:59.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5452" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3000,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:08:59.426: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-8cf3fff0-983b-4f8b-bc5f-79950fd0df03
STEP: Creating a pod to test consume configMaps
Feb 12 15:08:59.513: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7" in namespace "projected-544" to be "Succeeded or Failed"
Feb 12 15:08:59.518: INFO: Pod "pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.895396ms
Feb 12 15:09:01.525: INFO: Pod "pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012093667s
STEP: Saw pod success
Feb 12 15:09:01.526: INFO: Pod "pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7" satisfied condition "Succeeded or Failed"
Feb 12 15:09:01.530: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:09:01.569: INFO: Waiting for pod pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7 to disappear
Feb 12 15:09:01.576: INFO: Pod pod-projected-configmaps-4c0b9b9f-61a9-4654-8909-3ecc754bf3f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:09:01.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-544" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3007,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:09:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Feb 12 15:09:01.664: INFO: Waiting up to 5m0s for pod "downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39" in namespace "downward-api-5056" to be "Succeeded or Failed"
Feb 12 15:09:01.678: INFO: Pod "downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.874962ms
Feb 12 15:09:03.685: INFO: Pod "downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020307055s
STEP: Saw pod success
Feb 12 15:09:03.685: INFO: Pod "downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39" satisfied condition "Succeeded or Failed"
Feb 12 15:09:03.691: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39 container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:09:03.738: INFO: Waiting for pod downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39 to disappear
Feb 12 15:09:03.743: INFO: Pod downward-api-32b525ab-bc59-4725-b1f3-7d7df9850a39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:09:03.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5056" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:09:03.763: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:09:34.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6915" for this suite.
STEP: Destroying namespace "nsdeletetest-778" for this suite.
Feb 12 15:09:34.121: INFO: Namespace nsdeletetest-778 was already deleted
STEP: Destroying namespace "nsdeletetest-4145" for this suite.

• [SLOW TEST:30.371 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":190,"skipped":3039,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:09:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Feb 12 15:09:34.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-1436'
Feb 12 15:09:34.484: INFO: stderr: ""
Feb 12 15:09:34.484: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 12 15:09:34.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1436'
Feb 12 15:09:34.573: INFO: stderr: ""
Feb 12 15:09:34.573: INFO: stdout: "update-demo-nautilus-bbp5j update-demo-nautilus-sbf7v "
Feb 12 15:09:34.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-bbp5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1436'
Feb 12 15:09:34.652: INFO: stderr: ""
Feb 12 15:09:34.652: INFO: stdout: ""
Feb 12 15:09:34.652: INFO: update-demo-nautilus-bbp5j is created but not running
Feb 12 15:09:39.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1436'
Feb 12 15:09:39.754: INFO: stderr: ""
Feb 12 15:09:39.754: INFO: stdout: "update-demo-nautilus-bbp5j update-demo-nautilus-sbf7v "
Feb 12 15:09:39.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-bbp5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1436'
Feb 12 15:09:39.830: INFO: stderr: ""
Feb 12 15:09:39.830: INFO: stdout: "true"
Feb 12 15:09:39.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-bbp5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1436'
Feb 12 15:09:39.917: INFO: stderr: ""
Feb 12 15:09:39.917: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 15:09:39.917: INFO: validating pod update-demo-nautilus-bbp5j
Feb 12 15:09:40.014: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 15:09:40.014: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 15:09:40.014: INFO: update-demo-nautilus-bbp5j is verified up and running
Feb 12 15:09:40.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-sbf7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1436'
Feb 12 15:09:40.127: INFO: stderr: ""
Feb 12 15:09:40.127: INFO: stdout: "true"
Feb 12 15:09:40.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods update-demo-nautilus-sbf7v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1436'
Feb 12 15:09:40.212: INFO: stderr: ""
Feb 12 15:09:40.212: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 12 15:09:40.212: INFO: validating pod update-demo-nautilus-sbf7v
Feb 12 15:09:40.308: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 15:09:40.308: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 15:09:40.308: INFO: update-demo-nautilus-sbf7v is verified up and running
STEP: using delete to clean up resources
Feb 12 15:09:40.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-1436'
Feb 12 15:09:40.423: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:09:40.423: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 12 15:09:40.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1436'
Feb 12 15:09:40.509: INFO: stderr: "No resources found in kubectl-1436 namespace.\n"
Feb 12 15:09:40.509: INFO: stdout: ""
Feb 12 15:09:40.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -l name=update-demo --namespace=kubectl-1436 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 15:09:40.593: INFO: stderr: ""
Feb 12 15:09:40.593: INFO: stdout: "update-demo-nautilus-bbp5j\nupdate-demo-nautilus-sbf7v\n"
Feb 12 15:09:41.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1436'
Feb 12 15:09:41.194: INFO: stderr: "No resources found in kubectl-1436 namespace.\n"
Feb 12 15:09:41.194: INFO: stdout: ""
Feb 12 15:09:41.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -l name=update-demo --namespace=kubectl-1436 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 15:09:41.278: INFO: stderr: ""
Feb 12 15:09:41.278: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:09:41.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1436" for this suite.

• [SLOW TEST:7.170 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":191,"skipped":3039,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:09:41.304: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:09:45.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6183" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3053,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:09:45.438: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:10:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3916" for this suite.

• [SLOW TEST:16.345 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":193,"skipped":3062,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:10:01.784: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 12 15:10:07.946: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 15:10:07.952: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 15:10:09.952: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 15:10:09.963: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 15:10:11.952: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 15:10:11.963: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:10:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8951" for this suite.

• [SLOW TEST:10.205 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3063,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:10:11.989: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:12:12.088: INFO: Deleting pod "var-expansion-2d5b34fc-5942-4b43-a2dd-00ab3512b93c" in namespace "var-expansion-6647"
Feb 12 15:12:12.101: INFO: Wait up to 5m0s for pod "var-expansion-2d5b34fc-5942-4b43-a2dd-00ab3512b93c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:12:16.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6647" for this suite.

• [SLOW TEST:124.152 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":195,"skipped":3066,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:12:16.142: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Feb 12 15:12:16.235: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 15:13:16.284: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Feb 12 15:13:16.339: INFO: Created pod: pod0-sched-preemption-low-priority
Feb 12 15:13:16.364: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb 12 15:13:16.387: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:13:50.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4427" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:94.441 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":196,"skipped":3072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:13:50.588: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb 12 15:13:50.704: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 15:14:50.744: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:14:50.750: INFO: Starting informer...
STEP: Starting pods...
Feb 12 15:14:51.005: INFO: Pod1 is running on modest-nightingale-745f6d5bd4-h2tct. Tainting Node
Feb 12 15:14:53.244: INFO: Pod2 is running on modest-nightingale-745f6d5bd4-h2tct. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 12 15:15:00.206: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 12 15:15:27.463: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:15:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3208" for this suite.

• [SLOW TEST:96.954 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":197,"skipped":3110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:15:27.544: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:15:43.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3560" for this suite.

• [SLOW TEST:16.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":198,"skipped":3121,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:15:43.701: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6405
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 12 15:15:43.749: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 15:15:43.817: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 15:15:45.828: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:47.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:49.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:51.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:53.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:55.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:57.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:15:59.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 12 15:16:01.824: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 12 15:16:01.837: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 12 15:16:03.844: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 12 15:16:03.857: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 12 15:16:07.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.236 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6405 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:16:07.924: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:16:09.388: INFO: Found all expected endpoints: [netserver-0]
Feb 12 15:16:09.395: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.109 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6405 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:16:09.395: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:16:10.944: INFO: Found all expected endpoints: [netserver-1]
Feb 12 15:16:10.950: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.222 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6405 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:16:10.950: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:16:12.433: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:16:12.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6405" for this suite.

• [SLOW TEST:28.753 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3124,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:16:12.456: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:16:12.820: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:16:14.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739772, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739772, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739772, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748739772, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:16:17.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 12 15:16:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:16:18.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9715" for this suite.
STEP: Destroying namespace "webhook-9715-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.817 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":200,"skipped":3125,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:16:18.273: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5299, will wait for the garbage collector to delete the pods
Feb 12 15:16:22.418: INFO: Deleting Job.batch foo took: 12.353934ms
Feb 12 15:16:22.518: INFO: Terminating Job.batch foo pods took: 100.137659ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:16:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5299" for this suite.

• [SLOW TEST:39.269 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":201,"skipped":3131,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:16:57.543: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:16:57.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 version'
Feb 12 15:16:57.676: INFO: stderr: ""
Feb 12 15:16:57.676: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:41:49Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:16:57.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8176" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":202,"skipped":3142,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:16:57.694: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0212 15:17:07.807939      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0212 15:17:07.808064      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0212 15:17:07.808236      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 12 15:17:07.808: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:17:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5296" for this suite.

• [SLOW TEST:10.132 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":203,"skipped":3147,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:17:07.827: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-c09d8708-f416-4453-9817-3bb7a402abbe
STEP: Creating a pod to test consume secrets
Feb 12 15:17:07.910: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd" in namespace "projected-5144" to be "Succeeded or Failed"
Feb 12 15:17:07.921: INFO: Pod "pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.153764ms
Feb 12 15:17:09.928: INFO: Pod "pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017791164s
Feb 12 15:17:11.936: INFO: Pod "pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025986993s
STEP: Saw pod success
Feb 12 15:17:11.936: INFO: Pod "pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd" satisfied condition "Succeeded or Failed"
Feb 12 15:17:11.942: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:17:12.001: INFO: Waiting for pod pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd to disappear
Feb 12 15:17:12.008: INFO: Pod pod-projected-secrets-fef9aa04-53d3-400e-8cbe-6617c6362abd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:17:12.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5144" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:17:12.033: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-064ecc52-a873-43cf-900f-246e5feb2353
STEP: Creating a pod to test consume configMaps
Feb 12 15:17:12.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e" in namespace "configmap-8369" to be "Succeeded or Failed"
Feb 12 15:17:12.125: INFO: Pod "pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.252073ms
Feb 12 15:17:14.134: INFO: Pod "pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016742221s
Feb 12 15:17:16.141: INFO: Pod "pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023617818s
STEP: Saw pod success
Feb 12 15:17:16.141: INFO: Pod "pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e" satisfied condition "Succeeded or Failed"
Feb 12 15:17:16.146: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:17:16.187: INFO: Waiting for pod pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e to disappear
Feb 12 15:17:16.200: INFO: Pod pod-configmaps-0b144ece-b2c5-4000-81b5-aaa874bdd85e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:17:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8369" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:17:16.228: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:18:16.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5561" for this suite.

• [SLOW TEST:60.127 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3235,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:18:16.355: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:18:16.413: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:18:16.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5046" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":207,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:18:16.979: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 12 15:18:17.062: INFO: Waiting up to 5m0s for pod "pod-fe84d521-bc71-4238-b235-f5c04124f925" in namespace "emptydir-6405" to be "Succeeded or Failed"
Feb 12 15:18:17.070: INFO: Pod "pod-fe84d521-bc71-4238-b235-f5c04124f925": Phase="Pending", Reason="", readiness=false. Elapsed: 7.9508ms
Feb 12 15:18:19.075: INFO: Pod "pod-fe84d521-bc71-4238-b235-f5c04124f925": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013007881s
STEP: Saw pod success
Feb 12 15:18:19.075: INFO: Pod "pod-fe84d521-bc71-4238-b235-f5c04124f925" satisfied condition "Succeeded or Failed"
Feb 12 15:18:19.082: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-fe84d521-bc71-4238-b235-f5c04124f925 container test-container: <nil>
STEP: delete the pod
Feb 12 15:18:19.154: INFO: Waiting for pod pod-fe84d521-bc71-4238-b235-f5c04124f925 to disappear
Feb 12 15:18:19.158: INFO: Pod pod-fe84d521-bc71-4238-b235-f5c04124f925 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:18:19.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6405" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3300,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:18:19.178: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Feb 12 15:18:19.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-8980'
Feb 12 15:18:19.733: INFO: stderr: ""
Feb 12 15:18:19.733: INFO: stdout: "pod/pause created\n"
Feb 12 15:18:19.733: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 12 15:18:19.733: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8980" to be "running and ready"
Feb 12 15:18:19.739: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276413ms
Feb 12 15:18:21.744: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011657291s
Feb 12 15:18:21.744: INFO: Pod "pause" satisfied condition "running and ready"
Feb 12 15:18:21.744: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 12 15:18:21.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 label pods pause testing-label=testing-label-value --namespace=kubectl-8980'
Feb 12 15:18:21.836: INFO: stderr: ""
Feb 12 15:18:21.836: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 12 15:18:21.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pod pause -L testing-label --namespace=kubectl-8980'
Feb 12 15:18:21.940: INFO: stderr: ""
Feb 12 15:18:21.940: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 12 15:18:21.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 label pods pause testing-label- --namespace=kubectl-8980'
Feb 12 15:18:22.047: INFO: stderr: ""
Feb 12 15:18:22.047: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 12 15:18:22.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pod pause -L testing-label --namespace=kubectl-8980'
Feb 12 15:18:22.123: INFO: stderr: ""
Feb 12 15:18:22.123: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Feb 12 15:18:22.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-8980'
Feb 12 15:18:22.223: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:18:22.223: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 12 15:18:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get rc,svc -l name=pause --no-headers --namespace=kubectl-8980'
Feb 12 15:18:22.314: INFO: stderr: "No resources found in kubectl-8980 namespace.\n"
Feb 12 15:18:22.314: INFO: stdout: ""
Feb 12 15:18:22.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pods -l name=pause --namespace=kubectl-8980 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 15:18:22.399: INFO: stderr: ""
Feb 12 15:18:22.399: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:18:22.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8980" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":209,"skipped":3304,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:18:22.433: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7587
Feb 12 15:18:24.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 12 15:18:25.121: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 12 15:18:25.121: INFO: stdout: "ipvs"
Feb 12 15:18:25.121: INFO: proxyMode: ipvs
Feb 12 15:18:25.134: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 15:18:25.141: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 15:18:27.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 15:18:27.147: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 15:18:29.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 15:18:29.147: INFO: Pod kube-proxy-mode-detector still exists
Feb 12 15:18:31.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 15:18:31.146: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7587
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7587
I0212 15:18:31.180535      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7587, replica count: 3
I0212 15:18:34.231137      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 15:18:37.231443      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 15:18:37.257: INFO: Creating new exec pod
Feb 12 15:18:40.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Feb 12 15:18:40.881: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 12 15:18:40.881: INFO: stdout: ""
Feb 12 15:18:40.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c nc -zv -t -w 2 10.240.23.54 80'
Feb 12 15:18:41.464: INFO: stderr: "+ nc -zv -t -w 2 10.240.23.54 80\nConnection to 10.240.23.54 80 port [tcp/http] succeeded!\n"
Feb 12 15:18:41.464: INFO: stdout: ""
Feb 12 15:18:41.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c nc -zv -t -w 2 104.248.253.154 32361'
Feb 12 15:18:42.039: INFO: stderr: "+ nc -zv -t -w 2 104.248.253.154 32361\nConnection to 104.248.253.154 32361 port [tcp/32361] succeeded!\n"
Feb 12 15:18:42.039: INFO: stdout: ""
Feb 12 15:18:42.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c nc -zv -t -w 2 165.227.175.251 32361'
Feb 12 15:18:42.615: INFO: stderr: "+ nc -zv -t -w 2 165.227.175.251 32361\nConnection to 165.227.175.251 32361 port [tcp/32361] succeeded!\n"
Feb 12 15:18:42.615: INFO: stdout: ""
Feb 12 15:18:42.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://104.248.253.154:32361/ ; done'
Feb 12 15:18:43.207: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n"
Feb 12 15:18:43.207: INFO: stdout: "\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5\naffinity-nodeport-timeout-86vj5"
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Received response from host: affinity-nodeport-timeout-86vj5
Feb 12 15:18:43.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://104.248.253.154:32361/'
Feb 12 15:18:43.869: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n"
Feb 12 15:18:43.870: INFO: stdout: "affinity-nodeport-timeout-86vj5"
Feb 12 15:20:48.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://104.248.253.154:32361/'
Feb 12 15:20:49.441: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n"
Feb 12 15:20:49.441: INFO: stdout: "affinity-nodeport-timeout-86vj5"
Feb 12 15:22:54.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://104.248.253.154:32361/'
Feb 12 15:22:55.047: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n"
Feb 12 15:22:55.047: INFO: stdout: "affinity-nodeport-timeout-86vj5"
Feb 12 15:25:00.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-7587 execpod-affinityc4zm5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://104.248.253.154:32361/'
Feb 12 15:25:00.620: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://104.248.253.154:32361/\n"
Feb 12 15:25:00.620: INFO: stdout: "affinity-nodeport-timeout-54dmf"
Feb 12 15:25:00.620: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7587, will wait for the garbage collector to delete the pods
Feb 12 15:25:00.711: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 14.826782ms
Feb 12 15:25:01.311: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.144487ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:25:14.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7587" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:411.732 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":210,"skipped":3316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:25:14.170: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-hpvg
STEP: Creating a pod to test atomic-volume-subpath
Feb 12 15:25:14.262: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hpvg" in namespace "subpath-670" to be "Succeeded or Failed"
Feb 12 15:25:14.268: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.501762ms
Feb 12 15:25:16.275: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01293556s
Feb 12 15:25:18.282: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 4.020299676s
Feb 12 15:25:20.288: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 6.026421414s
Feb 12 15:25:22.296: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 8.033986984s
Feb 12 15:25:24.304: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 10.042574499s
Feb 12 15:25:26.311: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 12.048971048s
Feb 12 15:25:28.318: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 14.056228948s
Feb 12 15:25:30.329: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 16.067202726s
Feb 12 15:25:32.338: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 18.075726217s
Feb 12 15:25:34.346: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Running", Reason="", readiness=true. Elapsed: 20.084050847s
Feb 12 15:25:36.353: INFO: Pod "pod-subpath-test-projected-hpvg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.091570921s
STEP: Saw pod success
Feb 12 15:25:36.353: INFO: Pod "pod-subpath-test-projected-hpvg" satisfied condition "Succeeded or Failed"
Feb 12 15:25:36.359: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-subpath-test-projected-hpvg container test-container-subpath-projected-hpvg: <nil>
STEP: delete the pod
Feb 12 15:25:36.410: INFO: Waiting for pod pod-subpath-test-projected-hpvg to disappear
Feb 12 15:25:36.424: INFO: Pod pod-subpath-test-projected-hpvg no longer exists
STEP: Deleting pod pod-subpath-test-projected-hpvg
Feb 12 15:25:36.424: INFO: Deleting pod "pod-subpath-test-projected-hpvg" in namespace "subpath-670"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:25:36.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-670" for this suite.

• [SLOW TEST:22.285 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":211,"skipped":3353,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:25:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4567
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-4567
Feb 12 15:25:36.566: INFO: Found 0 stateful pods, waiting for 1
Feb 12 15:25:46.574: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 15:25:46.610: INFO: Deleting all statefulset in ns statefulset-4567
Feb 12 15:25:46.618: INFO: Scaling statefulset ss to 0
Feb 12 15:26:06.662: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 15:26:06.668: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4567" for this suite.

• [SLOW TEST:30.268 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":212,"skipped":3361,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:06.725: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-3477/secret-test-9e40bf53-f44e-4f4f-96cb-27bb7cc27e70
STEP: Creating a pod to test consume secrets
Feb 12 15:26:06.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f" in namespace "secrets-3477" to be "Succeeded or Failed"
Feb 12 15:26:06.807: INFO: Pod "pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.719565ms
Feb 12 15:26:08.824: INFO: Pod "pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024144631s
STEP: Saw pod success
Feb 12 15:26:08.824: INFO: Pod "pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f" satisfied condition "Succeeded or Failed"
Feb 12 15:26:08.829: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f container env-test: <nil>
STEP: delete the pod
Feb 12 15:26:08.866: INFO: Waiting for pod pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f to disappear
Feb 12 15:26:08.874: INFO: Pod pod-configmaps-396bb8e4-990e-49dd-af48-9b2440dc288f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:08.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3477" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3362,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:08.901: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Feb 12 15:26:08.954: INFO: created test-event-1
Feb 12 15:26:08.962: INFO: created test-event-2
Feb 12 15:26:08.971: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb 12 15:26:08.978: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb 12 15:26:09.018: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2480" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":214,"skipped":3381,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:09.049: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:11.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8991" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 12 15:26:11.250: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:26:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:27.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9688" for this suite.

• [SLOW TEST:16.013 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":216,"skipped":3411,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:27.201: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Feb 12 15:26:27.264: INFO: created test-podtemplate-1
Feb 12 15:26:27.278: INFO: created test-podtemplate-2
Feb 12 15:26:27.287: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb 12 15:26:27.293: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb 12 15:26:27.327: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:27.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2424" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":217,"skipped":3413,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:26:27.809: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:26:29.831: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740387, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740387, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740387, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740387, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:26:32.856: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:45.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4402" for this suite.
STEP: Destroying namespace "webhook-4402-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.224 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":218,"skipped":3419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:45.574: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-096e561f-4c0a-44ed-8711-c40407087b5d
STEP: Creating a pod to test consume secrets
Feb 12 15:26:45.652: INFO: Waiting up to 5m0s for pod "pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23" in namespace "secrets-5573" to be "Succeeded or Failed"
Feb 12 15:26:45.657: INFO: Pod "pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.577292ms
Feb 12 15:26:47.664: INFO: Pod "pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011890685s
STEP: Saw pod success
Feb 12 15:26:47.664: INFO: Pod "pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23" satisfied condition "Succeeded or Failed"
Feb 12 15:26:47.671: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23 container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:26:47.711: INFO: Waiting for pod pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23 to disappear
Feb 12 15:26:47.717: INFO: Pod pod-secrets-30bbbe98-b106-473c-98e9-cbeebd18cd23 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5573" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:47.740: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-911e3119-3ac3-48c3-9a0a-b42fb4dbbe49
STEP: Creating a pod to test consume configMaps
Feb 12 15:26:47.828: INFO: Waiting up to 5m0s for pod "pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c" in namespace "configmap-8140" to be "Succeeded or Failed"
Feb 12 15:26:47.834: INFO: Pod "pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035903ms
Feb 12 15:26:49.840: INFO: Pod "pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012326982s
Feb 12 15:26:51.848: INFO: Pod "pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019728628s
STEP: Saw pod success
Feb 12 15:26:51.848: INFO: Pod "pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c" satisfied condition "Succeeded or Failed"
Feb 12 15:26:51.853: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:26:51.928: INFO: Waiting for pod pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c to disappear
Feb 12 15:26:51.934: INFO: Pod pod-configmaps-036b7bb5-0a72-492c-aa41-33e66f189b7c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8140" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3492,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:51.958: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 12 15:26:52.010: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 15:26:52.026: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 15:26:52.032: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-h2tct before test
Feb 12 15:26:52.042: INFO: canal-zprb7 from kube-system started at 2021-02-12 12:16:38 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:26:52.042: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:26:52.042: INFO: kube-proxy-wl68v from kube-system started at 2021-02-12 12:16:38 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:26:52.042: INFO: logrotate-wp82m from kube-system started at 2021-02-12 15:15:27 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:26:52.042: INFO: node-local-dns-jrrg2 from kube-system started at 2021-02-12 15:15:27 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:26:52.042: INFO: user-ssh-keys-agent-tm2rj from kube-system started at 2021-02-12 13:11:47 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:26:52.042: INFO: busybox-host-aliases08ada421-bd63-4b76-b3a5-ab8307f080fc from kubelet-test-8991 started at 2021-02-12 15:26:09 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container busybox-host-aliases08ada421-bd63-4b76-b3a5-ab8307f080fc ready: false, restart count 0
Feb 12 15:26:52.042: INFO: sonobuoy from sonobuoy started at 2021-02-12 14:03:39 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 15:26:52.042: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-65n95 from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.042: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb 12 15:26:52.042: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 15:26:52.042: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-qftnt before test
Feb 12 15:26:52.053: INFO: canal-j2gzw from kube-system started at 2021-02-12 12:16:44 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:26:52.053: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:26:52.053: INFO: coredns-856777c9f5-tzdw7 from kube-system started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container coredns ready: true, restart count 0
Feb 12 15:26:52.053: INFO: kube-proxy-rfp6j from kube-system started at 2021-02-12 12:16:44 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:26:52.053: INFO: logrotate-zkvll from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:26:52.053: INFO: node-local-dns-2lwvq from kube-system started at 2021-02-12 12:17:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:26:52.053: INFO: user-ssh-keys-agent-vdvks from kube-system started at 2021-02-12 13:12:04 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:26:52.053: INFO: dashboard-metrics-scraper-975c84c89-7drg5 from kubernetes-dashboard started at 2021-02-12 12:23:08 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 15:26:52.053: INFO: sonobuoy-e2e-job-7631d608de454e9c from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container e2e ready: true, restart count 0
Feb 12 15:26:52.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 15:26:52.053: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-g2jsj from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.053: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb 12 15:26:52.053: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 15:26:52.053: INFO: 
Logging pods the apiserver thinks is on node modest-nightingale-745f6d5bd4-w42vq before test
Feb 12 15:26:52.066: INFO: canal-mx9gb from kube-system started at 2021-02-12 12:16:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.066: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 15:26:52.066: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 12 15:26:52.066: INFO: coredns-856777c9f5-b27nr from kube-system started at 2021-02-12 12:16:59 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.066: INFO: 	Container coredns ready: true, restart count 0
Feb 12 15:26:52.066: INFO: kube-proxy-s2hw4 from kube-system started at 2021-02-12 12:16:40 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.066: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 15:26:52.067: INFO: logrotate-54qnh from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container logrotate ready: true, restart count 0
Feb 12 15:26:52.067: INFO: node-local-dns-4dbnq from kube-system started at 2021-02-12 12:16:50 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 15:26:52.067: INFO: openvpn-client-6c9dd998bc-lr8jh from kube-system started at 2021-02-12 12:16:59 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container dnat-controller ready: true, restart count 1
Feb 12 15:26:52.067: INFO: 	Container openvpn-client ready: true, restart count 0
Feb 12 15:26:52.067: INFO: user-ssh-keys-agent-2llsr from kube-system started at 2021-02-12 13:12:10 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 12 15:26:52.067: INFO: dashboard-metrics-scraper-975c84c89-xfxv8 from kubernetes-dashboard started at 2021-02-12 12:16:57 +0000 UTC (1 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 12 15:26:52.067: INFO: sonobuoy-systemd-logs-daemon-set-7d24730cac324fe5-tvtnb from sonobuoy started at 2021-02-12 14:03:40 +0000 UTC (2 container statuses recorded)
Feb 12 15:26:52.067: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb 12 15:26:52.068: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16630991840aecb0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1663099185016990], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:53.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1316" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":221,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:53.139: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Feb 12 15:26:55.765: INFO: Successfully updated pod "labelsupdate2e562d02-5459-45d7-a579-933c12be836c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:26:57.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2401" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3518,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:26:57.838: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:26:58.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:27:00.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740418, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740418, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740418, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740418, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:27:03.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:03.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9604" for this suite.
STEP: Destroying namespace "webhook-9604-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.866 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":223,"skipped":3526,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:03.704: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:27:03.766: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685" in namespace "projected-480" to be "Succeeded or Failed"
Feb 12 15:27:03.772: INFO: Pod "downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685": Phase="Pending", Reason="", readiness=false. Elapsed: 6.095805ms
Feb 12 15:27:05.778: INFO: Pod "downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011756444s
STEP: Saw pod success
Feb 12 15:27:05.778: INFO: Pod "downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685" satisfied condition "Succeeded or Failed"
Feb 12 15:27:05.783: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685 container client-container: <nil>
STEP: delete the pod
Feb 12 15:27:05.822: INFO: Waiting for pod downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685 to disappear
Feb 12 15:27:05.827: INFO: Pod downwardapi-volume-57045da2-e3a3-4a2b-b13b-643dc1c7a685 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:05.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-480" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":224,"skipped":3539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:05.848: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:27:05.932: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06" in namespace "downward-api-5142" to be "Succeeded or Failed"
Feb 12 15:27:05.939: INFO: Pod "downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06": Phase="Pending", Reason="", readiness=false. Elapsed: 7.00207ms
Feb 12 15:27:07.947: INFO: Pod "downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015313004s
STEP: Saw pod success
Feb 12 15:27:07.947: INFO: Pod "downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06" satisfied condition "Succeeded or Failed"
Feb 12 15:27:07.953: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06 container client-container: <nil>
STEP: delete the pod
Feb 12 15:27:07.998: INFO: Waiting for pod downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06 to disappear
Feb 12 15:27:08.003: INFO: Pod downwardapi-volume-83b0cd7c-c444-400f-98fd-8c58617f2d06 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:08.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5142" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3567,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:08.026: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Feb 12 15:27:08.081: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9453" for this suite.

• [SLOW TEST:19.299 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":226,"skipped":3582,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:27.326: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 12 15:27:33.463: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 15:27:33.468: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 15:27:35.468: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 15:27:35.476: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 15:27:37.468: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 15:27:37.482: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 15:27:39.468: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 15:27:39.475: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 15:27:41.468: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 15:27:41.475: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:41.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-131" for this suite.

• [SLOW TEST:14.184 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3589,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:41.511: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-966b8f28-0fdb-4f03-a51e-6d942700ce8f
STEP: Creating a pod to test consume configMaps
Feb 12 15:27:41.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a" in namespace "configmap-4543" to be "Succeeded or Failed"
Feb 12 15:27:41.592: INFO: Pod "pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.515927ms
Feb 12 15:27:43.599: INFO: Pod "pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015861864s
Feb 12 15:27:45.605: INFO: Pod "pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022240974s
STEP: Saw pod success
Feb 12 15:27:45.605: INFO: Pod "pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a" satisfied condition "Succeeded or Failed"
Feb 12 15:27:45.647: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:27:45.690: INFO: Waiting for pod pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a to disappear
Feb 12 15:27:45.696: INFO: Pod pod-configmaps-ed8ea190-4425-4009-8b00-5bef90602b8a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:45.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4543" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3591,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:45.720: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 12 15:27:45.824: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88210 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 15:27:45.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88211 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 15:27:45.824: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88212 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 12 15:27:55.890: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88290 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 15:27:55.890: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88291 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 15:27:55.891: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7623 /api/v1/namespaces/watch-7623/configmaps/e2e-watch-test-label-changed 5d030964-d325-4c22-980e-d1e19f122afd 88292 0 2021-02-12 15:27:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-12 15:27:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:27:55.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7623" for this suite.

• [SLOW TEST:10.189 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":229,"skipped":3600,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:27:55.910: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:28:00.045: INFO: Waiting up to 5m0s for pod "client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6" in namespace "pods-4247" to be "Succeeded or Failed"
Feb 12 15:28:00.055: INFO: Pod "client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.661671ms
Feb 12 15:28:02.062: INFO: Pod "client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016505206s
STEP: Saw pod success
Feb 12 15:28:02.062: INFO: Pod "client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6" satisfied condition "Succeeded or Failed"
Feb 12 15:28:02.067: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6 container env3cont: <nil>
STEP: delete the pod
Feb 12 15:28:02.102: INFO: Waiting for pod client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6 to disappear
Feb 12 15:28:02.110: INFO: Pod client-envvars-81fbb9ef-bc9d-4dde-b000-e6fe25d842e6 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4247" for this suite.

• [SLOW TEST:6.219 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3616,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4353
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4353
STEP: creating replication controller externalsvc in namespace services-4353
I0212 15:28:02.231198      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4353, replica count: 2
I0212 15:28:05.281537      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 12 15:28:05.317: INFO: Creating new exec pod
Feb 12 15:28:07.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 exec --namespace=services-4353 execpodsdcd5 -- /bin/sh -x -c nslookup clusterip-service.services-4353.svc.cluster.local'
Feb 12 15:28:07.992: INFO: stderr: "+ nslookup clusterip-service.services-4353.svc.cluster.local\n"
Feb 12 15:28:07.992: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nclusterip-service.services-4353.svc.cluster.local\tcanonical name = externalsvc.services-4353.svc.cluster.local.\nName:\texternalsvc.services-4353.svc.cluster.local\nAddress: 10.240.27.166\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4353, will wait for the garbage collector to delete the pods
Feb 12 15:28:08.061: INFO: Deleting ReplicationController externalsvc took: 12.838914ms
Feb 12 15:28:08.162: INFO: Terminating ReplicationController externalsvc pods took: 100.171956ms
Feb 12 15:28:17.600: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4353" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.525 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":231,"skipped":3619,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:28:18.101: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:28:21.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:28:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4532-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:22.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9363" for this suite.
STEP: Destroying namespace "webhook-9363-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.502 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":232,"skipped":3629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:23.161: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:27.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-149" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":233,"skipped":3657,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:27.478: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:53.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7607" for this suite.

• [SLOW TEST:26.438 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":234,"skipped":3661,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:28:53.982: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125" in namespace "downward-api-1328" to be "Succeeded or Failed"
Feb 12 15:28:53.990: INFO: Pod "downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125": Phase="Pending", Reason="", readiness=false. Elapsed: 8.38532ms
Feb 12 15:28:55.998: INFO: Pod "downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015649708s
Feb 12 15:28:58.008: INFO: Pod "downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026237836s
STEP: Saw pod success
Feb 12 15:28:58.008: INFO: Pod "downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125" satisfied condition "Succeeded or Failed"
Feb 12 15:28:58.014: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125 container client-container: <nil>
STEP: delete the pod
Feb 12 15:28:58.101: INFO: Waiting for pod downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125 to disappear
Feb 12 15:28:58.106: INFO: Pod downwardapi-volume-6a9980f6-f26a-41ca-8200-b78154e39125 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:58.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1328" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3664,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:58.126: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:28:58.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1638" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":236,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:28:58.299: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 12 15:28:58.379: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 12 15:29:03.385: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8670" for this suite.

• [SLOW TEST:6.134 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":237,"skipped":3708,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:04.433: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4630" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":238,"skipped":3717,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:04.603: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:29:04.648: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 12 15:29:08.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-81 create -f -'
Feb 12 15:29:08.590: INFO: stderr: ""
Feb 12 15:29:08.590: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 12 15:29:08.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-81 delete e2e-test-crd-publish-openapi-3126-crds test-cr'
Feb 12 15:29:08.715: INFO: stderr: ""
Feb 12 15:29:08.715: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 12 15:29:08.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-81 apply -f -'
Feb 12 15:29:09.065: INFO: stderr: ""
Feb 12 15:29:09.065: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 12 15:29:09.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-81 delete e2e-test-crd-publish-openapi-3126-crds test-cr'
Feb 12 15:29:09.157: INFO: stderr: ""
Feb 12 15:29:09.157: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 12 15:29:09.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-3126-crds'
Feb 12 15:29:09.362: INFO: stderr: ""
Feb 12 15:29:09.362: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3126-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:12.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-81" for this suite.

• [SLOW TEST:7.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":239,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 12 15:29:12.380: INFO: Waiting up to 5m0s for pod "pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1" in namespace "emptydir-729" to be "Succeeded or Failed"
Feb 12 15:29:12.387: INFO: Pod "pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.890301ms
Feb 12 15:29:14.394: INFO: Pod "pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013855888s
Feb 12 15:29:16.401: INFO: Pod "pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020876633s
STEP: Saw pod success
Feb 12 15:29:16.401: INFO: Pod "pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1" satisfied condition "Succeeded or Failed"
Feb 12 15:29:16.406: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1 container test-container: <nil>
STEP: delete the pod
Feb 12 15:29:16.445: INFO: Waiting for pod pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1 to disappear
Feb 12 15:29:16.450: INFO: Pod pod-be216d4c-af1e-440f-b2d9-8edbd5781bb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:16.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-729" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3753,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:16.472: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:29:16.569: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"380e4e03-e343-46db-883d-df89ec412b24", Controller:(*bool)(0xc0093e5a9a), BlockOwnerDeletion:(*bool)(0xc0093e5a9b)}}
Feb 12 15:29:16.579: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"bcc854a0-003f-4aa6-a132-bc389a35d789", Controller:(*bool)(0xc009411016), BlockOwnerDeletion:(*bool)(0xc009411017)}}
Feb 12 15:29:16.592: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f71c9a27-21f2-4591-9490-542bfaa8159f", Controller:(*bool)(0xc0093e5c96), BlockOwnerDeletion:(*bool)(0xc0093e5c97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4207" for this suite.

• [SLOW TEST:5.163 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":241,"skipped":3758,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:21.637: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-a8e8ec0c-9902-43e5-acab-d3866e0ee416
STEP: Creating a pod to test consume secrets
Feb 12 15:29:21.711: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc" in namespace "projected-6788" to be "Succeeded or Failed"
Feb 12 15:29:21.716: INFO: Pod "pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.895714ms
Feb 12 15:29:23.721: INFO: Pod "pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010511254s
Feb 12 15:29:25.729: INFO: Pod "pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018147337s
STEP: Saw pod success
Feb 12 15:29:25.729: INFO: Pod "pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc" satisfied condition "Succeeded or Failed"
Feb 12 15:29:25.735: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:29:25.812: INFO: Waiting for pod pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc to disappear
Feb 12 15:29:25.817: INFO: Pod pod-projected-secrets-5acb66aa-7ba7-40da-941c-8ad57aaa61cc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:25.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6788" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":3771,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:25.836: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 12 15:29:28.944: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:28.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2301" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":3771,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:28.988: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Feb 12 15:29:29.039: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:33.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4688" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":244,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:33.508: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:29:33.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763" in namespace "downward-api-5367" to be "Succeeded or Failed"
Feb 12 15:29:33.582: INFO: Pod "downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893341ms
Feb 12 15:29:35.587: INFO: Pod "downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016751944s
STEP: Saw pod success
Feb 12 15:29:35.588: INFO: Pod "downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763" satisfied condition "Succeeded or Failed"
Feb 12 15:29:35.594: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763 container client-container: <nil>
STEP: delete the pod
Feb 12 15:29:35.631: INFO: Waiting for pod downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763 to disappear
Feb 12 15:29:35.635: INFO: Pod downwardapi-volume-127f3243-f166-4ec9-9123-5b0257ac0763 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:35.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5367" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3854,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:35.654: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Feb 12 15:29:35.712: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-249037184 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:35.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7509" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":246,"skipped":3868,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:35.797: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Feb 12 15:29:35.863: INFO: Waiting up to 5m0s for pod "client-containers-c5b099cb-4991-4460-abc0-3be444201234" in namespace "containers-368" to be "Succeeded or Failed"
Feb 12 15:29:35.881: INFO: Pod "client-containers-c5b099cb-4991-4460-abc0-3be444201234": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024753ms
Feb 12 15:29:37.896: INFO: Pod "client-containers-c5b099cb-4991-4460-abc0-3be444201234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032471636s
Feb 12 15:29:39.903: INFO: Pod "client-containers-c5b099cb-4991-4460-abc0-3be444201234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03974974s
STEP: Saw pod success
Feb 12 15:29:39.903: INFO: Pod "client-containers-c5b099cb-4991-4460-abc0-3be444201234" satisfied condition "Succeeded or Failed"
Feb 12 15:29:39.908: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod client-containers-c5b099cb-4991-4460-abc0-3be444201234 container test-container: <nil>
STEP: delete the pod
Feb 12 15:29:39.943: INFO: Waiting for pod client-containers-c5b099cb-4991-4460-abc0-3be444201234 to disappear
Feb 12 15:29:39.949: INFO: Pod client-containers-c5b099cb-4991-4460-abc0-3be444201234 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-368" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":3875,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:39.968: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-53
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-53
STEP: Creating statefulset with conflicting port in namespace statefulset-53
STEP: Waiting until pod test-pod will start running in namespace statefulset-53
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-53
Feb 12 15:29:44.083: INFO: Observed stateful pod in namespace: statefulset-53, name: ss-0, uid: a13ffe8b-6573-47a4-914b-e33472db9cc6, status phase: Pending. Waiting for statefulset controller to delete.
Feb 12 15:29:44.351: INFO: Observed stateful pod in namespace: statefulset-53, name: ss-0, uid: a13ffe8b-6573-47a4-914b-e33472db9cc6, status phase: Failed. Waiting for statefulset controller to delete.
Feb 12 15:29:44.366: INFO: Observed stateful pod in namespace: statefulset-53, name: ss-0, uid: a13ffe8b-6573-47a4-914b-e33472db9cc6, status phase: Failed. Waiting for statefulset controller to delete.
Feb 12 15:29:44.373: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-53
STEP: Removing pod with conflicting port in namespace statefulset-53
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-53 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 15:29:48.420: INFO: Deleting all statefulset in ns statefulset-53
Feb 12 15:29:48.426: INFO: Scaling statefulset ss to 0
Feb 12 15:29:58.453: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 15:29:58.458: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:29:58.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-53" for this suite.

• [SLOW TEST:18.540 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":248,"skipped":3893,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:29:58.508: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:30:11.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2066" for this suite.

• [SLOW TEST:13.187 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":249,"skipped":3899,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:30:11.698: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Feb 12 15:30:11.755: INFO: PodSpec: initContainers in spec.initContainers
Feb 12 15:31:00.344: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5b731827-a2e2-442a-a0c9-c973b1959d5e", GenerateName:"", Namespace:"init-container-2992", SelfLink:"/api/v1/namespaces/init-container-2992/pods/pod-init-5b731827-a2e2-442a-a0c9-c973b1959d5e", UID:"85e068f4-cada-4fc1-91b3-3611017e6d49", ResourceVersion:"90167", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63748740611, loc:(*time.Location)(0x770e880)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"755500427"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.0.20/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041348a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0041348c0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041348e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004134920)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004134960), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004134980)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-5fqkb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004422f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fqkb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fqkb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fqkb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0094e1e68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"modest-nightingale-745f6d5bd4-h2tct", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026a8ee0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0094e1ef0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0094e1f20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0094e1f28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0094e1f2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006dca420), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740611, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740611, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740611, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740611, loc:(*time.Location)(0x770e880)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"104.248.253.154", PodIP:"172.25.0.20", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.0.20"}}, StartTime:(*v1.Time)(0xc0041349a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026a8fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026a9030)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://b2c177f8cc034a7f634d08c76c5cdb3ad434641f61d65110daa4bc1c90fb736d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041349e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041349c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0094e1fcf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:00.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2992" for this suite.

• [SLOW TEST:48.668 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":250,"skipped":3903,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:00.366: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Feb 12 15:31:05.038: INFO: Successfully updated pod "labelsupdate2a23b555-e0d0-4564-b124-670e527b92cf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:07.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4425" for this suite.

• [SLOW TEST:6.747 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":3912,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:18.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5418" for this suite.

• [SLOW TEST:11.148 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":252,"skipped":3916,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:18.261: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4295" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":253,"skipped":3917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:18.391: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:31:19.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:31:21.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740679, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740679, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740679, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740678, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:31:24.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:25.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2467" for this suite.
STEP: Destroying namespace "webhook-2467-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.857 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":254,"skipped":3944,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:25.251: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:31:25.308: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 12 15:31:29.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 create -f -'
Feb 12 15:31:29.881: INFO: stderr: ""
Feb 12 15:31:29.881: INFO: stdout: "e2e-test-crd-publish-openapi-440-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 12 15:31:29.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 delete e2e-test-crd-publish-openapi-440-crds test-foo'
Feb 12 15:31:29.974: INFO: stderr: ""
Feb 12 15:31:29.974: INFO: stdout: "e2e-test-crd-publish-openapi-440-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 12 15:31:29.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 apply -f -'
Feb 12 15:31:30.301: INFO: stderr: ""
Feb 12 15:31:30.301: INFO: stdout: "e2e-test-crd-publish-openapi-440-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 12 15:31:30.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 delete e2e-test-crd-publish-openapi-440-crds test-foo'
Feb 12 15:31:30.399: INFO: stderr: ""
Feb 12 15:31:30.399: INFO: stdout: "e2e-test-crd-publish-openapi-440-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 12 15:31:30.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 create -f -'
Feb 12 15:31:30.598: INFO: rc: 1
Feb 12 15:31:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 apply -f -'
Feb 12 15:31:30.906: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 12 15:31:30.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 create -f -'
Feb 12 15:31:31.177: INFO: rc: 1
Feb 12 15:31:31.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-4153 apply -f -'
Feb 12 15:31:31.358: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 12 15:31:31.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-440-crds'
Feb 12 15:31:31.635: INFO: stderr: ""
Feb 12 15:31:31.635: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-440-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 12 15:31:31.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-440-crds.metadata'
Feb 12 15:31:31.833: INFO: stderr: ""
Feb 12 15:31:31.833: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-440-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 12 15:31:31.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-440-crds.spec'
Feb 12 15:31:32.134: INFO: stderr: ""
Feb 12 15:31:32.134: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-440-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 12 15:31:32.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-440-crds.spec.bars'
Feb 12 15:31:32.405: INFO: stderr: ""
Feb 12 15:31:32.405: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-440-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 12 15:31:32.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-440-crds.spec.bars2'
Feb 12 15:31:32.648: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:35.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4153" for this suite.

• [SLOW TEST:10.342 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":255,"skipped":3959,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:35.592: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 12 15:31:35.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8994'
Feb 12 15:31:35.742: INFO: stderr: ""
Feb 12 15:31:35.742: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 12 15:31:40.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 get pod e2e-test-httpd-pod --namespace=kubectl-8994 -o json'
Feb 12 15:31:40.882: INFO: stderr: ""
Feb 12 15:31:40.882: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.0.22/32\"\n        },\n        \"creationTimestamp\": \"2021-02-12T15:31:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-12T15:31:35Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-12T15:31:36Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.25.0.22\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-12T15:31:37Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8994\",\n        \"resourceVersion\": \"90565\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8994/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8b47f413-9847-4c73-86c8-bad3f36ab1a7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kzblr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"modest-nightingale-745f6d5bd4-h2tct\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kzblr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kzblr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T15:31:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T15:31:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T15:31:37Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-12T15:31:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6d9c0b4254496036ffcb7eb33c9ddfc8ffbc2dab6bb54f94092b60849c21740c\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-02-12T15:31:37Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"104.248.253.154\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.0.22\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.0.22\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-02-12T15:31:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 12 15:31:40.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 replace -f - --namespace=kubectl-8994'
Feb 12 15:31:41.187: INFO: stderr: ""
Feb 12 15:31:41.187: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Feb 12 15:31:41.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete pods e2e-test-httpd-pod --namespace=kubectl-8994'
Feb 12 15:31:47.469: INFO: stderr: ""
Feb 12 15:31:47.469: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:47.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8994" for this suite.

• [SLOW TEST:11.897 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":256,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:47.490: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:31:47.880: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:31:49.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740707, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740707, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740707, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740707, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:31:52.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:31:52.928: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3052-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:54.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1042" for this suite.
STEP: Destroying namespace "webhook-1042-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.000 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":257,"skipped":4030,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:54.491: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:31:54.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e" in namespace "projected-7025" to be "Succeeded or Failed"
Feb 12 15:31:54.570: INFO: Pod "downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.611282ms
Feb 12 15:31:56.578: INFO: Pod "downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013148251s
Feb 12 15:31:58.584: INFO: Pod "downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019917937s
STEP: Saw pod success
Feb 12 15:31:58.585: INFO: Pod "downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e" satisfied condition "Succeeded or Failed"
Feb 12 15:31:58.590: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e container client-container: <nil>
STEP: delete the pod
Feb 12 15:31:58.659: INFO: Waiting for pod downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e to disappear
Feb 12 15:31:58.665: INFO: Pod downwardapi-volume-39028490-ba6a-4c34-849c-44c5b742122e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:31:58.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7025" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4036,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:31:58.685: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Feb 12 15:31:58.749: INFO: Waiting up to 5m0s for pod "downward-api-5195003e-81f6-4fb9-876b-23942f28dfff" in namespace "downward-api-5554" to be "Succeeded or Failed"
Feb 12 15:31:58.757: INFO: Pod "downward-api-5195003e-81f6-4fb9-876b-23942f28dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.190032ms
Feb 12 15:32:00.765: INFO: Pod "downward-api-5195003e-81f6-4fb9-876b-23942f28dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015608815s
Feb 12 15:32:02.771: INFO: Pod "downward-api-5195003e-81f6-4fb9-876b-23942f28dfff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021901655s
STEP: Saw pod success
Feb 12 15:32:02.771: INFO: Pod "downward-api-5195003e-81f6-4fb9-876b-23942f28dfff" satisfied condition "Succeeded or Failed"
Feb 12 15:32:02.779: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod downward-api-5195003e-81f6-4fb9-876b-23942f28dfff container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:32:02.865: INFO: Waiting for pod downward-api-5195003e-81f6-4fb9-876b-23942f28dfff to disappear
Feb 12 15:32:02.871: INFO: Pod downward-api-5195003e-81f6-4fb9-876b-23942f28dfff no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:02.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5554" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4038,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:02.890: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a960ba9f-9f48-48aa-be9a-200a919b9261
STEP: Creating a pod to test consume configMaps
Feb 12 15:32:02.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b" in namespace "configmap-6371" to be "Succeeded or Failed"
Feb 12 15:32:02.980: INFO: Pod "pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.761155ms
Feb 12 15:32:04.987: INFO: Pod "pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020501981s
Feb 12 15:32:06.994: INFO: Pod "pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027262628s
STEP: Saw pod success
Feb 12 15:32:06.994: INFO: Pod "pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b" satisfied condition "Succeeded or Failed"
Feb 12 15:32:07.000: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b container configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:32:07.074: INFO: Waiting for pod pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b to disappear
Feb 12 15:32:07.081: INFO: Pod pod-configmaps-3ee12a27-70e7-4f1f-938a-48e37f9db57b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:07.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6371" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:07.104: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-0597312c-3a4f-48c9-a043-4c7e954c1a9d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:07.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9825" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":261,"skipped":4075,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:32:07.238: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 15:32:09.243: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 15:32:11.244: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:13.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:15.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:17.244: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:19.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:21.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:23.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:25.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = false)
Feb 12 15:32:27.245: INFO: The status of Pod test-webserver-6a460357-41a9-4e68-b449-7bf3523fafd3 is Running (Ready = true)
Feb 12 15:32:27.250: INFO: Container started at 2021-02-12 15:32:08 +0000 UTC, pod became ready at 2021-02-12 15:32:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:27.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7533" for this suite.

• [SLOW TEST:20.097 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4098,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:32:27.324: INFO: Creating ReplicaSet my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983
Feb 12 15:32:27.339: INFO: Pod name my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983: Found 0 pods out of 1
Feb 12 15:32:32.345: INFO: Pod name my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983: Found 1 pods out of 1
Feb 12 15:32:32.345: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983" is running
Feb 12 15:32:32.350: INFO: Pod "my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983-gqqpd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 15:32:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 15:32:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 15:32:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-12 15:32:27 +0000 UTC Reason: Message:}])
Feb 12 15:32:32.350: INFO: Trying to dial the pod
Feb 12 15:32:37.461: INFO: Controller my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983: Got expected result from replica 1 [my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983-gqqpd]: "my-hostname-basic-0a440fe9-4577-4125-b159-464fe1f23983-gqqpd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:37.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9203" for this suite.

• [SLOW TEST:10.212 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":263,"skipped":4106,"failed":0}
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:32:37.547: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-29412bda-3e35-4e5f-9b23-d2aec0ba0650" in namespace "security-context-test-1509" to be "Succeeded or Failed"
Feb 12 15:32:37.552: INFO: Pod "busybox-readonly-false-29412bda-3e35-4e5f-9b23-d2aec0ba0650": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694222ms
Feb 12 15:32:39.558: INFO: Pod "busybox-readonly-false-29412bda-3e35-4e5f-9b23-d2aec0ba0650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010876909s
Feb 12 15:32:41.564: INFO: Pod "busybox-readonly-false-29412bda-3e35-4e5f-9b23-d2aec0ba0650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017089171s
Feb 12 15:32:41.564: INFO: Pod "busybox-readonly-false-29412bda-3e35-4e5f-9b23-d2aec0ba0650" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:41.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1509" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4106,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:41.589: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:32:41.644: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:43.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9624" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":265,"skipped":4113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:43.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-586" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":266,"skipped":4138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:43.305: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-6c306838-db69-4f0e-964c-ea99f14d84b2
STEP: Creating a pod to test consume secrets
Feb 12 15:32:43.396: INFO: Waiting up to 5m0s for pod "pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee" in namespace "secrets-4461" to be "Succeeded or Failed"
Feb 12 15:32:43.406: INFO: Pod "pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 9.43317ms
Feb 12 15:32:45.414: INFO: Pod "pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017580176s
Feb 12 15:32:47.431: INFO: Pod "pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035188177s
STEP: Saw pod success
Feb 12 15:32:47.432: INFO: Pod "pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee" satisfied condition "Succeeded or Failed"
Feb 12 15:32:47.441: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:32:47.525: INFO: Waiting for pod pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee to disappear
Feb 12 15:32:47.532: INFO: Pod pod-secrets-2a6316a3-e203-48e4-884d-c57235dca6ee no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:47.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4461" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4184,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:47.557: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:51.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8987" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":268,"skipped":4192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:52.074: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 12 15:32:52.141: INFO: Waiting up to 5m0s for pod "pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8" in namespace "emptydir-7608" to be "Succeeded or Failed"
Feb 12 15:32:52.149: INFO: Pod "pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908088ms
Feb 12 15:32:54.155: INFO: Pod "pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013613122s
Feb 12 15:32:56.163: INFO: Pod "pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021311109s
STEP: Saw pod success
Feb 12 15:32:56.163: INFO: Pod "pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8" satisfied condition "Succeeded or Failed"
Feb 12 15:32:56.169: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8 container test-container: <nil>
STEP: delete the pod
Feb 12 15:32:56.238: INFO: Waiting for pod pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8 to disappear
Feb 12 15:32:56.243: INFO: Pod pod-6d9ca6dd-7dbf-406e-9831-d57f9093f4b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:32:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7608" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:32:56.261: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-698bf47d-915e-431a-8d79-82d057122b54
STEP: Creating configMap with name cm-test-opt-upd-6d4f2e57-3d0c-4733-ae94-d5042b090a6f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-698bf47d-915e-431a-8d79-82d057122b54
STEP: Updating configmap cm-test-opt-upd-6d4f2e57-3d0c-4733-ae94-d5042b090a6f
STEP: Creating configMap with name cm-test-opt-create-39871d9a-87e7-4162-9b2d-ae5311ac89d3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:00.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5961" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4263,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:00.891: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 12 15:33:00.960: INFO: Waiting up to 5m0s for pod "pod-b578ad70-851b-40c0-9a68-01e487745710" in namespace "emptydir-5575" to be "Succeeded or Failed"
Feb 12 15:33:00.972: INFO: Pod "pod-b578ad70-851b-40c0-9a68-01e487745710": Phase="Pending", Reason="", readiness=false. Elapsed: 12.374979ms
Feb 12 15:33:02.979: INFO: Pod "pod-b578ad70-851b-40c0-9a68-01e487745710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018636471s
STEP: Saw pod success
Feb 12 15:33:02.979: INFO: Pod "pod-b578ad70-851b-40c0-9a68-01e487745710" satisfied condition "Succeeded or Failed"
Feb 12 15:33:02.984: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-b578ad70-851b-40c0-9a68-01e487745710 container test-container: <nil>
STEP: delete the pod
Feb 12 15:33:03.024: INFO: Waiting for pod pod-b578ad70-851b-40c0-9a68-01e487745710 to disappear
Feb 12 15:33:03.030: INFO: Pod pod-b578ad70-851b-40c0-9a68-01e487745710 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:03.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5575" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4265,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:03.051: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 12 15:33:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Feb 12 15:33:03.527: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 12 15:33:05.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 15:33:07.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 15:33:09.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 15:33:11.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 15:33:13.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740783, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 15:33:17.042: INFO: Waited 1.420827892s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:17.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8112" for this suite.

• [SLOW TEST:14.949 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":272,"skipped":4273,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:18.003: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Feb 12 15:33:18.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 cluster-info'
Feb 12 15:33:18.159: INFO: stderr: ""
Feb 12 15:33:18.159: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mkube-dns\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns-tcp/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4963" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":273,"skipped":4291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:18.183: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:33:18.371: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 12 15:33:18.391: INFO: Number of nodes with available pods: 0
Feb 12 15:33:18.391: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 12 15:33:18.453: INFO: Number of nodes with available pods: 0
Feb 12 15:33:18.453: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:19.473: INFO: Number of nodes with available pods: 0
Feb 12 15:33:19.474: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:20.460: INFO: Number of nodes with available pods: 1
Feb 12 15:33:20.460: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 12 15:33:20.496: INFO: Number of nodes with available pods: 1
Feb 12 15:33:20.496: INFO: Number of running nodes: 0, number of available pods: 1
Feb 12 15:33:21.504: INFO: Number of nodes with available pods: 0
Feb 12 15:33:21.504: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 12 15:33:21.517: INFO: Number of nodes with available pods: 0
Feb 12 15:33:21.517: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:22.544: INFO: Number of nodes with available pods: 0
Feb 12 15:33:22.544: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:23.523: INFO: Number of nodes with available pods: 0
Feb 12 15:33:23.523: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:24.524: INFO: Number of nodes with available pods: 0
Feb 12 15:33:24.524: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:25.524: INFO: Number of nodes with available pods: 0
Feb 12 15:33:25.524: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:26.525: INFO: Number of nodes with available pods: 0
Feb 12 15:33:26.525: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:27.527: INFO: Number of nodes with available pods: 0
Feb 12 15:33:27.527: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:28.534: INFO: Number of nodes with available pods: 0
Feb 12 15:33:28.534: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:29.525: INFO: Number of nodes with available pods: 0
Feb 12 15:33:29.525: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:30.524: INFO: Number of nodes with available pods: 0
Feb 12 15:33:30.524: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:31.524: INFO: Number of nodes with available pods: 0
Feb 12 15:33:31.524: INFO: Node modest-nightingale-745f6d5bd4-w42vq is running more than one daemon pod
Feb 12 15:33:32.524: INFO: Number of nodes with available pods: 1
Feb 12 15:33:32.524: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5218, will wait for the garbage collector to delete the pods
Feb 12 15:33:32.607: INFO: Deleting DaemonSet.extensions daemon-set took: 13.778655ms
Feb 12 15:33:33.207: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.148779ms
Feb 12 15:33:36.317: INFO: Number of nodes with available pods: 0
Feb 12 15:33:36.317: INFO: Number of running nodes: 0, number of available pods: 0
Feb 12 15:33:36.321: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5218/daemonsets","resourceVersion":"91912"},"items":null}

Feb 12 15:33:36.326: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5218/pods","resourceVersion":"91912"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:36.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5218" for this suite.

• [SLOW TEST:18.214 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":274,"skipped":4333,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:36.398: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:33:37.097: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 12 15:33:39.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740817, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740817, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740817, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748740817, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:33:42.133: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:33:42.140: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:43.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3343" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.455 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":275,"skipped":4337,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 12 15:33:43.948: INFO: Waiting up to 5m0s for pod "pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d" in namespace "emptydir-8870" to be "Succeeded or Failed"
Feb 12 15:33:43.962: INFO: Pod "pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.528375ms
Feb 12 15:33:45.971: INFO: Pod "pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02234968s
STEP: Saw pod success
Feb 12 15:33:45.971: INFO: Pod "pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d" satisfied condition "Succeeded or Failed"
Feb 12 15:33:45.983: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d container test-container: <nil>
STEP: delete the pod
Feb 12 15:33:46.030: INFO: Waiting for pod pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d to disappear
Feb 12 15:33:46.034: INFO: Pod pod-f6a6e6d3-59aa-4ec2-b59c-9845b8fd813d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:46.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8870" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:46.057: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:33:57.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-331" for this suite.

• [SLOW TEST:11.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":277,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:33:57.238: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-6c293228-f5af-44ca-9a0a-f544fa39401f
STEP: Creating a pod to test consume configMaps
Feb 12 15:33:57.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241" in namespace "projected-6596" to be "Succeeded or Failed"
Feb 12 15:33:57.320: INFO: Pod "pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095151ms
Feb 12 15:33:59.331: INFO: Pod "pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016429662s
Feb 12 15:34:01.338: INFO: Pod "pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022960506s
STEP: Saw pod success
Feb 12 15:34:01.338: INFO: Pod "pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241" satisfied condition "Succeeded or Failed"
Feb 12 15:34:01.343: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 12 15:34:01.420: INFO: Waiting for pod pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241 to disappear
Feb 12 15:34:01.425: INFO: Pod pod-projected-configmaps-a10cd8a2-1916-4eb0-8c8f-357897e53241 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:01.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6596" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 12 15:34:05.549: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2906 PodName:pod-sharedvolume-353f0b43-3575-406e-8d3f-eda31dac7a08 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:05.549: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:06.024: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:06.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2906" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":279,"skipped":4460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:06.045: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Feb 12 15:34:06.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f -'
Feb 12 15:34:06.446: INFO: stderr: ""
Feb 12 15:34:06.446: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb 12 15:34:06.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 diff -f -'
Feb 12 15:34:06.754: INFO: rc: 1
Feb 12 15:34:06.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete -f -'
Feb 12 15:34:06.845: INFO: stderr: ""
Feb 12 15:34:06.845: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:06.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-996" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":280,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:06.871: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:06.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3891" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":281,"skipped":4507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:06.985: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Feb 12 15:34:07.052: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 12 15:34:07.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:07.271: INFO: stderr: ""
Feb 12 15:34:07.271: INFO: stdout: "service/agnhost-replica created\n"
Feb 12 15:34:07.271: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 12 15:34:07.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:07.699: INFO: stderr: ""
Feb 12 15:34:07.699: INFO: stdout: "service/agnhost-primary created\n"
Feb 12 15:34:07.699: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 12 15:34:07.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:07.999: INFO: stderr: ""
Feb 12 15:34:07.999: INFO: stdout: "service/frontend created\n"
Feb 12 15:34:08.000: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 12 15:34:08.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:08.255: INFO: stderr: ""
Feb 12 15:34:08.255: INFO: stdout: "deployment.apps/frontend created\n"
Feb 12 15:34:08.255: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 12 15:34:08.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:08.576: INFO: stderr: ""
Feb 12 15:34:08.576: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 12 15:34:08.576: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 12 15:34:08.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 create -f - --namespace=kubectl-7162'
Feb 12 15:34:08.898: INFO: stderr: ""
Feb 12 15:34:08.898: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb 12 15:34:08.898: INFO: Waiting for all frontend pods to be Running.
Feb 12 15:34:13.948: INFO: Waiting for frontend to serve content.
Feb 12 15:34:14.047: INFO: Trying to add a new entry to the guestbook.
Feb 12 15:34:14.187: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 12 15:34:14.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:14.350: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:14.350: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb 12 15:34:14.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:14.461: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:14.461: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 12 15:34:14.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:14.595: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:14.595: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 12 15:34:14.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:14.702: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:14.702: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 12 15:34:14.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:14.885: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:14.885: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 12 15:34:14.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Feb 12 15:34:15.032: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 15:34:15.032: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:15.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7162" for this suite.

• [SLOW TEST:8.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":282,"skipped":4541,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:15.053: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Feb 12 15:34:15.124: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-249037184 proxy --unix-socket=/tmp/kubectl-proxy-unix222179019/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6900" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":283,"skipped":4544,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:15.241: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:34:15.304: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3f881cc2-da39-4815-a827-a5a7862f3e57" in namespace "security-context-test-1427" to be "Succeeded or Failed"
Feb 12 15:34:15.315: INFO: Pod "alpine-nnp-false-3f881cc2-da39-4815-a827-a5a7862f3e57": Phase="Pending", Reason="", readiness=false. Elapsed: 11.689046ms
Feb 12 15:34:17.322: INFO: Pod "alpine-nnp-false-3f881cc2-da39-4815-a827-a5a7862f3e57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01810475s
Feb 12 15:34:17.322: INFO: Pod "alpine-nnp-false-3f881cc2-da39-4815-a827-a5a7862f3e57" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1427" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:17.404: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 12 15:34:21.516: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:21.516: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:21.958: INFO: Exec stderr: ""
Feb 12 15:34:21.958: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:21.958: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:22.491: INFO: Exec stderr: ""
Feb 12 15:34:22.491: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:22.491: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:23.018: INFO: Exec stderr: ""
Feb 12 15:34:23.018: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:23.018: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:23.479: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 12 15:34:23.479: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:24.037: INFO: Exec stderr: ""
Feb 12 15:34:24.037: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:24.037: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:24.569: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 12 15:34:24.569: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:24.569: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:25.144: INFO: Exec stderr: ""
Feb 12 15:34:25.144: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:25.144: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:25.743: INFO: Exec stderr: ""
Feb 12 15:34:25.743: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:25.743: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:26.238: INFO: Exec stderr: ""
Feb 12 15:34:26.238: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6843 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 12 15:34:26.238: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
Feb 12 15:34:26.724: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:34:26.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6843" for this suite.

• [SLOW TEST:9.339 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4660,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:34:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 12 15:34:27.223: INFO: Pod name wrapped-volume-race-727886ef-51e6-4037-ab54-83ddfe5beaa5: Found 0 pods out of 5
Feb 12 15:34:32.236: INFO: Pod name wrapped-volume-race-727886ef-51e6-4037-ab54-83ddfe5beaa5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-727886ef-51e6-4037-ab54-83ddfe5beaa5 in namespace emptydir-wrapper-5346, will wait for the garbage collector to delete the pods
Feb 12 15:34:44.359: INFO: Deleting ReplicationController wrapped-volume-race-727886ef-51e6-4037-ab54-83ddfe5beaa5 took: 12.946516ms
Feb 12 15:34:44.960: INFO: Terminating ReplicationController wrapped-volume-race-727886ef-51e6-4037-ab54-83ddfe5beaa5 pods took: 600.223277ms
STEP: Creating RC which spawns configmap-volume pods
Feb 12 15:34:50.394: INFO: Pod name wrapped-volume-race-e5e1d140-c3ed-4c25-8f49-f0867d6ae9b5: Found 0 pods out of 5
Feb 12 15:34:55.409: INFO: Pod name wrapped-volume-race-e5e1d140-c3ed-4c25-8f49-f0867d6ae9b5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e5e1d140-c3ed-4c25-8f49-f0867d6ae9b5 in namespace emptydir-wrapper-5346, will wait for the garbage collector to delete the pods
Feb 12 15:35:07.527: INFO: Deleting ReplicationController wrapped-volume-race-e5e1d140-c3ed-4c25-8f49-f0867d6ae9b5 took: 16.919039ms
Feb 12 15:35:08.127: INFO: Terminating ReplicationController wrapped-volume-race-e5e1d140-c3ed-4c25-8f49-f0867d6ae9b5 pods took: 600.13536ms
STEP: Creating RC which spawns configmap-volume pods
Feb 12 15:35:17.899: INFO: Pod name wrapped-volume-race-e522ad09-cede-4965-ab66-c27f7bafc680: Found 0 pods out of 5
Feb 12 15:35:22.910: INFO: Pod name wrapped-volume-race-e522ad09-cede-4965-ab66-c27f7bafc680: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e522ad09-cede-4965-ab66-c27f7bafc680 in namespace emptydir-wrapper-5346, will wait for the garbage collector to delete the pods
Feb 12 15:35:35.033: INFO: Deleting ReplicationController wrapped-volume-race-e522ad09-cede-4965-ab66-c27f7bafc680 took: 13.605347ms
Feb 12 15:35:35.634: INFO: Terminating ReplicationController wrapped-volume-race-e522ad09-cede-4965-ab66-c27f7bafc680 pods took: 600.237831ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:35:48.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5346" for this suite.

• [SLOW TEST:81.674 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":286,"skipped":4661,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:35:48.423: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Feb 12 15:35:48.503: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 15:36:48.547: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:36:48.552: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb 12 15:36:52.662: INFO: found a healthy node: modest-nightingale-745f6d5bd4-h2tct
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:37:04.801: INFO: pods created so far: [1 1 1]
Feb 12 15:37:04.801: INFO: length of pods created so far: 3
Feb 12 15:37:14.824: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:37:21.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-485" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:37:21.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2395" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:93.574 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":287,"skipped":4663,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:37:21.999: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4902/configmap-test-a4878580-49a2-4ed8-af8d-8175733d4b86
STEP: Creating a pod to test consume configMaps
Feb 12 15:37:22.082: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b" in namespace "configmap-4902" to be "Succeeded or Failed"
Feb 12 15:37:22.090: INFO: Pod "pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.854068ms
Feb 12 15:37:24.098: INFO: Pod "pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015355175s
STEP: Saw pod success
Feb 12 15:37:24.098: INFO: Pod "pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b" satisfied condition "Succeeded or Failed"
Feb 12 15:37:24.103: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b container env-test: <nil>
STEP: delete the pod
Feb 12 15:37:24.181: INFO: Waiting for pod pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b to disappear
Feb 12 15:37:24.187: INFO: Pod pod-configmaps-0d75bddc-abfe-4ad5-824d-c45c135a710b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:37:24.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4902" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":288,"skipped":4679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:37:24.210: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 12 15:37:28.409: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 15:37:28.422: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 12 15:37:30.422: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 15:37:30.433: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 12 15:37:32.422: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 15:37:32.428: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:37:32.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1005" for this suite.

• [SLOW TEST:8.240 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4701,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:37:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:37:33.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:37:35.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741053, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741053, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741053, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741053, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:37:38.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:37:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5205" for this suite.
STEP: Destroying namespace "webhook-5205-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.079 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":290,"skipped":4711,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:37:38.533: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2085
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Feb 12 15:37:38.615: INFO: Found 0 stateful pods, waiting for 3
Feb 12 15:37:48.625: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 15:37:48.625: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 15:37:48.625: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 12 15:37:48.674: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 12 15:37:58.739: INFO: Updating stateful set ss2
Feb 12 15:37:58.753: INFO: Waiting for Pod statefulset-2085/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 12 15:38:08.866: INFO: Found 2 stateful pods, waiting for 3
Feb 12 15:38:18.874: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 15:38:18.874: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 15:38:18.874: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 12 15:38:18.926: INFO: Updating stateful set ss2
Feb 12 15:38:18.950: INFO: Waiting for Pod statefulset-2085/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 12 15:38:28.990: INFO: Updating stateful set ss2
Feb 12 15:38:29.004: INFO: Waiting for StatefulSet statefulset-2085/ss2 to complete update
Feb 12 15:38:29.004: INFO: Waiting for Pod statefulset-2085/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 12 15:38:39.025: INFO: Waiting for StatefulSet statefulset-2085/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 12 15:38:49.017: INFO: Deleting all statefulset in ns statefulset-2085
Feb 12 15:38:49.022: INFO: Scaling statefulset ss2 to 0
Feb 12 15:39:09.053: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 15:39:09.059: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:39:09.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2085" for this suite.

• [SLOW TEST:90.595 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":291,"skipped":4719,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:39:09.130: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-6276c13a-4ba0-4d71-9d96-79b9d07f5a5c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:39:11.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-995" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4725,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:39:11.378: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:39:11.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549" in namespace "downward-api-1100" to be "Succeeded or Failed"
Feb 12 15:39:11.466: INFO: Pod "downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549": Phase="Pending", Reason="", readiness=false. Elapsed: 13.550439ms
Feb 12 15:39:13.473: INFO: Pod "downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020751977s
Feb 12 15:39:15.481: INFO: Pod "downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028594109s
STEP: Saw pod success
Feb 12 15:39:15.481: INFO: Pod "downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549" satisfied condition "Succeeded or Failed"
Feb 12 15:39:15.489: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549 container client-container: <nil>
STEP: delete the pod
Feb 12 15:39:15.531: INFO: Waiting for pod downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549 to disappear
Feb 12 15:39:15.536: INFO: Pod downwardapi-volume-3e3ce2e8-a581-4e53-b7c9-13da32e47549 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:39:15.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1100" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4728,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:39:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-3e1a74f2-f85f-487c-b965-47c3225f6c04
STEP: Creating secret with name s-test-opt-upd-8fbf6d1a-837f-4e74-a243-84f553e06dc4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3e1a74f2-f85f-487c-b965-47c3225f6c04
STEP: Updating secret s-test-opt-upd-8fbf6d1a-837f-4e74-a243-84f553e06dc4
STEP: Creating secret with name s-test-opt-create-cd89a8c5-f696-4029-9840-9a49ad9783f9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:40:28.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5489" for this suite.

• [SLOW TEST:73.284 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4736,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:40:28.841: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Feb 12 15:40:28.921: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 15:41:28.966: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Feb 12 15:41:29.003: INFO: Created pod: pod0-sched-preemption-low-priority
Feb 12 15:41:29.031: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb 12 15:41:29.063: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:41:49.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6103" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:80.369 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":295,"skipped":4751,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:41:49.211: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-ef607012-7019-4e24-a9ba-2b831e4358cc
STEP: Creating a pod to test consume secrets
Feb 12 15:41:49.283: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f" in namespace "projected-6465" to be "Succeeded or Failed"
Feb 12 15:41:49.294: INFO: Pod "pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.185981ms
Feb 12 15:41:51.301: INFO: Pod "pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018142602s
STEP: Saw pod success
Feb 12 15:41:51.301: INFO: Pod "pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f" satisfied condition "Succeeded or Failed"
Feb 12 15:41:51.306: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f container secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:41:51.341: INFO: Waiting for pod pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f to disappear
Feb 12 15:41:51.347: INFO: Pod pod-projected-secrets-4875981b-7fa6-44ae-b79f-e580a3bfb75f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:41:51.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6465" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":4763,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:41:51.376: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb 12 15:41:51.441: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb 12 15:41:51.453: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 12 15:41:51.453: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb 12 15:41:51.466: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 12 15:41:51.466: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb 12 15:41:51.480: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 12 15:41:51.480: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb 12 15:41:58.541: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:41:58.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5119" for this suite.

• [SLOW TEST:7.199 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":297,"skipped":4772,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:41:58.575: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Feb 12 15:41:58.646: INFO: Waiting up to 5m0s for pod "client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5" in namespace "containers-9262" to be "Succeeded or Failed"
Feb 12 15:41:58.656: INFO: Pod "client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398232ms
Feb 12 15:42:00.663: INFO: Pod "client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017349559s
STEP: Saw pod success
Feb 12 15:42:00.663: INFO: Pod "client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5" satisfied condition "Succeeded or Failed"
Feb 12 15:42:00.670: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5 container test-container: <nil>
STEP: delete the pod
Feb 12 15:42:00.706: INFO: Waiting for pod client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5 to disappear
Feb 12 15:42:00.716: INFO: Pod client-containers-993fa1c3-a760-4b1c-b908-470afd362ad5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:42:00.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9262" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4782,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:42:00.738: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Feb 12 15:42:00.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7" in namespace "downward-api-9339" to be "Succeeded or Failed"
Feb 12 15:42:00.820: INFO: Pod "downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866925ms
Feb 12 15:42:02.828: INFO: Pod "downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013339329s
STEP: Saw pod success
Feb 12 15:42:02.828: INFO: Pod "downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7" satisfied condition "Succeeded or Failed"
Feb 12 15:42:02.833: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-w42vq pod downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7 container client-container: <nil>
STEP: delete the pod
Feb 12 15:42:02.866: INFO: Waiting for pod downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7 to disappear
Feb 12 15:42:02.873: INFO: Pod downwardapi-volume-34c4f71e-45ce-478a-a5dc-026ad94ff3d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:42:02.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9339" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:42:02.898: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 12 15:42:03.294: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 15:42:05.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741323, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741323, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741323, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748741323, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 12 15:42:08.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:42:08.339: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1606-crds.webhook.example.com via the AdmissionRegistration API
Feb 12 15:42:08.893: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:42:09.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2658" for this suite.
STEP: Destroying namespace "webhook-2658-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.067 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":300,"skipped":4841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:42:09.970: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:42:54.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2802" for this suite.

• [SLOW TEST:44.255 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":301,"skipped":4868,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:42:54.225: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Feb 12 15:42:54.283: INFO: Waiting up to 5m0s for pod "var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48" in namespace "var-expansion-5788" to be "Succeeded or Failed"
Feb 12 15:42:54.294: INFO: Pod "var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.079429ms
Feb 12 15:42:56.299: INFO: Pod "var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015594748s
STEP: Saw pod success
Feb 12 15:42:56.300: INFO: Pod "var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48" satisfied condition "Succeeded or Failed"
Feb 12 15:42:56.304: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48 container dapi-container: <nil>
STEP: delete the pod
Feb 12 15:42:56.379: INFO: Waiting for pod var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48 to disappear
Feb 12 15:42:56.384: INFO: Pod var-expansion-f7fae85f-aa08-4c00-b100-72fc7bfe6b48 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:42:56.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5788" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":4872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:42:56.404: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Feb 12 15:42:56.459: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 12 15:42:59.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-1365 create -f -'
Feb 12 15:43:00.324: INFO: stderr: ""
Feb 12 15:43:00.324: INFO: stdout: "e2e-test-crd-publish-openapi-3359-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 12 15:43:00.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-1365 delete e2e-test-crd-publish-openapi-3359-crds test-cr'
Feb 12 15:43:00.420: INFO: stderr: ""
Feb 12 15:43:00.420: INFO: stdout: "e2e-test-crd-publish-openapi-3359-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 12 15:43:00.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-1365 apply -f -'
Feb 12 15:43:00.612: INFO: stderr: ""
Feb 12 15:43:00.612: INFO: stdout: "e2e-test-crd-publish-openapi-3359-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 12 15:43:00.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 --namespace=crd-publish-openapi-1365 delete e2e-test-crd-publish-openapi-3359-crds test-cr'
Feb 12 15:43:00.796: INFO: stderr: ""
Feb 12 15:43:00.796: INFO: stdout: "e2e-test-crd-publish-openapi-3359-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 12 15:43:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-249037184 explain e2e-test-crd-publish-openapi-3359-crds'
Feb 12 15:43:01.050: INFO: stderr: ""
Feb 12 15:43:01.050: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3359-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:43:04.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1365" for this suite.

• [SLOW TEST:8.127 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":303,"skipped":4899,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:43:04.534: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2020 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2020;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2020 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2020;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2020.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2020.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2020.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2020.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 181.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.181_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2020 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2020;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2020 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2020;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2020.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2020.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2020.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2020.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 181.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.181_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 12 15:43:06.747: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.790: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.799: INFO: Unable to read wheezy_udp@dns-test-service.dns-2020 from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.807: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2020 from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.816: INFO: Unable to read wheezy_udp@dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.834: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:06.843: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.368: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.379: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.389: INFO: Unable to read jessie_udp@dns-test-service.dns-2020 from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.397: INFO: Unable to read jessie_tcp@dns-test-service.dns-2020 from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.405: INFO: Unable to read jessie_udp@dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.414: INFO: Unable to read jessie_tcp@dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.423: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.431: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2020.svc from pod dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597: the server could not find the requested resource (get pods dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597)
Feb 12 15:43:07.920: INFO: Lookups using dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2020 wheezy_tcp@dns-test-service.dns-2020 wheezy_udp@dns-test-service.dns-2020.svc wheezy_tcp@dns-test-service.dns-2020.svc wheezy_udp@_http._tcp.dns-test-service.dns-2020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2020 jessie_tcp@dns-test-service.dns-2020 jessie_udp@dns-test-service.dns-2020.svc jessie_tcp@dns-test-service.dns-2020.svc jessie_udp@_http._tcp.dns-test-service.dns-2020.svc jessie_tcp@_http._tcp.dns-test-service.dns-2020.svc]

Feb 12 15:43:15.381: INFO: DNS probes using dns-2020/dns-test-32df92b9-e1a6-44a7-b3ed-a3b9f9d41597 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:43:15.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2020" for this suite.

• [SLOW TEST:10.974 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":304,"skipped":4910,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 12 15:43:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-249037184
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-1d3fc3e5-2c7c-4a02-aee9-31e943599d62
STEP: Creating a pod to test consume secrets
Feb 12 15:43:15.593: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b" in namespace "projected-1169" to be "Succeeded or Failed"
Feb 12 15:43:15.605: INFO: Pod "pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.255159ms
Feb 12 15:43:17.611: INFO: Pod "pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018487845s
Feb 12 15:43:19.621: INFO: Pod "pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028439188s
STEP: Saw pod success
Feb 12 15:43:19.621: INFO: Pod "pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b" satisfied condition "Succeeded or Failed"
Feb 12 15:43:19.632: INFO: Trying to get logs from node modest-nightingale-745f6d5bd4-h2tct pod pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 12 15:43:19.678: INFO: Waiting for pod pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b to disappear
Feb 12 15:43:19.684: INFO: Pod pod-projected-secrets-7f924d15-8b04-405f-8620-8852c4f9982b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 12 15:43:19.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1169" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":4927,"failed":0}
SFeb 12 15:43:19.700: INFO: Running AfterSuite actions on all nodes
Feb 12 15:43:19.701: INFO: Running AfterSuite actions on node 1
Feb 12 15:43:19.701: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4928,"failed":0}

Ran 305 of 5233 Specs in 5974.353 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4928 Skipped
PASS

Ginkgo ran 1 suite in 1h39m35.835657295s
Test Suite Passed
