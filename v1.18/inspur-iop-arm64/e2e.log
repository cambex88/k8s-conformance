I0814 03:30:34.701429      24 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-133789455
I0814 03:30:34.701488      24 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0814 03:30:34.701782      24 e2e.go:124] Starting e2e run "81cbf033-856f-4948-8fb5-e94cbcdbaf67" on Ginkgo node 1
{"msg":"Test Suite starting","total":275,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1597375832 - Will randomize all specs
Will run 275 of 4992 specs

Aug 14 03:30:34.836: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:30:34.844: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 14 03:30:34.894: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 14 03:30:35.015: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 14 03:30:35.016: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Aug 14 03:30:35.016: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 14 03:30:35.036: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 14 03:30:35.036: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Aug 14 03:30:35.036: INFO: e2e test version: v1.18.2-dirty
Aug 14 03:30:35.039: INFO: kube-apiserver version: v1.18.2
Aug 14 03:30:35.039: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:30:35.052: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:30:35.053: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
Aug 14 03:30:35.187: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 14 03:30:35.212: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:30:36.510: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:30:38.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972636, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972636, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972636, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972636, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:30:41.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:30:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8571" for this suite.
STEP: Destroying namespace "webhook-8571-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":275,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:30:41.928: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6173
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 14 03:30:42.189: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:30:53.436: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:20.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6173" for this suite.

• [SLOW TEST:38.864 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":275,"completed":2,"skipped":44,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 03:31:21.089: INFO: Waiting up to 5m0s for pod "pod-d8cecce2-a11f-40e9-b8f2-1e7618247182" in namespace "emptydir-2194" to be "Succeeded or Failed"
Aug 14 03:31:21.099: INFO: Pod "pod-d8cecce2-a11f-40e9-b8f2-1e7618247182": Phase="Pending", Reason="", readiness=false. Elapsed: 10.315397ms
Aug 14 03:31:23.109: INFO: Pod "pod-d8cecce2-a11f-40e9-b8f2-1e7618247182": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020418231s
Aug 14 03:31:25.118: INFO: Pod "pod-d8cecce2-a11f-40e9-b8f2-1e7618247182": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029214348s
STEP: Saw pod success
Aug 14 03:31:25.118: INFO: Pod "pod-d8cecce2-a11f-40e9-b8f2-1e7618247182" satisfied condition "Succeeded or Failed"
Aug 14 03:31:25.126: INFO: Trying to get logs from node worker2 pod pod-d8cecce2-a11f-40e9-b8f2-1e7618247182 container test-container: <nil>
STEP: delete the pod
Aug 14 03:31:25.196: INFO: Waiting for pod pod-d8cecce2-a11f-40e9-b8f2-1e7618247182 to disappear
Aug 14 03:31:25.205: INFO: Pod pod-d8cecce2-a11f-40e9-b8f2-1e7618247182 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:25.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2194" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":3,"skipped":59,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:25.231: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-33f13320-c795-4bf8-aa35-2d6cdd047e0e
STEP: Creating a pod to test consume configMaps
Aug 14 03:31:25.462: INFO: Waiting up to 5m0s for pod "pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601" in namespace "configmap-3629" to be "Succeeded or Failed"
Aug 14 03:31:25.470: INFO: Pod "pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601": Phase="Pending", Reason="", readiness=false. Elapsed: 8.287082ms
Aug 14 03:31:27.497: INFO: Pod "pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035780057s
STEP: Saw pod success
Aug 14 03:31:27.498: INFO: Pod "pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601" satisfied condition "Succeeded or Failed"
Aug 14 03:31:27.507: INFO: Trying to get logs from node worker2 pod pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:31:27.565: INFO: Waiting for pod pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601 to disappear
Aug 14 03:31:27.573: INFO: Pod pod-configmaps-010f1852-eb82-4ec0-bf2c-1997af36e601 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:27.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3629" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":4,"skipped":81,"failed":0}
SSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:27.598: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-6366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 14 03:31:27.809: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 14 03:31:27.828: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 14 03:31:27.828: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 14 03:31:27.854: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 14 03:31:27.854: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 14 03:31:27.878: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 14 03:31:27.878: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 14 03:31:35.088: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:35.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6366" for this suite.

• [SLOW TEST:7.537 seconds]
[sig-scheduling] LimitRange
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":275,"completed":5,"skipped":85,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:31:35.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26" in namespace "downward-api-6012" to be "Succeeded or Failed"
Aug 14 03:31:35.364: INFO: Pod "downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.933298ms
Aug 14 03:31:37.375: INFO: Pod "downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02083565s
Aug 14 03:31:39.385: INFO: Pod "downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030933563s
STEP: Saw pod success
Aug 14 03:31:39.385: INFO: Pod "downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26" satisfied condition "Succeeded or Failed"
Aug 14 03:31:39.393: INFO: Trying to get logs from node worker2 pod downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26 container client-container: <nil>
STEP: delete the pod
Aug 14 03:31:39.448: INFO: Waiting for pod downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26 to disappear
Aug 14 03:31:39.457: INFO: Pod downwardapi-volume-966d3be3-c593-4e24-9e54-f4c1f317ed26 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:39.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6012" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":6,"skipped":91,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:39.483: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:31:41.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:31:43.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972701, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972701, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972701, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972701, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:31:46.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:31:46.449: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2502-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:52.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9128" for this suite.
STEP: Destroying namespace "webhook-9128-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.935 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":275,"completed":7,"skipped":93,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:53.418: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-eb012813-259c-4c6e-a115-9ebe84d6369e
STEP: Creating a pod to test consume configMaps
Aug 14 03:31:53.760: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804" in namespace "projected-4798" to be "Succeeded or Failed"
Aug 14 03:31:53.794: INFO: Pod "pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804": Phase="Pending", Reason="", readiness=false. Elapsed: 34.130804ms
Aug 14 03:31:55.805: INFO: Pod "pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044744577s
Aug 14 03:31:57.816: INFO: Pod "pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055467329s
STEP: Saw pod success
Aug 14 03:31:57.816: INFO: Pod "pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804" satisfied condition "Succeeded or Failed"
Aug 14 03:31:57.825: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:31:57.875: INFO: Waiting for pod pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804 to disappear
Aug 14 03:31:57.883: INFO: Pod pod-projected-configmaps-4615aa18-825a-497e-81c1-7be4f67b8804 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4798" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":8,"skipped":93,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:57.909: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:31:58.198: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 71.888299ms)
Aug 14 03:31:58.208: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.268817ms)
Aug 14 03:31:58.228: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 19.855116ms)
Aug 14 03:31:58.242: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 13.421849ms)
Aug 14 03:31:58.253: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.638956ms)
Aug 14 03:31:58.264: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.536114ms)
Aug 14 03:31:58.277: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.138153ms)
Aug 14 03:31:58.287: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.092558ms)
Aug 14 03:31:58.298: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.198334ms)
Aug 14 03:31:58.309: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.520777ms)
Aug 14 03:31:58.321: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.645412ms)
Aug 14 03:31:58.334: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.180533ms)
Aug 14 03:31:58.345: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.520595ms)
Aug 14 03:31:58.356: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.808056ms)
Aug 14 03:31:58.368: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.451434ms)
Aug 14 03:31:58.393: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 25.545023ms)
Aug 14 03:31:58.404: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.216037ms)
Aug 14 03:31:58.416: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.717711ms)
Aug 14 03:31:58.428: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.823353ms)
Aug 14 03:31:58.439: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.877896ms)
[AfterEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:31:58.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-195" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":275,"completed":9,"skipped":105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:31:58.466: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:31:58.688: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5c83b678-c468-4021-9684-4847c0f84efd" in namespace "security-context-test-2002" to be "Succeeded or Failed"
Aug 14 03:31:58.698: INFO: Pod "busybox-readonly-false-5c83b678-c468-4021-9684-4847c0f84efd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.377719ms
Aug 14 03:32:00.707: INFO: Pod "busybox-readonly-false-5c83b678-c468-4021-9684-4847c0f84efd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018447735s
Aug 14 03:32:02.719: INFO: Pod "busybox-readonly-false-5c83b678-c468-4021-9684-4847c0f84efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030404545s
Aug 14 03:32:02.719: INFO: Pod "busybox-readonly-false-5c83b678-c468-4021-9684-4847c0f84efd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:02.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2002" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":275,"completed":10,"skipped":146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:02.794: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-291d8d51-8920-4d87-b6fc-093243cd7c1c
STEP: Creating a pod to test consume secrets
Aug 14 03:32:03.022: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada" in namespace "projected-8447" to be "Succeeded or Failed"
Aug 14 03:32:03.038: INFO: Pod "pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043684ms
Aug 14 03:32:05.048: INFO: Pod "pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026038759s
Aug 14 03:32:07.059: INFO: Pod "pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036663251s
STEP: Saw pod success
Aug 14 03:32:07.059: INFO: Pod "pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada" satisfied condition "Succeeded or Failed"
Aug 14 03:32:07.067: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:32:07.115: INFO: Waiting for pod pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada to disappear
Aug 14 03:32:07.124: INFO: Pod pod-projected-secrets-2bf90a5d-a5e4-449e-8b68-2b9259ea1ada no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:07.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8447" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":11,"skipped":184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:07.148: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:18.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7783" for this suite.

• [SLOW TEST:11.568 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":275,"completed":12,"skipped":232,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:18.717: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-b1c274c2-8280-4e3e-86c2-b6172e3d5fb2
STEP: Creating a pod to test consume configMaps
Aug 14 03:32:18.942: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5" in namespace "projected-3243" to be "Succeeded or Failed"
Aug 14 03:32:18.950: INFO: Pod "pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.491341ms
Aug 14 03:32:20.989: INFO: Pod "pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047014391s
Aug 14 03:32:22.999: INFO: Pod "pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057519464s
STEP: Saw pod success
Aug 14 03:32:22.999: INFO: Pod "pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5" satisfied condition "Succeeded or Failed"
Aug 14 03:32:23.009: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:32:23.123: INFO: Waiting for pod pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5 to disappear
Aug 14 03:32:23.133: INFO: Pod pod-projected-configmaps-9fe80085-c82c-47b9-a810-236995332df5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3243" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":13,"skipped":251,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:23.164: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 14 03:32:28.513: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:29.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7697" for this suite.

• [SLOW TEST:6.416 seconds]
[sig-apps] ReplicaSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":275,"completed":14,"skipped":253,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:29.580: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:31.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9237" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":275,"completed":15,"skipped":258,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:31.884: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:34.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7286" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":275,"completed":16,"skipped":259,"failed":0}

------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:34.170: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:34.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7254" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":275,"completed":17,"skipped":259,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:34.406: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Aug 14 03:32:34.626: INFO: Waiting up to 5m0s for pod "downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca" in namespace "downward-api-3591" to be "Succeeded or Failed"
Aug 14 03:32:34.637: INFO: Pod "downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 10.844556ms
Aug 14 03:32:36.648: INFO: Pod "downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021672668s
Aug 14 03:32:38.657: INFO: Pod "downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030179426s
STEP: Saw pod success
Aug 14 03:32:38.657: INFO: Pod "downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca" satisfied condition "Succeeded or Failed"
Aug 14 03:32:38.665: INFO: Trying to get logs from node worker2 pod downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca container dapi-container: <nil>
STEP: delete the pod
Aug 14 03:32:38.716: INFO: Waiting for pod downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca to disappear
Aug 14 03:32:38.724: INFO: Pod downward-api-94377cde-8b0b-475f-9d69-a5041530a5ca no longer exists
[AfterEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:38.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3591" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":275,"completed":18,"skipped":264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:38.747: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:32:38.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3" in namespace "projected-6575" to be "Succeeded or Failed"
Aug 14 03:32:38.973: INFO: Pod "downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.1361ms
Aug 14 03:32:40.983: INFO: Pod "downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018896414s
Aug 14 03:32:42.994: INFO: Pod "downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030032766s
STEP: Saw pod success
Aug 14 03:32:42.994: INFO: Pod "downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3" satisfied condition "Succeeded or Failed"
Aug 14 03:32:43.003: INFO: Trying to get logs from node worker1 pod downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3 container client-container: <nil>
STEP: delete the pod
Aug 14 03:32:43.073: INFO: Waiting for pod downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3 to disappear
Aug 14 03:32:43.082: INFO: Pod downwardapi-volume-4fb581a3-b949-4a27-9316-0b430f3de5e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:32:43.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6575" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":19,"skipped":293,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:32:43.109: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4919
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 03:32:43.306: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 14 03:32:43.436: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 03:32:45.448: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 03:32:47.448: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:49.445: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:51.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:53.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:55.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:57.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:32:59.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:33:01.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:33:03.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 03:33:05.446: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 14 03:33:05.462: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 14 03:33:05.482: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 14 03:33:07.492: INFO: The status of Pod netserver-2 is Running (Ready = true)
Aug 14 03:33:07.512: INFO: The status of Pod netserver-3 is Running (Ready = true)
Aug 14 03:33:07.529: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Aug 14 03:33:11.582: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.72:8080/dial?request=hostname&protocol=http&host=100.101.161.168&port=8080&tries=1'] Namespace:pod-network-test-4919 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 03:33:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:33:11.901: INFO: Waiting for responses: map[]
Aug 14 03:33:11.911: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.72:8080/dial?request=hostname&protocol=http&host=100.101.208.149&port=8080&tries=1'] Namespace:pod-network-test-4919 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 03:33:11.911: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:33:12.213: INFO: Waiting for responses: map[]
Aug 14 03:33:12.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.72:8080/dial?request=hostname&protocol=http&host=100.101.32.101&port=8080&tries=1'] Namespace:pod-network-test-4919 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 03:33:12.222: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:33:12.514: INFO: Waiting for responses: map[]
Aug 14 03:33:12.596: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.72:8080/dial?request=hostname&protocol=http&host=100.101.166.109&port=8080&tries=1'] Namespace:pod-network-test-4919 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 03:33:12.596: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:33:12.865: INFO: Waiting for responses: map[]
Aug 14 03:33:12.874: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.72:8080/dial?request=hostname&protocol=http&host=100.101.245.77&port=8080&tries=1'] Namespace:pod-network-test-4919 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 03:33:12.874: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:33:13.257: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:13.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4919" for this suite.

• [SLOW TEST:30.172 seconds]
[sig-network] Networking
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":275,"completed":20,"skipped":300,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:13.281: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4647
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1472
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:29.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8724" for this suite.
STEP: Destroying namespace "nsdeletetest-4647" for this suite.
Aug 14 03:33:29.980: INFO: Namespace nsdeletetest-4647 was already deleted
STEP: Destroying namespace "nsdeletetest-1472" for this suite.

• [SLOW TEST:16.718 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":275,"completed":21,"skipped":304,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:30.000: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:30.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8459" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":275,"completed":22,"skipped":310,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-e36f1406-7cf1-4488-b213-9db2a0c6eea4-5851
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:30.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9125" for this suite.
STEP: Destroying namespace "nspatchtest-e36f1406-7cf1-4488-b213-9db2a0c6eea4-5851" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":275,"completed":23,"skipped":318,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:30.718: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:33:32.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:33:34.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972812, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972812, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972812, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972812, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:33:37.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:37.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8328" for this suite.
STEP: Destroying namespace "webhook-8328-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.185 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":275,"completed":24,"skipped":335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Aug 14 03:33:42.708: INFO: Successfully updated pod "labelsupdate7d9f62e2-fce1-40a0-99ea-ba50c58eb851"
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:33:44.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6291" for this suite.

• [SLOW TEST:6.879 seconds]
[sig-storage] Downward API volume
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":25,"skipped":393,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:33:44.782: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4981
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:33:44.984: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 14 03:33:56.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4981 create -f -'
Aug 14 03:33:56.942: INFO: stderr: ""
Aug 14 03:33:56.942: INFO: stdout: "e2e-test-crd-publish-openapi-40-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 14 03:33:56.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4981 delete e2e-test-crd-publish-openapi-40-crds test-cr'
Aug 14 03:33:57.191: INFO: stderr: ""
Aug 14 03:33:57.191: INFO: stdout: "e2e-test-crd-publish-openapi-40-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 14 03:33:57.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4981 apply -f -'
Aug 14 03:33:57.804: INFO: stderr: ""
Aug 14 03:33:57.804: INFO: stdout: "e2e-test-crd-publish-openapi-40-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 14 03:33:57.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4981 delete e2e-test-crd-publish-openapi-40-crds test-cr'
Aug 14 03:33:57.979: INFO: stderr: ""
Aug 14 03:33:57.979: INFO: stdout: "e2e-test-crd-publish-openapi-40-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 14 03:33:57.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-40-crds'
Aug 14 03:33:58.454: INFO: stderr: ""
Aug 14 03:33:58.454: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-40-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:04.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4981" for this suite.

• [SLOW TEST:19.313 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":275,"completed":26,"skipped":397,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:04.096: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Aug 14 03:34:05.472: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 03:34:05.472392      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:05.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8209" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":275,"completed":27,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:05.498: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 03:34:05.716: INFO: Waiting up to 5m0s for pod "pod-8458c9e0-e495-4f14-8419-6f5d516b4666" in namespace "emptydir-472" to be "Succeeded or Failed"
Aug 14 03:34:05.724: INFO: Pod "pod-8458c9e0-e495-4f14-8419-6f5d516b4666": Phase="Pending", Reason="", readiness=false. Elapsed: 8.556801ms
Aug 14 03:34:07.734: INFO: Pod "pod-8458c9e0-e495-4f14-8419-6f5d516b4666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018116856s
STEP: Saw pod success
Aug 14 03:34:07.734: INFO: Pod "pod-8458c9e0-e495-4f14-8419-6f5d516b4666" satisfied condition "Succeeded or Failed"
Aug 14 03:34:07.743: INFO: Trying to get logs from node worker2 pod pod-8458c9e0-e495-4f14-8419-6f5d516b4666 container test-container: <nil>
STEP: delete the pod
Aug 14 03:34:07.801: INFO: Waiting for pod pod-8458c9e0-e495-4f14-8419-6f5d516b4666 to disappear
Aug 14 03:34:07.811: INFO: Pod pod-8458c9e0-e495-4f14-8419-6f5d516b4666 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:07.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-472" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":28,"skipped":421,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:07.839: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:34:09.340: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:34:11.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972849, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972849, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972849, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732972849, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:34:14.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 14 03:34:18.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 attach --namespace=webhook-3391 to-be-attached-pod -i -c=container1'
Aug 14 03:34:18.671: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:18.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3391" for this suite.
STEP: Destroying namespace "webhook-3391-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.009 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":275,"completed":29,"skipped":438,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:18.848: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3933
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-064d45e3-ae8c-4161-b11f-3e4ab628c13f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-064d45e3-ae8c-4161-b11f-3e4ab628c13f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:23.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3933" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":30,"skipped":439,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:23.312: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Aug 14 03:34:23.510: INFO: namespace kubectl-9450
Aug 14 03:34:23.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9450'
Aug 14 03:34:24.057: INFO: stderr: ""
Aug 14 03:34:24.057: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 14 03:34:25.067: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:34:25.067: INFO: Found 0 / 1
Aug 14 03:34:26.067: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:34:26.067: INFO: Found 0 / 1
Aug 14 03:34:27.068: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:34:27.068: INFO: Found 1 / 1
Aug 14 03:34:27.068: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 03:34:27.078: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:34:27.078: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 03:34:27.078: INFO: wait on agnhost-master startup in kubectl-9450 
Aug 14 03:34:27.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs agnhost-master-n7bzl agnhost-master --namespace=kubectl-9450'
Aug 14 03:34:27.293: INFO: stderr: ""
Aug 14 03:34:27.293: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 14 03:34:27.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9450'
Aug 14 03:34:27.540: INFO: stderr: ""
Aug 14 03:34:27.540: INFO: stdout: "service/rm2 exposed\n"
Aug 14 03:34:27.549: INFO: Service rm2 in namespace kubectl-9450 found.
STEP: exposing service
Aug 14 03:34:29.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9450'
Aug 14 03:34:29.767: INFO: stderr: ""
Aug 14 03:34:29.767: INFO: stdout: "service/rm3 exposed\n"
Aug 14 03:34:29.774: INFO: Service rm3 in namespace kubectl-9450 found.
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:31.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9450" for this suite.

• [SLOW TEST:8.501 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":275,"completed":31,"skipped":440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:31.814: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 03:34:32.036: INFO: Waiting up to 5m0s for pod "pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343" in namespace "emptydir-7399" to be "Succeeded or Failed"
Aug 14 03:34:32.045: INFO: Pod "pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343": Phase="Pending", Reason="", readiness=false. Elapsed: 9.483539ms
Aug 14 03:34:34.056: INFO: Pod "pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020258211s
Aug 14 03:34:36.065: INFO: Pod "pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029218688s
STEP: Saw pod success
Aug 14 03:34:36.065: INFO: Pod "pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343" satisfied condition "Succeeded or Failed"
Aug 14 03:34:36.074: INFO: Trying to get logs from node worker2 pod pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343 container test-container: <nil>
STEP: delete the pod
Aug 14 03:34:36.128: INFO: Waiting for pod pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343 to disappear
Aug 14 03:34:36.136: INFO: Pod pod-6e5fa3e5-a5aa-4d02-a3d9-4e87c7caf343 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7399" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":32,"skipped":471,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:36.163: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Aug 14 03:34:36.381: INFO: Waiting up to 5m0s for pod "client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489" in namespace "containers-4809" to be "Succeeded or Failed"
Aug 14 03:34:36.393: INFO: Pod "client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489": Phase="Pending", Reason="", readiness=false. Elapsed: 11.222895ms
Aug 14 03:34:38.402: INFO: Pod "client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021042589s
Aug 14 03:34:40.411: INFO: Pod "client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029319207s
STEP: Saw pod success
Aug 14 03:34:40.411: INFO: Pod "client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489" satisfied condition "Succeeded or Failed"
Aug 14 03:34:40.418: INFO: Trying to get logs from node worker2 pod client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489 container test-container: <nil>
STEP: delete the pod
Aug 14 03:34:40.467: INFO: Waiting for pod client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489 to disappear
Aug 14 03:34:40.475: INFO: Pod client-containers-8a9857a6-fb76-47a9-8f31-1b8540192489 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:34:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4809" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":275,"completed":33,"skipped":492,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:34:40.500: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:35:40.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8975" for this suite.

• [SLOW TEST:60.309 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":275,"completed":34,"skipped":511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:35:40.810: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 03:35:49.239: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 03:35:49.248: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 03:35:51.248: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 03:35:51.259: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 03:35:53.248: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 03:35:53.258: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 03:35:55.248: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 03:35:55.257: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 03:35:57.248: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 03:35:57.258: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:35:57.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6702" for this suite.

• [SLOW TEST:16.495 seconds]
[k8s.io] Container Lifecycle Hook
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":275,"completed":35,"skipped":551,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:35:57.305: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 14 03:35:57.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147825 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:35:57 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:35:57.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147825 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:35:57 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 14 03:36:07.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147880 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:36:07.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147880 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 14 03:36:17.559: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147919 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:36:17.559: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147919 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 14 03:36:27.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147956 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:36:27.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-a 821b1456-8104-4c94-aae0-e39457ca1a1e 4147956 0 2020-08-14 03:35:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 14 03:36:37.596: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-b cb8df64c-c1f0-4ac5-80a7-3d8030d2465c 4147990 0 2020-08-14 03:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:36:37.596: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-b cb8df64c-c1f0-4ac5-80a7-3d8030d2465c 4147990 0 2020-08-14 03:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 14 03:36:47.618: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-b cb8df64c-c1f0-4ac5-80a7-3d8030d2465c 4148022 0 2020-08-14 03:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:36:47.618: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6637 /api/v1/namespaces/watch-6637/configmaps/e2e-watch-test-configmap-b cb8df64c-c1f0-4ac5-80a7-3d8030d2465c 4148022 0 2020-08-14 03:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-14 03:36:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:36:57.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6637" for this suite.

• [SLOW TEST:60.337 seconds]
[sig-api-machinery] Watchers
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":275,"completed":36,"skipped":564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:36:57.644: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3442
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 14 03:36:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 03:37:08.509: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:37:35.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3442" for this suite.

• [SLOW TEST:38.068 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":275,"completed":37,"skipped":607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:37:35.713: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Aug 14 03:37:35.902: INFO: PodSpec: initContainers in spec.initContainers
Aug 14 03:38:25.056: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-30d408bc-d695-40b6-9b82-65c2b73007ae", GenerateName:"", Namespace:"init-container-7950", SelfLink:"/api/v1/namespaces/init-container-7950/pods/pod-init-30d408bc-d695-40b6-9b82-65c2b73007ae", UID:"1955a3d1-bc31-49d9-80aa-c6afe17a5075", ResourceVersion:"4148417", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63732973055, loc:(*time.Location)(0x68e32c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"902606569"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0x4002241660), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0x4002241680)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0x40022416a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0x40022416c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bvm2s", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0x4004fc2b40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bvm2s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bvm2s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bvm2s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x4003dc5778), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4002590690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x4003dc5840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x4003dc5880)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0x4003dc5888), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x4003dc588c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973055, loc:(*time.Location)(0x68e32c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973055, loc:(*time.Location)(0x68e32c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973055, loc:(*time.Location)(0x68e32c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973055, loc:(*time.Location)(0x68e32c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.21.97", PodIP:"100.101.245.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.101.245.83"}}, StartTime:(*v1.Time)(0x40022416e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4002590770)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x40025907e0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:669f9b20ccd59ba113166e6eb7782e87d701cb57f8b64bece7269b0a4c20d1c4", ContainerID:"docker://7c1990342bf76df8aa4329b165e27846602b905013cd688263f06fd4c7dbed16", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002241720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002241700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0x4003dc593f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:38:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7950" for this suite.

• [SLOW TEST:49.371 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":275,"completed":38,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:38:25.085: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-6f862e92-093b-4358-b984-ffcdcdebc542
STEP: Creating a pod to test consume secrets
Aug 14 03:38:25.310: INFO: Waiting up to 5m0s for pod "pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518" in namespace "secrets-2310" to be "Succeeded or Failed"
Aug 14 03:38:25.318: INFO: Pod "pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518": Phase="Pending", Reason="", readiness=false. Elapsed: 8.134962ms
Aug 14 03:38:27.327: INFO: Pod "pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017224738s
Aug 14 03:38:29.337: INFO: Pod "pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027505132s
STEP: Saw pod success
Aug 14 03:38:29.337: INFO: Pod "pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518" satisfied condition "Succeeded or Failed"
Aug 14 03:38:29.345: INFO: Trying to get logs from node worker1 pod pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:38:29.423: INFO: Waiting for pod pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518 to disappear
Aug 14 03:38:29.430: INFO: Pod pod-secrets-df414147-e8a1-475f-a292-4c50ec2d5518 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:38:29.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2310" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":39,"skipped":712,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:38:29.455: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:38:31.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:38:33.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973111, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973111, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973111, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973111, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:38:36.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:38:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6573-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:38:42.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8921" for this suite.
STEP: Destroying namespace "webhook-8921-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":275,"completed":40,"skipped":720,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:38:43.117: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Aug 14 03:38:43.368: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-133789455 proxy --unix-socket=/tmp/kubectl-proxy-unix060307270/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:38:43.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9350" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":275,"completed":41,"skipped":737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:38:43.525: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3587
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3587
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3587
Aug 14 03:38:44.161: INFO: Found 0 stateful pods, waiting for 1
Aug 14 03:38:54.193: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 14 03:38:54.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:38:54.631: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:38:54.631: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:38:54.631: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:38:54.641: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 03:39:04.652: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:39:04.652: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:39:04.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999932s
Aug 14 03:39:05.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986318212s
Aug 14 03:39:06.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975336898s
Aug 14 03:39:07.753: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965252583s
Aug 14 03:39:08.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955731786s
Aug 14 03:39:09.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.920093448s
Aug 14 03:39:10.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.907933857s
Aug 14 03:39:11.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.897928441s
Aug 14 03:39:12.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.888310504s
Aug 14 03:39:13.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 875.881674ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3587
Aug 14 03:39:14.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:39:15.393: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 03:39:15.394: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:39:15.394: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:39:15.405: INFO: Found 1 stateful pods, waiting for 3
Aug 14 03:39:25.417: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 03:39:25.417: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 03:39:25.417: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 14 03:39:25.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:39:25.924: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:39:25.924: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:39:25.924: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:39:25.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:39:26.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:39:26.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:39:26.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:39:26.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:39:27.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:39:27.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:39:27.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:39:27.217: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:39:27.225: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 14 03:39:37.245: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:39:37.245: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:39:37.245: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:39:37.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999956s
Aug 14 03:39:38.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989912324s
Aug 14 03:39:39.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980029728s
Aug 14 03:39:40.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967849397s
Aug 14 03:39:41.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956184685s
Aug 14 03:39:42.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944746033s
Aug 14 03:39:43.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934018538s
Aug 14 03:39:44.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.923038585s
Aug 14 03:39:45.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.91243531s
Aug 14 03:39:46.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 902.222235ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3587
Aug 14 03:39:47.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:39:47.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 03:39:47.807: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:39:47.807: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:39:47.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:39:48.285: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 03:39:48.285: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:39:48.285: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:39:48.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-3587 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:39:48.729: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 03:39:48.729: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:39:48.729: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:39:48.729: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 03:40:08.941: INFO: Deleting all statefulset in ns statefulset-3587
Aug 14 03:40:08.950: INFO: Scaling statefulset ss to 0
Aug 14 03:40:08.982: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:40:08.990: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3587" for this suite.

• [SLOW TEST:85.530 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":275,"completed":42,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:09.055: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 03:40:09.273: INFO: Waiting up to 5m0s for pod "pod-62d0846d-182b-408b-a874-7d6f1b05b691" in namespace "emptydir-1371" to be "Succeeded or Failed"
Aug 14 03:40:09.286: INFO: Pod "pod-62d0846d-182b-408b-a874-7d6f1b05b691": Phase="Pending", Reason="", readiness=false. Elapsed: 13.56183ms
Aug 14 03:40:11.296: INFO: Pod "pod-62d0846d-182b-408b-a874-7d6f1b05b691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023524864s
Aug 14 03:40:13.306: INFO: Pod "pod-62d0846d-182b-408b-a874-7d6f1b05b691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033883137s
STEP: Saw pod success
Aug 14 03:40:13.307: INFO: Pod "pod-62d0846d-182b-408b-a874-7d6f1b05b691" satisfied condition "Succeeded or Failed"
Aug 14 03:40:13.316: INFO: Trying to get logs from node worker2 pod pod-62d0846d-182b-408b-a874-7d6f1b05b691 container test-container: <nil>
STEP: delete the pod
Aug 14 03:40:13.450: INFO: Waiting for pod pod-62d0846d-182b-408b-a874-7d6f1b05b691 to disappear
Aug 14 03:40:13.456: INFO: Pod pod-62d0846d-182b-408b-a874-7d6f1b05b691 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:13.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1371" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":43,"skipped":782,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:13.481: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-6bd8e69f-be7b-4b10-b93d-264297ef4b12
STEP: Creating a pod to test consume configMaps
Aug 14 03:40:13.718: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d" in namespace "projected-2768" to be "Succeeded or Failed"
Aug 14 03:40:13.726: INFO: Pod "pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.002622ms
Aug 14 03:40:15.736: INFO: Pod "pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018296575s
Aug 14 03:40:17.745: INFO: Pod "pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027404191s
STEP: Saw pod success
Aug 14 03:40:17.745: INFO: Pod "pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d" satisfied condition "Succeeded or Failed"
Aug 14 03:40:17.756: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:40:17.805: INFO: Waiting for pod pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d to disappear
Aug 14 03:40:17.813: INFO: Pod pod-projected-configmaps-f377cd13-e1dd-4e48-b0c2-b53eb0b5ea3d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:17.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2768" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":44,"skipped":786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4655
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4655
I0814 03:40:18.109347      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4655, replica count: 2
Aug 14 03:40:21.160: INFO: Creating new exec pod
I0814 03:40:21.160121      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 03:40:24.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-4655 execpodk8grp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 14 03:40:24.675: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 14 03:40:24.675: INFO: stdout: ""
Aug 14 03:40:24.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-4655 execpodk8grp -- /bin/sh -x -c nc -zv -t -w 2 100.105.252.145 80'
Aug 14 03:40:25.116: INFO: stderr: "+ nc -zv -t -w 2 100.105.252.145 80\nConnection to 100.105.252.145 80 port [tcp/http] succeeded!\n"
Aug 14 03:40:25.116: INFO: stdout: ""
Aug 14 03:40:25.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-4655 execpodk8grp -- /bin/sh -x -c nc -zv -t -w 2 172.16.21.97 31768'
Aug 14 03:40:25.586: INFO: stderr: "+ nc -zv -t -w 2 172.16.21.97 31768\nConnection to 172.16.21.97 31768 port [tcp/31768] succeeded!\n"
Aug 14 03:40:25.586: INFO: stdout: ""
Aug 14 03:40:25.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-4655 execpodk8grp -- /bin/sh -x -c nc -zv -t -w 2 172.16.21.98 31768'
Aug 14 03:40:26.026: INFO: stderr: "+ nc -zv -t -w 2 172.16.21.98 31768\nConnection to 172.16.21.98 31768 port [tcp/31768] succeeded!\n"
Aug 14 03:40:26.026: INFO: stdout: ""
Aug 14 03:40:26.026: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:26.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4655" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:8.276 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":275,"completed":45,"skipped":857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Aug 14 03:40:26.311: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:30.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2001" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":275,"completed":46,"skipped":879,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:30.528: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7536" for this suite.

• [SLOW TEST:17.370 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":275,"completed":47,"skipped":882,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 14 03:40:48.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7625'
Aug 14 03:40:48.285: INFO: stderr: ""
Aug 14 03:40:48.285: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 14 03:40:53.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pod e2e-test-httpd-pod --namespace=kubectl-7625 -o json'
Aug 14 03:40:53.496: INFO: stderr: ""
Aug 14 03:40:53.497: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-08-14T03:40:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-14T03:40:48Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"100.101.245.94\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-14T03:40:50Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7625\",\n        \"resourceVersion\": \"4149573\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7625/pods/e2e-test-httpd-pod\",\n        \"uid\": \"f57bcad8-7098-441b-922c-26cd69a9bcce\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gqnd7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gqnd7\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gqnd7\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-14T03:40:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-14T03:40:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-14T03:40:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-14T03:40:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://720138a390a2aefdd21bf3d1ad4d34b5d7821d40937a1a502cfb5a9104531650\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-14T03:40:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.21.97\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.101.245.94\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.101.245.94\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-14T03:40:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 14 03:40:53.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 replace -f - --namespace=kubectl-7625'
Aug 14 03:40:54.056: INFO: stderr: ""
Aug 14 03:40:54.056: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Aug 14 03:40:54.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete pods e2e-test-httpd-pod --namespace=kubectl-7625'
Aug 14 03:40:56.405: INFO: stderr: ""
Aug 14 03:40:56.406: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:56.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7625" for this suite.

• [SLOW TEST:8.531 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":275,"completed":48,"skipped":883,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:40:56.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 version'
Aug 14 03:40:56.791: INFO: stderr: ""
Aug 14 03:40:56.791: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18+\", GitVersion:\"v1.18.2-dirty\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"dirty\", BuildDate:\"2020-08-14T03:12:20Z\", GoVersion:\"go1.15rc2\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.2\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:48:36Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:56.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4305" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":275,"completed":49,"skipped":888,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:56.897: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Aug 14 03:40:57.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 api-versions'
Aug 14 03:40:57.274: INFO: stderr: ""
Aug 14 03:40:57.274: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncke.inspur.com/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:57.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6027" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":275,"completed":50,"skipped":896,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:57.300: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:40:57.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb" in namespace "downward-api-1173" to be "Succeeded or Failed"
Aug 14 03:40:57.532: INFO: Pod "downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.957658ms
Aug 14 03:40:59.541: INFO: Pod "downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018946794s
STEP: Saw pod success
Aug 14 03:40:59.541: INFO: Pod "downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb" satisfied condition "Succeeded or Failed"
Aug 14 03:40:59.549: INFO: Trying to get logs from node worker2 pod downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb container client-container: <nil>
STEP: delete the pod
Aug 14 03:40:59.604: INFO: Waiting for pod downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb to disappear
Aug 14 03:40:59.616: INFO: Pod downwardapi-volume-8b2f4887-9c44-4680-b3f8-c91dfd5286eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:40:59.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1173" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":51,"skipped":912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:40:59.642: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-jg5t
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 03:40:59.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jg5t" in namespace "subpath-6073" to be "Succeeded or Failed"
Aug 14 03:40:59.893: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.258253ms
Aug 14 03:41:01.901: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.020402911s
Aug 14 03:41:03.911: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 4.031044284s
Aug 14 03:41:05.922: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 6.041482277s
Aug 14 03:41:07.932: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 8.051587771s
Aug 14 03:41:09.942: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 10.061523045s
Aug 14 03:41:11.952: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 12.071512239s
Aug 14 03:41:13.965: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 14.084149367s
Aug 14 03:41:15.976: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 16.095622998s
Aug 14 03:41:17.988: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 18.107228568s
Aug 14 03:41:19.997: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 20.116198045s
Aug 14 03:41:22.088: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Running", Reason="", readiness=true. Elapsed: 22.207984815s
Aug 14 03:41:24.100: INFO: Pod "pod-subpath-test-configmap-jg5t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.219240327s
STEP: Saw pod success
Aug 14 03:41:24.100: INFO: Pod "pod-subpath-test-configmap-jg5t" satisfied condition "Succeeded or Failed"
Aug 14 03:41:24.108: INFO: Trying to get logs from node worker2 pod pod-subpath-test-configmap-jg5t container test-container-subpath-configmap-jg5t: <nil>
STEP: delete the pod
Aug 14 03:41:24.156: INFO: Waiting for pod pod-subpath-test-configmap-jg5t to disappear
Aug 14 03:41:24.165: INFO: Pod pod-subpath-test-configmap-jg5t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jg5t
Aug 14 03:41:24.165: INFO: Deleting pod "pod-subpath-test-configmap-jg5t" in namespace "subpath-6073"
[AfterEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:24.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6073" for this suite.

• [SLOW TEST:24.556 seconds]
[sig-storage] Subpath
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":275,"completed":52,"skipped":938,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:24.198: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Aug 14 03:41:24.420: INFO: Waiting up to 5m0s for pod "var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b" in namespace "var-expansion-9964" to be "Succeeded or Failed"
Aug 14 03:41:24.432: INFO: Pod "var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.700954ms
Aug 14 03:41:26.496: INFO: Pod "var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076234786s
STEP: Saw pod success
Aug 14 03:41:26.496: INFO: Pod "var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b" satisfied condition "Succeeded or Failed"
Aug 14 03:41:26.505: INFO: Trying to get logs from node worker2 pod var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b container dapi-container: <nil>
STEP: delete the pod
Aug 14 03:41:26.556: INFO: Waiting for pod var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b to disappear
Aug 14 03:41:26.564: INFO: Pod var-expansion-d6627994-5d53-4331-b64f-1e6dc86d806b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9964" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":275,"completed":53,"skipped":941,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-5693
STEP: creating replication controller nodeport-test in namespace services-5693
I0814 03:41:26.828230      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5693, replica count: 2
I0814 03:41:29.878921      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 03:41:29.879: INFO: Creating new exec pod
Aug 14 03:41:32.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5693 execpodl9rtx -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 14 03:41:33.366: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 14 03:41:33.366: INFO: stdout: ""
Aug 14 03:41:33.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5693 execpodl9rtx -- /bin/sh -x -c nc -zv -t -w 2 100.105.135.64 80'
Aug 14 03:41:33.802: INFO: stderr: "+ nc -zv -t -w 2 100.105.135.64 80\nConnection to 100.105.135.64 80 port [tcp/http] succeeded!\n"
Aug 14 03:41:33.803: INFO: stdout: ""
Aug 14 03:41:33.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5693 execpodl9rtx -- /bin/sh -x -c nc -zv -t -w 2 172.16.21.93 30139'
Aug 14 03:41:34.313: INFO: stderr: "+ nc -zv -t -w 2 172.16.21.93 30139\nConnection to 172.16.21.93 30139 port [tcp/30139] succeeded!\n"
Aug 14 03:41:34.313: INFO: stdout: ""
Aug 14 03:41:34.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5693 execpodl9rtx -- /bin/sh -x -c nc -zv -t -w 2 172.16.21.99 30139'
Aug 14 03:41:34.753: INFO: stderr: "+ nc -zv -t -w 2 172.16.21.99 30139\nConnection to 172.16.21.99 30139 port [tcp/30139] succeeded!\n"
Aug 14 03:41:34.753: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:34.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5693" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:8.219 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":275,"completed":54,"skipped":948,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 03:41:38.088: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:38.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9868" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":55,"skipped":955,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:38.146: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 14 03:41:38.367: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 14 03:41:43.380: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:44.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9133" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-apps] ReplicationController
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":275,"completed":56,"skipped":957,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:44.507: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8578
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:41:44.717: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8578" for this suite.

• [SLOW TEST:6.573 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":275,"completed":57,"skipped":964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:51.081: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:41:52.970: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 14 03:41:54.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973313, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973313, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973313, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973312, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:41:58.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:41:58.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5960" for this suite.
STEP: Destroying namespace "webhook-5960-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.277 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":275,"completed":58,"skipped":996,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:41:58.358: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:42:00.652: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:42:02.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973320, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973320, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973320, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973320, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:42:05.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:42:05.721: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8257-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:11.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9071" for this suite.
STEP: Destroying namespace "webhook-9071-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":275,"completed":59,"skipped":1005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:12.100: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Aug 14 03:42:12.346: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1062" to be "Succeeded or Failed"
Aug 14 03:42:12.355: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.85498ms
Aug 14 03:42:14.365: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018882734s
Aug 14 03:42:16.375: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029740066s
STEP: Saw pod success
Aug 14 03:42:16.376: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Aug 14 03:42:16.383: INFO: Trying to get logs from node worker2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 14 03:42:16.435: INFO: Waiting for pod pod-host-path-test to disappear
Aug 14 03:42:16.444: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1062" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":60,"skipped":1028,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:16.471: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Aug 14 03:42:16.687: INFO: Waiting up to 5m0s for pod "downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92" in namespace "downward-api-1468" to be "Succeeded or Failed"
Aug 14 03:42:16.695: INFO: Pod "downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.844662ms
Aug 14 03:42:18.705: INFO: Pod "downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017840856s
Aug 14 03:42:20.716: INFO: Pod "downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028404489s
STEP: Saw pod success
Aug 14 03:42:20.716: INFO: Pod "downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92" satisfied condition "Succeeded or Failed"
Aug 14 03:42:20.726: INFO: Trying to get logs from node worker2 pod downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92 container dapi-container: <nil>
STEP: delete the pod
Aug 14 03:42:20.839: INFO: Waiting for pod downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92 to disappear
Aug 14 03:42:20.848: INFO: Pod downward-api-49480b28-d2d2-4448-9aa6-3cc5b9b0aa92 no longer exists
[AfterEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:20.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1468" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":275,"completed":61,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:42:21.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511" in namespace "downward-api-1498" to be "Succeeded or Failed"
Aug 14 03:42:21.103: INFO: Pod "downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511": Phase="Pending", Reason="", readiness=false. Elapsed: 11.774813ms
Aug 14 03:42:23.113: INFO: Pod "downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021275528s
STEP: Saw pod success
Aug 14 03:42:23.113: INFO: Pod "downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511" satisfied condition "Succeeded or Failed"
Aug 14 03:42:23.120: INFO: Trying to get logs from node worker2 pod downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511 container client-container: <nil>
STEP: delete the pod
Aug 14 03:42:23.172: INFO: Waiting for pod downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511 to disappear
Aug 14 03:42:23.179: INFO: Pod downwardapi-volume-415f788a-9ba3-4830-a5b5-d746e0f0c511 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:23.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1498" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":62,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:23.203: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-4652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:23.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4652" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":275,"completed":63,"skipped":1081,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:23.428: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:42:23.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-1770'
Aug 14 03:42:24.177: INFO: stderr: ""
Aug 14 03:42:24.178: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Aug 14 03:42:24.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-1770'
Aug 14 03:42:24.651: INFO: stderr: ""
Aug 14 03:42:24.651: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 14 03:42:25.667: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:42:25.667: INFO: Found 0 / 1
Aug 14 03:42:26.664: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:42:26.664: INFO: Found 1 / 1
Aug 14 03:42:26.664: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 03:42:26.673: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 03:42:26.673: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 03:42:26.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 describe pod agnhost-master-8k2rc --namespace=kubectl-1770'
Aug 14 03:42:26.928: INFO: stderr: ""
Aug 14 03:42:26.928: INFO: stdout: "Name:         agnhost-master-8k2rc\nNamespace:    kubectl-1770\nPriority:     0\nNode:         worker2/172.16.21.97\nStart Time:   Fri, 14 Aug 2020 03:42:24 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           100.101.245.102\nIPs:\n  IP:           100.101.245.102\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://6667b42dbb7438837138b1a69643b3e7fd105f2b3418c5f743ee2dda8df61812\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker://sha256:750232f684b5317e3e93c97d2443c81838ff822811a31cbedd7c96b1ec2c47b8\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 14 Aug 2020 03:42:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4cqw6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-4cqw6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4cqw6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-1770/agnhost-master-8k2rc to worker2\n  Normal  Pulled     1s         kubelet, worker2   Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    1s         kubelet, worker2   Created container agnhost-master\n  Normal  Started    1s         kubelet, worker2   Started container agnhost-master\n"
Aug 14 03:42:26.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 describe rc agnhost-master --namespace=kubectl-1770'
Aug 14 03:42:27.159: INFO: stderr: ""
Aug 14 03:42:27.159: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1770\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-8k2rc\n"
Aug 14 03:42:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 describe service agnhost-master --namespace=kubectl-1770'
Aug 14 03:42:27.408: INFO: stderr: ""
Aug 14 03:42:27.408: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1770\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                100.105.241.3\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.101.245.102:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 14 03:42:27.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 describe node master1'
Aug 14 03:42:27.652: INFO: stderr: ""
Aug 14 03:42:27.652: INFO: stdout: "Name:               master1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    node.beta.kubernetes.io/node-resources: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 28 Jul 2020 08:43:59 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 14 Aug 2020 03:42:22 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 04 Aug 2020 07:03:31 +0000   Tue, 04 Aug 2020 07:03:31 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 14 Aug 2020 03:41:52 +0000   Tue, 04 Aug 2020 07:03:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 14 Aug 2020 03:41:52 +0000   Tue, 04 Aug 2020 07:03:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 14 Aug 2020 03:41:52 +0000   Tue, 04 Aug 2020 07:03:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 14 Aug 2020 03:41:52 +0000   Fri, 14 Aug 2020 01:38:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.21.93\n  Hostname:    master1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      50311712Ki\n  hugepages-2Mi:          0\n  memory:                 16421212Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    7\n  ephemeral-storage:      45068832Ki\n  hugepages-2Mi:          0\n  memory:                 14896924Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 9d52b0d775c941c197e93448bf1d4445\n  System UUID:                3F0DD3B3-88FA-477A-8FFD-F840EC22D81F\n  Boot ID:                    5d55a575-ac4b-4b57-8f17-ddf7f891fb49\n  Kernel Version:             4.15.0-55-generic\n  OS Image:                   Ubuntu 18.04.3 LTS\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  docker://18.9.8\n  Kubelet Version:            v1.18.2\n  Kube-Proxy Version:         v1.18.2\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                               ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-qdxq4                  150m (2%)     300m (4%)   64M (0%)         500M (3%)      16d\n  kube-system                 cke-controller-manager-master1     100m (1%)     1 (14%)     256M (1%)        2G (13%)       9d\n  kube-system                 coredns-bwnkm                      100m (1%)     0 (0%)      70Mi (0%)        500Mi (3%)     16d\n  kube-system                 keepalived-master1                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d\n  kube-system                 kube-apiserver-master1             500m (7%)     1 (14%)     1024M (6%)       2G (13%)       124m\n  kube-system                 kube-controller-manager-master1    100m (1%)     1 (14%)     256M (1%)        2G (13%)       124m\n  kube-system                 kube-proxy-master1                 500m (7%)     500m (7%)   512M (3%)        2G (13%)       124m\n  kube-system                 kube-scheduler-master1             100m (1%)     1 (14%)     256M (1%)        2G (13%)       124m\n  kube-system                 nginx-proxy-master1                25m (0%)      500m (7%)   32M (0%)         1024M (6%)     9d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests          Limits\n  --------               --------          ------\n  cpu                    1575m (22%)       5300m (75%)\n  memory                 2473400320 (16%)  12048288k (78%)\n  ephemeral-storage      0 (0%)            0 (0%)\n  hugepages-2Mi          0 (0%)            0 (0%)\n  scheduling.k8s.io/foo  0                 0\nEvents:                  <none>\n"
Aug 14 03:42:27.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 describe namespace kubectl-1770'
Aug 14 03:42:27.850: INFO: stderr: ""
Aug 14 03:42:27.850: INFO: stdout: "Name:         kubectl-1770\nLabels:       e2e-framework=kubectl\n              e2e-run=81cbf033-856f-4948-8fb5-e94cbcdbaf67\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:27.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1770" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":275,"completed":64,"skipped":1096,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:27.913: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 03:42:28.263: INFO: Number of nodes with available pods: 0
Aug 14 03:42:28.263: INFO: Node master1 is running more than one daemon pod
Aug 14 03:42:29.295: INFO: Number of nodes with available pods: 0
Aug 14 03:42:29.295: INFO: Node master1 is running more than one daemon pod
Aug 14 03:42:30.289: INFO: Number of nodes with available pods: 0
Aug 14 03:42:30.289: INFO: Node master1 is running more than one daemon pod
Aug 14 03:42:31.288: INFO: Number of nodes with available pods: 4
Aug 14 03:42:31.288: INFO: Node master2 is running more than one daemon pod
Aug 14 03:42:32.309: INFO: Number of nodes with available pods: 5
Aug 14 03:42:32.309: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 14 03:42:32.365: INFO: Number of nodes with available pods: 4
Aug 14 03:42:32.365: INFO: Node worker1 is running more than one daemon pod
Aug 14 03:42:33.389: INFO: Number of nodes with available pods: 4
Aug 14 03:42:33.389: INFO: Node worker1 is running more than one daemon pod
Aug 14 03:42:34.393: INFO: Number of nodes with available pods: 4
Aug 14 03:42:34.393: INFO: Node worker1 is running more than one daemon pod
Aug 14 03:42:35.389: INFO: Number of nodes with available pods: 5
Aug 14 03:42:35.389: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9031, will wait for the garbage collector to delete the pods
Aug 14 03:42:35.483: INFO: Deleting DaemonSet.extensions daemon-set took: 19.280957ms
Aug 14 03:42:35.583: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.387155ms
Aug 14 03:42:46.694: INFO: Number of nodes with available pods: 0
Aug 14 03:42:46.694: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 03:42:46.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9031/daemonsets","resourceVersion":"4150871"},"items":null}

Aug 14 03:42:46.713: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9031/pods","resourceVersion":"4150871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:46.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9031" for this suite.

• [SLOW TEST:18.975 seconds]
[sig-apps] Daemon set [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":275,"completed":65,"skipped":1096,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:47.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1793" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":275,"completed":66,"skipped":1097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:47.344: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-2547/configmap-test-58bfb27a-f213-4fd7-a861-558058b5158a
STEP: Creating a pod to test consume configMaps
Aug 14 03:42:47.570: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279" in namespace "configmap-2547" to be "Succeeded or Failed"
Aug 14 03:42:47.579: INFO: Pod "pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466101ms
Aug 14 03:42:49.592: INFO: Pod "pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021747968s
Aug 14 03:42:51.602: INFO: Pod "pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031859002s
STEP: Saw pod success
Aug 14 03:42:51.602: INFO: Pod "pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279" satisfied condition "Succeeded or Failed"
Aug 14 03:42:51.611: INFO: Trying to get logs from node worker2 pod pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279 container env-test: <nil>
STEP: delete the pod
Aug 14 03:42:51.737: INFO: Waiting for pod pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279 to disappear
Aug 14 03:42:51.746: INFO: Pod pod-configmaps-6b70ac4c-ea21-451f-9c21-e0ee43c35279 no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2547" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":275,"completed":67,"skipped":1124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:51.774: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5894
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:42:51.999: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-13a7c62f-a6ac-460c-b17e-e9638e807b10" in namespace "security-context-test-5894" to be "Succeeded or Failed"
Aug 14 03:42:52.011: INFO: Pod "alpine-nnp-false-13a7c62f-a6ac-460c-b17e-e9638e807b10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.294452ms
Aug 14 03:42:54.021: INFO: Pod "alpine-nnp-false-13a7c62f-a6ac-460c-b17e-e9638e807b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022507586s
Aug 14 03:42:54.022: INFO: Pod "alpine-nnp-false-13a7c62f-a6ac-460c-b17e-e9638e807b10" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:54.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5894" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":68,"skipped":1181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:54.066: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5596
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-b9f1a44b-e435-40d8-868b-79afc1b6d891
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:42:56.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5596" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":69,"skipped":1203,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:42:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 14 03:43:01.153: INFO: Successfully updated pod "adopt-release-7hzwm"
STEP: Checking that the Job readopts the Pod
Aug 14 03:43:01.153: INFO: Waiting up to 15m0s for pod "adopt-release-7hzwm" in namespace "job-9988" to be "adopted"
Aug 14 03:43:01.163: INFO: Pod "adopt-release-7hzwm": Phase="Running", Reason="", readiness=true. Elapsed: 10.103657ms
Aug 14 03:43:03.173: INFO: Pod "adopt-release-7hzwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020256591s
Aug 14 03:43:03.173: INFO: Pod "adopt-release-7hzwm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 14 03:43:03.696: INFO: Successfully updated pod "adopt-release-7hzwm"
STEP: Checking that the Job releases the Pod
Aug 14 03:43:03.697: INFO: Waiting up to 15m0s for pod "adopt-release-7hzwm" in namespace "job-9988" to be "released"
Aug 14 03:43:03.705: INFO: Pod "adopt-release-7hzwm": Phase="Running", Reason="", readiness=true. Elapsed: 7.995702ms
Aug 14 03:43:05.712: INFO: Pod "adopt-release-7hzwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.015721382s
Aug 14 03:43:05.712: INFO: Pod "adopt-release-7hzwm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9988" for this suite.

• [SLOW TEST:9.404 seconds]
[sig-apps] Job
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":275,"completed":70,"skipped":1222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:05.805: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 14 03:43:06.088: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151178 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:06 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:43:06.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151179 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:06 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:43:06.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151180 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:06 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 14 03:43:16.218: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151244 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:43:16.218: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151245 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:43:16.219: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9747 /api/v1/namespaces/watch-9747/configmaps/e2e-watch-test-label-changed 47bc6f33-8b24-4000-9596-a74468c35bd1 4151246 0 2020-08-14 03:43:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-14 03:43:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:16.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9747" for this suite.

• [SLOW TEST:10.444 seconds]
[sig-api-machinery] Watchers
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":275,"completed":71,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:16.249: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:20.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-279" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":275,"completed":72,"skipped":1285,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:43:20.811: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5839
I0814 03:43:20.837484      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5839, replica count: 1
I0814 03:43:21.888102      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 03:43:22.888467      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 03:43:23.888782      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 03:43:24.093: INFO: Created: latency-svc-szn96
Aug 14 03:43:24.110: INFO: Got endpoints: latency-svc-szn96 [121.813786ms]
Aug 14 03:43:24.136: INFO: Created: latency-svc-nqmsc
Aug 14 03:43:24.149: INFO: Got endpoints: latency-svc-nqmsc [38.578374ms]
Aug 14 03:43:24.152: INFO: Created: latency-svc-jw9nm
Aug 14 03:43:24.165: INFO: Got endpoints: latency-svc-jw9nm [53.846659ms]
Aug 14 03:43:24.168: INFO: Created: latency-svc-vbx2c
Aug 14 03:43:24.178: INFO: Got endpoints: latency-svc-vbx2c [66.99105ms]
Aug 14 03:43:24.189: INFO: Created: latency-svc-krffb
Aug 14 03:43:24.204: INFO: Got endpoints: latency-svc-krffb [93.60669ms]
Aug 14 03:43:24.215: INFO: Created: latency-svc-pjzdh
Aug 14 03:43:24.227: INFO: Got endpoints: latency-svc-pjzdh [115.669221ms]
Aug 14 03:43:24.232: INFO: Created: latency-svc-jzj7s
Aug 14 03:43:24.244: INFO: Got endpoints: latency-svc-jzj7s [133.50868ms]
Aug 14 03:43:24.251: INFO: Created: latency-svc-zfctm
Aug 14 03:43:24.263: INFO: Got endpoints: latency-svc-zfctm [152.511478ms]
Aug 14 03:43:24.265: INFO: Created: latency-svc-zg2pb
Aug 14 03:43:24.282: INFO: Created: latency-svc-ds7mm
Aug 14 03:43:24.282: INFO: Got endpoints: latency-svc-zg2pb [171.123277ms]
Aug 14 03:43:24.293: INFO: Got endpoints: latency-svc-ds7mm [181.800332ms]
Aug 14 03:43:24.302: INFO: Created: latency-svc-482cd
Aug 14 03:43:24.319: INFO: Created: latency-svc-2989w
Aug 14 03:43:24.320: INFO: Got endpoints: latency-svc-482cd [208.543452ms]
Aug 14 03:43:24.333: INFO: Got endpoints: latency-svc-2989w [221.805283ms]
Aug 14 03:43:24.338: INFO: Created: latency-svc-xpm6f
Aug 14 03:43:24.359: INFO: Got endpoints: latency-svc-xpm6f [247.756304ms]
Aug 14 03:43:24.366: INFO: Created: latency-svc-wwsw5
Aug 14 03:43:24.378: INFO: Got endpoints: latency-svc-wwsw5 [266.815702ms]
Aug 14 03:43:24.379: INFO: Created: latency-svc-wtdsf
Aug 14 03:43:24.391: INFO: Got endpoints: latency-svc-wtdsf [279.619553ms]
Aug 14 03:43:24.393: INFO: Created: latency-svc-mvz6j
Aug 14 03:43:24.406: INFO: Got endpoints: latency-svc-mvz6j [295.097458ms]
Aug 14 03:43:24.417: INFO: Created: latency-svc-9wxvm
Aug 14 03:43:24.431: INFO: Got endpoints: latency-svc-9wxvm [281.591009ms]
Aug 14 03:43:24.432: INFO: Created: latency-svc-k7vnl
Aug 14 03:43:24.444: INFO: Got endpoints: latency-svc-k7vnl [279.276554ms]
Aug 14 03:43:24.448: INFO: Created: latency-svc-cqx4w
Aug 14 03:43:24.463: INFO: Got endpoints: latency-svc-cqx4w [285.080941ms]
Aug 14 03:43:24.467: INFO: Created: latency-svc-x69gx
Aug 14 03:43:24.486: INFO: Created: latency-svc-6x7gx
Aug 14 03:43:24.500: INFO: Created: latency-svc-wpzc7
Aug 14 03:43:24.501: INFO: Got endpoints: latency-svc-x69gx [296.038496ms]
Aug 14 03:43:24.501: INFO: Got endpoints: latency-svc-6x7gx [273.774347ms]
Aug 14 03:43:24.516: INFO: Got endpoints: latency-svc-wpzc7 [271.083433ms]
Aug 14 03:43:24.523: INFO: Created: latency-svc-hdjr9
Aug 14 03:43:24.533: INFO: Got endpoints: latency-svc-hdjr9 [269.004417ms]
Aug 14 03:43:24.538: INFO: Created: latency-svc-gznq8
Aug 14 03:43:24.551: INFO: Got endpoints: latency-svc-gznq8 [268.950117ms]
Aug 14 03:43:24.557: INFO: Created: latency-svc-vr2ck
Aug 14 03:43:24.569: INFO: Got endpoints: latency-svc-vr2ck [276.256681ms]
Aug 14 03:43:24.576: INFO: Created: latency-svc-rjhmt
Aug 14 03:43:24.599: INFO: Got endpoints: latency-svc-rjhmt [278.734035ms]
Aug 14 03:43:24.604: INFO: Created: latency-svc-8bqf8
Aug 14 03:43:24.621: INFO: Got endpoints: latency-svc-8bqf8 [288.340614ms]
Aug 14 03:43:24.625: INFO: Created: latency-svc-2jdn4
Aug 14 03:43:24.640: INFO: Got endpoints: latency-svc-2jdn4 [280.715191ms]
Aug 14 03:43:24.642: INFO: Created: latency-svc-j28vv
Aug 14 03:43:24.659: INFO: Got endpoints: latency-svc-j28vv [280.788551ms]
Aug 14 03:43:24.665: INFO: Created: latency-svc-ssq48
Aug 14 03:43:24.679: INFO: Got endpoints: latency-svc-ssq48 [288.163214ms]
Aug 14 03:43:24.682: INFO: Created: latency-svc-h5jbq
Aug 14 03:43:24.698: INFO: Got endpoints: latency-svc-h5jbq [291.227367ms]
Aug 14 03:43:24.700: INFO: Created: latency-svc-gvkqd
Aug 14 03:43:24.718: INFO: Got endpoints: latency-svc-gvkqd [287.107036ms]
Aug 14 03:43:24.722: INFO: Created: latency-svc-4hr7m
Aug 14 03:43:24.736: INFO: Got endpoints: latency-svc-4hr7m [291.918605ms]
Aug 14 03:43:24.741: INFO: Created: latency-svc-8jbrz
Aug 14 03:43:24.756: INFO: Got endpoints: latency-svc-8jbrz [293.057643ms]
Aug 14 03:43:24.761: INFO: Created: latency-svc-2l99b
Aug 14 03:43:24.776: INFO: Created: latency-svc-pbttl
Aug 14 03:43:24.776: INFO: Got endpoints: latency-svc-2l99b [274.866564ms]
Aug 14 03:43:24.795: INFO: Created: latency-svc-nq6jc
Aug 14 03:43:24.802: INFO: Got endpoints: latency-svc-pbttl [301.580884ms]
Aug 14 03:43:24.807: INFO: Got endpoints: latency-svc-nq6jc [291.212267ms]
Aug 14 03:43:24.813: INFO: Created: latency-svc-vznpd
Aug 14 03:43:24.826: INFO: Got endpoints: latency-svc-vznpd [293.738021ms]
Aug 14 03:43:24.829: INFO: Created: latency-svc-fq8q2
Aug 14 03:43:24.842: INFO: Got endpoints: latency-svc-fq8q2 [290.351769ms]
Aug 14 03:43:24.844: INFO: Created: latency-svc-kkqzv
Aug 14 03:43:24.863: INFO: Created: latency-svc-hn2lz
Aug 14 03:43:24.877: INFO: Created: latency-svc-hkwmk
Aug 14 03:43:24.896: INFO: Created: latency-svc-8bckj
Aug 14 03:43:24.901: INFO: Got endpoints: latency-svc-kkqzv [331.356458ms]
Aug 14 03:43:24.905: INFO: Got endpoints: latency-svc-hkwmk [283.523504ms]
Aug 14 03:43:24.907: INFO: Got endpoints: latency-svc-hn2lz [308.335209ms]
Aug 14 03:43:24.912: INFO: Got endpoints: latency-svc-8bckj [272.06277ms]
Aug 14 03:43:24.922: INFO: Created: latency-svc-pb5c4
Aug 14 03:43:24.935: INFO: Got endpoints: latency-svc-pb5c4 [275.747702ms]
Aug 14 03:43:24.946: INFO: Created: latency-svc-pfkrf
Aug 14 03:43:24.959: INFO: Got endpoints: latency-svc-pfkrf [279.235354ms]
Aug 14 03:43:24.967: INFO: Created: latency-svc-8h67p
Aug 14 03:43:24.987: INFO: Created: latency-svc-hptdz
Aug 14 03:43:25.002: INFO: Created: latency-svc-8fl8b
Aug 14 03:43:25.002: INFO: Got endpoints: latency-svc-8h67p [303.933058ms]
Aug 14 03:43:25.002: INFO: Got endpoints: latency-svc-hptdz [283.595004ms]
Aug 14 03:43:25.016: INFO: Got endpoints: latency-svc-8fl8b [279.551973ms]
Aug 14 03:43:25.022: INFO: Created: latency-svc-c2kxp
Aug 14 03:43:25.042: INFO: Created: latency-svc-pb5rd
Aug 14 03:43:25.059: INFO: Created: latency-svc-ldhbw
Aug 14 03:43:25.059: INFO: Got endpoints: latency-svc-c2kxp [302.725402ms]
Aug 14 03:43:25.074: INFO: Created: latency-svc-fkr6d
Aug 14 03:43:25.089: INFO: Created: latency-svc-qs6kk
Aug 14 03:43:25.109: INFO: Got endpoints: latency-svc-pb5rd [333.026973ms]
Aug 14 03:43:25.110: INFO: Created: latency-svc-2b8r8
Aug 14 03:43:25.125: INFO: Created: latency-svc-zfjts
Aug 14 03:43:25.142: INFO: Created: latency-svc-cbsl9
Aug 14 03:43:25.160: INFO: Created: latency-svc-skxh6
Aug 14 03:43:25.161: INFO: Got endpoints: latency-svc-ldhbw [358.705016ms]
Aug 14 03:43:25.176: INFO: Created: latency-svc-kcd48
Aug 14 03:43:25.191: INFO: Created: latency-svc-6z254
Aug 14 03:43:25.207: INFO: Created: latency-svc-zc5pd
Aug 14 03:43:25.209: INFO: Got endpoints: latency-svc-fkr6d [401.908139ms]
Aug 14 03:43:25.224: INFO: Created: latency-svc-25t5d
Aug 14 03:43:25.245: INFO: Created: latency-svc-m7tf9
Aug 14 03:43:25.260: INFO: Got endpoints: latency-svc-qs6kk [433.339289ms]
Aug 14 03:43:25.261: INFO: Created: latency-svc-rwrpm
Aug 14 03:43:25.278: INFO: Created: latency-svc-nq87g
Aug 14 03:43:25.294: INFO: Created: latency-svc-wbsnh
Aug 14 03:43:25.308: INFO: Got endpoints: latency-svc-2b8r8 [466.786354ms]
Aug 14 03:43:25.314: INFO: Created: latency-svc-zqgvc
Aug 14 03:43:25.332: INFO: Created: latency-svc-nrvwl
Aug 14 03:43:25.351: INFO: Created: latency-svc-827ph
Aug 14 03:43:25.363: INFO: Got endpoints: latency-svc-zfjts [461.729345ms]
Aug 14 03:43:25.370: INFO: Created: latency-svc-cm2lc
Aug 14 03:43:25.391: INFO: Created: latency-svc-f7ljg
Aug 14 03:43:25.494: INFO: Got endpoints: latency-svc-skxh6 [587.201203ms]
Aug 14 03:43:25.494: INFO: Got endpoints: latency-svc-cbsl9 [589.072779ms]
Aug 14 03:43:25.509: INFO: Got endpoints: latency-svc-kcd48 [596.737682ms]
Aug 14 03:43:25.523: INFO: Created: latency-svc-2vl4n
Aug 14 03:43:25.538: INFO: Created: latency-svc-wwbfq
Aug 14 03:43:25.555: INFO: Created: latency-svc-nhttr
Aug 14 03:43:25.558: INFO: Got endpoints: latency-svc-6z254 [623.568882ms]
Aug 14 03:43:25.581: INFO: Created: latency-svc-qjcng
Aug 14 03:43:25.611: INFO: Got endpoints: latency-svc-zc5pd [652.241318ms]
Aug 14 03:43:25.640: INFO: Created: latency-svc-868nk
Aug 14 03:43:25.666: INFO: Got endpoints: latency-svc-25t5d [664.139511ms]
Aug 14 03:43:25.693: INFO: Created: latency-svc-bfgtn
Aug 14 03:43:25.708: INFO: Got endpoints: latency-svc-m7tf9 [705.673178ms]
Aug 14 03:43:25.736: INFO: Created: latency-svc-plt9f
Aug 14 03:43:25.758: INFO: Got endpoints: latency-svc-rwrpm [742.489516ms]
Aug 14 03:43:25.782: INFO: Created: latency-svc-6fzns
Aug 14 03:43:25.808: INFO: Got endpoints: latency-svc-nq87g [748.729002ms]
Aug 14 03:43:25.835: INFO: Created: latency-svc-lc7fm
Aug 14 03:43:25.859: INFO: Got endpoints: latency-svc-wbsnh [749.58772ms]
Aug 14 03:43:25.883: INFO: Created: latency-svc-7n74d
Aug 14 03:43:25.910: INFO: Got endpoints: latency-svc-zqgvc [748.313882ms]
Aug 14 03:43:25.940: INFO: Created: latency-svc-6mcp9
Aug 14 03:43:25.958: INFO: Got endpoints: latency-svc-nrvwl [748.259043ms]
Aug 14 03:43:25.981: INFO: Created: latency-svc-zswhw
Aug 14 03:43:26.010: INFO: Got endpoints: latency-svc-827ph [749.820279ms]
Aug 14 03:43:26.035: INFO: Created: latency-svc-65x9n
Aug 14 03:43:26.057: INFO: Got endpoints: latency-svc-cm2lc [748.661721ms]
Aug 14 03:43:26.088: INFO: Created: latency-svc-wqgbb
Aug 14 03:43:26.110: INFO: Got endpoints: latency-svc-f7ljg [747.105785ms]
Aug 14 03:43:26.135: INFO: Created: latency-svc-vdpmg
Aug 14 03:43:26.158: INFO: Got endpoints: latency-svc-2vl4n [663.729952ms]
Aug 14 03:43:26.189: INFO: Created: latency-svc-rb468
Aug 14 03:43:26.208: INFO: Got endpoints: latency-svc-wwbfq [713.91036ms]
Aug 14 03:43:26.235: INFO: Created: latency-svc-t2pg8
Aug 14 03:43:26.257: INFO: Got endpoints: latency-svc-nhttr [748.646061ms]
Aug 14 03:43:26.282: INFO: Created: latency-svc-xsj57
Aug 14 03:43:26.311: INFO: Got endpoints: latency-svc-qjcng [752.005654ms]
Aug 14 03:43:26.342: INFO: Created: latency-svc-44z7h
Aug 14 03:43:26.359: INFO: Got endpoints: latency-svc-868nk [748.127363ms]
Aug 14 03:43:26.397: INFO: Created: latency-svc-b5vwv
Aug 14 03:43:26.409: INFO: Got endpoints: latency-svc-bfgtn [743.355774ms]
Aug 14 03:43:26.435: INFO: Created: latency-svc-zqt6g
Aug 14 03:43:26.461: INFO: Got endpoints: latency-svc-plt9f [753.83907ms]
Aug 14 03:43:26.487: INFO: Created: latency-svc-hqc8p
Aug 14 03:43:26.509: INFO: Got endpoints: latency-svc-6fzns [750.787197ms]
Aug 14 03:43:26.539: INFO: Created: latency-svc-kp42n
Aug 14 03:43:26.559: INFO: Got endpoints: latency-svc-lc7fm [750.683018ms]
Aug 14 03:43:26.587: INFO: Created: latency-svc-gtqws
Aug 14 03:43:26.610: INFO: Got endpoints: latency-svc-7n74d [750.893857ms]
Aug 14 03:43:26.641: INFO: Created: latency-svc-h95qg
Aug 14 03:43:26.659: INFO: Got endpoints: latency-svc-6mcp9 [749.29366ms]
Aug 14 03:43:26.684: INFO: Created: latency-svc-pjt9h
Aug 14 03:43:26.710: INFO: Got endpoints: latency-svc-zswhw [752.328674ms]
Aug 14 03:43:26.742: INFO: Created: latency-svc-hx4m8
Aug 14 03:43:26.759: INFO: Got endpoints: latency-svc-65x9n [748.943221ms]
Aug 14 03:43:26.789: INFO: Created: latency-svc-khpql
Aug 14 03:43:26.809: INFO: Got endpoints: latency-svc-wqgbb [751.287036ms]
Aug 14 03:43:26.837: INFO: Created: latency-svc-q4rq4
Aug 14 03:43:26.859: INFO: Got endpoints: latency-svc-vdpmg [748.802161ms]
Aug 14 03:43:26.886: INFO: Created: latency-svc-j577t
Aug 14 03:43:26.911: INFO: Got endpoints: latency-svc-rb468 [753.040812ms]
Aug 14 03:43:26.939: INFO: Created: latency-svc-lhpkj
Aug 14 03:43:26.959: INFO: Got endpoints: latency-svc-t2pg8 [750.207719ms]
Aug 14 03:43:26.985: INFO: Created: latency-svc-94kpr
Aug 14 03:43:27.011: INFO: Got endpoints: latency-svc-xsj57 [753.206752ms]
Aug 14 03:43:27.037: INFO: Created: latency-svc-xkffv
Aug 14 03:43:27.059: INFO: Got endpoints: latency-svc-44z7h [748.517902ms]
Aug 14 03:43:27.087: INFO: Created: latency-svc-fqsx4
Aug 14 03:43:27.110: INFO: Got endpoints: latency-svc-b5vwv [750.372898ms]
Aug 14 03:43:27.136: INFO: Created: latency-svc-4m2fn
Aug 14 03:43:27.157: INFO: Got endpoints: latency-svc-zqt6g [747.618864ms]
Aug 14 03:43:27.183: INFO: Created: latency-svc-ld9bn
Aug 14 03:43:27.209: INFO: Got endpoints: latency-svc-hqc8p [747.128605ms]
Aug 14 03:43:27.234: INFO: Created: latency-svc-tsdx2
Aug 14 03:43:27.259: INFO: Got endpoints: latency-svc-kp42n [749.44746ms]
Aug 14 03:43:27.283: INFO: Created: latency-svc-nhpzh
Aug 14 03:43:27.310: INFO: Got endpoints: latency-svc-gtqws [750.659698ms]
Aug 14 03:43:27.334: INFO: Created: latency-svc-cxz65
Aug 14 03:43:27.359: INFO: Got endpoints: latency-svc-h95qg [749.682919ms]
Aug 14 03:43:27.389: INFO: Created: latency-svc-sxdvn
Aug 14 03:43:27.408: INFO: Got endpoints: latency-svc-pjt9h [748.875221ms]
Aug 14 03:43:27.435: INFO: Created: latency-svc-cjnht
Aug 14 03:43:27.458: INFO: Got endpoints: latency-svc-hx4m8 [747.954783ms]
Aug 14 03:43:27.483: INFO: Created: latency-svc-rxz82
Aug 14 03:43:27.509: INFO: Got endpoints: latency-svc-khpql [750.199659ms]
Aug 14 03:43:27.536: INFO: Created: latency-svc-bjszn
Aug 14 03:43:27.559: INFO: Got endpoints: latency-svc-q4rq4 [749.592199ms]
Aug 14 03:43:27.582: INFO: Created: latency-svc-s69mk
Aug 14 03:43:27.608: INFO: Got endpoints: latency-svc-j577t [748.996741ms]
Aug 14 03:43:27.641: INFO: Created: latency-svc-z4csj
Aug 14 03:43:27.658: INFO: Got endpoints: latency-svc-lhpkj [747.057205ms]
Aug 14 03:43:27.683: INFO: Created: latency-svc-ds4np
Aug 14 03:43:27.707: INFO: Got endpoints: latency-svc-94kpr [748.816742ms]
Aug 14 03:43:27.735: INFO: Created: latency-svc-gj7qr
Aug 14 03:43:27.759: INFO: Got endpoints: latency-svc-xkffv [748.168683ms]
Aug 14 03:43:27.783: INFO: Created: latency-svc-lc9jm
Aug 14 03:43:27.810: INFO: Got endpoints: latency-svc-fqsx4 [750.692637ms]
Aug 14 03:43:27.836: INFO: Created: latency-svc-9sdlr
Aug 14 03:43:27.857: INFO: Got endpoints: latency-svc-4m2fn [746.940465ms]
Aug 14 03:43:27.884: INFO: Created: latency-svc-grpdk
Aug 14 03:43:27.909: INFO: Got endpoints: latency-svc-ld9bn [751.166776ms]
Aug 14 03:43:27.932: INFO: Created: latency-svc-cq4ns
Aug 14 03:43:27.960: INFO: Got endpoints: latency-svc-tsdx2 [751.390715ms]
Aug 14 03:43:27.986: INFO: Created: latency-svc-mk8qq
Aug 14 03:43:28.008: INFO: Got endpoints: latency-svc-nhpzh [748.655902ms]
Aug 14 03:43:28.036: INFO: Created: latency-svc-pqd8l
Aug 14 03:43:28.059: INFO: Got endpoints: latency-svc-cxz65 [748.933421ms]
Aug 14 03:43:28.084: INFO: Created: latency-svc-mmmrf
Aug 14 03:43:28.109: INFO: Got endpoints: latency-svc-sxdvn [749.57288ms]
Aug 14 03:43:28.136: INFO: Created: latency-svc-7kmb5
Aug 14 03:43:28.161: INFO: Got endpoints: latency-svc-cjnht [753.175492ms]
Aug 14 03:43:28.187: INFO: Created: latency-svc-fsc4m
Aug 14 03:43:28.210: INFO: Got endpoints: latency-svc-rxz82 [752.016894ms]
Aug 14 03:43:28.237: INFO: Created: latency-svc-6w8dr
Aug 14 03:43:28.258: INFO: Got endpoints: latency-svc-bjszn [749.100121ms]
Aug 14 03:43:28.285: INFO: Created: latency-svc-5czxp
Aug 14 03:43:28.307: INFO: Got endpoints: latency-svc-s69mk [748.339842ms]
Aug 14 03:43:28.334: INFO: Created: latency-svc-65ptp
Aug 14 03:43:28.358: INFO: Got endpoints: latency-svc-z4csj [750.200599ms]
Aug 14 03:43:28.384: INFO: Created: latency-svc-q4szh
Aug 14 03:43:28.409: INFO: Got endpoints: latency-svc-ds4np [750.442677ms]
Aug 14 03:43:28.434: INFO: Created: latency-svc-95lvc
Aug 14 03:43:28.462: INFO: Got endpoints: latency-svc-gj7qr [754.602568ms]
Aug 14 03:43:28.489: INFO: Created: latency-svc-96m6z
Aug 14 03:43:28.510: INFO: Got endpoints: latency-svc-lc9jm [750.651817ms]
Aug 14 03:43:28.539: INFO: Created: latency-svc-lnn7w
Aug 14 03:43:28.558: INFO: Got endpoints: latency-svc-9sdlr [748.272543ms]
Aug 14 03:43:28.587: INFO: Created: latency-svc-t9mcr
Aug 14 03:43:28.607: INFO: Got endpoints: latency-svc-grpdk [750.474998ms]
Aug 14 03:43:28.636: INFO: Created: latency-svc-2f8hg
Aug 14 03:43:28.659: INFO: Got endpoints: latency-svc-cq4ns [749.998759ms]
Aug 14 03:43:28.687: INFO: Created: latency-svc-pq7ln
Aug 14 03:43:28.711: INFO: Got endpoints: latency-svc-mk8qq [750.296338ms]
Aug 14 03:43:28.741: INFO: Created: latency-svc-wtzwk
Aug 14 03:43:28.758: INFO: Got endpoints: latency-svc-pqd8l [749.976879ms]
Aug 14 03:43:28.781: INFO: Created: latency-svc-vvn7v
Aug 14 03:43:28.808: INFO: Got endpoints: latency-svc-mmmrf [749.42518ms]
Aug 14 03:43:28.834: INFO: Created: latency-svc-75q8g
Aug 14 03:43:28.859: INFO: Got endpoints: latency-svc-7kmb5 [749.7454ms]
Aug 14 03:43:28.890: INFO: Created: latency-svc-zsmjl
Aug 14 03:43:28.912: INFO: Got endpoints: latency-svc-fsc4m [750.473318ms]
Aug 14 03:43:28.940: INFO: Created: latency-svc-b2jvg
Aug 14 03:43:28.959: INFO: Got endpoints: latency-svc-6w8dr [748.725022ms]
Aug 14 03:43:28.985: INFO: Created: latency-svc-fn2s6
Aug 14 03:43:29.010: INFO: Got endpoints: latency-svc-5czxp [751.121796ms]
Aug 14 03:43:29.036: INFO: Created: latency-svc-t972w
Aug 14 03:43:29.060: INFO: Got endpoints: latency-svc-65ptp [752.869952ms]
Aug 14 03:43:29.085: INFO: Created: latency-svc-cqts8
Aug 14 03:43:29.108: INFO: Got endpoints: latency-svc-q4szh [749.72256ms]
Aug 14 03:43:29.133: INFO: Created: latency-svc-bkw5m
Aug 14 03:43:29.156: INFO: Got endpoints: latency-svc-95lvc [747.485725ms]
Aug 14 03:43:29.182: INFO: Created: latency-svc-rlb7c
Aug 14 03:43:29.210: INFO: Got endpoints: latency-svc-96m6z [747.819063ms]
Aug 14 03:43:29.236: INFO: Created: latency-svc-gjh56
Aug 14 03:43:29.260: INFO: Got endpoints: latency-svc-lnn7w [750.007519ms]
Aug 14 03:43:29.287: INFO: Created: latency-svc-zmj4s
Aug 14 03:43:29.317: INFO: Got endpoints: latency-svc-t9mcr [757.983081ms]
Aug 14 03:43:29.344: INFO: Created: latency-svc-j2gsn
Aug 14 03:43:29.360: INFO: Got endpoints: latency-svc-2f8hg [752.354454ms]
Aug 14 03:43:29.386: INFO: Created: latency-svc-bf6kd
Aug 14 03:43:29.411: INFO: Got endpoints: latency-svc-pq7ln [751.869874ms]
Aug 14 03:43:29.436: INFO: Created: latency-svc-zs284
Aug 14 03:43:29.459: INFO: Got endpoints: latency-svc-wtzwk [748.027703ms]
Aug 14 03:43:29.493: INFO: Created: latency-svc-jwvhc
Aug 14 03:43:29.510: INFO: Got endpoints: latency-svc-vvn7v [751.879615ms]
Aug 14 03:43:29.534: INFO: Created: latency-svc-9cxlr
Aug 14 03:43:29.561: INFO: Got endpoints: latency-svc-75q8g [752.497093ms]
Aug 14 03:43:29.587: INFO: Created: latency-svc-tjbdz
Aug 14 03:43:29.608: INFO: Got endpoints: latency-svc-zsmjl [748.967061ms]
Aug 14 03:43:29.635: INFO: Created: latency-svc-ldzxs
Aug 14 03:43:29.669: INFO: Got endpoints: latency-svc-b2jvg [756.772563ms]
Aug 14 03:43:29.695: INFO: Created: latency-svc-fhqhz
Aug 14 03:43:29.708: INFO: Got endpoints: latency-svc-fn2s6 [748.318322ms]
Aug 14 03:43:29.733: INFO: Created: latency-svc-c7gj8
Aug 14 03:43:29.760: INFO: Got endpoints: latency-svc-t972w [749.997439ms]
Aug 14 03:43:29.787: INFO: Created: latency-svc-jlrxd
Aug 14 03:43:29.808: INFO: Got endpoints: latency-svc-cqts8 [747.301625ms]
Aug 14 03:43:29.835: INFO: Created: latency-svc-s2dc6
Aug 14 03:43:29.859: INFO: Got endpoints: latency-svc-bkw5m [750.563618ms]
Aug 14 03:43:29.883: INFO: Created: latency-svc-ffx4j
Aug 14 03:43:29.910: INFO: Got endpoints: latency-svc-rlb7c [752.940412ms]
Aug 14 03:43:29.938: INFO: Created: latency-svc-hffvg
Aug 14 03:43:29.961: INFO: Got endpoints: latency-svc-gjh56 [750.413758ms]
Aug 14 03:43:29.988: INFO: Created: latency-svc-2gl6v
Aug 14 03:43:30.011: INFO: Got endpoints: latency-svc-zmj4s [751.271656ms]
Aug 14 03:43:30.036: INFO: Created: latency-svc-j6f2p
Aug 14 03:43:30.058: INFO: Got endpoints: latency-svc-j2gsn [741.444658ms]
Aug 14 03:43:30.084: INFO: Created: latency-svc-wkp8d
Aug 14 03:43:30.108: INFO: Got endpoints: latency-svc-bf6kd [748.558782ms]
Aug 14 03:43:30.135: INFO: Created: latency-svc-pgkhf
Aug 14 03:43:30.159: INFO: Got endpoints: latency-svc-zs284 [748.197003ms]
Aug 14 03:43:30.182: INFO: Created: latency-svc-pq24w
Aug 14 03:43:30.208: INFO: Got endpoints: latency-svc-jwvhc [749.547719ms]
Aug 14 03:43:30.235: INFO: Created: latency-svc-7rk7p
Aug 14 03:43:30.258: INFO: Got endpoints: latency-svc-9cxlr [748.109703ms]
Aug 14 03:43:30.285: INFO: Created: latency-svc-mbhxk
Aug 14 03:43:30.308: INFO: Got endpoints: latency-svc-tjbdz [747.057505ms]
Aug 14 03:43:30.332: INFO: Created: latency-svc-mlc2c
Aug 14 03:43:30.359: INFO: Got endpoints: latency-svc-ldzxs [749.51518ms]
Aug 14 03:43:30.383: INFO: Created: latency-svc-6sl47
Aug 14 03:43:30.412: INFO: Got endpoints: latency-svc-fhqhz [743.352994ms]
Aug 14 03:43:30.436: INFO: Created: latency-svc-z9rcm
Aug 14 03:43:30.457: INFO: Got endpoints: latency-svc-c7gj8 [749.35388ms]
Aug 14 03:43:30.484: INFO: Created: latency-svc-b6hb6
Aug 14 03:43:30.508: INFO: Got endpoints: latency-svc-jlrxd [747.950083ms]
Aug 14 03:43:30.532: INFO: Created: latency-svc-lpsvl
Aug 14 03:43:30.558: INFO: Got endpoints: latency-svc-s2dc6 [750.697277ms]
Aug 14 03:43:30.583: INFO: Created: latency-svc-qkjb7
Aug 14 03:43:30.613: INFO: Got endpoints: latency-svc-ffx4j [754.08631ms]
Aug 14 03:43:30.640: INFO: Created: latency-svc-l5j7l
Aug 14 03:43:30.659: INFO: Got endpoints: latency-svc-hffvg [749.48242ms]
Aug 14 03:43:30.686: INFO: Created: latency-svc-ksrnm
Aug 14 03:43:30.720: INFO: Got endpoints: latency-svc-2gl6v [758.863879ms]
Aug 14 03:43:30.747: INFO: Created: latency-svc-hd7rh
Aug 14 03:43:30.760: INFO: Got endpoints: latency-svc-j6f2p [748.234443ms]
Aug 14 03:43:30.786: INFO: Created: latency-svc-hdrsd
Aug 14 03:43:30.809: INFO: Got endpoints: latency-svc-wkp8d [750.201258ms]
Aug 14 03:43:30.832: INFO: Created: latency-svc-b79hc
Aug 14 03:43:30.860: INFO: Got endpoints: latency-svc-pgkhf [751.182896ms]
Aug 14 03:43:30.884: INFO: Created: latency-svc-rchjv
Aug 14 03:43:30.909: INFO: Got endpoints: latency-svc-pq24w [749.889559ms]
Aug 14 03:43:30.937: INFO: Created: latency-svc-qctnh
Aug 14 03:43:30.959: INFO: Got endpoints: latency-svc-7rk7p [750.238758ms]
Aug 14 03:43:30.982: INFO: Created: latency-svc-gkd6x
Aug 14 03:43:31.010: INFO: Got endpoints: latency-svc-mbhxk [751.856335ms]
Aug 14 03:43:31.034: INFO: Created: latency-svc-8dxtv
Aug 14 03:43:31.059: INFO: Got endpoints: latency-svc-mlc2c [751.035516ms]
Aug 14 03:43:31.086: INFO: Created: latency-svc-vdndq
Aug 14 03:43:31.110: INFO: Got endpoints: latency-svc-6sl47 [750.972237ms]
Aug 14 03:43:31.135: INFO: Created: latency-svc-hhxv6
Aug 14 03:43:31.159: INFO: Got endpoints: latency-svc-z9rcm [746.956965ms]
Aug 14 03:43:31.184: INFO: Created: latency-svc-brgjx
Aug 14 03:43:31.209: INFO: Got endpoints: latency-svc-b6hb6 [752.148394ms]
Aug 14 03:43:31.236: INFO: Created: latency-svc-6xwwx
Aug 14 03:43:31.260: INFO: Got endpoints: latency-svc-lpsvl [751.875274ms]
Aug 14 03:43:31.297: INFO: Created: latency-svc-xl6j2
Aug 14 03:43:31.311: INFO: Got endpoints: latency-svc-qkjb7 [752.875732ms]
Aug 14 03:43:31.341: INFO: Created: latency-svc-6zxbp
Aug 14 03:43:31.358: INFO: Got endpoints: latency-svc-l5j7l [744.458371ms]
Aug 14 03:43:31.387: INFO: Created: latency-svc-wpxg7
Aug 14 03:43:31.412: INFO: Got endpoints: latency-svc-ksrnm [752.721052ms]
Aug 14 03:43:31.437: INFO: Created: latency-svc-rkvbh
Aug 14 03:43:31.458: INFO: Got endpoints: latency-svc-hd7rh [738.498065ms]
Aug 14 03:43:31.483: INFO: Created: latency-svc-stjb7
Aug 14 03:43:31.515: INFO: Got endpoints: latency-svc-hdrsd [755.098528ms]
Aug 14 03:43:31.542: INFO: Created: latency-svc-6h56c
Aug 14 03:43:31.559: INFO: Got endpoints: latency-svc-b79hc [750.414197ms]
Aug 14 03:43:31.587: INFO: Created: latency-svc-grkhk
Aug 14 03:43:31.609: INFO: Got endpoints: latency-svc-rchjv [749.17862ms]
Aug 14 03:43:31.634: INFO: Created: latency-svc-s8t99
Aug 14 03:43:31.657: INFO: Got endpoints: latency-svc-qctnh [747.666063ms]
Aug 14 03:43:31.681: INFO: Created: latency-svc-n8slt
Aug 14 03:43:31.710: INFO: Got endpoints: latency-svc-gkd6x [751.245376ms]
Aug 14 03:43:31.734: INFO: Created: latency-svc-p8pqs
Aug 14 03:43:31.759: INFO: Got endpoints: latency-svc-8dxtv [749.4317ms]
Aug 14 03:43:31.784: INFO: Created: latency-svc-vsf48
Aug 14 03:43:31.809: INFO: Got endpoints: latency-svc-vdndq [749.38586ms]
Aug 14 03:43:31.836: INFO: Created: latency-svc-4cctc
Aug 14 03:43:31.858: INFO: Got endpoints: latency-svc-hhxv6 [748.239602ms]
Aug 14 03:43:31.885: INFO: Created: latency-svc-hs6kp
Aug 14 03:43:31.908: INFO: Got endpoints: latency-svc-brgjx [749.24918ms]
Aug 14 03:43:31.934: INFO: Created: latency-svc-vf7s7
Aug 14 03:43:31.957: INFO: Got endpoints: latency-svc-6xwwx [747.808104ms]
Aug 14 03:43:32.009: INFO: Got endpoints: latency-svc-xl6j2 [748.606902ms]
Aug 14 03:43:32.059: INFO: Got endpoints: latency-svc-6zxbp [747.334304ms]
Aug 14 03:43:32.110: INFO: Got endpoints: latency-svc-wpxg7 [751.779835ms]
Aug 14 03:43:32.159: INFO: Got endpoints: latency-svc-rkvbh [747.171145ms]
Aug 14 03:43:32.208: INFO: Got endpoints: latency-svc-stjb7 [749.909698ms]
Aug 14 03:43:32.260: INFO: Got endpoints: latency-svc-6h56c [745.21019ms]
Aug 14 03:43:32.308: INFO: Got endpoints: latency-svc-grkhk [748.891921ms]
Aug 14 03:43:32.359: INFO: Got endpoints: latency-svc-s8t99 [749.781139ms]
Aug 14 03:43:32.408: INFO: Got endpoints: latency-svc-n8slt [751.494076ms]
Aug 14 03:43:32.463: INFO: Got endpoints: latency-svc-p8pqs [752.533433ms]
Aug 14 03:43:32.510: INFO: Got endpoints: latency-svc-vsf48 [750.093339ms]
Aug 14 03:43:32.560: INFO: Got endpoints: latency-svc-4cctc [750.876377ms]
Aug 14 03:43:32.607: INFO: Got endpoints: latency-svc-hs6kp [749.202521ms]
Aug 14 03:43:32.658: INFO: Got endpoints: latency-svc-vf7s7 [749.180841ms]
Aug 14 03:43:32.658: INFO: Latencies: [38.578374ms 53.846659ms 66.99105ms 93.60669ms 115.669221ms 133.50868ms 152.511478ms 171.123277ms 181.800332ms 208.543452ms 221.805283ms 247.756304ms 266.815702ms 268.950117ms 269.004417ms 271.083433ms 272.06277ms 273.774347ms 274.866564ms 275.747702ms 276.256681ms 278.734035ms 279.235354ms 279.276554ms 279.551973ms 279.619553ms 280.715191ms 280.788551ms 281.591009ms 283.523504ms 283.595004ms 285.080941ms 287.107036ms 288.163214ms 288.340614ms 290.351769ms 291.212267ms 291.227367ms 291.918605ms 293.057643ms 293.738021ms 295.097458ms 296.038496ms 301.580884ms 302.725402ms 303.933058ms 308.335209ms 331.356458ms 333.026973ms 358.705016ms 401.908139ms 433.339289ms 461.729345ms 466.786354ms 587.201203ms 589.072779ms 596.737682ms 623.568882ms 652.241318ms 663.729952ms 664.139511ms 705.673178ms 713.91036ms 738.498065ms 741.444658ms 742.489516ms 743.352994ms 743.355774ms 744.458371ms 745.21019ms 746.940465ms 746.956965ms 747.057205ms 747.057505ms 747.105785ms 747.128605ms 747.171145ms 747.301625ms 747.334304ms 747.485725ms 747.618864ms 747.666063ms 747.808104ms 747.819063ms 747.950083ms 747.954783ms 748.027703ms 748.109703ms 748.127363ms 748.168683ms 748.197003ms 748.234443ms 748.239602ms 748.259043ms 748.272543ms 748.313882ms 748.318322ms 748.339842ms 748.517902ms 748.558782ms 748.606902ms 748.646061ms 748.655902ms 748.661721ms 748.725022ms 748.729002ms 748.802161ms 748.816742ms 748.875221ms 748.891921ms 748.933421ms 748.943221ms 748.967061ms 748.996741ms 749.100121ms 749.17862ms 749.180841ms 749.202521ms 749.24918ms 749.29366ms 749.35388ms 749.38586ms 749.42518ms 749.4317ms 749.44746ms 749.48242ms 749.51518ms 749.547719ms 749.57288ms 749.58772ms 749.592199ms 749.682919ms 749.72256ms 749.7454ms 749.781139ms 749.820279ms 749.889559ms 749.909698ms 749.976879ms 749.997439ms 749.998759ms 750.007519ms 750.093339ms 750.199659ms 750.200599ms 750.201258ms 750.207719ms 750.238758ms 750.296338ms 750.372898ms 750.413758ms 750.414197ms 750.442677ms 750.473318ms 750.474998ms 750.563618ms 750.651817ms 750.659698ms 750.683018ms 750.692637ms 750.697277ms 750.787197ms 750.876377ms 750.893857ms 750.972237ms 751.035516ms 751.121796ms 751.166776ms 751.182896ms 751.245376ms 751.271656ms 751.287036ms 751.390715ms 751.494076ms 751.779835ms 751.856335ms 751.869874ms 751.875274ms 751.879615ms 752.005654ms 752.016894ms 752.148394ms 752.328674ms 752.354454ms 752.497093ms 752.533433ms 752.721052ms 752.869952ms 752.875732ms 752.940412ms 753.040812ms 753.175492ms 753.206752ms 753.83907ms 754.08631ms 754.602568ms 755.098528ms 756.772563ms 757.983081ms 758.863879ms]
Aug 14 03:43:32.658: INFO: 50 %ile: 748.606902ms
Aug 14 03:43:32.658: INFO: 90 %ile: 752.016894ms
Aug 14 03:43:32.658: INFO: 99 %ile: 757.983081ms
Aug 14 03:43:32.658: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:32.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5839" for this suite.

• [SLOW TEST:12.074 seconds]
[sig-network] Service endpoints latency
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":275,"completed":73,"skipped":1297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:32.690: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:36.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-85" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":74,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:37.001: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-6ca406a8-ac4f-41e7-aeb3-219b205ed579
STEP: Creating a pod to test consume secrets
Aug 14 03:43:37.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264" in namespace "projected-6691" to be "Succeeded or Failed"
Aug 14 03:43:37.307: INFO: Pod "pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023377ms
Aug 14 03:43:39.318: INFO: Pod "pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02091457s
Aug 14 03:43:41.329: INFO: Pod "pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031529582s
STEP: Saw pod success
Aug 14 03:43:41.329: INFO: Pod "pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264" satisfied condition "Succeeded or Failed"
Aug 14 03:43:41.339: INFO: Trying to get logs from node worker1 pod pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:43:41.423: INFO: Waiting for pod pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264 to disappear
Aug 14 03:43:41.433: INFO: Pod pod-projected-secrets-61e31a71-c9d1-4747-a0a8-47a40d2b2264 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:41.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6691" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":75,"skipped":1341,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:41.457: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:43:43.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:43:45.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 03:43:47.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732973423, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:43:50.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:51.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:52.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:53.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:54.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:55.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:56.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 14 03:43:57.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:43:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6802" for this suite.
STEP: Destroying namespace "webhook-6802-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":275,"completed":76,"skipped":1342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:43:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-d76cd4e9-347c-45ec-8536-c430303ba2ec
STEP: Creating a pod to test consume secrets
Aug 14 03:43:59.100: INFO: Waiting up to 5m0s for pod "pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c" in namespace "secrets-4262" to be "Succeeded or Failed"
Aug 14 03:43:59.113: INFO: Pod "pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.711151ms
Aug 14 03:44:01.124: INFO: Pod "pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023958782s
STEP: Saw pod success
Aug 14 03:44:01.124: INFO: Pod "pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c" satisfied condition "Succeeded or Failed"
Aug 14 03:44:01.134: INFO: Trying to get logs from node worker2 pod pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:44:01.188: INFO: Waiting for pod pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c to disappear
Aug 14 03:44:01.196: INFO: Pod pod-secrets-c1fdf174-200b-4d3f-90e6-bad63833396c no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:44:01.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4262" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":77,"skipped":1404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:44:01.221: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-fb235b41-0651-4f20-8b52-62f0a39980cf
STEP: Creating a pod to test consume configMaps
Aug 14 03:44:01.443: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06" in namespace "configmap-4222" to be "Succeeded or Failed"
Aug 14 03:44:01.452: INFO: Pod "pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06": Phase="Pending", Reason="", readiness=false. Elapsed: 9.162259ms
Aug 14 03:44:03.460: INFO: Pod "pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017538857s
Aug 14 03:44:05.494: INFO: Pod "pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051359838s
STEP: Saw pod success
Aug 14 03:44:05.494: INFO: Pod "pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06" satisfied condition "Succeeded or Failed"
Aug 14 03:44:05.503: INFO: Trying to get logs from node worker2 pod pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:44:05.551: INFO: Waiting for pod pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06 to disappear
Aug 14 03:44:05.562: INFO: Pod pod-configmaps-4e510fe2-5fe5-45e1-a107-196e00ce6e06 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:44:05.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4222" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":78,"skipped":1444,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:44:05.587: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-4943fe77-2199-4a7c-a1a1-839101d6e416
STEP: Creating a pod to test consume secrets
Aug 14 03:44:05.814: INFO: Waiting up to 5m0s for pod "pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1" in namespace "secrets-7425" to be "Succeeded or Failed"
Aug 14 03:44:05.888: INFO: Pod "pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 74.393713ms
Aug 14 03:44:07.898: INFO: Pod "pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084175487s
Aug 14 03:44:09.909: INFO: Pod "pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095168699s
STEP: Saw pod success
Aug 14 03:44:09.909: INFO: Pod "pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1" satisfied condition "Succeeded or Failed"
Aug 14 03:44:09.917: INFO: Trying to get logs from node worker1 pod pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:44:09.967: INFO: Waiting for pod pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1 to disappear
Aug 14 03:44:09.974: INFO: Pod pod-secrets-0cb4b362-8ec1-4a5b-83f7-93c013bf9fb1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:44:09.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7425" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":79,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:44:10.000: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Aug 14 03:44:10.195: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Aug 14 03:44:10.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:10.924: INFO: stderr: ""
Aug 14 03:44:10.924: INFO: stdout: "service/agnhost-slave created\n"
Aug 14 03:44:10.925: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Aug 14 03:44:10.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:11.527: INFO: stderr: ""
Aug 14 03:44:11.527: INFO: stdout: "service/agnhost-master created\n"
Aug 14 03:44:11.528: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 14 03:44:11.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:11.989: INFO: stderr: ""
Aug 14 03:44:11.989: INFO: stdout: "service/frontend created\n"
Aug 14 03:44:11.989: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 14 03:44:11.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:12.707: INFO: stderr: ""
Aug 14 03:44:12.707: INFO: stdout: "deployment.apps/frontend created\n"
Aug 14 03:44:12.708: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 14 03:44:12.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:13.298: INFO: stderr: ""
Aug 14 03:44:13.298: INFO: stdout: "deployment.apps/agnhost-master created\n"
Aug 14 03:44:13.299: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 14 03:44:13.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-9018'
Aug 14 03:44:13.926: INFO: stderr: ""
Aug 14 03:44:13.926: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Aug 14 03:44:13.926: INFO: Waiting for all frontend pods to be Running.
Aug 14 03:44:18.977: INFO: Waiting for frontend to serve content.
Aug 14 03:44:19.002: INFO: Trying to add a new entry to the guestbook.
Aug 14 03:44:19.022: INFO: Verifying that added entry can be retrieved.
Aug 14 03:44:19.146: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Aug 14 03:44:24.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:24.432: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:24.432: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 03:44:24.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:24.662: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:24.662: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 03:44:24.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:24.889: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:24.889: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 03:44:24.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:25.062: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:25.063: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 03:44:25.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:25.239: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:25.239: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 03:44:25.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-9018'
Aug 14 03:44:25.452: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 03:44:25.452: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:44:25.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9018" for this suite.

• [SLOW TEST:15.476 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":275,"completed":80,"skipped":1464,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:44:25.476: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5438
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-5438
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5438
Aug 14 03:44:25.713: INFO: Found 0 stateful pods, waiting for 1
Aug 14 03:44:35.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 14 03:44:35.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:44:36.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:44:36.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:44:36.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:44:36.274: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 03:44:46.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:44:46.285: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:44:46.406: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:44:46.406: INFO: ss-0  worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  }]
Aug 14 03:44:46.406: INFO: 
Aug 14 03:44:46.406: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 14 03:44:47.419: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988457427s
Aug 14 03:44:48.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975510018s
Aug 14 03:44:49.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963856646s
Aug 14 03:44:50.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953074752s
Aug 14 03:44:51.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.899639753s
Aug 14 03:44:52.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.888314941s
Aug 14 03:44:53.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.875022652s
Aug 14 03:44:54.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.863879059s
Aug 14 03:44:55.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 805.266212ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5438
Aug 14 03:44:56.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:44:57.115: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 03:44:57.115: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:44:57.115: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:44:57.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:44:57.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 03:44:57.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:44:57.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:44:57.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:44:58.167: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 03:44:58.167: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 03:44:58.167: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 03:44:58.195: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 03:44:58.195: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 03:44:58.195: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 14 03:44:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:44:58.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:44:58.654: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:44:58.654: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:44:58.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:44:59.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:44:59.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:44:59.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:44:59.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 03:44:59.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 03:44:59.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 03:44:59.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 03:44:59.802: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:44:59.811: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 14 03:45:09.898: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:45:09.898: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:45:09.898: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 03:45:09.929: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:09.929: INFO: ss-0  worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  }]
Aug 14 03:45:09.929: INFO: ss-1  worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:09.929: INFO: ss-2  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:09.929: INFO: 
Aug 14 03:45:09.929: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 03:45:10.942: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:10.942: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  }]
Aug 14 03:45:10.942: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:10.942: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:10.942: INFO: 
Aug 14 03:45:10.942: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 03:45:11.994: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:11.994: INFO: ss-0  worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:25 +0000 UTC  }]
Aug 14 03:45:11.994: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:11.994: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:11.994: INFO: 
Aug 14 03:45:11.994: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 03:45:13.007: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:13.007: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:13.007: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:13.007: INFO: 
Aug 14 03:45:13.007: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 03:45:14.019: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:14.019: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:14.019: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:14.019: INFO: 
Aug 14 03:45:14.019: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 03:45:15.031: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:15.031: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:15.031: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:15.031: INFO: 
Aug 14 03:45:15.031: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 03:45:16.094: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:16.095: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:16.095: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:45:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:16.095: INFO: 
Aug 14 03:45:16.095: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 03:45:17.105: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:17.105: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:17.105: INFO: 
Aug 14 03:45:17.105: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 14 03:45:18.115: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:18.115: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:18.115: INFO: 
Aug 14 03:45:18.115: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 14 03:45:19.126: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Aug 14 03:45:19.126: INFO: ss-1  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-14 03:44:46 +0000 UTC  }]
Aug 14 03:45:19.126: INFO: 
Aug 14 03:45:19.126: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5438
Aug 14 03:45:20.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:45:20.436: INFO: rc: 1
Aug 14 03:45:20.436: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Aug 14 03:45:30.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:45:30.597: INFO: rc: 1
Aug 14 03:45:30.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:45:40.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:45:40.759: INFO: rc: 1
Aug 14 03:45:40.759: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:45:50.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:45:50.932: INFO: rc: 1
Aug 14 03:45:50.932: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:00.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:01.116: INFO: rc: 1
Aug 14 03:46:01.116: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:11.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:11.287: INFO: rc: 1
Aug 14 03:46:11.287: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:21.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:21.469: INFO: rc: 1
Aug 14 03:46:21.469: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:31.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:31.660: INFO: rc: 1
Aug 14 03:46:31.660: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:41.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:41.828: INFO: rc: 1
Aug 14 03:46:41.828: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:46:51.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:46:52.005: INFO: rc: 1
Aug 14 03:46:52.005: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:02.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:02.174: INFO: rc: 1
Aug 14 03:47:02.174: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:12.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:12.334: INFO: rc: 1
Aug 14 03:47:12.334: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:22.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:22.506: INFO: rc: 1
Aug 14 03:47:22.506: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:32.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:32.678: INFO: rc: 1
Aug 14 03:47:32.678: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:42.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:42.869: INFO: rc: 1
Aug 14 03:47:42.869: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:47:52.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:47:53.048: INFO: rc: 1
Aug 14 03:47:53.048: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:03.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:03.223: INFO: rc: 1
Aug 14 03:48:03.223: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:13.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:13.408: INFO: rc: 1
Aug 14 03:48:13.408: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:23.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:23.578: INFO: rc: 1
Aug 14 03:48:23.578: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:33.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:33.748: INFO: rc: 1
Aug 14 03:48:33.748: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:43.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:43.916: INFO: rc: 1
Aug 14 03:48:43.916: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:48:53.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:48:54.078: INFO: rc: 1
Aug 14 03:48:54.078: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:04.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:04.302: INFO: rc: 1
Aug 14 03:49:04.302: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:14.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:14.475: INFO: rc: 1
Aug 14 03:49:14.475: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:24.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:24.643: INFO: rc: 1
Aug 14 03:49:24.643: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:34.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:34.809: INFO: rc: 1
Aug 14 03:49:34.809: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:44.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:44.970: INFO: rc: 1
Aug 14 03:49:44.970: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:49:54.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:49:55.144: INFO: rc: 1
Aug 14 03:49:55.144: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:50:05.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:50:05.321: INFO: rc: 1
Aug 14 03:50:05.321: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:50:15.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:50:15.510: INFO: rc: 1
Aug 14 03:50:15.510: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 03:50:25.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-5438 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 03:50:25.692: INFO: rc: 1
Aug 14 03:50:25.692: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Aug 14 03:50:25.692: INFO: Scaling statefulset ss to 0
Aug 14 03:50:25.723: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 03:50:25.732: INFO: Deleting all statefulset in ns statefulset-5438
Aug 14 03:50:25.740: INFO: Scaling statefulset ss to 0
Aug 14 03:50:25.766: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 03:50:25.774: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5438" for this suite.

• [SLOW TEST:360.357 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":275,"completed":81,"skipped":1473,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:25.833: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3284
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 14 03:50:26.049: INFO: Waiting up to 5m0s for pod "pod-89116f3b-d63c-4c25-b9c4-6af505bcf611" in namespace "emptydir-3284" to be "Succeeded or Failed"
Aug 14 03:50:26.057: INFO: Pod "pod-89116f3b-d63c-4c25-b9c4-6af505bcf611": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082961ms
Aug 14 03:50:28.066: INFO: Pod "pod-89116f3b-d63c-4c25-b9c4-6af505bcf611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017089679s
STEP: Saw pod success
Aug 14 03:50:28.066: INFO: Pod "pod-89116f3b-d63c-4c25-b9c4-6af505bcf611" satisfied condition "Succeeded or Failed"
Aug 14 03:50:28.075: INFO: Trying to get logs from node worker2 pod pod-89116f3b-d63c-4c25-b9c4-6af505bcf611 container test-container: <nil>
STEP: delete the pod
Aug 14 03:50:28.153: INFO: Waiting for pod pod-89116f3b-d63c-4c25-b9c4-6af505bcf611 to disappear
Aug 14 03:50:28.162: INFO: Pod pod-89116f3b-d63c-4c25-b9c4-6af505bcf611 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:28.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3284" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":82,"skipped":1490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:28.189: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-14228be7-5ea6-4847-b5be-a9b195ddf561
STEP: Creating a pod to test consume configMaps
Aug 14 03:50:28.423: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23" in namespace "projected-1002" to be "Succeeded or Failed"
Aug 14 03:50:28.432: INFO: Pod "pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23": Phase="Pending", Reason="", readiness=false. Elapsed: 8.909659ms
Aug 14 03:50:30.441: INFO: Pod "pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018154176s
STEP: Saw pod success
Aug 14 03:50:30.441: INFO: Pod "pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23" satisfied condition "Succeeded or Failed"
Aug 14 03:50:30.451: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:50:30.500: INFO: Waiting for pod pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23 to disappear
Aug 14 03:50:30.508: INFO: Pod pod-projected-configmaps-c01d47df-451f-4a72-8def-70da7227ab23 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:30.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1002" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":83,"skipped":1574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:30.532: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 03:50:30.746: INFO: Waiting up to 5m0s for pod "pod-f86edeb4-7e31-443b-b514-91fe41c049f8" in namespace "emptydir-5853" to be "Succeeded or Failed"
Aug 14 03:50:30.754: INFO: Pod "pod-f86edeb4-7e31-443b-b514-91fe41c049f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.69822ms
Aug 14 03:50:32.765: INFO: Pod "pod-f86edeb4-7e31-443b-b514-91fe41c049f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019441293s
STEP: Saw pod success
Aug 14 03:50:32.765: INFO: Pod "pod-f86edeb4-7e31-443b-b514-91fe41c049f8" satisfied condition "Succeeded or Failed"
Aug 14 03:50:32.776: INFO: Trying to get logs from node worker2 pod pod-f86edeb4-7e31-443b-b514-91fe41c049f8 container test-container: <nil>
STEP: delete the pod
Aug 14 03:50:32.826: INFO: Waiting for pod pod-f86edeb4-7e31-443b-b514-91fe41c049f8 to disappear
Aug 14 03:50:32.835: INFO: Pod pod-f86edeb4-7e31-443b-b514-91fe41c049f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:32.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5853" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":84,"skipped":1602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:32.861: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-bc544b63-c65c-4e92-8821-dcc72330561f
STEP: Creating a pod to test consume configMaps
Aug 14 03:50:33.089: INFO: Waiting up to 5m0s for pod "pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973" in namespace "configmap-7212" to be "Succeeded or Failed"
Aug 14 03:50:33.097: INFO: Pod "pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973": Phase="Pending", Reason="", readiness=false. Elapsed: 8.141641ms
Aug 14 03:50:35.108: INFO: Pod "pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018966135s
STEP: Saw pod success
Aug 14 03:50:35.108: INFO: Pod "pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973" satisfied condition "Succeeded or Failed"
Aug 14 03:50:35.117: INFO: Trying to get logs from node worker2 pod pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 03:50:35.172: INFO: Waiting for pod pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973 to disappear
Aug 14 03:50:35.180: INFO: Pod pod-configmaps-a32eaa23-db95-49a3-bc05-4d7598698973 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7212" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":85,"skipped":1626,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:35.211: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 14 03:50:35.468: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6908 /api/v1/namespaces/watch-6908/configmaps/e2e-watch-test-resource-version 80a4725e-d4a3-44b3-baea-670dddba383c 4155481 0 2020-08-14 03:50:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-08-14 03:50:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 03:50:35.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6908 /api/v1/namespaces/watch-6908/configmaps/e2e-watch-test-resource-version 80a4725e-d4a3-44b3-baea-670dddba383c 4155482 0 2020-08-14 03:50:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-08-14 03:50:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:35.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6908" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":275,"completed":86,"skipped":1637,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:35.491: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:50:35.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be" in namespace "projected-2841" to be "Succeeded or Failed"
Aug 14 03:50:35.718: INFO: Pod "downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be": Phase="Pending", Reason="", readiness=false. Elapsed: 9.751937ms
Aug 14 03:50:37.789: INFO: Pod "downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080868571s
Aug 14 03:50:39.798: INFO: Pod "downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090339848s
STEP: Saw pod success
Aug 14 03:50:39.798: INFO: Pod "downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be" satisfied condition "Succeeded or Failed"
Aug 14 03:50:39.807: INFO: Trying to get logs from node worker2 pod downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be container client-container: <nil>
STEP: delete the pod
Aug 14 03:50:39.860: INFO: Waiting for pod downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be to disappear
Aug 14 03:50:39.869: INFO: Pod downwardapi-volume-262fda93-3e1c-4f91-9b39-8e24103ee1be no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:39.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2841" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":87,"skipped":1650,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Aug 14 03:50:40.655: INFO: created pod pod-service-account-defaultsa
Aug 14 03:50:40.655: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 14 03:50:40.667: INFO: created pod pod-service-account-mountsa
Aug 14 03:50:40.668: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 14 03:50:40.680: INFO: created pod pod-service-account-nomountsa
Aug 14 03:50:40.680: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 14 03:50:40.692: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 14 03:50:40.692: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 14 03:50:40.706: INFO: created pod pod-service-account-mountsa-mountspec
Aug 14 03:50:40.707: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 14 03:50:40.725: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 14 03:50:40.726: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 14 03:50:40.742: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 14 03:50:40.743: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 14 03:50:40.755: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 14 03:50:40.755: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 14 03:50:40.766: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 14 03:50:40.767: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:50:40.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1866" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":275,"completed":88,"skipped":1670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:50:40.799: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-cbc7b380-5203-4174-be15-c1210b6e8bae in namespace container-probe-1004
Aug 14 03:50:45.093: INFO: Started pod liveness-cbc7b380-5203-4174-be15-c1210b6e8bae in namespace container-probe-1004
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 03:50:45.188: INFO: Initial restart count of pod liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is 0
Aug 14 03:50:59.294: INFO: Restart count of pod container-probe-1004/liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is now 1 (14.106346365s elapsed)
Aug 14 03:51:17.386: INFO: Restart count of pod container-probe-1004/liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is now 2 (32.198431399s elapsed)
Aug 14 03:51:37.512: INFO: Restart count of pod container-probe-1004/liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is now 3 (52.324674114s elapsed)
Aug 14 03:51:57.616: INFO: Restart count of pod container-probe-1004/liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is now 4 (1m12.4282028s elapsed)
Aug 14 03:53:06.113: INFO: Restart count of pod container-probe-1004/liveness-cbc7b380-5203-4174-be15-c1210b6e8bae is now 5 (2m20.925159844s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:06.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1004" for this suite.

• [SLOW TEST:145.369 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":275,"completed":89,"skipped":1695,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:06.169: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-706e1676-623e-42cc-8cab-c242cfc49465
STEP: Creating a pod to test consume secrets
Aug 14 03:53:06.405: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef" in namespace "projected-8878" to be "Succeeded or Failed"
Aug 14 03:53:06.413: INFO: Pod "pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.505741ms
Aug 14 03:53:08.424: INFO: Pod "pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019087415s
Aug 14 03:53:10.435: INFO: Pod "pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029727129s
STEP: Saw pod success
Aug 14 03:53:10.435: INFO: Pod "pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef" satisfied condition "Succeeded or Failed"
Aug 14 03:53:10.443: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:53:10.546: INFO: Waiting for pod pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef to disappear
Aug 14 03:53:10.555: INFO: Pod pod-projected-secrets-cc7f59e6-96c5-4751-bb32-14794f17dbef no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:10.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8878" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":90,"skipped":1699,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:10.580: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-d3a0fab0-ca78-4c4c-a73f-d0d40694cc67
STEP: Creating a pod to test consume secrets
Aug 14 03:53:10.815: INFO: Waiting up to 5m0s for pod "pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d" in namespace "secrets-8985" to be "Succeeded or Failed"
Aug 14 03:53:10.825: INFO: Pod "pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.819738ms
Aug 14 03:53:12.894: INFO: Pod "pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078867457s
Aug 14 03:53:14.904: INFO: Pod "pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088704293s
STEP: Saw pod success
Aug 14 03:53:14.904: INFO: Pod "pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d" satisfied condition "Succeeded or Failed"
Aug 14 03:53:14.913: INFO: Trying to get logs from node worker2 pod pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d container secret-env-test: <nil>
STEP: delete the pod
Aug 14 03:53:14.967: INFO: Waiting for pod pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d to disappear
Aug 14 03:53:14.976: INFO: Pod pod-secrets-c52eb40b-991d-45e6-881f-8a6b19ea5a1d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:14.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8985" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":275,"completed":91,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:15.002: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-1333a346-795d-4362-8e04-06835a05c390
STEP: Creating a pod to test consume secrets
Aug 14 03:53:15.236: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb" in namespace "projected-1235" to be "Succeeded or Failed"
Aug 14 03:53:15.244: INFO: Pod "pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.655383ms
Aug 14 03:53:17.254: INFO: Pod "pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018029817s
STEP: Saw pod success
Aug 14 03:53:17.254: INFO: Pod "pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb" satisfied condition "Succeeded or Failed"
Aug 14 03:53:17.263: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:53:17.315: INFO: Waiting for pod pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb to disappear
Aug 14 03:53:17.323: INFO: Pod pod-projected-secrets-cd9f44d5-ebf8-4c46-b1fe-ef79239539bb no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:17.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1235" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":92,"skipped":1756,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:17.348: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4012
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4012
STEP: creating replication controller externalsvc in namespace services-4012
I0814 03:53:17.605119      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4012, replica count: 2
I0814 03:53:20.655810      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 14 03:53:20.699: INFO: Creating new exec pod
Aug 14 03:53:24.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-4012 execpodv5rlc -- /bin/sh -x -c nslookup clusterip-service'
Aug 14 03:53:25.308: INFO: stderr: "+ nslookup clusterip-service\n"
Aug 14 03:53:25.308: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nclusterip-service.services-4012.svc.cluster.local\tcanonical name = externalsvc.services-4012.svc.cluster.local.\nName:\texternalsvc.services-4012.svc.cluster.local\nAddress: 100.105.6.141\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4012, will wait for the garbage collector to delete the pods
Aug 14 03:53:25.388: INFO: Deleting ReplicationController externalsvc took: 20.408572ms
Aug 14 03:53:25.788: INFO: Terminating ReplicationController externalsvc pods took: 400.337235ms
Aug 14 03:53:41.435: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4012" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:24.151 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":275,"completed":93,"skipped":1762,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:41.500: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-73713b05-5339-427d-bf49-94319a8559e1
STEP: Creating a pod to test consume secrets
Aug 14 03:53:41.731: INFO: Waiting up to 5m0s for pod "pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9" in namespace "secrets-5606" to be "Succeeded or Failed"
Aug 14 03:53:41.740: INFO: Pod "pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54454ms
Aug 14 03:53:43.751: INFO: Pod "pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019482793s
Aug 14 03:53:45.761: INFO: Pod "pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029302089s
STEP: Saw pod success
Aug 14 03:53:45.761: INFO: Pod "pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9" satisfied condition "Succeeded or Failed"
Aug 14 03:53:45.769: INFO: Trying to get logs from node worker1 pod pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 03:53:45.835: INFO: Waiting for pod pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9 to disappear
Aug 14 03:53:45.843: INFO: Pod pod-secrets-aeb622f2-c523-4a7e-8173-002a690e47c9 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:45.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5606" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":94,"skipped":1779,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:45.867: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:53:50.154: INFO: Waiting up to 5m0s for pod "client-envvars-df211300-64ac-472f-a0b7-116ae72060f1" in namespace "pods-4079" to be "Succeeded or Failed"
Aug 14 03:53:50.162: INFO: Pod "client-envvars-df211300-64ac-472f-a0b7-116ae72060f1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.260061ms
Aug 14 03:53:52.173: INFO: Pod "client-envvars-df211300-64ac-472f-a0b7-116ae72060f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018605716s
STEP: Saw pod success
Aug 14 03:53:52.173: INFO: Pod "client-envvars-df211300-64ac-472f-a0b7-116ae72060f1" satisfied condition "Succeeded or Failed"
Aug 14 03:53:52.181: INFO: Trying to get logs from node worker2 pod client-envvars-df211300-64ac-472f-a0b7-116ae72060f1 container env3cont: <nil>
STEP: delete the pod
Aug 14 03:53:52.231: INFO: Waiting for pod client-envvars-df211300-64ac-472f-a0b7-116ae72060f1 to disappear
Aug 14 03:53:52.239: INFO: Pod client-envvars-df211300-64ac-472f-a0b7-116ae72060f1 no longer exists
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:52.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4079" for this suite.

• [SLOW TEST:6.400 seconds]
[k8s.io] Pods
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":275,"completed":95,"skipped":1798,"failed":0}
SSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1851
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1851" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":275,"completed":96,"skipped":1804,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:53:52.799: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:53:57.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5282" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":275,"completed":97,"skipped":1805,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:53:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 03:53:57.438: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"49cdcd93-82ef-437a-a07d-d44bf249c6b0", Controller:(*bool)(0x40013370d2), BlockOwnerDeletion:(*bool)(0x40013370d3)}}
Aug 14 03:53:57.452: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f277c503-e134-492d-9087-4039aa015baf", Controller:(*bool)(0x4002c79e02), BlockOwnerDeletion:(*bool)(0x4002c79e03)}}
Aug 14 03:53:57.466: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d6c05fa5-b288-4064-82ff-0c6bfda89fb7", Controller:(*bool)(0x40034326e2), BlockOwnerDeletion:(*bool)(0x40034326e3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:54:02.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7439" for this suite.

• [SLOW TEST:5.330 seconds]
[sig-api-machinery] Garbage collector
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":275,"completed":98,"skipped":1806,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:54:02.519: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:54:10.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2392" for this suite.

• [SLOW TEST:8.251 seconds]
[sig-apps] Job
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":275,"completed":99,"skipped":1811,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:54:10.770: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 03:54:10.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73" in namespace "projected-3750" to be "Succeeded or Failed"
Aug 14 03:54:11.007: INFO: Pod "downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.978532ms
Aug 14 03:54:13.017: INFO: Pod "downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021886348s
Aug 14 03:54:15.027: INFO: Pod "downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032163103s
STEP: Saw pod success
Aug 14 03:54:15.027: INFO: Pod "downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73" satisfied condition "Succeeded or Failed"
Aug 14 03:54:15.035: INFO: Trying to get logs from node worker1 pod downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73 container client-container: <nil>
STEP: delete the pod
Aug 14 03:54:15.087: INFO: Waiting for pod downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73 to disappear
Aug 14 03:54:15.095: INFO: Pod downwardapi-volume-cc7f2996-e043-499c-b5c1-94e0862e3e73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:54:15.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3750" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":100,"skipped":1824,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:54:15.122: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Aug 14 03:54:15.338: INFO: Waiting up to 5m0s for pod "var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc" in namespace "var-expansion-6116" to be "Succeeded or Failed"
Aug 14 03:54:15.345: INFO: Pod "var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503263ms
Aug 14 03:54:17.355: INFO: Pod "var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0166985s
Aug 14 03:54:19.364: INFO: Pod "var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025726038s
STEP: Saw pod success
Aug 14 03:54:19.364: INFO: Pod "var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc" satisfied condition "Succeeded or Failed"
Aug 14 03:54:19.372: INFO: Trying to get logs from node worker1 pod var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc container dapi-container: <nil>
STEP: delete the pod
Aug 14 03:54:19.422: INFO: Waiting for pod var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc to disappear
Aug 14 03:54:19.430: INFO: Pod var-expansion-8d2b206e-d79e-4784-8bb6-375b46192cbc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:54:19.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6116" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":275,"completed":101,"skipped":1836,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:54:19.457: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-034a7014-ebf5-4c47-bedf-632383a03822 in namespace container-probe-162
Aug 14 03:54:23.695: INFO: Started pod liveness-034a7014-ebf5-4c47-bedf-632383a03822 in namespace container-probe-162
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 03:54:23.703: INFO: Initial restart count of pod liveness-034a7014-ebf5-4c47-bedf-632383a03822 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:25.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-162" for this suite.

• [SLOW TEST:245.870 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":275,"completed":102,"skipped":1847,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Aug 14 03:58:25.524: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-133789455 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:25.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2792" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":275,"completed":103,"skipped":1856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:25.685: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 03:58:27.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 03:58:29.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974307, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974307, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974308, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974307, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 03:58:32.979: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:33.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3994" for this suite.
STEP: Destroying namespace "webhook-3994-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.848 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":275,"completed":104,"skipped":1918,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 14 03:58:38.301: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1913 pod-service-account-e9d494a0-601f-4f8a-8ff0-174327c7d2b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 14 03:58:38.869: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1913 pod-service-account-e9d494a0-601f-4f8a-8ff0-174327c7d2b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 14 03:58:39.336: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1913 pod-service-account-e9d494a0-601f-4f8a-8ff0-174327c7d2b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:39.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1913" for this suite.

• [SLOW TEST:6.363 seconds]
[sig-auth] ServiceAccounts
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":275,"completed":105,"skipped":1930,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:39.898: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:42.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6832" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":106,"skipped":1944,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:42.255: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 03:58:42.466: INFO: Waiting up to 5m0s for pod "pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c" in namespace "emptydir-2724" to be "Succeeded or Failed"
Aug 14 03:58:42.477: INFO: Pod "pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.497217ms
Aug 14 03:58:44.487: INFO: Pod "pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020935236s
Aug 14 03:58:46.498: INFO: Pod "pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032048834s
STEP: Saw pod success
Aug 14 03:58:46.499: INFO: Pod "pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c" satisfied condition "Succeeded or Failed"
Aug 14 03:58:46.508: INFO: Trying to get logs from node worker1 pod pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c container test-container: <nil>
STEP: delete the pod
Aug 14 03:58:46.657: INFO: Waiting for pod pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c to disappear
Aug 14 03:58:46.666: INFO: Pod pod-c24607ef-beaf-4ec0-bc4b-3352c47ea99c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 03:58:46.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2724" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":107,"skipped":1947,"failed":0}

------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 03:58:46.692: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-207
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-8d0dc775-d431-4af7-ae83-b4169d2d922e
STEP: Creating secret with name s-test-opt-upd-7b54477e-0eeb-4af0-92ec-f46d81b3c428
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8d0dc775-d431-4af7-ae83-b4169d2d922e
STEP: Updating secret s-test-opt-upd-7b54477e-0eeb-4af0-92ec-f46d81b3c428
STEP: Creating secret with name s-test-opt-create-566cd727-bb64-4b20-9c52-311d84389cc3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:00:10.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-207" for this suite.

• [SLOW TEST:83.543 seconds]
[sig-storage] Secrets
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":108,"skipped":1947,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:00:10.236: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-a93dbbbf-725f-4ff3-862e-0bede7b0cf37 in namespace container-probe-942
Aug 14 04:00:14.470: INFO: Started pod liveness-a93dbbbf-725f-4ff3-862e-0bede7b0cf37 in namespace container-probe-942
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 04:00:14.479: INFO: Initial restart count of pod liveness-a93dbbbf-725f-4ff3-862e-0bede7b0cf37 is 0
Aug 14 04:00:32.613: INFO: Restart count of pod container-probe-942/liveness-a93dbbbf-725f-4ff3-862e-0bede7b0cf37 is now 1 (18.133742425s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:00:32.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-942" for this suite.

• [SLOW TEST:22.509 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":109,"skipped":1948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:00:32.746: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:00:32.977: INFO: (0) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 13.099691ms)
Aug 14 04:00:32.989: INFO: (1) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 11.760494ms)
Aug 14 04:00:33.000: INFO: (2) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 11.335815ms)
Aug 14 04:00:33.013: INFO: (3) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 12.753112ms)
Aug 14 04:00:33.025: INFO: (4) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 11.248215ms)
Aug 14 04:00:33.036: INFO: (5) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.615756ms)
Aug 14 04:00:33.046: INFO: (6) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.698896ms)
Aug 14 04:00:33.058: INFO: (7) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 11.108435ms)
Aug 14 04:00:33.071: INFO: (8) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 13.46539ms)
Aug 14 04:00:33.082: INFO: (9) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.741216ms)
Aug 14 04:00:33.092: INFO: (10) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.070917ms)
Aug 14 04:00:33.102: INFO: (11) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.106817ms)
Aug 14 04:00:33.112: INFO: (12) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.065418ms)
Aug 14 04:00:33.124: INFO: (13) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 11.387374ms)
Aug 14 04:00:33.135: INFO: (14) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.787556ms)
Aug 14 04:00:33.145: INFO: (15) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 9.955358ms)
Aug 14 04:00:33.155: INFO: (16) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.061418ms)
Aug 14 04:00:33.165: INFO: (17) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.281517ms)
Aug 14 04:00:33.179: INFO: (18) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 13.610929ms)
Aug 14 04:00:33.189: INFO: (19) /api/v1/nodes/worker2:10250/proxy/logs/: <pre>
<a href="README">README</a>
<a href="alternatives.log">alternatives.log</a>
<a href="altern... (200; 10.327577ms)
[AfterEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:00:33.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8347" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":275,"completed":110,"skipped":1985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:00:33.217: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2345.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2345.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2345.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2345.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.21.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.21.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.21.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.21.212_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2345.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2345.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2345.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2345.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2345.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2345.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.21.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.21.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.21.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.21.212_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:00:37.521: INFO: Unable to read wheezy_udp@dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.533: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.623: INFO: Unable to read jessie_udp@dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.632: INFO: Unable to read jessie_tcp@dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.641: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.656: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local from pod dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6: the server could not find the requested resource (get pods dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6)
Aug 14 04:00:37.717: INFO: Lookups using dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6 failed for: [wheezy_udp@dns-test-service.dns-2345.svc.cluster.local wheezy_tcp@dns-test-service.dns-2345.svc.cluster.local jessie_udp@dns-test-service.dns-2345.svc.cluster.local jessie_tcp@dns-test-service.dns-2345.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2345.svc.cluster.local]

Aug 14 04:00:42.941: INFO: DNS probes using dns-2345/dns-test-b87b9561-4f84-4b68-be26-853f89c3b9d6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:00:43.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2345" for this suite.

• [SLOW TEST:9.878 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":275,"completed":111,"skipped":2009,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:00:43.095: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-rjnx
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 04:00:43.342: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rjnx" in namespace "subpath-4664" to be "Succeeded or Failed"
Aug 14 04:00:43.357: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Pending", Reason="", readiness=false. Elapsed: 14.635647ms
Aug 14 04:00:45.368: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025195146s
Aug 14 04:00:47.389: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 4.046201602s
Aug 14 04:00:49.398: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 6.055057885s
Aug 14 04:00:51.408: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 8.065107945s
Aug 14 04:00:53.418: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 10.075681684s
Aug 14 04:00:55.429: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 12.086486982s
Aug 14 04:00:57.440: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 14.097435681s
Aug 14 04:00:59.449: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 16.106813002s
Aug 14 04:01:01.459: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 18.116437083s
Aug 14 04:01:03.469: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Running", Reason="", readiness=true. Elapsed: 20.126387804s
Aug 14 04:01:05.479: INFO: Pod "pod-subpath-test-configmap-rjnx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.136501604s
STEP: Saw pod success
Aug 14 04:01:05.479: INFO: Pod "pod-subpath-test-configmap-rjnx" satisfied condition "Succeeded or Failed"
Aug 14 04:01:05.488: INFO: Trying to get logs from node worker2 pod pod-subpath-test-configmap-rjnx container test-container-subpath-configmap-rjnx: <nil>
STEP: delete the pod
Aug 14 04:01:05.552: INFO: Waiting for pod pod-subpath-test-configmap-rjnx to disappear
Aug 14 04:01:05.567: INFO: Pod pod-subpath-test-configmap-rjnx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rjnx
Aug 14 04:01:05.568: INFO: Deleting pod "pod-subpath-test-configmap-rjnx" in namespace "subpath-4664"
[AfterEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:01:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4664" for this suite.

• [SLOW TEST:22.504 seconds]
[sig-storage] Subpath
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":275,"completed":112,"skipped":2015,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:01:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:01:07.048: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:01:09.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974467, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974467, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974467, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974467, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:01:12.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:01:22.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8046" for this suite.
STEP: Destroying namespace "webhook-8046-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.040 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":275,"completed":113,"skipped":2020,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:01:22.641: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3791
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:01:22.840: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:02:24.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3791" for this suite.

• [SLOW TEST:62.063 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":275,"completed":114,"skipped":2026,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:02:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c
Aug 14 04:02:25.103: INFO: Pod name my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c: Found 1 pods out of 1
Aug 14 04:02:25.103: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c" are running
Aug 14 04:02:29.124: INFO: Pod "my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c-v4tgv" is running (conditions: [])
Aug 14 04:02:29.124: INFO: Trying to dial the pod
Aug 14 04:02:34.190: INFO: Controller my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c: Got expected result from replica 1 [my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c-v4tgv]: "my-hostname-basic-93e3f618-f4ca-4f85-9275-388ea668099c-v4tgv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:02:34.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4202" for this suite.

• [SLOW TEST:9.511 seconds]
[sig-apps] ReplicationController
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":115,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:02:34.216: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:02:37.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1907" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":275,"completed":116,"skipped":2077,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:02:37.558: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5584
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 04:02:37.753: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 14 04:02:37.854: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:02:39.865: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:02:41.864: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:43.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:45.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:47.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:49.862: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:51.864: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:53.864: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:55.864: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:57.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:02:59.864: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 14 04:02:59.881: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 14 04:02:59.901: INFO: The status of Pod netserver-2 is Running (Ready = true)
Aug 14 04:02:59.917: INFO: The status of Pod netserver-3 is Running (Ready = true)
Aug 14 04:02:59.936: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Aug 14 04:03:04.021: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.161.172 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:04.021: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:05.310: INFO: Found all expected endpoints: [netserver-0]
Aug 14 04:03:05.319: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.208.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:05.320: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:06.668: INFO: Found all expected endpoints: [netserver-1]
Aug 14 04:03:06.678: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.32.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:06.678: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:08.018: INFO: Found all expected endpoints: [netserver-2]
Aug 14 04:03:08.028: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.166.144 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:09.640: INFO: Found all expected endpoints: [netserver-3]
Aug 14 04:03:09.649: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.245.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:09.649: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:10.976: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5584" for this suite.

• [SLOW TEST:33.444 seconds]
[sig-network] Networking
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":117,"skipped":2091,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 14 04:03:11.224: INFO: Created pod &Pod{ObjectMeta:{dns-4385  dns-4385 /api/v1/namespaces/dns-4385/pods/dns-4385 02c79812-484a-4b96-a540-3cf624f1faa1 4159776 0 2020-08-14 04:03:11 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-08-14 04:03:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nfx7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nfx7z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nfx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:03:11.234: INFO: The status of Pod dns-4385 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:03:13.243: INFO: The status of Pod dns-4385 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:03:15.243: INFO: The status of Pod dns-4385 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 14 04:03:15.244: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4385 PodName:dns-4385 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:15.244: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Verifying customized DNS server is configured on pod...
Aug 14 04:03:15.600: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4385 PodName:dns-4385 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:15.600: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:15.952: INFO: Deleting pod dns-4385...
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:16.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4385" for this suite.

• [SLOW TEST:5.022 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":275,"completed":118,"skipped":2106,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:16.025: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7711
STEP: Creating secret with name secret-test-413f3260-d73c-4ea4-861c-68b7189cb10c
STEP: Creating a pod to test consume secrets
Aug 14 04:03:16.494: INFO: Waiting up to 5m0s for pod "pod-secrets-36583bc0-9831-418b-94c2-e45843e78733" in namespace "secrets-7707" to be "Succeeded or Failed"
Aug 14 04:03:16.508: INFO: Pod "pod-secrets-36583bc0-9831-418b-94c2-e45843e78733": Phase="Pending", Reason="", readiness=false. Elapsed: 13.145831ms
Aug 14 04:03:18.519: INFO: Pod "pod-secrets-36583bc0-9831-418b-94c2-e45843e78733": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024534568s
Aug 14 04:03:20.528: INFO: Pod "pod-secrets-36583bc0-9831-418b-94c2-e45843e78733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033218151s
STEP: Saw pod success
Aug 14 04:03:20.528: INFO: Pod "pod-secrets-36583bc0-9831-418b-94c2-e45843e78733" satisfied condition "Succeeded or Failed"
Aug 14 04:03:20.537: INFO: Trying to get logs from node worker1 pod pod-secrets-36583bc0-9831-418b-94c2-e45843e78733 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 04:03:20.607: INFO: Waiting for pod pod-secrets-36583bc0-9831-418b-94c2-e45843e78733 to disappear
Aug 14 04:03:20.616: INFO: Pod pod-secrets-36583bc0-9831-418b-94c2-e45843e78733 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:20.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7707" for this suite.
STEP: Destroying namespace "secret-namespace-7711" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":275,"completed":119,"skipped":2107,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:20.654: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5411
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 04:03:20.852: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 14 04:03:20.965: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:03:22.975: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:03:24.976: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:26.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:28.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:30.975: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:32.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:34.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:36.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:38.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:40.993: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:03:42.974: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 14 04:03:42.992: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 14 04:03:43.010: INFO: The status of Pod netserver-2 is Running (Ready = true)
Aug 14 04:03:43.027: INFO: The status of Pod netserver-3 is Running (Ready = true)
Aug 14 04:03:43.045: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Aug 14 04:03:47.101: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.156:8080/dial?request=hostname&protocol=udp&host=100.101.161.173&port=8081&tries=1'] Namespace:pod-network-test-5411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:47.101: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:47.478: INFO: Waiting for responses: map[]
Aug 14 04:03:47.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.156:8080/dial?request=hostname&protocol=udp&host=100.101.208.153&port=8081&tries=1'] Namespace:pod-network-test-5411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:47.486: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:47.813: INFO: Waiting for responses: map[]
Aug 14 04:03:47.821: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.156:8080/dial?request=hostname&protocol=udp&host=100.101.32.104&port=8081&tries=1'] Namespace:pod-network-test-5411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:47.821: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:48.156: INFO: Waiting for responses: map[]
Aug 14 04:03:48.166: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.156:8080/dial?request=hostname&protocol=udp&host=100.101.166.146&port=8081&tries=1'] Namespace:pod-network-test-5411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:48.472: INFO: Waiting for responses: map[]
Aug 14 04:03:48.482: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.245.156:8080/dial?request=hostname&protocol=udp&host=100.101.245.155&port=8081&tries=1'] Namespace:pod-network-test-5411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:03:48.482: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:03:48.915: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:48.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5411" for this suite.

• [SLOW TEST:28.341 seconds]
[sig-network] Networking
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":275,"completed":120,"skipped":2116,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:48.995: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Aug 14 04:03:49.215: INFO: Waiting up to 5m0s for pod "client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f" in namespace "containers-3909" to be "Succeeded or Failed"
Aug 14 04:03:49.223: INFO: Pod "client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821743ms
Aug 14 04:03:51.232: INFO: Pod "client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017255425s
STEP: Saw pod success
Aug 14 04:03:51.232: INFO: Pod "client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f" satisfied condition "Succeeded or Failed"
Aug 14 04:03:51.241: INFO: Trying to get logs from node worker2 pod client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f container test-container: <nil>
STEP: delete the pod
Aug 14 04:03:51.313: INFO: Waiting for pod client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f to disappear
Aug 14 04:03:51.321: INFO: Pod client-containers-7d049eb1-117b-493f-bad9-19b2d9c1387f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:51.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3909" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":275,"completed":121,"skipped":2117,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:51.344: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-46
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:03:51.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7" in namespace "projected-46" to be "Succeeded or Failed"
Aug 14 04:03:51.572: INFO: Pod "downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.980338ms
Aug 14 04:03:53.594: INFO: Pod "downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032056072s
Aug 14 04:03:55.602: INFO: Pod "downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040682615s
STEP: Saw pod success
Aug 14 04:03:55.602: INFO: Pod "downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7" satisfied condition "Succeeded or Failed"
Aug 14 04:03:55.610: INFO: Trying to get logs from node worker2 pod downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7 container client-container: <nil>
STEP: delete the pod
Aug 14 04:03:55.665: INFO: Waiting for pod downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7 to disappear
Aug 14 04:03:55.673: INFO: Pod downwardapi-volume-63eb1054-f1a8-4613-9ee8-e9f113c0d1a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:03:55.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-46" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":122,"skipped":2128,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:03:55.700: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3063.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3063.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3063.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3063.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:04:00.001: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.010: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.020: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.071: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.081: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.094: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3063.svc.cluster.local from pod dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762: the server could not find the requested resource (get pods dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762)
Aug 14 04:04:00.117: INFO: Lookups using dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3063.svc.cluster.local jessie_udp@dns-test-service-2.dns-3063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3063.svc.cluster.local]

Aug 14 04:04:05.637: INFO: DNS probes using dns-3063/dns-test-0d1fbead-db51-447a-8b6a-d4f0c9685762 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:05.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3063" for this suite.

• [SLOW TEST:10.048 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":275,"completed":123,"skipped":2130,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:05.748: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-a384c00a-8977-41ec-a326-686fa784278e
STEP: Creating a pod to test consume configMaps
Aug 14 04:04:05.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8" in namespace "configmap-3112" to be "Succeeded or Failed"
Aug 14 04:04:06.000: INFO: Pod "pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.565176ms
Aug 14 04:04:08.009: INFO: Pod "pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019516499s
STEP: Saw pod success
Aug 14 04:04:08.009: INFO: Pod "pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8" satisfied condition "Succeeded or Failed"
Aug 14 04:04:08.019: INFO: Trying to get logs from node worker2 pod pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:04:08.073: INFO: Waiting for pod pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8 to disappear
Aug 14 04:04:08.082: INFO: Pod pod-configmaps-85ee698f-aca3-473d-a704-1829f136cda8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:08.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3112" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":124,"skipped":2143,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:08.117: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:04:08.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c" in namespace "downward-api-3971" to be "Succeeded or Failed"
Aug 14 04:04:08.350: INFO: Pod "downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.060515ms
Aug 14 04:04:10.361: INFO: Pod "downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022134213s
STEP: Saw pod success
Aug 14 04:04:10.361: INFO: Pod "downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c" satisfied condition "Succeeded or Failed"
Aug 14 04:04:10.370: INFO: Trying to get logs from node worker2 pod downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c container client-container: <nil>
STEP: delete the pod
Aug 14 04:04:10.437: INFO: Waiting for pod downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c to disappear
Aug 14 04:04:10.446: INFO: Pod downwardapi-volume-654f68f3-047b-4839-a27e-152b6924f30c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:10.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3971" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":125,"skipped":2152,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:10.472: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-1005/secret-test-eea9ee09-6ad9-4ae4-a132-3cd2c9fda7b9
STEP: Creating a pod to test consume secrets
Aug 14 04:04:10.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef" in namespace "secrets-1005" to be "Succeeded or Failed"
Aug 14 04:04:10.713: INFO: Pod "pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.9983ms
Aug 14 04:04:12.725: INFO: Pod "pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021667894s
Aug 14 04:04:14.736: INFO: Pod "pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031883654s
STEP: Saw pod success
Aug 14 04:04:14.736: INFO: Pod "pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef" satisfied condition "Succeeded or Failed"
Aug 14 04:04:14.745: INFO: Trying to get logs from node worker2 pod pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef container env-test: <nil>
STEP: delete the pod
Aug 14 04:04:14.794: INFO: Waiting for pod pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef to disappear
Aug 14 04:04:14.803: INFO: Pod pod-configmaps-c35844b7-8aa0-4f95-90b8-54878a7c82ef no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:14.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1005" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":126,"skipped":2154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:14.834: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 04:04:17.089: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:17.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2795" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":275,"completed":127,"skipped":2190,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:17.153: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-fch6
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 04:04:17.396: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fch6" in namespace "subpath-6706" to be "Succeeded or Failed"
Aug 14 04:04:17.406: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.213979ms
Aug 14 04:04:19.416: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019405699s
Aug 14 04:04:21.425: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 4.02903606s
Aug 14 04:04:23.436: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 6.0394197s
Aug 14 04:04:25.446: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 8.04916574s
Aug 14 04:04:27.457: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 10.060543838s
Aug 14 04:04:29.467: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 12.070908617s
Aug 14 04:04:31.494: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 14.097339301s
Aug 14 04:04:33.503: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 16.106756342s
Aug 14 04:04:35.512: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 18.115963824s
Aug 14 04:04:37.522: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 20.125631925s
Aug 14 04:04:39.532: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Running", Reason="", readiness=true. Elapsed: 22.135234247s
Aug 14 04:04:41.541: INFO: Pod "pod-subpath-test-downwardapi-fch6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.144297709s
STEP: Saw pod success
Aug 14 04:04:41.541: INFO: Pod "pod-subpath-test-downwardapi-fch6" satisfied condition "Succeeded or Failed"
Aug 14 04:04:41.549: INFO: Trying to get logs from node worker2 pod pod-subpath-test-downwardapi-fch6 container test-container-subpath-downwardapi-fch6: <nil>
STEP: delete the pod
Aug 14 04:04:41.601: INFO: Waiting for pod pod-subpath-test-downwardapi-fch6 to disappear
Aug 14 04:04:41.609: INFO: Pod pod-subpath-test-downwardapi-fch6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fch6
Aug 14 04:04:41.609: INFO: Deleting pod "pod-subpath-test-downwardapi-fch6" in namespace "subpath-6706"
[AfterEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:41.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6706" for this suite.

• [SLOW TEST:24.490 seconds]
[sig-storage] Subpath
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":275,"completed":128,"skipped":2195,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:41.643: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-b015801f-20be-4bb2-8548-3a25e698206e
STEP: Creating a pod to test consume configMaps
Aug 14 04:04:41.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50" in namespace "configmap-6677" to be "Succeeded or Failed"
Aug 14 04:04:41.887: INFO: Pod "pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50": Phase="Pending", Reason="", readiness=false. Elapsed: 8.342261ms
Aug 14 04:04:43.897: INFO: Pod "pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018087102s
STEP: Saw pod success
Aug 14 04:04:43.897: INFO: Pod "pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50" satisfied condition "Succeeded or Failed"
Aug 14 04:04:43.905: INFO: Trying to get logs from node worker2 pod pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:04:43.957: INFO: Waiting for pod pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50 to disappear
Aug 14 04:04:43.965: INFO: Pod pod-configmaps-1c25a964-13b9-42e2-9203-51591c7dfa50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:43.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6677" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":129,"skipped":2197,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 04:04:44.208: INFO: Waiting up to 5m0s for pod "pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84" in namespace "emptydir-6995" to be "Succeeded or Failed"
Aug 14 04:04:44.217: INFO: Pod "pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2058ms
Aug 14 04:04:46.228: INFO: Pod "pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019685479s
STEP: Saw pod success
Aug 14 04:04:46.228: INFO: Pod "pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84" satisfied condition "Succeeded or Failed"
Aug 14 04:04:46.238: INFO: Trying to get logs from node worker2 pod pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84 container test-container: <nil>
STEP: delete the pod
Aug 14 04:04:46.291: INFO: Waiting for pod pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84 to disappear
Aug 14 04:04:46.299: INFO: Pod pod-b0d2b2ca-aa8e-41b3-9686-ff3fc58b2b84 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:46.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6995" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":130,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:46.324: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-eddea612-cd83-4d10-8a54-8d8518be07e9
STEP: Creating a pod to test consume configMaps
Aug 14 04:04:46.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83" in namespace "projected-6759" to be "Succeeded or Failed"
Aug 14 04:04:46.566: INFO: Pod "pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.586821ms
Aug 14 04:04:48.577: INFO: Pod "pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019536139s
STEP: Saw pod success
Aug 14 04:04:48.577: INFO: Pod "pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83" satisfied condition "Succeeded or Failed"
Aug 14 04:04:48.586: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:04:48.644: INFO: Waiting for pod pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83 to disappear
Aug 14 04:04:48.653: INFO: Pod pod-projected-configmaps-da0d1d15-240c-44a1-a3c4-d706bdaa6a83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6759" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":131,"skipped":2259,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:48.677: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Aug 14 04:04:48.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-6722'
Aug 14 04:04:49.587: INFO: stderr: ""
Aug 14 04:04:49.587: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 04:04:49.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6722'
Aug 14 04:04:49.761: INFO: stderr: ""
Aug 14 04:04:49.761: INFO: stdout: "update-demo-nautilus-d5d2c update-demo-nautilus-skhq6 "
Aug 14 04:04:49.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-d5d2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6722'
Aug 14 04:04:49.925: INFO: stderr: ""
Aug 14 04:04:49.925: INFO: stdout: ""
Aug 14 04:04:49.925: INFO: update-demo-nautilus-d5d2c is created but not running
Aug 14 04:04:54.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6722'
Aug 14 04:04:55.088: INFO: stderr: ""
Aug 14 04:04:55.088: INFO: stdout: "update-demo-nautilus-d5d2c update-demo-nautilus-skhq6 "
Aug 14 04:04:55.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-d5d2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6722'
Aug 14 04:04:55.248: INFO: stderr: ""
Aug 14 04:04:55.248: INFO: stdout: "true"
Aug 14 04:04:55.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-d5d2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6722'
Aug 14 04:04:55.415: INFO: stderr: ""
Aug 14 04:04:55.415: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:04:55.415: INFO: validating pod update-demo-nautilus-d5d2c
Aug 14 04:04:55.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:04:55.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:04:55.427: INFO: update-demo-nautilus-d5d2c is verified up and running
Aug 14 04:04:55.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-skhq6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6722'
Aug 14 04:04:55.604: INFO: stderr: ""
Aug 14 04:04:55.604: INFO: stdout: "true"
Aug 14 04:04:55.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-skhq6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6722'
Aug 14 04:04:55.759: INFO: stderr: ""
Aug 14 04:04:55.759: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:04:55.759: INFO: validating pod update-demo-nautilus-skhq6
Aug 14 04:04:55.770: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:04:55.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:04:55.770: INFO: update-demo-nautilus-skhq6 is verified up and running
STEP: using delete to clean up resources
Aug 14 04:04:55.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-6722'
Aug 14 04:04:56.004: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 04:04:56.005: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 04:04:56.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6722'
Aug 14 04:04:56.170: INFO: stderr: "No resources found in kubectl-6722 namespace.\n"
Aug 14 04:04:56.170: INFO: stdout: ""
Aug 14 04:04:56.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -l name=update-demo --namespace=kubectl-6722 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 04:04:56.327: INFO: stderr: ""
Aug 14 04:04:56.327: INFO: stdout: "update-demo-nautilus-d5d2c\nupdate-demo-nautilus-skhq6\n"
Aug 14 04:04:56.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6722'
Aug 14 04:04:56.997: INFO: stderr: "No resources found in kubectl-6722 namespace.\n"
Aug 14 04:04:56.998: INFO: stdout: ""
Aug 14 04:04:56.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -l name=update-demo --namespace=kubectl-6722 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 04:04:57.167: INFO: stderr: ""
Aug 14 04:04:57.167: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:04:57.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6722" for this suite.

• [SLOW TEST:8.515 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":275,"completed":132,"skipped":2275,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:04:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4352
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-46ac4d26-92fc-49c8-b232-55dc7e21018a
STEP: Creating configMap with name cm-test-opt-upd-631e137b-dbcf-4db2-80c8-1a3a0edd84b8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-46ac4d26-92fc-49c8-b232-55dc7e21018a
STEP: Updating configmap cm-test-opt-upd-631e137b-dbcf-4db2-80c8-1a3a0edd84b8
STEP: Creating configMap with name cm-test-opt-create-87bd811f-92e0-494c-a570-02a8bf257ba0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:05:05.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4352" for this suite.

• [SLOW TEST:8.536 seconds]
[sig-storage] ConfigMap
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":133,"skipped":2283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:05:05.729: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-400
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-2936f472-2782-461c-858c-7eba48dd4ed9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2936f472-2782-461c-858c-7eba48dd4ed9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:06:25.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-400" for this suite.

• [SLOW TEST:79.356 seconds]
[sig-storage] ConfigMap
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":134,"skipped":2313,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:06:25.085: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Aug 14 04:06:25.315: INFO: Waiting up to 5m0s for pod "downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280" in namespace "downward-api-3823" to be "Succeeded or Failed"
Aug 14 04:06:25.395: INFO: Pod "downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280": Phase="Pending", Reason="", readiness=false. Elapsed: 79.732656ms
Aug 14 04:06:27.406: INFO: Pod "downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.090433372s
STEP: Saw pod success
Aug 14 04:06:27.406: INFO: Pod "downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280" satisfied condition "Succeeded or Failed"
Aug 14 04:06:27.414: INFO: Trying to get logs from node worker2 pod downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280 container dapi-container: <nil>
STEP: delete the pod
Aug 14 04:06:27.470: INFO: Waiting for pod downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280 to disappear
Aug 14 04:06:27.477: INFO: Pod downward-api-dfa7d57e-a7a5-4f89-9ef6-0ed307591280 no longer exists
[AfterEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:06:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3823" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":275,"completed":135,"skipped":2314,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:06:27.505: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6114
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Aug 14 04:06:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:07:04.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6114" for this suite.

• [SLOW TEST:37.230 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":275,"completed":136,"skipped":2315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:07:04.737: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9915
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:07:04.938: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Creating first CR 
Aug 14 04:07:10.576: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:10Z]] name:name1 resourceVersion:4161564 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6aaeb745-0d5e-4526-9865-e78eea08c164] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 14 04:07:20.591: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:20Z]] name:name2 resourceVersion:4161601 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5b0ee3f2-ce4e-4172-853b-aa5975d0fbfc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 14 04:07:30.603: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:30Z]] name:name1 resourceVersion:4161633 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6aaeb745-0d5e-4526-9865-e78eea08c164] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 14 04:07:40.615: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:40Z]] name:name2 resourceVersion:4161667 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5b0ee3f2-ce4e-4172-853b-aa5975d0fbfc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 14 04:07:50.636: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:30Z]] name:name1 resourceVersion:4161701 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6aaeb745-0d5e-4526-9865-e78eea08c164] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 14 04:08:00.656: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-14T04:07:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-14T04:07:40Z]] name:name2 resourceVersion:4161734 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5b0ee3f2-ce4e-4172-853b-aa5975d0fbfc] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:11.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9915" for this suite.

• [SLOW TEST:66.469 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":275,"completed":137,"skipped":2356,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:11.207: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:08:11.955: INFO: Waiting up to 5m0s for pod "busybox-user-65534-27fa7173-76fb-4bcc-a39f-ab69548cdcd6" in namespace "security-context-test-3930" to be "Succeeded or Failed"
Aug 14 04:08:11.965: INFO: Pod "busybox-user-65534-27fa7173-76fb-4bcc-a39f-ab69548cdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.144858ms
Aug 14 04:08:13.976: INFO: Pod "busybox-user-65534-27fa7173-76fb-4bcc-a39f-ab69548cdcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020767038s
Aug 14 04:08:13.976: INFO: Pod "busybox-user-65534-27fa7173-76fb-4bcc-a39f-ab69548cdcd6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:13.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3930" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":138,"skipped":2370,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2634" for this suite.

• [SLOW TEST:13.434 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":275,"completed":139,"skipped":2375,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:27.434: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:08:27.655: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 14 04:08:32.665: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 04:08:32.665: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 14 04:08:34.673: INFO: Creating deployment "test-rollover-deployment"
Aug 14 04:08:34.691: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 14 04:08:36.705: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 14 04:08:36.789: INFO: Ensure that both replica sets have 1 created replica
Aug 14 04:08:36.802: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 14 04:08:36.822: INFO: Updating deployment test-rollover-deployment
Aug 14 04:08:36.822: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 14 04:08:38.836: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 14 04:08:38.896: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 14 04:08:38.910: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:38.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974916, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:40.993: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:40.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974919, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:42.926: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:42.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974919, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:44.926: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:44.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974919, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:46.926: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:46.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974919, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:48.996: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 04:08:48.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974919, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732974914, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:08:50.927: INFO: 
Aug 14 04:08:50.927: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Aug 14 04:08:51.003: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7130 /apis/apps/v1/namespaces/deployment-7130/deployments/test-rollover-deployment 366929b6-5315-483a-9b81-4ccba74fdba5 4162063 2 2020-08-14 04:08:34 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-14 04:08:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:08:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4004e70148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-14 04:08:34 +0000 UTC,LastTransitionTime:2020-08-14 04:08:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-08-14 04:08:49 +0000 UTC,LastTransitionTime:2020-08-14 04:08:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 04:08:51.012: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-7130 /apis/apps/v1/namespaces/deployment-7130/replicasets/test-rollover-deployment-84f7f6f64b 0cdaedc9-1ca4-42bc-8447-118e1990605f 4162051 2 2020-08-14 04:08:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 366929b6-5315-483a-9b81-4ccba74fdba5 0x4004dd3a67 0x4004dd3a68}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:08:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 54 54 57 50 57 98 54 45 53 51 49 53 45 52 56 51 97 45 57 98 56 49 45 52 99 99 98 97 55 52 102 100 98 97 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4004dd3b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:08:51.012: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 14 04:08:51.012: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7130 /apis/apps/v1/namespaces/deployment-7130/replicasets/test-rollover-controller 49498e9e-7d9e-4070-8cbc-fa99fc5a2f7c 4162062 2 2020-08-14 04:08:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 366929b6-5315-483a-9b81-4ccba74fdba5 0x4004dd3807 0x4004dd3808}] []  [{e2e.test Update apps/v1 2020-08-14 04:08:27 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:08:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 54 54 57 50 57 98 54 45 53 51 49 53 45 52 56 51 97 45 57 98 56 49 45 52 99 99 98 97 55 52 102 100 98 97 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4004dd38c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:08:51.013: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-7130 /apis/apps/v1/namespaces/deployment-7130/replicasets/test-rollover-deployment-5686c4cfd5 b3b2246c-1783-4ec5-a916-2ede3bc4390c 4161994 2 2020-08-14 04:08:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 366929b6-5315-483a-9b81-4ccba74fdba5 0x4004dd3937 0x4004dd3938}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:08:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 54 54 57 50 57 98 54 45 53 51 49 53 45 52 56 51 97 45 57 98 56 49 45 52 99 99 98 97 55 52 102 100 98 97 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4004dd39d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:08:51.025: INFO: Pod "test-rollover-deployment-84f7f6f64b-d69cp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-d69cp test-rollover-deployment-84f7f6f64b- deployment-7130 /api/v1/namespaces/deployment-7130/pods/test-rollover-deployment-84f7f6f64b-d69cp 362437e6-b92a-4c80-93e8-b17e91617b5a 4162017 0 2020-08-14 04:08:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 0cdaedc9-1ca4-42bc-8447-118e1990605f 0x4004ee0187 0x4004ee0188}] []  [{kube-controller-manager Update v1 2020-08-14 04:08:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 99 100 97 101 100 99 57 45 49 99 97 52 45 52 50 98 99 45 56 52 52 55 45 49 49 56 101 49 57 57 48 54 48 53 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:08:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 49 54 54 46 49 52 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5hpp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5hpp6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5hpp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:08:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:100.101.166.142,StartTime:2020-08-14 04:08:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:08:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker://sha256:750232f684b5317e3e93c97d2443c81838ff822811a31cbedd7c96b1ec2c47b8,ContainerID:docker://9d1568762404e0f70be0ac60287ba2373525cc0eafb379e0c66ab225bb7c5dca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.166.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:51.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7130" for this suite.

• [SLOW TEST:23.616 seconds]
[sig-apps] Deployment
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":275,"completed":140,"skipped":2377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:51.051: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Aug 14 04:08:51.272: INFO: Waiting up to 5m0s for pod "downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5" in namespace "downward-api-5333" to be "Succeeded or Failed"
Aug 14 04:08:51.284: INFO: Pod "downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.891474ms
Aug 14 04:08:53.295: INFO: Pod "downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023043533s
Aug 14 04:08:55.306: INFO: Pod "downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034107713s
STEP: Saw pod success
Aug 14 04:08:55.306: INFO: Pod "downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5" satisfied condition "Succeeded or Failed"
Aug 14 04:08:55.314: INFO: Trying to get logs from node worker2 pod downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5 container dapi-container: <nil>
STEP: delete the pod
Aug 14 04:08:55.387: INFO: Waiting for pod downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5 to disappear
Aug 14 04:08:55.396: INFO: Pod downward-api-8b769a8d-4e3e-4fd1-beae-04464f381bd5 no longer exists
[AfterEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:55.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5333" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":275,"completed":141,"skipped":2399,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:55.419: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Aug 14 04:08:55.628: INFO: Waiting up to 5m0s for pod "client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f" in namespace "containers-1961" to be "Succeeded or Failed"
Aug 14 04:08:55.637: INFO: Pod "client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.630261ms
Aug 14 04:08:57.692: INFO: Pod "client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.063843004s
STEP: Saw pod success
Aug 14 04:08:57.692: INFO: Pod "client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f" satisfied condition "Succeeded or Failed"
Aug 14 04:08:57.702: INFO: Trying to get logs from node worker2 pod client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f container test-container: <nil>
STEP: delete the pod
Aug 14 04:08:57.754: INFO: Waiting for pod client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f to disappear
Aug 14 04:08:57.764: INFO: Pod client-containers-0d7fabb0-0b6b-49d8-ae6f-fbe2b5703d4f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:57.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1961" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":275,"completed":142,"skipped":2401,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:57.789: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:08:58.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6931" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":275,"completed":143,"skipped":2422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:08:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Aug 14 04:08:58.302: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9811" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":275,"completed":144,"skipped":2461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:02.931: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5322" for this suite.

• [SLOW TEST:7.261 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":275,"completed":145,"skipped":2509,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:10.193: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7814
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:09:10.388: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:15.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7814" for this suite.

• [SLOW TEST:5.807 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":275,"completed":146,"skipped":2518,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:15.999: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:16.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4433" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":275,"completed":147,"skipped":2521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:16.402: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-6271
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6271
STEP: Deleting pre-stop pod
Aug 14 04:09:27.896: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:27.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6271" for this suite.

• [SLOW TEST:11.539 seconds]
[k8s.io] [sig-node] PreStop
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":275,"completed":148,"skipped":2574,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:27.942: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0814 04:09:29.258629      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 04:09:29.258: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:09:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1470" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":275,"completed":149,"skipped":2579,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:09:29.283: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:09:29.561: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 14 04:09:29.582: INFO: Number of nodes with available pods: 0
Aug 14 04:09:29.582: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 14 04:09:29.625: INFO: Number of nodes with available pods: 0
Aug 14 04:09:29.625: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:30.636: INFO: Number of nodes with available pods: 0
Aug 14 04:09:30.636: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:31.637: INFO: Number of nodes with available pods: 0
Aug 14 04:09:31.637: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:32.637: INFO: Number of nodes with available pods: 1
Aug 14 04:09:32.637: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 14 04:09:32.677: INFO: Number of nodes with available pods: 1
Aug 14 04:09:32.677: INFO: Number of running nodes: 0, number of available pods: 1
Aug 14 04:09:33.686: INFO: Number of nodes with available pods: 0
Aug 14 04:09:33.687: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 14 04:09:33.715: INFO: Number of nodes with available pods: 0
Aug 14 04:09:33.715: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:34.724: INFO: Number of nodes with available pods: 0
Aug 14 04:09:34.724: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:35.724: INFO: Number of nodes with available pods: 0
Aug 14 04:09:35.724: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:36.726: INFO: Number of nodes with available pods: 0
Aug 14 04:09:36.726: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:37.726: INFO: Number of nodes with available pods: 0
Aug 14 04:09:37.726: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:38.726: INFO: Number of nodes with available pods: 0
Aug 14 04:09:38.726: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:39.724: INFO: Number of nodes with available pods: 0
Aug 14 04:09:39.724: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:40.725: INFO: Number of nodes with available pods: 0
Aug 14 04:09:40.725: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:41.725: INFO: Number of nodes with available pods: 0
Aug 14 04:09:41.726: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:42.728: INFO: Number of nodes with available pods: 0
Aug 14 04:09:42.728: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:43.728: INFO: Number of nodes with available pods: 0
Aug 14 04:09:43.728: INFO: Node master2 is running more than one daemon pod
Aug 14 04:09:44.726: INFO: Number of nodes with available pods: 1
Aug 14 04:09:44.726: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6409, will wait for the garbage collector to delete the pods
Aug 14 04:09:44.823: INFO: Deleting DaemonSet.extensions daemon-set took: 21.518193ms
Aug 14 04:09:44.923: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.339901ms
Aug 14 04:10:00.732: INFO: Number of nodes with available pods: 0
Aug 14 04:10:00.732: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 04:10:00.741: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6409/daemonsets","resourceVersion":"4162747"},"items":null}

Aug 14 04:10:00.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6409/pods","resourceVersion":"4162747"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:10:00.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6409" for this suite.

• [SLOW TEST:31.559 seconds]
[sig-apps] Daemon set [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":275,"completed":150,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:10:00.843: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 04:10:05.143: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 04:10:05.152: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 04:10:07.152: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 04:10:07.163: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 04:10:09.152: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 04:10:09.163: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 04:10:11.152: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 04:10:11.161: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:10:11.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1538" for this suite.

• [SLOW TEST:10.384 seconds]
[k8s.io] Container Lifecycle Hook
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":275,"completed":151,"skipped":2638,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:10:11.227: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-nhzvm in namespace proxy-1266
I0814 04:10:11.489369      24 runners.go:190] Created replication controller with name: proxy-service-nhzvm, namespace: proxy-1266, replica count: 1
I0814 04:10:12.540980      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 04:10:13.541357      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:14.542493      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:15.542916      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:16.543243      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:17.543614      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:18.544002      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:19.544381      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:20.544720      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:21.545174      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 04:10:22.545542      24 runners.go:190] proxy-service-nhzvm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 04:10:22.554: INFO: setup took 11.105277091s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 24.656786ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 24.849086ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 24.782666ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 24.794466ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 24.884746ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 24.848886ms)
Aug 14 04:10:22.579: INFO: (0) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 24.826965ms)
Aug 14 04:10:22.580: INFO: (0) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 25.662344ms)
Aug 14 04:10:22.581: INFO: (0) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 26.401302ms)
Aug 14 04:10:22.581: INFO: (0) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 26.606162ms)
Aug 14 04:10:22.581: INFO: (0) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 26.585722ms)
Aug 14 04:10:22.583: INFO: (0) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 28.496718ms)
Aug 14 04:10:22.583: INFO: (0) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 28.514718ms)
Aug 14 04:10:22.596: INFO: (0) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 41.937608ms)
Aug 14 04:10:22.596: INFO: (0) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 41.999708ms)
Aug 14 04:10:22.596: INFO: (0) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 42.053648ms)
Aug 14 04:10:22.609: INFO: (1) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 12.504493ms)
Aug 14 04:10:22.611: INFO: (1) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 14.091049ms)
Aug 14 04:10:22.611: INFO: (1) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 14.393729ms)
Aug 14 04:10:22.611: INFO: (1) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 14.214729ms)
Aug 14 04:10:22.611: INFO: (1) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 14.288309ms)
Aug 14 04:10:22.611: INFO: (1) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.217989ms)
Aug 14 04:10:22.615: INFO: (1) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 17.561221ms)
Aug 14 04:10:22.615: INFO: (1) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 17.637901ms)
Aug 14 04:10:22.615: INFO: (1) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 17.582221ms)
Aug 14 04:10:22.615: INFO: (1) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 17.632281ms)
Aug 14 04:10:22.617: INFO: (1) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 20.563936ms)
Aug 14 04:10:22.617: INFO: (1) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 20.673615ms)
Aug 14 04:10:22.617: INFO: (1) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 20.316596ms)
Aug 14 04:10:22.617: INFO: (1) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 20.741875ms)
Aug 14 04:10:22.618: INFO: (1) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 21.412693ms)
Aug 14 04:10:22.619: INFO: (1) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 21.530413ms)
Aug 14 04:10:22.696: INFO: (2) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 77.295292ms)
Aug 14 04:10:22.697: INFO: (2) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 77.473351ms)
Aug 14 04:10:22.697: INFO: (2) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 77.626711ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 80.064445ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 80.194965ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 80.412185ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 80.372045ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 80.343305ms)
Aug 14 04:10:22.699: INFO: (2) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 80.407645ms)
Aug 14 04:10:22.700: INFO: (2) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 80.994904ms)
Aug 14 04:10:22.700: INFO: (2) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 81.447123ms)
Aug 14 04:10:22.700: INFO: (2) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 81.041984ms)
Aug 14 04:10:22.702: INFO: (2) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 82.78078ms)
Aug 14 04:10:22.702: INFO: (2) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 83.136518ms)
Aug 14 04:10:22.705: INFO: (2) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 86.097252ms)
Aug 14 04:10:22.705: INFO: (2) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 86.413172ms)
Aug 14 04:10:22.718: INFO: (3) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 11.876334ms)
Aug 14 04:10:22.718: INFO: (3) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 12.687373ms)
Aug 14 04:10:22.718: INFO: (3) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 12.930432ms)
Aug 14 04:10:22.719: INFO: (3) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 13.030311ms)
Aug 14 04:10:22.719: INFO: (3) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 12.954491ms)
Aug 14 04:10:22.721: INFO: (3) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 15.463906ms)
Aug 14 04:10:22.721: INFO: (3) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 15.252427ms)
Aug 14 04:10:22.721: INFO: (3) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 15.373947ms)
Aug 14 04:10:22.721: INFO: (3) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 15.370807ms)
Aug 14 04:10:22.721: INFO: (3) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 15.318427ms)
Aug 14 04:10:22.723: INFO: (3) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 17.611602ms)
Aug 14 04:10:22.787: INFO: (3) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 81.086004ms)
Aug 14 04:10:22.787: INFO: (3) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 80.948604ms)
Aug 14 04:10:22.787: INFO: (3) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 81.067564ms)
Aug 14 04:10:22.787: INFO: (3) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 81.027243ms)
Aug 14 04:10:22.788: INFO: (3) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 81.757742ms)
Aug 14 04:10:22.804: INFO: (4) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 16.391144ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 16.432584ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 16.623804ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 16.860703ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 16.673184ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 17.089143ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 16.896963ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 17.259183ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 17.296622ms)
Aug 14 04:10:22.805: INFO: (4) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 17.226722ms)
Aug 14 04:10:22.806: INFO: (4) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 18.0089ms)
Aug 14 04:10:22.809: INFO: (4) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 21.496773ms)
Aug 14 04:10:22.809: INFO: (4) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 21.393513ms)
Aug 14 04:10:22.809: INFO: (4) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 21.609613ms)
Aug 14 04:10:22.809: INFO: (4) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 21.417173ms)
Aug 14 04:10:22.809: INFO: (4) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 21.420253ms)
Aug 14 04:10:22.821: INFO: (5) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 11.670535ms)
Aug 14 04:10:22.824: INFO: (5) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 14.098309ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 76.786733ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 77.077392ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 76.835613ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 76.979732ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 76.987172ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 76.996212ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 77.062612ms)
Aug 14 04:10:22.887: INFO: (5) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 77.201852ms)
Aug 14 04:10:22.888: INFO: (5) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 78.747168ms)
Aug 14 04:10:22.888: INFO: (5) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 78.720168ms)
Aug 14 04:10:22.888: INFO: (5) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 78.582809ms)
Aug 14 04:10:22.888: INFO: (5) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 78.628009ms)
Aug 14 04:10:22.888: INFO: (5) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 78.542029ms)
Aug 14 04:10:22.889: INFO: (5) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 78.700608ms)
Aug 14 04:10:22.901: INFO: (6) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 12.441233ms)
Aug 14 04:10:22.901: INFO: (6) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 12.431973ms)
Aug 14 04:10:22.904: INFO: (6) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 14.934628ms)
Aug 14 04:10:22.904: INFO: (6) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 14.740948ms)
Aug 14 04:10:22.904: INFO: (6) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 15.132147ms)
Aug 14 04:10:22.904: INFO: (6) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 15.120188ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 17.730761ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 17.786901ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 17.396942ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 17.672421ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 17.764361ms)
Aug 14 04:10:22.906: INFO: (6) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 17.703941ms)
Aug 14 04:10:22.907: INFO: (6) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 17.644502ms)
Aug 14 04:10:22.913: INFO: (6) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 24.225867ms)
Aug 14 04:10:22.913: INFO: (6) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 24.385007ms)
Aug 14 04:10:22.913: INFO: (6) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 24.279387ms)
Aug 14 04:10:22.924: INFO: (7) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 10.016798ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 75.739255ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 75.690195ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 75.530556ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 75.619456ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 75.859135ms)
Aug 14 04:10:22.989: INFO: (7) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 75.571876ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 76.092534ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 76.221234ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 75.960795ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 75.958535ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 75.989175ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 76.088734ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 76.036215ms)
Aug 14 04:10:22.990: INFO: (7) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 76.081775ms)
Aug 14 04:10:22.991: INFO: (7) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 77.686871ms)
Aug 14 04:10:23.002: INFO: (8) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 10.513737ms)
Aug 14 04:10:23.002: INFO: (8) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 10.496977ms)
Aug 14 04:10:23.004: INFO: (8) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 12.117273ms)
Aug 14 04:10:23.004: INFO: (8) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 12.278313ms)
Aug 14 04:10:23.004: INFO: (8) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 12.238133ms)
Aug 14 04:10:23.004: INFO: (8) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 12.380993ms)
Aug 14 04:10:23.004: INFO: (8) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 12.473173ms)
Aug 14 04:10:23.007: INFO: (8) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 15.617186ms)
Aug 14 04:10:23.007: INFO: (8) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 15.654106ms)
Aug 14 04:10:23.007: INFO: (8) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 15.635306ms)
Aug 14 04:10:23.010: INFO: (8) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 18.714379ms)
Aug 14 04:10:23.010: INFO: (8) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 18.814699ms)
Aug 14 04:10:23.010: INFO: (8) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 18.940638ms)
Aug 14 04:10:23.010: INFO: (8) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 18.937638ms)
Aug 14 04:10:23.011: INFO: (8) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 19.081298ms)
Aug 14 04:10:23.011: INFO: (8) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 19.829237ms)
Aug 14 04:10:23.025: INFO: (9) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 13.62007ms)
Aug 14 04:10:23.025: INFO: (9) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 13.78479ms)
Aug 14 04:10:23.025: INFO: (9) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 13.59243ms)
Aug 14 04:10:23.025: INFO: (9) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 13.546151ms)
Aug 14 04:10:23.026: INFO: (9) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 13.82311ms)
Aug 14 04:10:23.026: INFO: (9) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 13.978889ms)
Aug 14 04:10:23.086: INFO: (9) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 74.283678ms)
Aug 14 04:10:23.088: INFO: (9) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 76.403914ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 76.728293ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 76.841752ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 77.113172ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 76.649673ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 77.116392ms)
Aug 14 04:10:23.089: INFO: (9) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 76.685753ms)
Aug 14 04:10:23.095: INFO: (9) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 83.149359ms)
Aug 14 04:10:23.099: INFO: (9) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 87.42171ms)
Aug 14 04:10:23.109: INFO: (10) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 10.140738ms)
Aug 14 04:10:23.109: INFO: (10) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 9.746998ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 14.291969ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.475108ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 14.509148ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 14.488428ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 14.608688ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 14.636828ms)
Aug 14 04:10:23.114: INFO: (10) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 14.524328ms)
Aug 14 04:10:23.115: INFO: (10) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 15.348326ms)
Aug 14 04:10:23.116: INFO: (10) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 16.614603ms)
Aug 14 04:10:23.118: INFO: (10) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 18.845239ms)
Aug 14 04:10:23.118: INFO: (10) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 18.3454ms)
Aug 14 04:10:23.119: INFO: (10) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 19.466037ms)
Aug 14 04:10:23.121: INFO: (10) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 21.556673ms)
Aug 14 04:10:23.121: INFO: (10) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 21.264913ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 68.301811ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 67.636012ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 67.760392ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 67.785532ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 68.067652ms)
Aug 14 04:10:23.189: INFO: (11) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 67.767372ms)
Aug 14 04:10:23.190: INFO: (11) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 68.048011ms)
Aug 14 04:10:23.194: INFO: (11) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 73.026461ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 75.723835ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 75.549056ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 75.519515ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 75.614935ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 75.798915ms)
Aug 14 04:10:23.197: INFO: (11) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 75.628315ms)
Aug 14 04:10:23.199: INFO: (11) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 77.460832ms)
Aug 14 04:10:23.199: INFO: (11) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 77.185811ms)
Aug 14 04:10:23.210: INFO: (12) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 10.928256ms)
Aug 14 04:10:23.210: INFO: (12) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 10.762696ms)
Aug 14 04:10:23.213: INFO: (12) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 13.87865ms)
Aug 14 04:10:23.213: INFO: (12) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 13.95429ms)
Aug 14 04:10:23.213: INFO: (12) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 14.208469ms)
Aug 14 04:10:23.213: INFO: (12) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 13.90685ms)
Aug 14 04:10:23.213: INFO: (12) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 14.09175ms)
Aug 14 04:10:23.214: INFO: (12) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 14.684648ms)
Aug 14 04:10:23.214: INFO: (12) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.466409ms)
Aug 14 04:10:23.214: INFO: (12) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 14.490589ms)
Aug 14 04:10:23.214: INFO: (12) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 14.616328ms)
Aug 14 04:10:23.217: INFO: (12) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 17.380722ms)
Aug 14 04:10:23.217: INFO: (12) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 17.492401ms)
Aug 14 04:10:23.217: INFO: (12) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 17.262042ms)
Aug 14 04:10:23.219: INFO: (12) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 19.913997ms)
Aug 14 04:10:23.219: INFO: (12) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 20.320275ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 69.684889ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 69.768348ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 69.677709ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 69.819447ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 69.974747ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 70.052467ms)
Aug 14 04:10:23.290: INFO: (13) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 70.012027ms)
Aug 14 04:10:23.295: INFO: (13) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 75.294896ms)
Aug 14 04:10:23.297: INFO: (13) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 77.520752ms)
Aug 14 04:10:23.297: INFO: (13) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 77.449212ms)
Aug 14 04:10:23.298: INFO: (13) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 78.20415ms)
Aug 14 04:10:23.298: INFO: (13) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 78.442529ms)
Aug 14 04:10:23.298: INFO: (13) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 78.25127ms)
Aug 14 04:10:23.298: INFO: (13) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 78.30761ms)
Aug 14 04:10:23.298: INFO: (13) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 78.613789ms)
Aug 14 04:10:23.300: INFO: (13) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 80.219345ms)
Aug 14 04:10:23.313: INFO: (14) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 12.006034ms)
Aug 14 04:10:23.314: INFO: (14) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 13.62437ms)
Aug 14 04:10:23.315: INFO: (14) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 14.736388ms)
Aug 14 04:10:23.315: INFO: (14) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 15.079647ms)
Aug 14 04:10:23.315: INFO: (14) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.666488ms)
Aug 14 04:10:23.316: INFO: (14) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 14.843288ms)
Aug 14 04:10:23.316: INFO: (14) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 15.028567ms)
Aug 14 04:10:23.316: INFO: (14) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 15.008067ms)
Aug 14 04:10:23.317: INFO: (14) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 16.226345ms)
Aug 14 04:10:23.317: INFO: (14) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 16.067745ms)
Aug 14 04:10:23.317: INFO: (14) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 16.321624ms)
Aug 14 04:10:23.318: INFO: (14) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 17.161362ms)
Aug 14 04:10:23.319: INFO: (14) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 18.25938ms)
Aug 14 04:10:23.319: INFO: (14) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 18.477739ms)
Aug 14 04:10:23.319: INFO: (14) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 18.262ms)
Aug 14 04:10:23.320: INFO: (14) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 19.740277ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 67.704233ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 67.894752ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 67.778932ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 68.209511ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 68.192751ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 68.70543ms)
Aug 14 04:10:23.389: INFO: (15) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 68.72993ms)
Aug 14 04:10:23.394: INFO: (15) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 72.358443ms)
Aug 14 04:10:23.397: INFO: (15) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 76.649174ms)
Aug 14 04:10:23.397: INFO: (15) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 76.734773ms)
Aug 14 04:10:23.398: INFO: (15) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 76.537433ms)
Aug 14 04:10:23.398: INFO: (15) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 76.421233ms)
Aug 14 04:10:23.398: INFO: (15) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 76.685273ms)
Aug 14 04:10:23.401: INFO: (15) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 80.203085ms)
Aug 14 04:10:23.404: INFO: (15) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 82.5543ms)
Aug 14 04:10:23.404: INFO: (15) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 82.6431ms)
Aug 14 04:10:23.416: INFO: (16) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 12.337313ms)
Aug 14 04:10:23.425: INFO: (16) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 20.534135ms)
Aug 14 04:10:23.425: INFO: (16) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 20.818955ms)
Aug 14 04:10:23.425: INFO: (16) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 20.956354ms)
Aug 14 04:10:23.425: INFO: (16) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 20.985634ms)
Aug 14 04:10:23.425: INFO: (16) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 21.102814ms)
Aug 14 04:10:23.426: INFO: (16) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 21.756593ms)
Aug 14 04:10:23.427: INFO: (16) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 22.382951ms)
Aug 14 04:10:23.427: INFO: (16) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 22.309212ms)
Aug 14 04:10:23.427: INFO: (16) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 22.711911ms)
Aug 14 04:10:23.427: INFO: (16) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 22.455632ms)
Aug 14 04:10:23.432: INFO: (16) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 27.767019ms)
Aug 14 04:10:23.433: INFO: (16) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 28.835617ms)
Aug 14 04:10:23.487: INFO: (16) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 82.65286ms)
Aug 14 04:10:23.487: INFO: (16) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 82.5911ms)
Aug 14 04:10:23.487: INFO: (16) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 82.763759ms)
Aug 14 04:10:23.500: INFO: (17) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 12.721132ms)
Aug 14 04:10:23.500: INFO: (17) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 12.620333ms)
Aug 14 04:10:23.500: INFO: (17) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 12.729433ms)
Aug 14 04:10:23.500: INFO: (17) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 12.651053ms)
Aug 14 04:10:23.500: INFO: (17) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 12.686793ms)
Aug 14 04:10:23.501: INFO: (17) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.006929ms)
Aug 14 04:10:23.501: INFO: (17) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 13.73865ms)
Aug 14 04:10:23.501: INFO: (17) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 13.94111ms)
Aug 14 04:10:23.502: INFO: (17) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 14.051629ms)
Aug 14 04:10:23.502: INFO: (17) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 13.89645ms)
Aug 14 04:10:23.503: INFO: (17) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 15.123227ms)
Aug 14 04:10:23.504: INFO: (17) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 16.775004ms)
Aug 14 04:10:23.504: INFO: (17) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 16.852424ms)
Aug 14 04:10:23.507: INFO: (17) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 19.738437ms)
Aug 14 04:10:23.507: INFO: (17) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 19.782137ms)
Aug 14 04:10:23.507: INFO: (17) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 19.688737ms)
Aug 14 04:10:23.518: INFO: (18) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 10.579217ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 79.971066ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 79.988366ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 80.011786ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 79.643147ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 79.844646ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 79.840626ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 79.931726ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 80.347925ms)
Aug 14 04:10:23.588: INFO: (18) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 79.627387ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 81.470502ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 80.884864ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 81.286703ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 81.209103ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 80.865144ms)
Aug 14 04:10:23.589: INFO: (18) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 80.913864ms)
Aug 14 04:10:23.600: INFO: (19) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">... (200; 10.473117ms)
Aug 14 04:10:23.602: INFO: (19) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:443/proxy/tlsrewritem... (200; 12.414033ms)
Aug 14 04:10:23.602: INFO: (19) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:1080/proxy/rewriteme">test<... (200; 12.249874ms)
Aug 14 04:10:23.602: INFO: (19) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 12.454732ms)
Aug 14 04:10:23.602: INFO: (19) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:162/proxy/: bar (200; 12.516532ms)
Aug 14 04:10:23.602: INFO: (19) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:462/proxy/: tls qux (200; 12.487093ms)
Aug 14 04:10:23.604: INFO: (19) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.077829ms)
Aug 14 04:10:23.604: INFO: (19) /api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/: <a href="/api/v1/namespaces/proxy-1266/pods/proxy-service-nhzvm-hg52d/proxy/rewriteme">test</a> (200; 14.103709ms)
Aug 14 04:10:23.604: INFO: (19) /api/v1/namespaces/proxy-1266/pods/https:proxy-service-nhzvm-hg52d:460/proxy/: tls baz (200; 13.97933ms)
Aug 14 04:10:23.604: INFO: (19) /api/v1/namespaces/proxy-1266/pods/http:proxy-service-nhzvm-hg52d:160/proxy/: foo (200; 14.293769ms)
Aug 14 04:10:23.604: INFO: (19) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname2/proxy/: bar (200; 14.619068ms)
Aug 14 04:10:23.608: INFO: (19) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname2/proxy/: tls qux (200; 17.665962ms)
Aug 14 04:10:23.608: INFO: (19) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname2/proxy/: bar (200; 17.841381ms)
Aug 14 04:10:23.608: INFO: (19) /api/v1/namespaces/proxy-1266/services/proxy-service-nhzvm:portname1/proxy/: foo (200; 17.847661ms)
Aug 14 04:10:23.608: INFO: (19) /api/v1/namespaces/proxy-1266/services/https:proxy-service-nhzvm:tlsportname1/proxy/: tls baz (200; 18.445199ms)
Aug 14 04:10:23.609: INFO: (19) /api/v1/namespaces/proxy-1266/services/http:proxy-service-nhzvm:portname1/proxy/: foo (200; 19.079538ms)
STEP: deleting ReplicationController proxy-service-nhzvm in namespace proxy-1266, will wait for the garbage collector to delete the pods
Aug 14 04:10:23.704: INFO: Deleting ReplicationController proxy-service-nhzvm took: 35.616163ms
Aug 14 04:10:24.104: INFO: Terminating ReplicationController proxy-service-nhzvm pods took: 400.353828ms
[AfterEach] version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:10:36.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1266" for this suite.

• [SLOW TEST:25.103 seconds]
[sig-network] Proxy
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":275,"completed":152,"skipped":2642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:10:36.331: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-6ed84b33-7c4b-4b9a-af56-7a361826f923
STEP: Creating secret with name secret-projected-all-test-volume-ddc16942-28bd-4e9f-b5ea-b687ac9fa72e
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 14 04:10:36.560: INFO: Waiting up to 5m0s for pod "projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25" in namespace "projected-7124" to be "Succeeded or Failed"
Aug 14 04:10:36.569: INFO: Pod "projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25": Phase="Pending", Reason="", readiness=false. Elapsed: 9.17018ms
Aug 14 04:10:38.596: INFO: Pod "projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036053225s
Aug 14 04:10:40.606: INFO: Pod "projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045984047s
STEP: Saw pod success
Aug 14 04:10:40.606: INFO: Pod "projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25" satisfied condition "Succeeded or Failed"
Aug 14 04:10:40.616: INFO: Trying to get logs from node worker2 pod projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 14 04:10:40.738: INFO: Waiting for pod projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25 to disappear
Aug 14 04:10:40.746: INFO: Pod projected-volume-d6b73d63-c162-4ad5-bb04-306e414a7c25 no longer exists
[AfterEach] [sig-storage] Projected combined
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:10:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7124" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":275,"completed":153,"skipped":2686,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:10:40.772: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-2p7g
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 04:10:41.005: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2p7g" in namespace "subpath-300" to be "Succeeded or Failed"
Aug 14 04:10:41.013: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Pending", Reason="", readiness=false. Elapsed: 7.790503ms
Aug 14 04:10:43.024: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019029682s
Aug 14 04:10:45.034: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 4.028523685s
Aug 14 04:10:47.044: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 6.038588707s
Aug 14 04:10:49.053: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 8.04789081s
Aug 14 04:10:51.063: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 10.057337333s
Aug 14 04:10:53.093: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 12.087378732s
Aug 14 04:10:55.102: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 14.096588735s
Aug 14 04:10:57.112: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 16.106329738s
Aug 14 04:10:59.124: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 18.118584595s
Aug 14 04:11:01.134: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 20.128834356s
Aug 14 04:11:03.192: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Running", Reason="", readiness=true. Elapsed: 22.186236195s
Aug 14 04:11:05.202: INFO: Pod "pod-subpath-test-projected-2p7g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.196114957s
STEP: Saw pod success
Aug 14 04:11:05.202: INFO: Pod "pod-subpath-test-projected-2p7g" satisfied condition "Succeeded or Failed"
Aug 14 04:11:05.210: INFO: Trying to get logs from node worker2 pod pod-subpath-test-projected-2p7g container test-container-subpath-projected-2p7g: <nil>
STEP: delete the pod
Aug 14 04:11:05.263: INFO: Waiting for pod pod-subpath-test-projected-2p7g to disappear
Aug 14 04:11:05.271: INFO: Pod pod-subpath-test-projected-2p7g no longer exists
STEP: Deleting pod pod-subpath-test-projected-2p7g
Aug 14 04:11:05.271: INFO: Deleting pod "pod-subpath-test-projected-2p7g" in namespace "subpath-300"
[AfterEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:11:05.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-300" for this suite.

• [SLOW TEST:24.531 seconds]
[sig-storage] Subpath
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":275,"completed":154,"skipped":2694,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:11:05.304: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-59
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-0095d9f3-97c8-4efe-92c7-bfd04a0abd23
STEP: Creating a pod to test consume secrets
Aug 14 04:11:05.533: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9" in namespace "projected-59" to be "Succeeded or Failed"
Aug 14 04:11:05.541: INFO: Pod "pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.347722ms
Aug 14 04:11:07.552: INFO: Pod "pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019635681s
Aug 14 04:11:09.562: INFO: Pod "pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029262044s
STEP: Saw pod success
Aug 14 04:11:09.562: INFO: Pod "pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9" satisfied condition "Succeeded or Failed"
Aug 14 04:11:09.571: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 04:11:09.623: INFO: Waiting for pod pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9 to disappear
Aug 14 04:11:09.632: INFO: Pod pod-projected-secrets-4e682295-0910-4c75-beaf-9399ea6b38a9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:11:09.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-59" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":155,"skipped":2694,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:11:09.655: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-652e2f41-c2c6-4883-ba21-f51b64a32168 in namespace container-probe-8158
Aug 14 04:11:11.886: INFO: Started pod busybox-652e2f41-c2c6-4883-ba21-f51b64a32168 in namespace container-probe-8158
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 04:11:11.895: INFO: Initial restart count of pod busybox-652e2f41-c2c6-4883-ba21-f51b64a32168 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:13.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8158" for this suite.

• [SLOW TEST:243.906 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":156,"skipped":2694,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:13.561: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 04:15:16.361: INFO: Successfully updated pod "pod-update-5bec6cd2-54bc-44d8-ba25-aff0328e7c0d"
STEP: verifying the updated pod is in kubernetes
Aug 14 04:15:16.381: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:16.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1482" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":275,"completed":157,"skipped":2700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:16.414: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9853
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 14 04:15:16.639: INFO: Waiting up to 5m0s for pod "pod-90268945-c084-4678-9804-d13c60e3f755" in namespace "emptydir-9853" to be "Succeeded or Failed"
Aug 14 04:15:16.649: INFO: Pod "pod-90268945-c084-4678-9804-d13c60e3f755": Phase="Pending", Reason="", readiness=false. Elapsed: 9.668519ms
Aug 14 04:15:18.659: INFO: Pod "pod-90268945-c084-4678-9804-d13c60e3f755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019968336s
Aug 14 04:15:20.670: INFO: Pod "pod-90268945-c084-4678-9804-d13c60e3f755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030995772s
STEP: Saw pod success
Aug 14 04:15:20.670: INFO: Pod "pod-90268945-c084-4678-9804-d13c60e3f755" satisfied condition "Succeeded or Failed"
Aug 14 04:15:20.679: INFO: Trying to get logs from node worker2 pod pod-90268945-c084-4678-9804-d13c60e3f755 container test-container: <nil>
STEP: delete the pod
Aug 14 04:15:20.761: INFO: Waiting for pod pod-90268945-c084-4678-9804-d13c60e3f755 to disappear
Aug 14 04:15:20.770: INFO: Pod pod-90268945-c084-4678-9804-d13c60e3f755 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9853" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":158,"skipped":2793,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:20.796: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 14 04:15:23.051: INFO: &Pod{ObjectMeta:{send-events-08354eb7-ee02-44e8-89a6-ee915d150635  events-3818 /api/v1/namespaces/events-3818/pods/send-events-08354eb7-ee02-44e8-89a6-ee915d150635 7b27d699-71aa-4a6a-a23a-e6c6818c4ae5 4164184 0 2020-08-14 04:15:21 +0000 UTC <nil> <nil> map[name:foo time:993344479] map[] [] []  [{e2e.test Update v1 2020-08-14 04:15:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:15:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 49 56 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lxwbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lxwbd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lxwbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.188,StartTime:2020-08-14 04:15:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:15:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker://sha256:750232f684b5317e3e93c97d2443c81838ff822811a31cbedd7c96b1ec2c47b8,ContainerID:docker://5e6e5e983d8c8f1dad4102b68610398b78f19ea203078913e3a90f6488e67645,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 14 04:15:25.060: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 14 04:15:27.068: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:27.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3818" for this suite.

• [SLOW TEST:6.317 seconds]
[k8s.io] [sig-node] Events
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":275,"completed":159,"skipped":2795,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:27.113: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3152" for this suite.

• [SLOW TEST:16.464 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":275,"completed":160,"skipped":2801,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:43.578: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:15:43.803: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 14 04:15:48.813: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 04:15:48.813: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Aug 14 04:15:48.860: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/deployments/test-cleanup-deployment 09f88ae6-f706-4406-a283-43949473efce 4164383 1 2020-08-14 04:15:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-08-14 04:15:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4005123888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 14 04:15:48.869: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/replicasets/test-cleanup-deployment-b4867b47f e8dc769d-b5ff-42ca-8448-327914fa430e 4164386 1 2020-08-14 04:15:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 09f88ae6-f706-4406-a283-43949473efce 0x40034321d0 0x40034321d1}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:15:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 57 102 56 56 97 101 54 45 102 55 48 54 45 52 52 48 54 45 97 50 56 51 45 52 51 57 52 57 52 55 51 101 102 99 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4003432248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:15:48.869: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 14 04:15:48.870: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/replicasets/test-cleanup-controller 7347de17-c781-4e4d-803e-d7a0b3f1d320 4164385 1 2020-08-14 04:15:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 09f88ae6-f706-4406-a283-43949473efce 0x40034320c7 0x40034320c8}] []  [{e2e.test Update apps/v1 2020-08-14 04:15:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:15:48 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 57 102 56 56 97 101 54 45 102 55 48 54 45 52 52 48 54 45 97 50 56 51 45 52 51 57 52 57 52 55 51 101 102 99 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4003432168 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:15:48.884: INFO: Pod "test-cleanup-controller-ccbkb" is available:
&Pod{ObjectMeta:{test-cleanup-controller-ccbkb test-cleanup-controller- deployment-6986 /api/v1/namespaces/deployment-6986/pods/test-cleanup-controller-ccbkb 599e8a0e-e641-4f06-ac79-cb33669b90ec 4164366 0 2020-08-14 04:15:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 7347de17-c781-4e4d-803e-d7a0b3f1d320 0x4005123c47 0x4005123c48}] []  [{kube-controller-manager Update v1 2020-08-14 04:15:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 51 52 55 100 101 49 55 45 99 55 56 49 45 52 101 52 100 45 56 48 51 101 45 100 55 97 48 98 51 102 49 100 51 50 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:15:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 49 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zhmhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zhmhv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zhmhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:15:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.111,StartTime:2020-08-14 04:15:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:15:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://2e1675ea4ab6feed078e7d9b2e58d357607c7db904e896bb2b1e57443c96f08f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:15:48.884: INFO: Pod "test-cleanup-deployment-b4867b47f-gvpt8" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-gvpt8 test-cleanup-deployment-b4867b47f- deployment-6986 /api/v1/namespaces/deployment-6986/pods/test-cleanup-deployment-b4867b47f-gvpt8 9b64a690-8da9-4e5d-bc5f-3313d787bccb 4164389 0 2020-08-14 04:15:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f e8dc769d-b5ff-42ca-8448-327914fa430e 0x4005123e00 0x4005123e01}] []  [{kube-controller-manager Update v1 2020-08-14 04:15:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 56 100 99 55 54 57 100 45 98 53 102 102 45 52 50 99 97 45 56 52 52 56 45 51 50 55 57 49 52 102 97 52 51 48 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zhmhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zhmhv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zhmhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:48.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6986" for this suite.

• [SLOW TEST:5.348 seconds]
[sig-apps] Deployment
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":275,"completed":161,"skipped":2817,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:48.927: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 04:15:49.152: INFO: Waiting up to 5m0s for pod "pod-c1bc8c95-8764-4615-a25d-8c10610a818c" in namespace "emptydir-1773" to be "Succeeded or Failed"
Aug 14 04:15:49.160: INFO: Pod "pod-c1bc8c95-8764-4615-a25d-8c10610a818c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.964443ms
Aug 14 04:15:51.193: INFO: Pod "pod-c1bc8c95-8764-4615-a25d-8c10610a818c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040163993s
STEP: Saw pod success
Aug 14 04:15:51.193: INFO: Pod "pod-c1bc8c95-8764-4615-a25d-8c10610a818c" satisfied condition "Succeeded or Failed"
Aug 14 04:15:51.203: INFO: Trying to get logs from node worker2 pod pod-c1bc8c95-8764-4615-a25d-8c10610a818c container test-container: <nil>
STEP: delete the pod
Aug 14 04:15:51.258: INFO: Waiting for pod pod-c1bc8c95-8764-4615-a25d-8c10610a818c to disappear
Aug 14 04:15:51.265: INFO: Pod pod-c1bc8c95-8764-4615-a25d-8c10610a818c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:15:51.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1773" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":162,"skipped":2825,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:15:51.294: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Aug 14 04:15:51.493: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 04:15:51.533: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 04:15:51.543: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 04:15:51.578: INFO: kube-apiserver-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:15:51.578: INFO: cke-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:15:51.578: INFO: kube-scheduler-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:15:51.578: INFO: keepalived-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:15:51.578: INFO: coredns-bwnkm from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:15:51.578: INFO: nginx-proxy-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:15:51.578: INFO: kube-proxy-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:15:51.578: INFO: calico-node-qdxq4 from kube-system started at 2020-07-28 08:56:36 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:15:51.578: INFO: kube-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.578: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:15:51.578: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 04:15:51.614: INFO: kube-scheduler-master2 from kube-system started at 2020-08-04 08:03:47 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:15:51.614: INFO: calico-node-pk8lr from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:15:51.614: INFO: coredns-r5mc7 from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:15:51.614: INFO: kube-controller-manager-master2 from kube-system started at 2020-08-04 07:58:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:15:51.614: INFO: nginx-proxy-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:15:51.614: INFO: keepalived-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:15:51.614: INFO: kube-apiserver-master2 from kube-system started at 2020-08-04 07:58:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container kube-apiserver ready: true, restart count 1
Aug 14 04:15:51.614: INFO: kube-proxy-master2 from kube-system started at 2020-08-14 01:40:02 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:15:51.614: INFO: cke-controller-manager-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.614: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:15:51.614: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 04:15:51.649: INFO: keepalived-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:15:51.649: INFO: cke-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:15:51.649: INFO: coredns-s2kck from kube-system started at 2020-07-28 08:58:10 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:15:51.649: INFO: nginx-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:15:51.649: INFO: kube-apiserver-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:15:51.649: INFO: calico-node-vv7tl from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:15:51.649: INFO: kube-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:15:51.649: INFO: kube-scheduler-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:15:51.649: INFO: kube-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:15:51.649: INFO: tiller-deploy-cf7dd5dc6-thqqn from kube-system started at 2020-08-04 03:48:03 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.649: INFO: 	Container tiller ready: true, restart count 1
Aug 14 04:15:51.649: INFO: 
Logging pods the kubelet thinks is on node worker1 before test
Aug 14 04:15:51.683: INFO: calico-node-5qkcf from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.683: INFO: 	Container calico-node ready: true, restart count 2
Aug 14 04:15:51.683: INFO: auto-office-deploy-6d96bc4bc6-smxlw from auto-office started at 2020-08-12 10:37:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.683: INFO: 	Container auto-office ready: true, restart count 0
Aug 14 04:15:51.683: INFO: sonobuoy-e2e-job-a0c50eafb2fc4380 from sonobuoy started at 2020-08-14 03:30:29 +0000 UTC (2 container statuses recorded)
Aug 14 04:15:51.683: INFO: 	Container e2e ready: true, restart count 0
Aug 14 04:15:51.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 04:15:51.683: INFO: test-cleanup-deployment-b4867b47f-gvpt8 from deployment-6986 started at 2020-08-14 04:15:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.683: INFO: 	Container agnhost ready: true, restart count 0
Aug 14 04:15:51.683: INFO: kube-proxy-worker1 from kube-system started at 2020-08-14 01:41:42 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.683: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:15:51.683: INFO: 
Logging pods the kubelet thinks is on node worker2 before test
Aug 14 04:15:51.701: INFO: sonobuoy from sonobuoy started at 2020-08-14 03:30:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.701: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 14 04:15:51.701: INFO: send-events-08354eb7-ee02-44e8-89a6-ee915d150635 from events-3818 started at 2020-08-14 04:15:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.701: INFO: 	Container p ready: true, restart count 0
Aug 14 04:15:51.701: INFO: calico-node-77qqt from kube-system started at 2020-08-11 09:14:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.701: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:15:51.701: INFO: calico-kube-controllers-575796f75c-qrr79 from kube-system started at 2020-08-11 13:00:14 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.701: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 04:15:51.701: INFO: kube-proxy-worker2 from kube-system started at 2020-08-14 01:41:56 +0000 UTC (1 container statuses recorded)
Aug 14 04:15:51.701: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4b91d0a3-e4c9-49d6-87a7-d839003891f6 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4b91d0a3-e4c9-49d6-87a7-d839003891f6 off the node worker2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4b91d0a3-e4c9-49d6-87a7-d839003891f6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:16:08.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3165" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:16.742 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":275,"completed":163,"skipped":2838,"failed":0}
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:16:08.036: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 14 04:16:08.277: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 14 04:16:17.414: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:16:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4773" for this suite.

• [SLOW TEST:9.411 seconds]
[k8s.io] Pods
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":275,"completed":164,"skipped":2838,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:16:17.447: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:16:20.733: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:16:22.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975380, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975380, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975380, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975380, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:16:25.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 14 04:16:25.935: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:16:25.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1524" for this suite.
STEP: Destroying namespace "webhook-1524-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.670 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":275,"completed":165,"skipped":2838,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:16:26.117: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:16:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2399" for this suite.

• [SLOW TEST:16.619 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":275,"completed":166,"skipped":2842,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:16:42.737: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Aug 14 04:16:42.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-3290 -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 14 04:16:43.277: INFO: stderr: ""
Aug 14 04:16:43.277: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Aug 14 04:16:43.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 14 04:16:43.277: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3290" to be "running and ready, or succeeded"
Aug 14 04:16:43.293: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 15.427867ms
Aug 14 04:16:45.303: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025951023s
Aug 14 04:16:45.303: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 14 04:16:45.303: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 14 04:16:45.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290'
Aug 14 04:16:45.506: INFO: stderr: ""
Aug 14 04:16:45.506: INFO: stdout: "I0814 04:16:44.810754       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d72 347\nI0814 04:16:45.011015       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/7mz 572\nI0814 04:16:45.210960       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/glvm 423\nI0814 04:16:45.410972       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/tsz 360\n"
STEP: limiting log lines
Aug 14 04:16:45.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290 --tail=1'
Aug 14 04:16:45.716: INFO: stderr: ""
Aug 14 04:16:45.716: INFO: stdout: "I0814 04:16:45.610951       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zr9f 435\n"
Aug 14 04:16:45.716: INFO: got output "I0814 04:16:45.610951       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zr9f 435\n"
STEP: limiting log bytes
Aug 14 04:16:45.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290 --limit-bytes=1'
Aug 14 04:16:45.893: INFO: stderr: ""
Aug 14 04:16:45.893: INFO: stdout: "I"
Aug 14 04:16:45.893: INFO: got output "I"
STEP: exposing timestamps
Aug 14 04:16:45.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290 --tail=1 --timestamps'
Aug 14 04:16:46.087: INFO: stderr: ""
Aug 14 04:16:46.087: INFO: stdout: "2020-08-14T04:16:46.01124088Z I0814 04:16:46.010994       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/zldz 558\n"
Aug 14 04:16:46.087: INFO: got output "2020-08-14T04:16:46.01124088Z I0814 04:16:46.010994       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/zldz 558\n"
STEP: restricting to a time range
Aug 14 04:16:48.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290 --since=1s'
Aug 14 04:16:48.788: INFO: stderr: ""
Aug 14 04:16:48.788: INFO: stdout: "I0814 04:16:47.810933       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/dvbf 202\nI0814 04:16:48.010931       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/8b6c 517\nI0814 04:16:48.210966       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mgkx 438\nI0814 04:16:48.410953       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/fzz 267\nI0814 04:16:48.610965       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/x99b 349\n"
Aug 14 04:16:48.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 logs logs-generator logs-generator --namespace=kubectl-3290 --since=24h'
Aug 14 04:16:49.001: INFO: stderr: ""
Aug 14 04:16:49.001: INFO: stdout: "I0814 04:16:44.810754       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d72 347\nI0814 04:16:45.011015       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/7mz 572\nI0814 04:16:45.210960       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/glvm 423\nI0814 04:16:45.410972       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/tsz 360\nI0814 04:16:45.610951       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zr9f 435\nI0814 04:16:45.810957       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/p9pf 360\nI0814 04:16:46.010994       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/zldz 558\nI0814 04:16:46.210960       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/jrq5 265\nI0814 04:16:46.410951       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/pwsc 212\nI0814 04:16:46.610954       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/hqkq 536\nI0814 04:16:46.810990       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/nrn 389\nI0814 04:16:47.011014       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/qdz 287\nI0814 04:16:47.210962       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/pmls 591\nI0814 04:16:47.410940       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/ws5s 372\nI0814 04:16:47.610940       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/7j8c 577\nI0814 04:16:47.810933       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/dvbf 202\nI0814 04:16:48.010931       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/8b6c 517\nI0814 04:16:48.210966       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mgkx 438\nI0814 04:16:48.410953       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/fzz 267\nI0814 04:16:48.610965       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/x99b 349\nI0814 04:16:48.810948       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/zhk 378\n"
[AfterEach] Kubectl logs
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Aug 14 04:16:49.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete pod logs-generator --namespace=kubectl-3290'
Aug 14 04:16:56.166: INFO: stderr: ""
Aug 14 04:16:56.167: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:16:56.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3290" for this suite.

• [SLOW TEST:13.456 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":275,"completed":167,"skipped":2863,"failed":0}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:16:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 14 04:17:04.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:04.541: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:04.842: INFO: Exec stderr: ""
Aug 14 04:17:04.842: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:04.842: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:05.254: INFO: Exec stderr: ""
Aug 14 04:17:05.254: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:05.254: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:05.547: INFO: Exec stderr: ""
Aug 14 04:17:05.547: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:05.890: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 14 04:17:05.890: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:05.890: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:06.189: INFO: Exec stderr: ""
Aug 14 04:17:06.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:06.189: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:06.561: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 14 04:17:06.561: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:06.561: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:06.829: INFO: Exec stderr: ""
Aug 14 04:17:06.829: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:06.829: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:07.082: INFO: Exec stderr: ""
Aug 14 04:17:07.082: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:07.336: INFO: Exec stderr: ""
Aug 14 04:17:07.336: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:17:07.336: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:17:07.623: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:17:07.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2577" for this suite.

• [SLOW TEST:11.455 seconds]
[k8s.io] KubeletManagedEtcHosts
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":168,"skipped":2868,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:17:07.650: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Aug 14 04:17:07.844: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 04:17:07.879: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 04:17:07.886: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 04:17:07.902: INFO: kube-scheduler-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:17:07.902: INFO: keepalived-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:17:07.902: INFO: coredns-bwnkm from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:17:07.902: INFO: nginx-proxy-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:17:07.902: INFO: kube-proxy-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:17:07.902: INFO: calico-node-qdxq4 from kube-system started at 2020-07-28 08:56:36 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.902: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:17:07.903: INFO: kube-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.903: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:17:07.903: INFO: kube-apiserver-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.903: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:17:07.903: INFO: cke-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.903: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:17:07.903: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 04:17:07.923: INFO: kube-apiserver-master2 from kube-system started at 2020-08-04 07:58:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container kube-apiserver ready: true, restart count 1
Aug 14 04:17:07.923: INFO: calico-node-pk8lr from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:17:07.923: INFO: coredns-r5mc7 from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:17:07.923: INFO: kube-controller-manager-master2 from kube-system started at 2020-08-04 07:58:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:17:07.923: INFO: nginx-proxy-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:17:07.923: INFO: keepalived-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:17:07.923: INFO: kube-proxy-master2 from kube-system started at 2020-08-14 01:40:02 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:17:07.923: INFO: cke-controller-manager-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:17:07.923: INFO: kube-scheduler-master2 from kube-system started at 2020-08-04 08:03:47 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.923: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:17:07.923: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 04:17:07.942: INFO: kube-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:17:07.942: INFO: kube-scheduler-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:17:07.942: INFO: kube-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:17:07.942: INFO: tiller-deploy-cf7dd5dc6-thqqn from kube-system started at 2020-08-04 03:48:03 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container tiller ready: true, restart count 1
Aug 14 04:17:07.942: INFO: keepalived-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:17:07.942: INFO: cke-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:17:07.942: INFO: coredns-s2kck from kube-system started at 2020-07-28 08:58:10 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:17:07.942: INFO: nginx-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:17:07.942: INFO: kube-apiserver-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:17:07.942: INFO: calico-node-vv7tl from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.942: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:17:07.942: INFO: 
Logging pods the kubelet thinks is on node worker1 before test
Aug 14 04:17:07.958: INFO: calico-node-5qkcf from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.958: INFO: 	Container calico-node ready: true, restart count 2
Aug 14 04:17:07.958: INFO: auto-office-deploy-6d96bc4bc6-smxlw from auto-office started at 2020-08-12 10:37:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.958: INFO: 	Container auto-office ready: true, restart count 0
Aug 14 04:17:07.958: INFO: sonobuoy-e2e-job-a0c50eafb2fc4380 from sonobuoy started at 2020-08-14 03:30:29 +0000 UTC (2 container statuses recorded)
Aug 14 04:17:07.958: INFO: 	Container e2e ready: true, restart count 0
Aug 14 04:17:07.958: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 04:17:07.958: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-2577 started at 2020-08-14 04:17:00 +0000 UTC (2 container statuses recorded)
Aug 14 04:17:07.958: INFO: 	Container busybox-1 ready: true, restart count 0
Aug 14 04:17:07.958: INFO: 	Container busybox-2 ready: true, restart count 0
Aug 14 04:17:07.958: INFO: kube-proxy-worker1 from kube-system started at 2020-08-14 01:41:42 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.958: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:17:07.958: INFO: 
Logging pods the kubelet thinks is on node worker2 before test
Aug 14 04:17:07.974: INFO: kube-proxy-worker2 from kube-system started at 2020-08-14 01:41:56 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.974: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:17:07.974: INFO: test-pod from e2e-kubelet-etc-hosts-2577 started at 2020-08-14 04:16:56 +0000 UTC (3 container statuses recorded)
Aug 14 04:17:07.974: INFO: 	Container busybox-1 ready: true, restart count 0
Aug 14 04:17:07.974: INFO: 	Container busybox-2 ready: true, restart count 0
Aug 14 04:17:07.974: INFO: 	Container busybox-3 ready: true, restart count 0
Aug 14 04:17:07.974: INFO: calico-node-77qqt from kube-system started at 2020-08-11 09:14:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.974: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:17:07.974: INFO: calico-kube-controllers-575796f75c-qrr79 from kube-system started at 2020-08-11 13:00:14 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.974: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 04:17:07.974: INFO: sonobuoy from sonobuoy started at 2020-08-14 03:30:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:17:07.974: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5370995a-4276-4cbf-a12b-e043f59039b1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-5370995a-4276-4cbf-a12b-e043f59039b1 off the node worker2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5370995a-4276-4cbf-a12b-e043f59039b1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:14.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5180" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:306.618 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":275,"completed":169,"skipped":2876,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:14.269: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:22:14.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797" in namespace "downward-api-9053" to be "Succeeded or Failed"
Aug 14 04:22:14.509: INFO: Pod "downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797": Phase="Pending", Reason="", readiness=false. Elapsed: 9.769199ms
Aug 14 04:22:16.519: INFO: Pod "downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020129656s
Aug 14 04:22:18.528: INFO: Pod "downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028947216s
STEP: Saw pod success
Aug 14 04:22:18.528: INFO: Pod "downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797" satisfied condition "Succeeded or Failed"
Aug 14 04:22:18.535: INFO: Trying to get logs from node worker2 pod downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797 container client-container: <nil>
STEP: delete the pod
Aug 14 04:22:18.604: INFO: Waiting for pod downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797 to disappear
Aug 14 04:22:18.612: INFO: Pod downwardapi-volume-5dd91abb-8160-41c2-8008-cbe109847797 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:18.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9053" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":275,"completed":170,"skipped":2890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Aug 14 04:22:18.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-5482'
Aug 14 04:22:19.459: INFO: stderr: ""
Aug 14 04:22:19.459: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 14 04:22:20.468: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 04:22:20.469: INFO: Found 0 / 1
Aug 14 04:22:21.472: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 04:22:21.473: INFO: Found 1 / 1
Aug 14 04:22:21.473: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 14 04:22:21.483: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 04:22:21.483: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 04:22:21.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 patch pod agnhost-master-p8pcm --namespace=kubectl-5482 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 14 04:22:21.660: INFO: stderr: ""
Aug 14 04:22:21.660: INFO: stdout: "pod/agnhost-master-p8pcm patched\n"
STEP: checking annotations
Aug 14 04:22:21.670: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 14 04:22:21.671: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:21.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5482" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":275,"completed":171,"skipped":2925,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:21.696: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:22:21.895: INFO: Creating deployment "webserver-deployment"
Aug 14 04:22:21.905: INFO: Waiting for observed generation 1
Aug 14 04:22:23.926: INFO: Waiting for all required pods to come up
Aug 14 04:22:23.942: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 14 04:22:27.971: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 14 04:22:27.986: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 14 04:22:28.005: INFO: Updating deployment webserver-deployment
Aug 14 04:22:28.005: INFO: Waiting for observed generation 2
Aug 14 04:22:30.019: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 14 04:22:30.026: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 14 04:22:30.034: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 14 04:22:30.056: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 14 04:22:30.056: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 14 04:22:30.063: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 14 04:22:30.078: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 14 04:22:30.078: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 14 04:22:30.099: INFO: Updating deployment webserver-deployment
Aug 14 04:22:30.099: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 14 04:22:30.118: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 14 04:22:30.126: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Aug 14 04:22:30.153: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-127 /apis/apps/v1/namespaces/deployment-127/deployments/webserver-deployment 2f26c739-b68d-4a39-8332-a2c983a63eed 4166606 3 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4005822be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-08-14 04:22:28 +0000 UTC,LastTransitionTime:2020-08-14 04:22:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-14 04:22:30 +0000 UTC,LastTransitionTime:2020-08-14 04:22:30 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 14 04:22:30.162: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-127 /apis/apps/v1/namespaces/deployment-127/replicasets/webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 4166603 3 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2f26c739-b68d-4a39-8332-a2c983a63eed 0x40058230a7 0x40058230a8}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 102 50 54 99 55 51 57 45 98 54 56 100 45 52 97 51 57 45 56 51 51 50 45 97 50 99 57 56 51 97 54 51 101 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4005823128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:22:30.162: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 14 04:22:30.163: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-127 /apis/apps/v1/namespaces/deployment-127/replicasets/webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 4166600 3 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2f26c739-b68d-4a39-8332-a2c983a63eed 0x40058231b7 0x40058231b8}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 102 50 54 99 55 51 57 45 98 54 56 100 45 52 97 51 57 45 56 51 51 50 45 97 50 99 57 56 51 97 54 51 101 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4005823228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:22:30.194: INFO: Pod "webserver-deployment-6676bcd6d4-25f8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-25f8f webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-25f8f 853f7b21-3336-4292-8731-04bba2fb35d8 4166610 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237837 0x4005237838}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.195: INFO: Pod "webserver-deployment-6676bcd6d4-2mzbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-2mzbs webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-2mzbs b13b0713-9965-414d-a23c-ba1c82ca2674 4166555 0 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237950 0x4005237951}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:,StartTime:2020-08-14 04:22:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.196: INFO: Pod "webserver-deployment-6676bcd6d4-d6c9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-d6c9h webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-d6c9h 79458c17-da27-405e-bc30-3d8566030148 4166613 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237af7 0x4005237af8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.196: INFO: Pod "webserver-deployment-6676bcd6d4-dzr56" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-dzr56 webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-dzr56 cdbc380f-fc5d-42af-b3a1-a0774864b48d 4166571 0 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237c20 0x4005237c21}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:,StartTime:2020-08-14 04:22:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.199: INFO: Pod "webserver-deployment-6676bcd6d4-gt5hm" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-gt5hm webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-gt5hm 86da06ce-cc76-4b12-a87f-ae4038bdd7b1 4166596 0 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237dc7 0x4005237dc8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 50 48 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.208,StartTime:2020-08-14 04:22:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 100.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.203: INFO: Pod "webserver-deployment-6676bcd6d4-jxmnf" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-jxmnf webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-jxmnf 46d3f2b6-ad3a-440b-aa1c-2d141a36cf8b 4166554 0 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x4005237fa7 0x4005237fa8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:,StartTime:2020-08-14 04:22:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.205: INFO: Pod "webserver-deployment-6676bcd6d4-kbrj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kbrj9 webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-kbrj9 bf2f74de-ef28-482f-93c0-410011760eb8 4166614 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x40043f4177 0x40043f4178}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.206: INFO: Pod "webserver-deployment-6676bcd6d4-m6j8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-m6j8s webserver-deployment-6676bcd6d4- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-6676bcd6d4-m6j8s 980a35fd-aae1-4811-85ec-22dcc088566d 4166569 0 2020-08-14 04:22:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 2efc108e-187a-48b0-bce9-aeef0be6938f 0x40043f42b7 0x40043f42b8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 101 102 99 49 48 56 101 45 49 56 55 97 45 52 56 98 48 45 98 99 101 57 45 97 101 101 102 48 98 101 54 57 51 56 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:28 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:,StartTime:2020-08-14 04:22:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.207: INFO: Pod "webserver-deployment-84855cf797-4bhkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-4bhkn webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-4bhkn e5ef4cab-6cd8-44d4-8158-5db600295060 4166617 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4467 0x40043f4468}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.207: INFO: Pod "webserver-deployment-84855cf797-4nfp4" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-4nfp4 webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-4nfp4 99d7a484-c53b-44f1-801c-71731e066a7d 4166604 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4597 0x40043f4598}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.208: INFO: Pod "webserver-deployment-84855cf797-9fclx" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9fclx webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-9fclx 21888424-f878-4882-8374-1b103ffeabb7 4166440 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f46c7 0x40043f46c8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 50 48 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.206,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://ec8269a226fb2bed91aab546980ed53c7e71ed46b03b4fa1641f3d1dfc87d0d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.209: INFO: Pod "webserver-deployment-84855cf797-b8q6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-b8q6q webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-b8q6q 6af42061-5c32-4a60-b833-0b26efd13fa3 4166616 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4877 0x40043f4878}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.210: INFO: Pod "webserver-deployment-84855cf797-g6pkd" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-g6pkd webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-g6pkd 2cf30737-74e6-4940-a839-2df176d71928 4166466 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4980 0x40043f4981}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 49 54 54 46 49 53 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:100.101.166.156,StartTime:2020-08-14 04:22:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://faa704e1e36ee874d6cacd069807ebdd3783f88cf80e49b50030671b19013a2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.166.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.211: INFO: Pod "webserver-deployment-84855cf797-n47cl" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-n47cl webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-n47cl 130ea5f4-b3d8-471f-8775-cacafa43c5c6 4166458 0 2020-08-14 04:22:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4b27 0x40043f4b28}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 49 57 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.190,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://684e93ade008b8a9a8ca6a739350e9c4adcc6c650e4837313935ad59692eae94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.212: INFO: Pod "webserver-deployment-84855cf797-smr5t" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-smr5t webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-smr5t 8e38e64f-b216-4069-b11c-9b2bb6d559a2 4166469 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4ce7 0x40043f4ce8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 49 54 54 46 49 53 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:100.101.166.159,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://45b7ba408950dc8817bf3321cd9961e6941782e9519545d8a01270c660684a1e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.166.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.214: INFO: Pod "webserver-deployment-84855cf797-sqcqp" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-sqcqp webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-sqcqp 7eb5b20c-d019-43a4-9e79-d2169a5e1e64 4166494 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f4e97 0x40043f4e98}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 49 54 54 46 49 54 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:100.101.166.168,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://299cc9b000a309dc2228aa90f1e0f3173e8c9c4b695e7b58ac894d0ad815de29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.166.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.215: INFO: Pod "webserver-deployment-84855cf797-wd6l8" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-wd6l8 webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-wd6l8 05caf972-c61c-48d3-8b0e-6a579cc25007 4166438 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f5047 0x40043f5048}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 49 57 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.193,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://090fb529e30bcee0d83e26d7d8c36f0bc112412b826279a2868f1b361ac084e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.216: INFO: Pod "webserver-deployment-84855cf797-whr2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-whr2z webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-whr2z 5e32b4aa-73d7-4360-ae86-6b995615c3fc 4166609 0 2020-08-14 04:22:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f5217 0x40043f5218}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.220: INFO: Pod "webserver-deployment-84855cf797-wk5zr" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-wk5zr webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-wk5zr 395c1c69-2009-4172-a25c-59186234aa30 4166444 0 2020-08-14 04:22:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f5320 0x40043f5321}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 50 48 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.201,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://161bb938cd873298297c9f25218d0506c0ac647c79d75aed9e45d6afca1e54fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 14 04:22:30.221: INFO: Pod "webserver-deployment-84855cf797-xsbhp" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-xsbhp webserver-deployment-84855cf797- deployment-127 /api/v1/namespaces/deployment-127/pods/webserver-deployment-84855cf797-xsbhp 39526804-40b7-4a65-abd1-692badba1786 4166447 0 2020-08-14 04:22:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 29d750e1-13de-4f02-b725-957c5c4fba62 0x40043f54d7 0x40043f54d8}] []  [{kube-controller-manager Update v1 2020-08-14 04:22:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 57 100 55 53 48 101 49 45 49 51 100 101 45 52 102 48 50 45 98 55 50 53 45 57 53 55 99 53 99 52 102 98 97 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:22:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 50 52 53 46 50 48 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzxdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzxdz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzxdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:22:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:100.101.245.207,StartTime:2020-08-14 04:22:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:22:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:f3f141fc8609b34723212d42afe66e41a498e1eb63ec1dda0de32ef16384d0df,ContainerID:docker://5fb5f35e1fcfd0d46d4338dfd8261e335acb3c6e35aa919d00809c263d12de60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.245.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:30.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-127" for this suite.

• [SLOW TEST:8.557 seconds]
[sig-apps] Deployment
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":275,"completed":172,"skipped":2928,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:30.254: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:22:31.968: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 14 04:22:34.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975751, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975751, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975752, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975751, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:22:37.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:37.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-664" for this suite.
STEP: Destroying namespace "webhook-664-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.099 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":275,"completed":173,"skipped":2944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:37.354: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:22:37.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d" in namespace "projected-8845" to be "Succeeded or Failed"
Aug 14 04:22:37.622: INFO: Pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.722489ms
Aug 14 04:22:39.632: INFO: Pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023697729s
Aug 14 04:22:41.643: INFO: Pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034326607s
Aug 14 04:22:43.653: INFO: Pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044132067s
STEP: Saw pod success
Aug 14 04:22:43.653: INFO: Pod "downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d" satisfied condition "Succeeded or Failed"
Aug 14 04:22:43.661: INFO: Trying to get logs from node worker2 pod downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d container client-container: <nil>
STEP: delete the pod
Aug 14 04:22:43.723: INFO: Waiting for pod downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d to disappear
Aug 14 04:22:43.733: INFO: Pod downwardapi-volume-cf88fd96-0b73-4b8b-becd-24aa0cc4ad8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8845" for this suite.

• [SLOW TEST:6.404 seconds]
[sig-storage] Projected downwardAPI
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":174,"skipped":2979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:43.759: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:22:43.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad" in namespace "projected-7984" to be "Succeeded or Failed"
Aug 14 04:22:43.997: INFO: Pod "downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614594ms
Aug 14 04:22:46.006: INFO: Pod "downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020625656s
Aug 14 04:22:48.016: INFO: Pod "downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031330354s
STEP: Saw pod success
Aug 14 04:22:48.016: INFO: Pod "downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad" satisfied condition "Succeeded or Failed"
Aug 14 04:22:48.024: INFO: Trying to get logs from node worker2 pod downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad container client-container: <nil>
STEP: delete the pod
Aug 14 04:22:48.136: INFO: Waiting for pod downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad to disappear
Aug 14 04:22:48.144: INFO: Pod downwardapi-volume-7c467078-d2d3-427c-80d2-6a5ce447caad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:22:48.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7984" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":175,"skipped":3003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:22:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8499
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8499
STEP: creating replication controller externalsvc in namespace services-8499
I0814 04:22:48.435613      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8499, replica count: 2
I0814 04:22:51.486369      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 14 04:22:51.628: INFO: Creating new exec pod
Aug 14 04:22:53.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-8499 execpodhrj6r -- /bin/sh -x -c nslookup nodeport-service'
Aug 14 04:22:54.125: INFO: stderr: "+ nslookup nodeport-service\n"
Aug 14 04:22:54.125: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nnodeport-service.services-8499.svc.cluster.local\tcanonical name = externalsvc.services-8499.svc.cluster.local.\nName:\texternalsvc.services-8499.svc.cluster.local\nAddress: 100.105.18.23\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8499, will wait for the garbage collector to delete the pods
Aug 14 04:22:54.205: INFO: Deleting ReplicationController externalsvc took: 21.334833ms
Aug 14 04:22:54.305: INFO: Terminating ReplicationController externalsvc pods took: 100.458798ms
Aug 14 04:23:01.557: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8499" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:13.446 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":275,"completed":176,"skipped":3027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:01.616: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Aug 14 04:23:01.815: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:06.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1159" for this suite.

• [SLOW TEST:5.376 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":275,"completed":177,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:23:08.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:23:10.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975788, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975788, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975788, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975788, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:23:13.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:25.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8228" for this suite.
STEP: Destroying namespace "webhook-8228-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":275,"completed":178,"skipped":3094,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:25.735: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:23:26.958: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:23:29.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975806, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975806, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975807, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975806, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:23:32.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:32.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7579" for this suite.
STEP: Destroying namespace "webhook-7579-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.449 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":275,"completed":179,"skipped":3102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:32.185: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:23:33.803: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 14 04:23:35.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975813, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975813, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975813, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975813, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:23:38.859: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:23:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:45.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6404" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.948 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":275,"completed":180,"skipped":3140,"failed":0}
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 04:23:49.725: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8509" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":181,"skipped":3140,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 14 04:23:50.136: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Aug 14 04:23:51.089: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 14 04:23:53.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975831, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975831, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975831, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975831, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:23:57.367: INFO: Waited 2.151649747s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:23:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4537" for this suite.

• [SLOW TEST:8.713 seconds]
[sig-api-machinery] Aggregator
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":275,"completed":182,"skipped":3151,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:23:58.534: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-45db0a2b-3a5b-47f7-8152-665324a1cc73
STEP: Creating a pod to test consume configMaps
Aug 14 04:23:58.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812" in namespace "configmap-746" to be "Succeeded or Failed"
Aug 14 04:23:58.859: INFO: Pod "pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93258ms
Aug 14 04:24:00.869: INFO: Pod "pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019029499s
STEP: Saw pod success
Aug 14 04:24:00.869: INFO: Pod "pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812" satisfied condition "Succeeded or Failed"
Aug 14 04:24:00.879: INFO: Trying to get logs from node worker2 pod pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:24:00.931: INFO: Waiting for pod pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812 to disappear
Aug 14 04:24:00.940: INFO: Pod pod-configmaps-fb756d7b-c64c-4f83-bf41-d83c79d8c812 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-746" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":183,"skipped":3156,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:00.965: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-e731fdbc-84c4-4b83-aebb-2e9108e2f796
STEP: Creating a pod to test consume configMaps
Aug 14 04:24:01.194: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38" in namespace "projected-7033" to be "Succeeded or Failed"
Aug 14 04:24:01.205: INFO: Pod "pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38": Phase="Pending", Reason="", readiness=false. Elapsed: 10.723436ms
Aug 14 04:24:03.216: INFO: Pod "pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021172014s
STEP: Saw pod success
Aug 14 04:24:03.216: INFO: Pod "pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38" satisfied condition "Succeeded or Failed"
Aug 14 04:24:03.224: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:24:03.280: INFO: Waiting for pod pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38 to disappear
Aug 14 04:24:03.288: INFO: Pod pod-projected-configmaps-0f92df5a-c755-4da3-93e4-04b7daf6be38 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:03.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7033" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":184,"skipped":3170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:03.315: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-636aa3d0-fb48-4f11-a7db-e6ec9fd21c51
STEP: Creating a pod to test consume secrets
Aug 14 04:24:03.548: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320" in namespace "projected-5302" to be "Succeeded or Failed"
Aug 14 04:24:03.558: INFO: Pod "pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320": Phase="Pending", Reason="", readiness=false. Elapsed: 9.973158ms
Aug 14 04:24:05.593: INFO: Pod "pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045126162s
Aug 14 04:24:07.604: INFO: Pod "pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056828877s
STEP: Saw pod success
Aug 14 04:24:07.605: INFO: Pod "pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320" satisfied condition "Succeeded or Failed"
Aug 14 04:24:07.613: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 04:24:07.663: INFO: Waiting for pod pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320 to disappear
Aug 14 04:24:07.671: INFO: Pod pod-projected-secrets-bf91f190-8ee7-40bc-a865-799194618320 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:07.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5302" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":185,"skipped":3214,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:07.694: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Aug 14 04:24:07.895: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 04:24:08.012: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 04:24:08.019: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 04:24:08.053: INFO: kube-scheduler-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:24:08.053: INFO: nginx-proxy-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:24:08.053: INFO: keepalived-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:24:08.053: INFO: coredns-bwnkm from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:24:08.053: INFO: kube-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:24:08.053: INFO: kube-proxy-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:24:08.053: INFO: calico-node-qdxq4 from kube-system started at 2020-07-28 08:56:36 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:24:08.053: INFO: cke-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:24:08.053: INFO: kube-apiserver-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.053: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:24:08.053: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 04:24:08.089: INFO: kube-scheduler-master2 from kube-system started at 2020-08-04 08:03:47 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:24:08.089: INFO: calico-node-pk8lr from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:24:08.089: INFO: coredns-r5mc7 from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:24:08.089: INFO: kube-controller-manager-master2 from kube-system started at 2020-08-04 07:58:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:24:08.089: INFO: nginx-proxy-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:24:08.089: INFO: keepalived-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:24:08.089: INFO: kube-apiserver-master2 from kube-system started at 2020-08-04 07:58:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container kube-apiserver ready: true, restart count 1
Aug 14 04:24:08.089: INFO: kube-proxy-master2 from kube-system started at 2020-08-14 01:40:02 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:24:08.089: INFO: cke-controller-manager-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.089: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:24:08.089: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 04:24:08.123: INFO: nginx-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:24:08.123: INFO: kube-apiserver-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:24:08.123: INFO: calico-node-vv7tl from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:24:08.123: INFO: kube-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:24:08.123: INFO: kube-scheduler-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:24:08.123: INFO: kube-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:24:08.123: INFO: tiller-deploy-cf7dd5dc6-thqqn from kube-system started at 2020-08-04 03:48:03 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container tiller ready: true, restart count 1
Aug 14 04:24:08.123: INFO: keepalived-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:24:08.123: INFO: cke-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:24:08.123: INFO: coredns-s2kck from kube-system started at 2020-07-28 08:58:10 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.123: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:24:08.123: INFO: 
Logging pods the kubelet thinks is on node worker1 before test
Aug 14 04:24:08.157: INFO: auto-office-deploy-6d96bc4bc6-smxlw from auto-office started at 2020-08-12 10:37:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.157: INFO: 	Container auto-office ready: true, restart count 0
Aug 14 04:24:08.157: INFO: sonobuoy-e2e-job-a0c50eafb2fc4380 from sonobuoy started at 2020-08-14 03:30:29 +0000 UTC (2 container statuses recorded)
Aug 14 04:24:08.157: INFO: 	Container e2e ready: true, restart count 0
Aug 14 04:24:08.157: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 04:24:08.157: INFO: kube-proxy-worker1 from kube-system started at 2020-08-14 01:41:42 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.157: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:24:08.157: INFO: calico-node-5qkcf from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.157: INFO: 	Container calico-node ready: true, restart count 2
Aug 14 04:24:08.157: INFO: 
Logging pods the kubelet thinks is on node worker2 before test
Aug 14 04:24:08.171: INFO: calico-kube-controllers-575796f75c-qrr79 from kube-system started at 2020-08-11 13:00:14 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.171: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 04:24:08.171: INFO: sonobuoy from sonobuoy started at 2020-08-14 03:30:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.171: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 14 04:24:08.171: INFO: calico-node-77qqt from kube-system started at 2020-08-11 09:14:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.171: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:24:08.171: INFO: kube-proxy-worker2 from kube-system started at 2020-08-14 01:41:56 +0000 UTC (1 container statuses recorded)
Aug 14 04:24:08.171: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162b07c7f5fe7288], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162b07c7f95406dc], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:09.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1202" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":275,"completed":186,"skipped":3222,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:09.269: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:24:09.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b" in namespace "projected-5031" to be "Succeeded or Failed"
Aug 14 04:24:09.497: INFO: Pod "downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.703861ms
Aug 14 04:24:11.508: INFO: Pod "downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019438339s
Aug 14 04:24:13.518: INFO: Pod "downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030283936s
STEP: Saw pod success
Aug 14 04:24:13.518: INFO: Pod "downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b" satisfied condition "Succeeded or Failed"
Aug 14 04:24:13.531: INFO: Trying to get logs from node worker2 pod downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b container client-container: <nil>
STEP: delete the pod
Aug 14 04:24:13.580: INFO: Waiting for pod downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b to disappear
Aug 14 04:24:13.589: INFO: Pod downwardapi-volume-04e7a119-2fbd-45fe-bcdd-6a7d4e98910b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:13.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5031" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":187,"skipped":3231,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:13.614: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 14 04:24:23.903: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 04:24:23.903071      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:23.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8013" for this suite.

• [SLOW TEST:10.314 seconds]
[sig-api-machinery] Garbage collector
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":275,"completed":188,"skipped":3239,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:23.928: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-881
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 04:24:24.322: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 14 04:24:24.441: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:24:26.451: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:24:28.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:24:30.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:24:32.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:24:34.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:24:36.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 14 04:24:38.452: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 14 04:24:38.472: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 14 04:24:40.480: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 14 04:24:42.482: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 14 04:24:44.482: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 14 04:24:44.499: INFO: The status of Pod netserver-2 is Running (Ready = true)
Aug 14 04:24:44.517: INFO: The status of Pod netserver-3 is Running (Ready = true)
Aug 14 04:24:44.537: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Aug 14 04:24:48.628: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.161.176:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-881 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:24:48.628: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:24:48.901: INFO: Found all expected endpoints: [netserver-0]
Aug 14 04:24:48.911: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.208.157:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-881 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:24:48.911: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:24:49.254: INFO: Found all expected endpoints: [netserver-1]
Aug 14 04:24:49.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.32.106:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-881 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:24:49.263: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:24:49.663: INFO: Found all expected endpoints: [netserver-2]
Aug 14 04:24:49.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.166.170:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-881 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:24:49.693: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:24:50.003: INFO: Found all expected endpoints: [netserver-3]
Aug 14 04:24:50.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.245.229:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-881 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:24:50.013: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:24:50.324: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:24:50.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-881" for this suite.

• [SLOW TEST:26.422 seconds]
[sig-network] Networking
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":189,"skipped":3243,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:24:50.351: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-6z2d
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 04:24:50.591: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6z2d" in namespace "subpath-2107" to be "Succeeded or Failed"
Aug 14 04:24:50.601: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044518ms
Aug 14 04:24:52.612: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020973855s
Aug 14 04:24:54.622: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 4.031524494s
Aug 14 04:24:56.634: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 6.04286437s
Aug 14 04:24:58.645: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 8.054060787s
Aug 14 04:25:00.657: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 10.066109322s
Aug 14 04:25:02.669: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 12.077821978s
Aug 14 04:25:04.679: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 14.087883337s
Aug 14 04:25:06.688: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 16.097102118s
Aug 14 04:25:08.698: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 18.107357057s
Aug 14 04:25:10.708: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 20.117562816s
Aug 14 04:25:12.719: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Running", Reason="", readiness=true. Elapsed: 22.128629553s
Aug 14 04:25:14.729: INFO: Pod "pod-subpath-test-secret-6z2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.138381153s
STEP: Saw pod success
Aug 14 04:25:14.729: INFO: Pod "pod-subpath-test-secret-6z2d" satisfied condition "Succeeded or Failed"
Aug 14 04:25:14.740: INFO: Trying to get logs from node worker2 pod pod-subpath-test-secret-6z2d container test-container-subpath-secret-6z2d: <nil>
STEP: delete the pod
Aug 14 04:25:14.795: INFO: Waiting for pod pod-subpath-test-secret-6z2d to disappear
Aug 14 04:25:14.805: INFO: Pod pod-subpath-test-secret-6z2d no longer exists
STEP: Deleting pod pod-subpath-test-secret-6z2d
Aug 14 04:25:14.805: INFO: Deleting pod "pod-subpath-test-secret-6z2d" in namespace "subpath-2107"
[AfterEach] [sig-storage] Subpath
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:14.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2107" for this suite.

• [SLOW TEST:24.497 seconds]
[sig-storage] Subpath
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":275,"completed":190,"skipped":3244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:14.849: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Aug 14 04:25:19.650: INFO: Successfully updated pod "annotationupdate99c8f561-efb2-4651-bce6-d2928ae15739"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:21.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9490" for this suite.

• [SLOW TEST:6.867 seconds]
[sig-storage] Projected downwardAPI
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":191,"skipped":3273,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:21.717: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:25:22.217: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7" in namespace "security-context-test-9300" to be "Succeeded or Failed"
Aug 14 04:25:22.226: INFO: Pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.82838ms
Aug 14 04:25:24.237: INFO: Pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019521878s
Aug 14 04:25:26.294: INFO: Pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076725994s
Aug 14 04:25:26.294: INFO: Pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7" satisfied condition "Succeeded or Failed"
Aug 14 04:25:26.318: INFO: Got logs for pod "busybox-privileged-false-7db97924-ad49-4a82-9d6a-60cada1ecea7": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:26.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9300" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":192,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:26.345: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:25:26.554: INFO: Creating deployment "test-recreate-deployment"
Aug 14 04:25:26.565: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 14 04:25:26.578: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 14 04:25:28.595: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 14 04:25:28.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975926, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975926, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975926, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732975926, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:25:30.609: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 14 04:25:30.630: INFO: Updating deployment test-recreate-deployment
Aug 14 04:25:30.630: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Aug 14 04:25:30.769: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5632 /apis/apps/v1/namespaces/deployment-5632/deployments/test-recreate-deployment 2c7d7c16-1dba-4e3c-ad0b-03ff40d11b65 4168898 2 2020-08-14 04:25:26 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x40022afa68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-14 04:25:30 +0000 UTC,LastTransitionTime:2020-08-14 04:25:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-08-14 04:25:30 +0000 UTC,LastTransitionTime:2020-08-14 04:25:26 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 14 04:25:30.777: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-5632 /apis/apps/v1/namespaces/deployment-5632/replicasets/test-recreate-deployment-d5667d9c7 2a9ce453-7588-4b8c-9e7e-9a493f2783d9 4168896 1 2020-08-14 04:25:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2c7d7c16-1dba-4e3c-ad0b-03ff40d11b65 0x4004bee2c0 0x4004bee2c1}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 99 55 100 55 99 49 54 45 49 100 98 97 45 52 101 51 99 45 97 100 48 98 45 48 51 102 102 52 48 100 49 49 98 54 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4004bee3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:25:30.777: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 14 04:25:30.778: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-5632 /apis/apps/v1/namespaces/deployment-5632/replicasets/test-recreate-deployment-74d98b5f7c c3b41f5f-cd1e-4d86-a67f-51b6de8d186d 4168887 2 2020-08-14 04:25:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2c7d7c16-1dba-4e3c-ad0b-03ff40d11b65 0x4004bee017 0x4004bee018}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 99 55 100 55 99 49 54 45 49 100 98 97 45 52 101 51 99 45 97 100 48 98 45 48 51 102 102 52 48 100 49 49 98 54 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4004bee0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:25:30.789: INFO: Pod "test-recreate-deployment-d5667d9c7-26jj2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-26jj2 test-recreate-deployment-d5667d9c7- deployment-5632 /api/v1/namespaces/deployment-5632/pods/test-recreate-deployment-d5667d9c7-26jj2 1bb490cc-b109-4aa0-bb86-3e5b8c7815ac 4168899 0 2020-08-14 04:25:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 2a9ce453-7588-4b8c-9e7e-9a493f2783d9 0x4004beea50 0x4004beea51}] []  [{kube-controller-manager Update v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 97 57 99 101 52 53 51 45 55 53 56 56 45 52 98 56 99 45 57 101 55 101 45 57 97 52 57 51 102 50 55 56 51 100 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:25:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbtkv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbtkv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbtkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:25:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:25:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:25:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:25:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.97,PodIP:,StartTime:2020-08-14 04:25:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:30.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5632" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":193,"skipped":3296,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:30.815: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:25:31.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb" in namespace "downward-api-3771" to be "Succeeded or Failed"
Aug 14 04:25:31.048: INFO: Pod "downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.293433ms
Aug 14 04:25:33.058: INFO: Pod "downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022483052s
STEP: Saw pod success
Aug 14 04:25:33.059: INFO: Pod "downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb" satisfied condition "Succeeded or Failed"
Aug 14 04:25:33.068: INFO: Trying to get logs from node worker2 pod downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb container client-container: <nil>
STEP: delete the pod
Aug 14 04:25:33.122: INFO: Waiting for pod downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb to disappear
Aug 14 04:25:33.131: INFO: Pod downwardapi-volume-7b222763-3a02-4880-b2cb-025ba23f9adb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:33.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3771" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":194,"skipped":3299,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:33.157: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:25:33.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c" in namespace "projected-4473" to be "Succeeded or Failed"
Aug 14 04:25:33.374: INFO: Pod "downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.445441ms
Aug 14 04:25:35.388: INFO: Pod "downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022548752s
Aug 14 04:25:37.398: INFO: Pod "downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03295793s
STEP: Saw pod success
Aug 14 04:25:37.398: INFO: Pod "downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c" satisfied condition "Succeeded or Failed"
Aug 14 04:25:37.407: INFO: Trying to get logs from node worker1 pod downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c container client-container: <nil>
STEP: delete the pod
Aug 14 04:25:37.533: INFO: Waiting for pod downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c to disappear
Aug 14 04:25:37.541: INFO: Pod downwardapi-volume-43871af0-b4f8-4891-bdfa-34b60217d09c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:37.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4473" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":275,"completed":195,"skipped":3314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:37.565: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 04:25:37.786: INFO: Waiting up to 5m0s for pod "pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318" in namespace "emptydir-9143" to be "Succeeded or Failed"
Aug 14 04:25:37.795: INFO: Pod "pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318": Phase="Pending", Reason="", readiness=false. Elapsed: 9.10176ms
Aug 14 04:25:39.805: INFO: Pod "pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018530221s
Aug 14 04:25:41.813: INFO: Pod "pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026939264s
STEP: Saw pod success
Aug 14 04:25:41.813: INFO: Pod "pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318" satisfied condition "Succeeded or Failed"
Aug 14 04:25:41.821: INFO: Trying to get logs from node worker2 pod pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318 container test-container: <nil>
STEP: delete the pod
Aug 14 04:25:41.868: INFO: Waiting for pod pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318 to disappear
Aug 14 04:25:41.876: INFO: Pod pod-4e3738fb-6f62-46e4-a5d0-4dec826f4318 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:41.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9143" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":196,"skipped":3360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:41.903: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Aug 14 04:25:42.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 cluster-info'
Aug 14 04:25:42.250: INFO: stderr: ""
Aug 14 04:25:42.250: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:42.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7881" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":275,"completed":197,"skipped":3390,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:42.278: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9251
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:25:42.478: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:48.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9251" for this suite.

• [SLOW TEST:6.272 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":275,"completed":198,"skipped":3402,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:48.550: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 14 04:25:55.495: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 04:25:55.494999      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:25:55.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4635" for this suite.

• [SLOW TEST:6.972 seconds]
[sig-api-machinery] Garbage collector
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":275,"completed":199,"skipped":3408,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:25:55.523: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Aug 14 04:25:55.742: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 04:25:55.778: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 04:25:55.786: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 04:25:55.830: INFO: cke-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.830: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:25:55.830: INFO: kube-apiserver-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.830: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:25:55.830: INFO: kube-scheduler-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.830: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:25:55.830: INFO: coredns-bwnkm from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.831: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:25:55.831: INFO: nginx-proxy-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.831: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:25:55.831: INFO: keepalived-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.831: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:25:55.831: INFO: calico-node-qdxq4 from kube-system started at 2020-07-28 08:56:36 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.831: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:25:55.832: INFO: kube-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.832: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:25:55.832: INFO: kube-proxy-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.832: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:25:55.832: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 04:25:55.872: INFO: kube-proxy-master2 from kube-system started at 2020-08-14 01:40:02 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:25:55.872: INFO: cke-controller-manager-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:25:55.872: INFO: kube-scheduler-master2 from kube-system started at 2020-08-04 08:03:47 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:25:55.872: INFO: calico-node-pk8lr from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:25:55.872: INFO: coredns-r5mc7 from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:25:55.872: INFO: kube-controller-manager-master2 from kube-system started at 2020-08-04 07:58:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:25:55.872: INFO: nginx-proxy-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:25:55.872: INFO: keepalived-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:25:55.872: INFO: kube-apiserver-master2 from kube-system started at 2020-08-04 07:58:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.872: INFO: 	Container kube-apiserver ready: true, restart count 1
Aug 14 04:25:55.872: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 04:25:55.909: INFO: kube-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:25:55.909: INFO: kube-scheduler-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:25:55.909: INFO: kube-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:25:55.909: INFO: tiller-deploy-cf7dd5dc6-thqqn from kube-system started at 2020-08-04 03:48:03 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container tiller ready: true, restart count 1
Aug 14 04:25:55.909: INFO: keepalived-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:25:55.909: INFO: cke-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:25:55.909: INFO: coredns-s2kck from kube-system started at 2020-07-28 08:58:10 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:25:55.909: INFO: nginx-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:25:55.909: INFO: kube-apiserver-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:25:55.909: INFO: calico-node-vv7tl from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.909: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:25:55.909: INFO: 
Logging pods the kubelet thinks is on node worker1 before test
Aug 14 04:25:55.927: INFO: auto-office-deploy-6d96bc4bc6-smxlw from auto-office started at 2020-08-12 10:37:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.927: INFO: 	Container auto-office ready: true, restart count 0
Aug 14 04:25:55.927: INFO: sonobuoy-e2e-job-a0c50eafb2fc4380 from sonobuoy started at 2020-08-14 03:30:29 +0000 UTC (2 container statuses recorded)
Aug 14 04:25:55.927: INFO: 	Container e2e ready: true, restart count 0
Aug 14 04:25:55.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 04:25:55.927: INFO: kube-proxy-worker1 from kube-system started at 2020-08-14 01:41:42 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.927: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:25:55.927: INFO: calico-node-5qkcf from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.927: INFO: 	Container calico-node ready: true, restart count 2
Aug 14 04:25:55.927: INFO: 
Logging pods the kubelet thinks is on node worker2 before test
Aug 14 04:25:55.941: INFO: kube-proxy-worker2 from kube-system started at 2020-08-14 01:41:56 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.941: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:25:55.941: INFO: calico-kube-controllers-575796f75c-qrr79 from kube-system started at 2020-08-11 13:00:14 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.941: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 04:25:55.941: INFO: sonobuoy from sonobuoy started at 2020-08-14 03:30:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.941: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 14 04:25:55.941: INFO: calico-node-77qqt from kube-system started at 2020-08-11 09:14:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:25:55.941: INFO: 	Container calico-node ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
STEP: verifying the node has the label node worker1
STEP: verifying the node has the label node worker2
Aug 14 04:25:56.167: INFO: Pod auto-office-deploy-6d96bc4bc6-smxlw requesting resource cpu=0m on Node worker1
Aug 14 04:25:56.167: INFO: Pod calico-kube-controllers-575796f75c-qrr79 requesting resource cpu=30m on Node worker2
Aug 14 04:25:56.167: INFO: Pod calico-node-5qkcf requesting resource cpu=150m on Node worker1
Aug 14 04:25:56.167: INFO: Pod calico-node-77qqt requesting resource cpu=150m on Node worker2
Aug 14 04:25:56.167: INFO: Pod calico-node-pk8lr requesting resource cpu=150m on Node master2
Aug 14 04:25:56.167: INFO: Pod calico-node-qdxq4 requesting resource cpu=150m on Node master1
Aug 14 04:25:56.167: INFO: Pod calico-node-vv7tl requesting resource cpu=150m on Node master3
Aug 14 04:25:56.167: INFO: Pod cke-controller-manager-master1 requesting resource cpu=100m on Node master1
Aug 14 04:25:56.167: INFO: Pod cke-controller-manager-master2 requesting resource cpu=100m on Node master2
Aug 14 04:25:56.167: INFO: Pod cke-controller-manager-master3 requesting resource cpu=100m on Node master3
Aug 14 04:25:56.167: INFO: Pod coredns-bwnkm requesting resource cpu=100m on Node master1
Aug 14 04:25:56.167: INFO: Pod coredns-r5mc7 requesting resource cpu=100m on Node master2
Aug 14 04:25:56.167: INFO: Pod coredns-s2kck requesting resource cpu=100m on Node master3
Aug 14 04:25:56.167: INFO: Pod keepalived-master1 requesting resource cpu=0m on Node master1
Aug 14 04:25:56.167: INFO: Pod keepalived-master2 requesting resource cpu=0m on Node master2
Aug 14 04:25:56.167: INFO: Pod keepalived-master3 requesting resource cpu=0m on Node master3
Aug 14 04:25:56.167: INFO: Pod kube-apiserver-master1 requesting resource cpu=500m on Node master1
Aug 14 04:25:56.167: INFO: Pod kube-apiserver-master2 requesting resource cpu=500m on Node master2
Aug 14 04:25:56.167: INFO: Pod kube-apiserver-master3 requesting resource cpu=500m on Node master3
Aug 14 04:25:56.167: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Aug 14 04:25:56.167: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Aug 14 04:25:56.167: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Aug 14 04:25:56.167: INFO: Pod kube-proxy-master1 requesting resource cpu=500m on Node master1
Aug 14 04:25:56.167: INFO: Pod kube-proxy-master2 requesting resource cpu=500m on Node master2
Aug 14 04:25:56.167: INFO: Pod kube-proxy-master3 requesting resource cpu=500m on Node master3
Aug 14 04:25:56.167: INFO: Pod kube-proxy-worker1 requesting resource cpu=500m on Node worker1
Aug 14 04:25:56.167: INFO: Pod kube-proxy-worker2 requesting resource cpu=500m on Node worker2
Aug 14 04:25:56.167: INFO: Pod kube-scheduler-master1 requesting resource cpu=100m on Node master1
Aug 14 04:25:56.167: INFO: Pod kube-scheduler-master2 requesting resource cpu=100m on Node master2
Aug 14 04:25:56.167: INFO: Pod kube-scheduler-master3 requesting resource cpu=100m on Node master3
Aug 14 04:25:56.167: INFO: Pod nginx-proxy-master1 requesting resource cpu=25m on Node master1
Aug 14 04:25:56.167: INFO: Pod nginx-proxy-master2 requesting resource cpu=25m on Node master2
Aug 14 04:25:56.167: INFO: Pod nginx-proxy-master3 requesting resource cpu=25m on Node master3
Aug 14 04:25:56.167: INFO: Pod tiller-deploy-cf7dd5dc6-thqqn requesting resource cpu=0m on Node master3
Aug 14 04:25:56.167: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker2
Aug 14 04:25:56.167: INFO: Pod sonobuoy-e2e-job-a0c50eafb2fc4380 requesting resource cpu=0m on Node worker1
STEP: Starting Pods to consume most of the cluster CPU.
Aug 14 04:25:56.167: INFO: Creating a pod which consumes cpu=3797m on Node master1
Aug 14 04:25:56.189: INFO: Creating a pod which consumes cpu=3797m on Node master2
Aug 14 04:25:56.204: INFO: Creating a pod which consumes cpu=3797m on Node master3
Aug 14 04:25:56.219: INFO: Creating a pod which consumes cpu=4725m on Node worker1
Aug 14 04:25:56.235: INFO: Creating a pod which consumes cpu=4704m on Node worker2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648.162b07e11a634f04], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8026/filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648 to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648.162b07e1990c8077], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648.162b07e19fc2b8db], Reason = [Created], Message = [Created container filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648.162b07e1c2a64615], Reason = [Started], Message = [Started container filler-pod-39f3e11b-85e2-4a27-b484-96a90e2bf648]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4df59d6b-590c-43d5-af93-fc055725af35.162b07e11ecb1554], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8026/filler-pod-4df59d6b-590c-43d5-af93-fc055725af35 to worker2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4df59d6b-590c-43d5-af93-fc055725af35.162b07e1745c7826], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4df59d6b-590c-43d5-af93-fc055725af35.162b07e176c2de12], Reason = [Created], Message = [Created container filler-pod-4df59d6b-590c-43d5-af93-fc055725af35]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4df59d6b-590c-43d5-af93-fc055725af35.162b07e1886d4de0], Reason = [Started], Message = [Started container filler-pod-4df59d6b-590c-43d5-af93-fc055725af35]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196.162b07e11be688a0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8026/filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196 to worker1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196.162b07e18ee239d5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196.162b07e193d4b1e9], Reason = [Created], Message = [Created container filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196.162b07e1ac1f44c1], Reason = [Started], Message = [Started container filler-pod-7ecc453e-85f7-49ec-a767-33ab65d9d196]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748.162b07e11b4d5694], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8026/filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748 to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748.162b07e186ad8255], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748.162b07e18d9fe215], Reason = [Created], Message = [Created container filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748.162b07e1a6cb2f01], Reason = [Started], Message = [Started container filler-pod-c7b03ff0-06ca-49f7-a5c4-1f53eef1f748]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5.162b07e11919a6ac], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8026/filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5 to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5.162b07e17e3f480c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5.162b07e184a13e80], Reason = [Created], Message = [Created container filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5.162b07e1ac48f9dc], Reason = [Started], Message = [Started container filler-pod-ca4bf0e2-c5d5-4004-969f-593b309ec8a5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162b07e210400250], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162b07e211822c38], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node worker2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:01.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8026" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:6.013 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":275,"completed":200,"skipped":3420,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4898
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 14 04:26:01.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4898'
Aug 14 04:26:01.922: INFO: stderr: ""
Aug 14 04:26:01.922: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Aug 14 04:26:01.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete pods e2e-test-httpd-pod --namespace=kubectl-4898'
Aug 14 04:26:06.915: INFO: stderr: ""
Aug 14 04:26:06.915: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:06.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4898" for this suite.

• [SLOW TEST:5.403 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":275,"completed":201,"skipped":3431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:06.941: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3961" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":275,"completed":202,"skipped":3465,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-5e9cbd50-0157-4b52-94b3-bf9825f48131
STEP: Creating a pod to test consume configMaps
Aug 14 04:26:07.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d" in namespace "configmap-1834" to be "Succeeded or Failed"
Aug 14 04:26:07.616: INFO: Pod "pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.982118ms
Aug 14 04:26:09.627: INFO: Pod "pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020700196s
Aug 14 04:26:11.636: INFO: Pod "pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030044057s
STEP: Saw pod success
Aug 14 04:26:11.636: INFO: Pod "pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d" satisfied condition "Succeeded or Failed"
Aug 14 04:26:11.645: INFO: Trying to get logs from node worker2 pod pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:26:11.722: INFO: Waiting for pod pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d to disappear
Aug 14 04:26:11.732: INFO: Pod pod-configmaps-8a072ad2-0d71-4674-8cab-182396079a1d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:11.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1834" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":203,"skipped":3475,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:26:11.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8" in namespace "downward-api-6482" to be "Succeeded or Failed"
Aug 14 04:26:11.984: INFO: Pod "downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.15816ms
Aug 14 04:26:13.994: INFO: Pod "downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0189933s
Aug 14 04:26:16.005: INFO: Pod "downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029489878s
STEP: Saw pod success
Aug 14 04:26:16.005: INFO: Pod "downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8" satisfied condition "Succeeded or Failed"
Aug 14 04:26:16.014: INFO: Trying to get logs from node worker2 pod downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8 container client-container: <nil>
STEP: delete the pod
Aug 14 04:26:16.065: INFO: Waiting for pod downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8 to disappear
Aug 14 04:26:16.073: INFO: Pod downwardapi-volume-ac19c10e-67c6-4984-8601-315c21eec6f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:16.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6482" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":204,"skipped":3484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:16.100: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 04:26:16.410: INFO: Number of nodes with available pods: 0
Aug 14 04:26:16.410: INFO: Node master1 is running more than one daemon pod
Aug 14 04:26:17.440: INFO: Number of nodes with available pods: 0
Aug 14 04:26:17.441: INFO: Node master1 is running more than one daemon pod
Aug 14 04:26:18.434: INFO: Number of nodes with available pods: 0
Aug 14 04:26:18.434: INFO: Node master1 is running more than one daemon pod
Aug 14 04:26:19.432: INFO: Number of nodes with available pods: 5
Aug 14 04:26:19.432: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 14 04:26:19.490: INFO: Number of nodes with available pods: 4
Aug 14 04:26:19.490: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:20.523: INFO: Number of nodes with available pods: 4
Aug 14 04:26:20.523: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:21.511: INFO: Number of nodes with available pods: 4
Aug 14 04:26:21.511: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:22.515: INFO: Number of nodes with available pods: 4
Aug 14 04:26:22.515: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:23.513: INFO: Number of nodes with available pods: 4
Aug 14 04:26:23.513: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:24.515: INFO: Number of nodes with available pods: 4
Aug 14 04:26:24.515: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:26:25.513: INFO: Number of nodes with available pods: 5
Aug 14 04:26:25.513: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-966, will wait for the garbage collector to delete the pods
Aug 14 04:26:25.598: INFO: Deleting DaemonSet.extensions daemon-set took: 18.260999ms
Aug 14 04:26:25.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.401416ms
Aug 14 04:26:36.808: INFO: Number of nodes with available pods: 0
Aug 14 04:26:36.808: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 04:26:36.816: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-966/daemonsets","resourceVersion":"4170003"},"items":null}

Aug 14 04:26:36.824: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-966/pods","resourceVersion":"4170003"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:36.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-966" for this suite.

• [SLOW TEST:20.801 seconds]
[sig-apps] Daemon set [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":275,"completed":205,"skipped":3510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:36.902: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:37.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3139" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":275,"completed":206,"skipped":3551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:26:37.322: INFO: Creating ReplicaSet my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8
Aug 14 04:26:37.341: INFO: Pod name my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8: Found 0 pods out of 1
Aug 14 04:26:42.352: INFO: Pod name my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8: Found 1 pods out of 1
Aug 14 04:26:42.352: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8" is running
Aug 14 04:26:42.360: INFO: Pod "my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8-j8tqf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-14 04:26:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-14 04:26:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-14 04:26:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-14 04:26:37 +0000 UTC Reason: Message:}])
Aug 14 04:26:42.360: INFO: Trying to dial the pod
Aug 14 04:26:47.390: INFO: Controller my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8: Got expected result from replica 1 [my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8-j8tqf]: "my-hostname-basic-471fe686-e8aa-4766-ac32-80c879c366f8-j8tqf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:47.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9454" for this suite.

• [SLOW TEST:10.283 seconds]
[sig-apps] ReplicaSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":207,"skipped":3581,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:47.415: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:51.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9024" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":275,"completed":208,"skipped":3591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Aug 14 04:26:56.518: INFO: Successfully updated pod "annotationupdate1aa12f5f-1308-48df-87ec-face0575209f"
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:26:58.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1544" for this suite.

• [SLOW TEST:6.870 seconds]
[sig-storage] Downward API volume
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":209,"skipped":3658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:26:58.589: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5896
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 14 04:27:02.862: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5896 PodName:pod-sharedvolume-70fd63ff-129f-4fc5-aa9c-764d607bca19 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 04:27:02.862: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:27:03.106: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:03.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5896" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":275,"completed":210,"skipped":3683,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:03.194: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Aug 14 04:27:03.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-4901'
Aug 14 04:27:04.281: INFO: stderr: ""
Aug 14 04:27:04.281: INFO: stdout: "pod/pause created\n"
Aug 14 04:27:04.281: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 14 04:27:04.281: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4901" to be "running and ready"
Aug 14 04:27:04.291: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.859198ms
Aug 14 04:27:06.302: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020725736s
Aug 14 04:27:08.313: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.031829633s
Aug 14 04:27:08.313: INFO: Pod "pause" satisfied condition "running and ready"
Aug 14 04:27:08.313: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 14 04:27:08.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 label pods pause testing-label=testing-label-value --namespace=kubectl-4901'
Aug 14 04:27:08.506: INFO: stderr: ""
Aug 14 04:27:08.506: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 14 04:27:08.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pod pause -L testing-label --namespace=kubectl-4901'
Aug 14 04:27:08.680: INFO: stderr: ""
Aug 14 04:27:08.680: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 14 04:27:08.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 label pods pause testing-label- --namespace=kubectl-4901'
Aug 14 04:27:08.874: INFO: stderr: ""
Aug 14 04:27:08.874: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 14 04:27:08.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pod pause -L testing-label --namespace=kubectl-4901'
Aug 14 04:27:09.043: INFO: stderr: ""
Aug 14 04:27:09.043: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Aug 14 04:27:09.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-4901'
Aug 14 04:27:09.228: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 04:27:09.228: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 14 04:27:09.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get rc,svc -l name=pause --no-headers --namespace=kubectl-4901'
Aug 14 04:27:09.415: INFO: stderr: "No resources found in kubectl-4901 namespace.\n"
Aug 14 04:27:09.415: INFO: stdout: ""
Aug 14 04:27:09.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -l name=pause --namespace=kubectl-4901 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 04:27:09.601: INFO: stderr: ""
Aug 14 04:27:09.601: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:09.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4901" for this suite.

• [SLOW TEST:6.435 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1203
    should update the label on a resource  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":275,"completed":211,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:09.629: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Aug 14 04:27:09.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 04:27:09.865: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 04:27:09.874: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 04:27:09.890: INFO: kube-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:27:09.890: INFO: kube-proxy-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:27:09.890: INFO: calico-node-qdxq4 from kube-system started at 2020-07-28 08:56:36 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:27:09.890: INFO: cke-controller-manager-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:27:09.890: INFO: kube-apiserver-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:27:09.890: INFO: kube-scheduler-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:27:09.890: INFO: nginx-proxy-master1 from kube-system started at 2020-08-04 07:04:34 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:27:09.890: INFO: keepalived-master1 from kube-system started at 2020-08-14 01:38:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:27:09.890: INFO: coredns-bwnkm from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.890: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:27:09.890: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 04:27:09.912: INFO: kube-apiserver-master2 from kube-system started at 2020-08-04 07:58:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container kube-apiserver ready: true, restart count 1
Aug 14 04:27:09.912: INFO: calico-node-pk8lr from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:27:09.912: INFO: coredns-r5mc7 from kube-system started at 2020-07-28 08:58:06 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:27:09.912: INFO: kube-controller-manager-master2 from kube-system started at 2020-08-04 07:58:21 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:27:09.912: INFO: nginx-proxy-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:27:09.912: INFO: keepalived-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:27:09.912: INFO: kube-proxy-master2 from kube-system started at 2020-08-14 01:40:02 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:27:09.912: INFO: cke-controller-manager-master2 from kube-system started at 2020-07-28 08:58:22 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:27:09.912: INFO: kube-scheduler-master2 from kube-system started at 2020-08-04 08:03:47 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.912: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:27:09.912: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 04:27:09.929: INFO: kube-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:27:09.929: INFO: kube-scheduler-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 14 04:27:09.929: INFO: kube-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 04:27:09.929: INFO: tiller-deploy-cf7dd5dc6-thqqn from kube-system started at 2020-08-04 03:48:03 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container tiller ready: true, restart count 1
Aug 14 04:27:09.929: INFO: keepalived-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container keepalived ready: true, restart count 0
Aug 14 04:27:09.929: INFO: cke-controller-manager-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container cke-controller-manager ready: true, restart count 0
Aug 14 04:27:09.929: INFO: coredns-s2kck from kube-system started at 2020-07-28 08:58:10 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container coredns ready: true, restart count 1
Aug 14 04:27:09.929: INFO: nginx-proxy-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 04:27:09.929: INFO: kube-apiserver-master3 from kube-system started at 2020-08-14 01:40:12 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 04:27:09.929: INFO: calico-node-vv7tl from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.929: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:27:09.929: INFO: 
Logging pods the kubelet thinks is on node worker1 before test
Aug 14 04:27:09.948: INFO: auto-office-deploy-6d96bc4bc6-smxlw from auto-office started at 2020-08-12 10:37:01 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.948: INFO: 	Container auto-office ready: true, restart count 0
Aug 14 04:27:09.948: INFO: sonobuoy-e2e-job-a0c50eafb2fc4380 from sonobuoy started at 2020-08-14 03:30:29 +0000 UTC (2 container statuses recorded)
Aug 14 04:27:09.948: INFO: 	Container e2e ready: true, restart count 0
Aug 14 04:27:09.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 04:27:09.948: INFO: kube-proxy-worker1 from kube-system started at 2020-08-14 01:41:42 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.949: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:27:09.949: INFO: calico-node-5qkcf from kube-system started at 2020-07-28 08:56:35 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.949: INFO: 	Container calico-node ready: true, restart count 2
Aug 14 04:27:09.949: INFO: 
Logging pods the kubelet thinks is on node worker2 before test
Aug 14 04:27:09.964: INFO: calico-node-77qqt from kube-system started at 2020-08-11 09:14:48 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.965: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 04:27:09.965: INFO: annotationupdate1aa12f5f-1308-48df-87ec-face0575209f from downward-api-1544 started at 2020-08-14 04:26:51 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.965: INFO: 	Container client-container ready: false, restart count 0
Aug 14 04:27:09.965: INFO: kube-proxy-worker2 from kube-system started at 2020-08-14 01:41:56 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.965: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 04:27:09.965: INFO: calico-kube-controllers-575796f75c-qrr79 from kube-system started at 2020-08-11 13:00:14 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.965: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 04:27:09.965: INFO: sonobuoy from sonobuoy started at 2020-08-14 03:30:27 +0000 UTC (1 container statuses recorded)
Aug 14 04:27:09.965: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5874a60e-6948-4571-9f9b-e002f95d163b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5874a60e-6948-4571-9f9b-e002f95d163b off the node worker1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5874a60e-6948-4571-9f9b-e002f95d163b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:18.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4971" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.609 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":275,"completed":212,"skipped":3707,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:18.238: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:23.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5733" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":275,"completed":213,"skipped":3712,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:23.155: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-0abc3ff3-17ad-4bb9-9727-ff8074030841
STEP: Creating a pod to test consume configMaps
Aug 14 04:27:23.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414" in namespace "projected-7725" to be "Succeeded or Failed"
Aug 14 04:27:23.397: INFO: Pod "pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414": Phase="Pending", Reason="", readiness=false. Elapsed: 13.046951ms
Aug 14 04:27:25.408: INFO: Pod "pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023879289s
Aug 14 04:27:27.418: INFO: Pod "pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033372589s
STEP: Saw pod success
Aug 14 04:27:27.418: INFO: Pod "pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414" satisfied condition "Succeeded or Failed"
Aug 14 04:27:27.426: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 04:27:27.542: INFO: Waiting for pod pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414 to disappear
Aug 14 04:27:27.551: INFO: Pod pod-projected-configmaps-b71c5f87-b115-4da8-a0d9-550a7a1b1414 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:27.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7725" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":214,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:27.575: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1210.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1210.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1210.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1210.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1210.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1210.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:27:31.941: INFO: DNS probes using dns-1210/dns-test-1a4cd833-3d4f-4fb8-86df-fc7dc6750aae succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:27:32.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1210" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":275,"completed":215,"skipped":3770,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:27:32.033: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1489
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 14 04:27:32.229: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 14 04:27:59.908: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
Aug 14 04:28:10.576: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:28:38.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1489" for this suite.

• [SLOW TEST:66.661 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":275,"completed":216,"skipped":3771,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:28:38.694: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-a70271e5-240d-4d28-a6a1-f9e49e02236e in namespace container-probe-2219
Aug 14 04:28:42.933: INFO: Started pod busybox-a70271e5-240d-4d28-a6a1-f9e49e02236e in namespace container-probe-2219
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 04:28:42.943: INFO: Initial restart count of pod busybox-a70271e5-240d-4d28-a6a1-f9e49e02236e is 0
Aug 14 04:29:33.411: INFO: Restart count of pod container-probe-2219/busybox-a70271e5-240d-4d28-a6a1-f9e49e02236e is now 1 (50.468733304s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:29:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2219" for this suite.

• [SLOW TEST:54.826 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":217,"skipped":3786,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:29:33.520: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 04:29:39.862: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 04:29:39.871: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 04:29:41.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 04:29:41.882: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 04:29:43.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 04:29:43.881: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 04:29:45.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 04:29:45.883: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 04:29:47.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 04:29:47.882: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:29:47.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1738" for this suite.

• [SLOW TEST:14.388 seconds]
[k8s.io] Container Lifecycle Hook
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":275,"completed":218,"skipped":3799,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:29:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-5450e64d-e928-4753-be55-4ebc7c82d11f in namespace container-probe-2909
Aug 14 04:29:52.146: INFO: Started pod test-webserver-5450e64d-e928-4753-be55-4ebc7c82d11f in namespace container-probe-2909
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 04:29:52.154: INFO: Initial restart count of pod test-webserver-5450e64d-e928-4753-be55-4ebc7c82d11f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:33:53.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2909" for this suite.

• [SLOW TEST:245.903 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":219,"skipped":3811,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:33:53.813: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1503
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-6d18fb99-f25d-404d-a0b2-3d4fe779abdc
STEP: Creating configMap with name cm-test-opt-upd-fd7e3ada-3ed9-45ad-a7b2-592a33e290df
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6d18fb99-f25d-404d-a0b2-3d4fe779abdc
STEP: Updating configmap cm-test-opt-upd-fd7e3ada-3ed9-45ad-a7b2-592a33e290df
STEP: Creating configMap with name cm-test-opt-create-e8e55ce3-ccef-41df-b591-ec09fd11d890
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:02.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1503" for this suite.

• [SLOW TEST:8.548 seconds]
[sig-storage] Projected configMap
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":220,"skipped":3822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:34:02.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430" in namespace "downward-api-5259" to be "Succeeded or Failed"
Aug 14 04:34:02.603: INFO: Pod "downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430": Phase="Pending", Reason="", readiness=false. Elapsed: 11.157597ms
Aug 14 04:34:04.615: INFO: Pod "downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022658092s
Aug 14 04:34:06.625: INFO: Pod "downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033483909s
STEP: Saw pod success
Aug 14 04:34:06.626: INFO: Pod "downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430" satisfied condition "Succeeded or Failed"
Aug 14 04:34:06.634: INFO: Trying to get logs from node worker1 pod downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430 container client-container: <nil>
STEP: delete the pod
Aug 14 04:34:06.709: INFO: Waiting for pod downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430 to disappear
Aug 14 04:34:06.717: INFO: Pod downwardapi-volume-a02b2fdc-97f5-4b45-a8dd-07cb4f048430 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:06.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5259" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":221,"skipped":3848,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:06.744: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5747
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5747
I0814 04:34:07.004503      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5747, replica count: 2
Aug 14 04:34:10.055: INFO: Creating new exec pod
I0814 04:34:10.055129      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 04:34:15.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5747 execpodfsrcb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 14 04:34:15.572: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 14 04:34:15.572: INFO: stdout: ""
Aug 14 04:34:15.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=services-5747 execpodfsrcb -- /bin/sh -x -c nc -zv -t -w 2 100.105.251.187 80'
Aug 14 04:34:16.093: INFO: stderr: "+ nc -zv -t -w 2 100.105.251.187 80\nConnection to 100.105.251.187 80 port [tcp/http] succeeded!\n"
Aug 14 04:34:16.093: INFO: stdout: ""
Aug 14 04:34:16.093: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:16.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5747" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.434 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":275,"completed":222,"skipped":3850,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:16.178: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 04:34:16.405: INFO: Waiting up to 5m0s for pod "pod-dc3e7018-daa8-4f03-9527-a74a285fad87" in namespace "emptydir-3308" to be "Succeeded or Failed"
Aug 14 04:34:16.415: INFO: Pod "pod-dc3e7018-daa8-4f03-9527-a74a285fad87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.5222ms
Aug 14 04:34:18.424: INFO: Pod "pod-dc3e7018-daa8-4f03-9527-a74a285fad87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018717841s
Aug 14 04:34:20.434: INFO: Pod "pod-dc3e7018-daa8-4f03-9527-a74a285fad87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028952259s
STEP: Saw pod success
Aug 14 04:34:20.434: INFO: Pod "pod-dc3e7018-daa8-4f03-9527-a74a285fad87" satisfied condition "Succeeded or Failed"
Aug 14 04:34:20.443: INFO: Trying to get logs from node worker2 pod pod-dc3e7018-daa8-4f03-9527-a74a285fad87 container test-container: <nil>
STEP: delete the pod
Aug 14 04:34:20.494: INFO: Waiting for pod pod-dc3e7018-daa8-4f03-9527-a74a285fad87 to disappear
Aug 14 04:34:20.503: INFO: Pod pod-dc3e7018-daa8-4f03-9527-a74a285fad87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:20.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3308" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":223,"skipped":3857,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:20.527: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Aug 14 04:34:20.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 create -f - --namespace=kubectl-4746'
Aug 14 04:34:21.434: INFO: stderr: ""
Aug 14 04:34:21.434: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 04:34:21.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:21.605: INFO: stderr: ""
Aug 14 04:34:21.605: INFO: stdout: "update-demo-nautilus-ll9ks update-demo-nautilus-pbs77 "
Aug 14 04:34:21.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-ll9ks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:21.761: INFO: stderr: ""
Aug 14 04:34:21.761: INFO: stdout: ""
Aug 14 04:34:21.761: INFO: update-demo-nautilus-ll9ks is created but not running
Aug 14 04:34:26.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:26.939: INFO: stderr: ""
Aug 14 04:34:26.939: INFO: stdout: "update-demo-nautilus-ll9ks update-demo-nautilus-pbs77 "
Aug 14 04:34:26.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-ll9ks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:27.118: INFO: stderr: ""
Aug 14 04:34:27.118: INFO: stdout: "true"
Aug 14 04:34:27.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-ll9ks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:27.278: INFO: stderr: ""
Aug 14 04:34:27.278: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:34:27.278: INFO: validating pod update-demo-nautilus-ll9ks
Aug 14 04:34:27.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:34:27.289: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:34:27.289: INFO: update-demo-nautilus-ll9ks is verified up and running
Aug 14 04:34:27.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:27.443: INFO: stderr: ""
Aug 14 04:34:27.443: INFO: stdout: "true"
Aug 14 04:34:27.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:27.593: INFO: stderr: ""
Aug 14 04:34:27.593: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:34:27.593: INFO: validating pod update-demo-nautilus-pbs77
Aug 14 04:34:27.691: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:34:27.691: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:34:27.691: INFO: update-demo-nautilus-pbs77 is verified up and running
STEP: scaling down the replication controller
Aug 14 04:34:27.698: INFO: scanned /root for discovery docs: <nil>
Aug 14 04:34:27.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4746'
Aug 14 04:34:28.908: INFO: stderr: ""
Aug 14 04:34:28.908: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 04:34:28.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:29.091: INFO: stderr: ""
Aug 14 04:34:29.091: INFO: stdout: "update-demo-nautilus-ll9ks update-demo-nautilus-pbs77 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 04:34:34.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:34.253: INFO: stderr: ""
Aug 14 04:34:34.253: INFO: stdout: "update-demo-nautilus-ll9ks update-demo-nautilus-pbs77 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 04:34:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:39.413: INFO: stderr: ""
Aug 14 04:34:39.413: INFO: stdout: "update-demo-nautilus-pbs77 "
Aug 14 04:34:39.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:39.568: INFO: stderr: ""
Aug 14 04:34:39.568: INFO: stdout: "true"
Aug 14 04:34:39.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:39.717: INFO: stderr: ""
Aug 14 04:34:39.717: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:34:39.717: INFO: validating pod update-demo-nautilus-pbs77
Aug 14 04:34:39.728: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:34:39.728: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:34:39.728: INFO: update-demo-nautilus-pbs77 is verified up and running
STEP: scaling up the replication controller
Aug 14 04:34:39.733: INFO: scanned /root for discovery docs: <nil>
Aug 14 04:34:39.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4746'
Aug 14 04:34:40.929: INFO: stderr: ""
Aug 14 04:34:40.929: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 04:34:40.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:41.108: INFO: stderr: ""
Aug 14 04:34:41.109: INFO: stdout: "update-demo-nautilus-n9cs4 update-demo-nautilus-pbs77 "
Aug 14 04:34:41.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-n9cs4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:41.282: INFO: stderr: ""
Aug 14 04:34:41.282: INFO: stdout: ""
Aug 14 04:34:41.282: INFO: update-demo-nautilus-n9cs4 is created but not running
Aug 14 04:34:46.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4746'
Aug 14 04:34:46.440: INFO: stderr: ""
Aug 14 04:34:46.440: INFO: stdout: "update-demo-nautilus-n9cs4 update-demo-nautilus-pbs77 "
Aug 14 04:34:46.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-n9cs4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:46.600: INFO: stderr: ""
Aug 14 04:34:46.600: INFO: stdout: "true"
Aug 14 04:34:46.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-n9cs4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:46.768: INFO: stderr: ""
Aug 14 04:34:46.768: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:34:46.768: INFO: validating pod update-demo-nautilus-n9cs4
Aug 14 04:34:46.781: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:34:46.781: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:34:46.781: INFO: update-demo-nautilus-n9cs4 is verified up and running
Aug 14 04:34:46.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:46.948: INFO: stderr: ""
Aug 14 04:34:46.948: INFO: stdout: "true"
Aug 14 04:34:46.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods update-demo-nautilus-pbs77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4746'
Aug 14 04:34:47.123: INFO: stderr: ""
Aug 14 04:34:47.124: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 04:34:47.124: INFO: validating pod update-demo-nautilus-pbs77
Aug 14 04:34:47.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 04:34:47.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 04:34:47.135: INFO: update-demo-nautilus-pbs77 is verified up and running
STEP: using delete to clean up resources
Aug 14 04:34:47.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 delete --grace-period=0 --force -f - --namespace=kubectl-4746'
Aug 14 04:34:47.310: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 04:34:47.310: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 04:34:47.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4746'
Aug 14 04:34:47.486: INFO: stderr: "No resources found in kubectl-4746 namespace.\n"
Aug 14 04:34:47.486: INFO: stdout: ""
Aug 14 04:34:47.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 get pods -l name=update-demo --namespace=kubectl-4746 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 04:34:47.654: INFO: stderr: ""
Aug 14 04:34:47.654: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4746" for this suite.

• [SLOW TEST:27.155 seconds]
[sig-cli] Kubectl client
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":275,"completed":224,"skipped":3869,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:47.682: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Aug 14 04:34:47.899: INFO: Waiting up to 5m0s for pod "downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db" in namespace "downward-api-4249" to be "Succeeded or Failed"
Aug 14 04:34:47.908: INFO: Pod "downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db": Phase="Pending", Reason="", readiness=false. Elapsed: 9.099781ms
Aug 14 04:34:49.920: INFO: Pod "downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020987635s
Aug 14 04:34:51.930: INFO: Pod "downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031120514s
STEP: Saw pod success
Aug 14 04:34:51.930: INFO: Pod "downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db" satisfied condition "Succeeded or Failed"
Aug 14 04:34:51.939: INFO: Trying to get logs from node worker2 pod downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db container dapi-container: <nil>
STEP: delete the pod
Aug 14 04:34:51.987: INFO: Waiting for pod downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db to disappear
Aug 14 04:34:51.995: INFO: Pod downward-api-8fe46bf8-59e3-44f4-9ff4-1a661abd25db no longer exists
[AfterEach] [sig-node] Downward API
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:51.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4249" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":275,"completed":225,"skipped":3872,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:52.020: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 04:34:52.236: INFO: Waiting up to 5m0s for pod "pod-a23cd345-01c0-4026-b4f8-3637e318577c" in namespace "emptydir-3905" to be "Succeeded or Failed"
Aug 14 04:34:52.244: INFO: Pod "pod-a23cd345-01c0-4026-b4f8-3637e318577c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.995923ms
Aug 14 04:34:54.253: INFO: Pod "pod-a23cd345-01c0-4026-b4f8-3637e318577c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017458163s
Aug 14 04:34:56.263: INFO: Pod "pod-a23cd345-01c0-4026-b4f8-3637e318577c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027042602s
STEP: Saw pod success
Aug 14 04:34:56.263: INFO: Pod "pod-a23cd345-01c0-4026-b4f8-3637e318577c" satisfied condition "Succeeded or Failed"
Aug 14 04:34:56.272: INFO: Trying to get logs from node worker2 pod pod-a23cd345-01c0-4026-b4f8-3637e318577c container test-container: <nil>
STEP: delete the pod
Aug 14 04:34:56.329: INFO: Waiting for pod pod-a23cd345-01c0-4026-b4f8-3637e318577c to disappear
Aug 14 04:34:56.336: INFO: Pod pod-a23cd345-01c0-4026-b4f8-3637e318577c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:34:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3905" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":226,"skipped":3889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:34:56.359: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-51e98e2b-974f-4b91-8168-59dab813c850
STEP: Creating a pod to test consume secrets
Aug 14 04:34:56.586: INFO: Waiting up to 5m0s for pod "pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1" in namespace "secrets-8162" to be "Succeeded or Failed"
Aug 14 04:34:56.603: INFO: Pod "pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.283425ms
Aug 14 04:34:58.612: INFO: Pod "pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025656445s
Aug 14 04:35:00.623: INFO: Pod "pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036521702s
STEP: Saw pod success
Aug 14 04:35:00.623: INFO: Pod "pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1" satisfied condition "Succeeded or Failed"
Aug 14 04:35:00.634: INFO: Trying to get logs from node worker2 pod pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 04:35:00.730: INFO: Waiting for pod pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1 to disappear
Aug 14 04:35:00.739: INFO: Pod pod-secrets-02e37765-1a38-40b6-902e-68757d3729b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:35:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8162" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":227,"skipped":3927,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:35:00.765: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4567.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4567.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4567.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4567.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4567.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4567.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:35:05.180: INFO: DNS probes using dns-4567/dns-test-ca7e1d22-43a8-444c-aae1-a60e7913619e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:35:05.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4567" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":275,"completed":228,"skipped":3934,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:35:05.234: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 04:35:10.088: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b81695e5-e8b6-433d-81de-3a1c2e35103d"
Aug 14 04:35:10.088: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b81695e5-e8b6-433d-81de-3a1c2e35103d" in namespace "pods-5591" to be "terminated due to deadline exceeded"
Aug 14 04:35:10.098: INFO: Pod "pod-update-activedeadlineseconds-b81695e5-e8b6-433d-81de-3a1c2e35103d": Phase="Running", Reason="", readiness=true. Elapsed: 10.130958ms
Aug 14 04:35:12.108: INFO: Pod "pod-update-activedeadlineseconds-b81695e5-e8b6-433d-81de-3a1c2e35103d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.020079257s
Aug 14 04:35:12.109: INFO: Pod "pod-update-activedeadlineseconds-b81695e5-e8b6-433d-81de-3a1c2e35103d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:35:12.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5591" for this suite.

• [SLOW TEST:6.976 seconds]
[k8s.io] Pods
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":275,"completed":229,"skipped":3940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:35:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:35:12.415: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 14 04:35:14.506: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:35:15.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1979" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":275,"completed":230,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:35:15.554: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 14 04:35:15.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1397 /api/v1/namespaces/watch-1397/configmaps/e2e-watch-test-watch-closed 2d846a0a-a036-4984-b534-b094fcf2d7f8 4173037 0 2020-08-14 04:35:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-14 04:35:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 04:35:15.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1397 /api/v1/namespaces/watch-1397/configmaps/e2e-watch-test-watch-closed 2d846a0a-a036-4984-b534-b094fcf2d7f8 4173038 0 2020-08-14 04:35:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-14 04:35:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 14 04:35:15.819: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1397 /api/v1/namespaces/watch-1397/configmaps/e2e-watch-test-watch-closed 2d846a0a-a036-4984-b534-b094fcf2d7f8 4173039 0 2020-08-14 04:35:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-14 04:35:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 14 04:35:15.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1397 /api/v1/namespaces/watch-1397/configmaps/e2e-watch-test-watch-closed 2d846a0a-a036-4984-b534-b094fcf2d7f8 4173041 0 2020-08-14 04:35:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-14 04:35:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:35:15.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1397" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":275,"completed":231,"skipped":4003,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:35:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:35:16.122: INFO: Create a RollingUpdate DaemonSet
Aug 14 04:35:16.135: INFO: Check that daemon pods launch on every node of the cluster
Aug 14 04:35:16.157: INFO: Number of nodes with available pods: 0
Aug 14 04:35:16.157: INFO: Node master1 is running more than one daemon pod
Aug 14 04:35:17.184: INFO: Number of nodes with available pods: 0
Aug 14 04:35:17.184: INFO: Node master1 is running more than one daemon pod
Aug 14 04:35:18.187: INFO: Number of nodes with available pods: 0
Aug 14 04:35:18.187: INFO: Node master1 is running more than one daemon pod
Aug 14 04:35:19.209: INFO: Number of nodes with available pods: 3
Aug 14 04:35:19.209: INFO: Node master1 is running more than one daemon pod
Aug 14 04:35:20.181: INFO: Number of nodes with available pods: 5
Aug 14 04:35:20.181: INFO: Number of running nodes: 5, number of available pods: 5
Aug 14 04:35:20.181: INFO: Update the DaemonSet to trigger a rollout
Aug 14 04:35:20.200: INFO: Updating DaemonSet daemon-set
Aug 14 04:35:26.243: INFO: Roll back the DaemonSet before rollout is complete
Aug 14 04:35:26.264: INFO: Updating DaemonSet daemon-set
Aug 14 04:35:26.264: INFO: Make sure DaemonSet rollback is complete
Aug 14 04:35:26.275: INFO: Wrong image for pod: daemon-set-kmjxp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 14 04:35:26.275: INFO: Pod daemon-set-kmjxp is not available
Aug 14 04:35:27.298: INFO: Wrong image for pod: daemon-set-kmjxp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 14 04:35:27.299: INFO: Pod daemon-set-kmjxp is not available
Aug 14 04:35:28.298: INFO: Wrong image for pod: daemon-set-kmjxp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 14 04:35:28.298: INFO: Pod daemon-set-kmjxp is not available
Aug 14 04:35:29.298: INFO: Wrong image for pod: daemon-set-kmjxp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 14 04:35:29.298: INFO: Pod daemon-set-kmjxp is not available
Aug 14 04:35:30.300: INFO: Wrong image for pod: daemon-set-kmjxp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 14 04:35:30.300: INFO: Pod daemon-set-kmjxp is not available
Aug 14 04:35:31.302: INFO: Pod daemon-set-kbp4m is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6811, will wait for the garbage collector to delete the pods
Aug 14 04:35:31.479: INFO: Deleting DaemonSet.extensions daemon-set took: 21.491734ms
Aug 14 04:35:31.879: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.393111ms
Aug 14 04:36:08.189: INFO: Number of nodes with available pods: 0
Aug 14 04:36:08.189: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 04:36:08.197: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6811/daemonsets","resourceVersion":"4173441"},"items":null}

Aug 14 04:36:08.206: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6811/pods","resourceVersion":"4173441"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:36:08.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6811" for this suite.

• [SLOW TEST:52.443 seconds]
[sig-apps] Daemon set [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":275,"completed":232,"skipped":4004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:36:08.288: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:36:08.506: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb" in namespace "projected-2176" to be "Succeeded or Failed"
Aug 14 04:36:08.517: INFO: Pod "downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.838017ms
Aug 14 04:36:10.527: INFO: Pod "downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020958156s
STEP: Saw pod success
Aug 14 04:36:10.527: INFO: Pod "downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb" satisfied condition "Succeeded or Failed"
Aug 14 04:36:10.537: INFO: Trying to get logs from node worker2 pod downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb container client-container: <nil>
STEP: delete the pod
Aug 14 04:36:10.596: INFO: Waiting for pod downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb to disappear
Aug 14 04:36:10.604: INFO: Pod downwardapi-volume-03ab835b-c20e-4ebb-a933-b4b2d8e1fadb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:36:10.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2176" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":233,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:36:10.630: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 04:36:10.850: INFO: Waiting up to 5m0s for pod "pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e" in namespace "emptydir-9875" to be "Succeeded or Failed"
Aug 14 04:36:10.859: INFO: Pod "pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.45116ms
Aug 14 04:36:12.868: INFO: Pod "pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018295222s
STEP: Saw pod success
Aug 14 04:36:12.868: INFO: Pod "pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e" satisfied condition "Succeeded or Failed"
Aug 14 04:36:12.878: INFO: Trying to get logs from node worker2 pod pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e container test-container: <nil>
STEP: delete the pod
Aug 14 04:36:12.937: INFO: Waiting for pod pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e to disappear
Aug 14 04:36:12.946: INFO: Pod pod-fd952a7d-e260-4c11-a5dd-0e1383f5a17e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:36:12.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9875" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":234,"skipped":4093,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:36:12.975: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-3610/configmap-test-320021ec-36c4-4f59-9805-69fab673ab69
STEP: Creating a pod to test consume configMaps
Aug 14 04:36:13.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8" in namespace "configmap-3610" to be "Succeeded or Failed"
Aug 14 04:36:13.213: INFO: Pod "pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.992761ms
Aug 14 04:36:15.222: INFO: Pod "pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018519661s
Aug 14 04:36:17.231: INFO: Pod "pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027611202s
STEP: Saw pod success
Aug 14 04:36:17.231: INFO: Pod "pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8" satisfied condition "Succeeded or Failed"
Aug 14 04:36:17.242: INFO: Trying to get logs from node worker2 pod pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8 container env-test: <nil>
STEP: delete the pod
Aug 14 04:36:17.295: INFO: Waiting for pod pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8 to disappear
Aug 14 04:36:17.303: INFO: Pod pod-configmaps-664e73b4-31c2-4624-b084-f4053f9a5bc8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:36:17.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3610" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":235,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:36:17.329: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 14 04:36:18.027: INFO: Pod name wrapped-volume-race-e3574fb1-4bfd-4a4e-9dc3-d58f762be4c4: Found 0 pods out of 5
Aug 14 04:36:23.046: INFO: Pod name wrapped-volume-race-e3574fb1-4bfd-4a4e-9dc3-d58f762be4c4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e3574fb1-4bfd-4a4e-9dc3-d58f762be4c4 in namespace emptydir-wrapper-6641, will wait for the garbage collector to delete the pods
Aug 14 04:36:35.212: INFO: Deleting ReplicationController wrapped-volume-race-e3574fb1-4bfd-4a4e-9dc3-d58f762be4c4 took: 24.687467ms
Aug 14 04:36:35.712: INFO: Terminating ReplicationController wrapped-volume-race-e3574fb1-4bfd-4a4e-9dc3-d58f762be4c4 pods took: 500.440959ms
STEP: Creating RC which spawns configmap-volume pods
Aug 14 04:36:51.567: INFO: Pod name wrapped-volume-race-3a36f2b2-9888-4b88-92eb-53dd95323e9e: Found 0 pods out of 5
Aug 14 04:36:56.584: INFO: Pod name wrapped-volume-race-3a36f2b2-9888-4b88-92eb-53dd95323e9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3a36f2b2-9888-4b88-92eb-53dd95323e9e in namespace emptydir-wrapper-6641, will wait for the garbage collector to delete the pods
Aug 14 04:37:08.744: INFO: Deleting ReplicationController wrapped-volume-race-3a36f2b2-9888-4b88-92eb-53dd95323e9e took: 26.566204ms
Aug 14 04:37:09.145: INFO: Terminating ReplicationController wrapped-volume-race-3a36f2b2-9888-4b88-92eb-53dd95323e9e pods took: 400.365471ms
STEP: Creating RC which spawns configmap-volume pods
Aug 14 04:37:27.005: INFO: Pod name wrapped-volume-race-d6ffb095-52d8-4f18-aac6-9e20d31af4cb: Found 0 pods out of 5
Aug 14 04:37:32.024: INFO: Pod name wrapped-volume-race-d6ffb095-52d8-4f18-aac6-9e20d31af4cb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d6ffb095-52d8-4f18-aac6-9e20d31af4cb in namespace emptydir-wrapper-6641, will wait for the garbage collector to delete the pods
Aug 14 04:37:44.174: INFO: Deleting ReplicationController wrapped-volume-race-d6ffb095-52d8-4f18-aac6-9e20d31af4cb took: 23.45241ms
Aug 14 04:37:44.574: INFO: Terminating ReplicationController wrapped-volume-race-d6ffb095-52d8-4f18-aac6-9e20d31af4cb pods took: 400.460271ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:38:01.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6641" for this suite.

• [SLOW TEST:104.611 seconds]
[sig-storage] EmptyDir wrapper volumes
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":275,"completed":236,"skipped":4134,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:38:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4937
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:38:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 14 04:38:12.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4937 create -f -'
Aug 14 04:38:13.545: INFO: stderr: ""
Aug 14 04:38:13.545: INFO: stdout: "e2e-test-crd-publish-openapi-5867-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 14 04:38:13.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4937 delete e2e-test-crd-publish-openapi-5867-crds test-cr'
Aug 14 04:38:13.803: INFO: stderr: ""
Aug 14 04:38:13.803: INFO: stdout: "e2e-test-crd-publish-openapi-5867-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 14 04:38:13.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4937 apply -f -'
Aug 14 04:38:14.164: INFO: stderr: ""
Aug 14 04:38:14.164: INFO: stdout: "e2e-test-crd-publish-openapi-5867-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 14 04:38:14.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4937 delete e2e-test-crd-publish-openapi-5867-crds test-cr'
Aug 14 04:38:14.352: INFO: stderr: ""
Aug 14 04:38:14.352: INFO: stdout: "e2e-test-crd-publish-openapi-5867-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 14 04:38:14.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-5867-crds'
Aug 14 04:38:14.943: INFO: stderr: ""
Aug 14 04:38:14.943: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5867-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:38:21.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4937" for this suite.

• [SLOW TEST:19.168 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":275,"completed":237,"skipped":4145,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:38:21.109: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6918
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-4616b49c-7bea-4326-a450-beb81f5bff91
STEP: Creating secret with name s-test-opt-upd-90876355-2dc3-43ca-ad88-7dc3be6ffb3e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4616b49c-7bea-4326-a450-beb81f5bff91
STEP: Updating secret s-test-opt-upd-90876355-2dc3-43ca-ad88-7dc3be6ffb3e
STEP: Creating secret with name s-test-opt-create-d83f01d1-8ade-4016-ab3d-05187c9cc1cd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:38:27.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6918" for this suite.

• [SLOW TEST:6.524 seconds]
[sig-storage] Projected secret
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":238,"skipped":4153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:38:27.634: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:38:28.574: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 14 04:38:30.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976708, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976708, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976708, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976708, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:38:33.634: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:38:33.693: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:38:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5082" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.585 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":275,"completed":239,"skipped":4182,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:38:41.219: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 14 04:39:21.613: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 04:39:21.613726      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:39:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4302" for this suite.

• [SLOW TEST:40.419 seconds]
[sig-api-machinery] Garbage collector
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":275,"completed":240,"skipped":4193,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:39:21.639: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Aug 14 04:39:23.900: INFO: Pod pod-hostip-facc1de4-14e6-410c-a96c-1a330da95b90 has hostIP: 172.16.21.97
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:39:23.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8803" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":275,"completed":241,"skipped":4199,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:39:23.929: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4576
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:39:24.125: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 14 04:39:34.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4576 create -f -'
Aug 14 04:39:35.551: INFO: stderr: ""
Aug 14 04:39:35.551: INFO: stdout: "e2e-test-crd-publish-openapi-2978-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 14 04:39:35.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4576 delete e2e-test-crd-publish-openapi-2978-crds test-cr'
Aug 14 04:39:35.816: INFO: stderr: ""
Aug 14 04:39:35.817: INFO: stdout: "e2e-test-crd-publish-openapi-2978-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 14 04:39:35.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4576 apply -f -'
Aug 14 04:39:36.479: INFO: stderr: ""
Aug 14 04:39:36.480: INFO: stdout: "e2e-test-crd-publish-openapi-2978-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 14 04:39:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-4576 delete e2e-test-crd-publish-openapi-2978-crds test-cr'
Aug 14 04:39:36.669: INFO: stderr: ""
Aug 14 04:39:36.669: INFO: stdout: "e2e-test-crd-publish-openapi-2978-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 14 04:39:36.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-2978-crds'
Aug 14 04:39:37.237: INFO: stderr: ""
Aug 14 04:39:37.237: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2978-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:39:43.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4576" for this suite.

• [SLOW TEST:19.154 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":275,"completed":242,"skipped":4200,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:39:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-b15035a0-87bb-468d-949c-e9be180c1cd0
STEP: Creating a pod to test consume secrets
Aug 14 04:39:43.312: INFO: Waiting up to 5m0s for pod "pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7" in namespace "secrets-7104" to be "Succeeded or Failed"
Aug 14 04:39:43.323: INFO: Pod "pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.095937ms
Aug 14 04:39:45.332: INFO: Pod "pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019944378s
Aug 14 04:39:47.395: INFO: Pod "pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082896843s
STEP: Saw pod success
Aug 14 04:39:47.395: INFO: Pod "pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7" satisfied condition "Succeeded or Failed"
Aug 14 04:39:47.407: INFO: Trying to get logs from node worker2 pod pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 04:39:47.459: INFO: Waiting for pod pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7 to disappear
Aug 14 04:39:47.467: INFO: Pod pod-secrets-c1784041-aed9-46e0-bda3-c2ff21703ef7 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:39:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7104" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":243,"skipped":4205,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:39:47.493: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:39:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5504" for this suite.

• [SLOW TEST:11.316 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":275,"completed":244,"skipped":4205,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:39:58.810: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3973
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:39:59.011: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 14 04:40:09.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 create -f -'
Aug 14 04:40:10.507: INFO: stderr: ""
Aug 14 04:40:10.507: INFO: stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 14 04:40:10.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 delete e2e-test-crd-publish-openapi-3435-crds test-foo'
Aug 14 04:40:10.694: INFO: stderr: ""
Aug 14 04:40:10.694: INFO: stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 14 04:40:10.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 14 04:40:11.224: INFO: stderr: ""
Aug 14 04:40:11.224: INFO: stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 14 04:40:11.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 delete e2e-test-crd-publish-openapi-3435-crds test-foo'
Aug 14 04:40:11.482: INFO: stderr: ""
Aug 14 04:40:11.482: INFO: stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 14 04:40:11.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 create -f -'
Aug 14 04:40:12.007: INFO: rc: 1
Aug 14 04:40:12.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 14 04:40:12.468: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 14 04:40:12.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 create -f -'
Aug 14 04:40:12.987: INFO: rc: 1
Aug 14 04:40:12.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 14 04:40:13.563: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 14 04:40:13.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-3435-crds'
Aug 14 04:40:14.099: INFO: stderr: ""
Aug 14 04:40:14.099: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3435-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 14 04:40:14.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-3435-crds.metadata'
Aug 14 04:40:14.558: INFO: stderr: ""
Aug 14 04:40:14.558: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3435-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 14 04:40:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-3435-crds.spec'
Aug 14 04:40:15.076: INFO: stderr: ""
Aug 14 04:40:15.076: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3435-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 14 04:40:15.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-3435-crds.spec.bars'
Aug 14 04:40:15.648: INFO: stderr: ""
Aug 14 04:40:15.648: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3435-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 14 04:40:15.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 explain e2e-test-crd-publish-openapi-3435-crds.spec.bars2'
Aug 14 04:40:16.322: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:40:21.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3973" for this suite.

• [SLOW TEST:23.178 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":275,"completed":245,"skipped":4211,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:40:21.988: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-e9101fa8-a04e-4e11-b6f9-c83e2f99f573
[AfterEach] [sig-node] ConfigMap
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:40:22.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1988" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":275,"completed":246,"skipped":4222,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:40:22.266: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:40:23.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:40:25.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976823, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976823, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976823, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732976823, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:40:28.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:40:28.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6215" for this suite.
STEP: Destroying namespace "webhook-6215-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":275,"completed":247,"skipped":4223,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:40:28.964: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8446
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Aug 14 04:40:29.204: INFO: Found 0 stateful pods, waiting for 3
Aug 14 04:40:39.215: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:40:39.216: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:40:39.216: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:40:39.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-8446 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 04:40:39.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 04:40:39.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 04:40:39.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 14 04:40:39.786: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 14 04:40:49.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-8446 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 04:40:50.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 04:40:50.248: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 04:40:50.248: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 04:41:00.303: INFO: Waiting for StatefulSet statefulset-8446/ss2 to complete update
Aug 14 04:41:00.303: INFO: Waiting for Pod statefulset-8446/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 14 04:41:00.303: INFO: Waiting for Pod statefulset-8446/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 14 04:41:00.303: INFO: Waiting for Pod statefulset-8446/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 14 04:41:10.323: INFO: Waiting for StatefulSet statefulset-8446/ss2 to complete update
Aug 14 04:41:10.323: INFO: Waiting for Pod statefulset-8446/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Aug 14 04:41:20.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-8446 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 14 04:41:20.858: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 14 04:41:20.858: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 14 04:41:20.858: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 14 04:41:30.930: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 14 04:41:40.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-133789455 exec --namespace=statefulset-8446 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 14 04:41:41.482: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 14 04:41:41.482: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 14 04:41:41.482: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 14 04:42:11.594: INFO: Waiting for StatefulSet statefulset-8446/ss2 to complete update
Aug 14 04:42:11.594: INFO: Waiting for Pod statefulset-8446/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 04:42:21.616: INFO: Deleting all statefulset in ns statefulset-8446
Aug 14 04:42:21.694: INFO: Scaling statefulset ss2 to 0
Aug 14 04:42:41.792: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 04:42:41.801: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:42:41.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8446" for this suite.

• [SLOW TEST:132.891 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":275,"completed":248,"skipped":4241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:42:41.856: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:42:42.051: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:42:44.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2641" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":275,"completed":249,"skipped":4278,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:42:44.179: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-9710
[It] Should recreate evicted statefulset [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9710
STEP: Creating statefulset with conflicting port in namespace statefulset-9710
STEP: Waiting until pod test-pod will start running in namespace statefulset-9710
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9710
Aug 14 04:42:50.498: INFO: Observed stateful pod in namespace: statefulset-9710, name: ss-0, uid: f9a6b113-5e9c-4c1f-a170-9335592eb40a, status phase: Pending. Waiting for statefulset controller to delete.
Aug 14 04:42:50.643: INFO: Observed stateful pod in namespace: statefulset-9710, name: ss-0, uid: f9a6b113-5e9c-4c1f-a170-9335592eb40a, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 04:42:50.663: INFO: Observed stateful pod in namespace: statefulset-9710, name: ss-0, uid: f9a6b113-5e9c-4c1f-a170-9335592eb40a, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 04:42:50.674: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9710
STEP: Removing pod with conflicting port in namespace statefulset-9710
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9710 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 04:42:56.747: INFO: Deleting all statefulset in ns statefulset-9710
Aug 14 04:42:56.757: INFO: Scaling statefulset ss to 0
Aug 14 04:43:16.888: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 04:43:16.898: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:43:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9710" for this suite.

• [SLOW TEST:32.777 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":275,"completed":250,"skipped":4297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:43:16.957: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-2020
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2020 to expose endpoints map[]
Aug 14 04:43:17.185: INFO: Get endpoints failed (11.512675ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 14 04:43:18.194: INFO: successfully validated that service multi-endpoint-test in namespace services-2020 exposes endpoints map[] (1.020199496s elapsed)
STEP: Creating pod pod1 in namespace services-2020
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2020 to expose endpoints map[pod1:[100]]
Aug 14 04:43:21.332: INFO: successfully validated that service multi-endpoint-test in namespace services-2020 exposes endpoints map[pod1:[100]] (3.118668921s elapsed)
STEP: Creating pod pod2 in namespace services-2020
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2020 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 14 04:43:24.450: INFO: successfully validated that service multi-endpoint-test in namespace services-2020 exposes endpoints map[pod1:[100] pod2:[101]] (3.103730394s elapsed)
STEP: Deleting pod pod1 in namespace services-2020
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2020 to expose endpoints map[pod2:[101]]
Aug 14 04:43:25.504: INFO: successfully validated that service multi-endpoint-test in namespace services-2020 exposes endpoints map[pod2:[101]] (1.037051379s elapsed)
STEP: Deleting pod pod2 in namespace services-2020
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2020 to expose endpoints map[]
Aug 14 04:43:26.591: INFO: successfully validated that service multi-endpoint-test in namespace services-2020 exposes endpoints map[] (1.069845427s elapsed)
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:43:26.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2020" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.710 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":275,"completed":251,"skipped":4362,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:43:26.669: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:43:26.898: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:43:28.910: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Pending, waiting for it to be Running (with Ready = true)
Aug 14 04:43:30.910: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:32.909: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:34.908: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:36.910: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:38.909: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:40.911: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:42.909: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = false)
Aug 14 04:43:44.909: INFO: The status of Pod test-webserver-2318b68a-f942-4a8f-8fb5-56ce4fc07551 is Running (Ready = true)
Aug 14 04:43:44.918: INFO: Container started at 2020-08-14 04:43:28 +0000 UTC, pod became ready at 2020-08-14 04:43:44 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:43:44.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1628" for this suite.

• [SLOW TEST:18.279 seconds]
[k8s.io] Probing container
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":275,"completed":252,"skipped":4364,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:43:44.948: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Aug 14 04:43:45.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c" in namespace "downward-api-4422" to be "Succeeded or Failed"
Aug 14 04:43:45.179: INFO: Pod "downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.840414ms
Aug 14 04:43:47.189: INFO: Pod "downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022440732s
Aug 14 04:43:49.199: INFO: Pod "downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032011773s
STEP: Saw pod success
Aug 14 04:43:49.199: INFO: Pod "downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c" satisfied condition "Succeeded or Failed"
Aug 14 04:43:49.208: INFO: Trying to get logs from node worker2 pod downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c container client-container: <nil>
STEP: delete the pod
Aug 14 04:43:49.350: INFO: Waiting for pod downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c to disappear
Aug 14 04:43:49.360: INFO: Pod downwardapi-volume-4ef4ba8d-1936-436e-82c4-5aaaac48ba2c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:43:49.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4422" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":253,"skipped":4367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:43:49.384: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 14 04:43:59.774: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 04:43:59.774590      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:43:59.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7532" for this suite.

• [SLOW TEST:10.419 seconds]
[sig-api-machinery] Garbage collector
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":275,"completed":254,"skipped":4390,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:43:59.804: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:44:16.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8930" for this suite.

• [SLOW TEST:16.405 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":275,"completed":255,"skipped":4407,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:44:16.209: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:44:40.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1232" for this suite.

• [SLOW TEST:24.011 seconds]
[k8s.io] Container Runtime
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":275,"completed":256,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:44:40.220: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 04:44:42.513: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:44:42.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9069" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":257,"skipped":4427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:44:42.587: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-6979
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Aug 14 04:44:42.829: INFO: Found 0 stateful pods, waiting for 3
Aug 14 04:44:52.840: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:44:52.840: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:44:52.840: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 14 04:44:52.902: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 14 04:45:02.975: INFO: Updating stateful set ss2
Aug 14 04:45:02.995: INFO: Waiting for Pod statefulset-6979/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 14 04:45:13.100: INFO: Found 2 stateful pods, waiting for 3
Aug 14 04:45:23.112: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:45:23.112: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 04:45:23.112: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 14 04:45:23.164: INFO: Updating stateful set ss2
Aug 14 04:45:23.186: INFO: Waiting for Pod statefulset-6979/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 14 04:45:33.239: INFO: Updating stateful set ss2
Aug 14 04:45:33.255: INFO: Waiting for StatefulSet statefulset-6979/ss2 to complete update
Aug 14 04:45:33.255: INFO: Waiting for Pod statefulset-6979/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 14 04:45:43.275: INFO: Waiting for StatefulSet statefulset-6979/ss2 to complete update
Aug 14 04:45:43.275: INFO: Waiting for Pod statefulset-6979/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 04:45:53.277: INFO: Deleting all statefulset in ns statefulset-6979
Aug 14 04:45:53.286: INFO: Scaling statefulset ss2 to 0
Aug 14 04:46:13.328: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 04:46:13.337: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:46:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6979" for this suite.

• [SLOW TEST:90.821 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":275,"completed":258,"skipped":4458,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:46:13.408: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2905, will wait for the garbage collector to delete the pods
Aug 14 04:46:17.714: INFO: Deleting Job.batch foo took: 20.263436ms
Aug 14 04:46:17.814: INFO: Terminating Job.batch foo pods took: 100.413939ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:47:01.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2905" for this suite.

• [SLOW TEST:48.039 seconds]
[sig-apps] Job
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":275,"completed":259,"skipped":4465,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:47:01.448: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Aug 14 04:47:06.344: INFO: Successfully updated pod "labelsupdatec25ca29e-d0d8-4e8f-95eb-adb26859b936"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:47:08.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4440" for this suite.

• [SLOW TEST:6.968 seconds]
[sig-storage] Projected downwardAPI
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":260,"skipped":4469,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:47:08.416: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7372
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2634
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:47:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1050" for this suite.
STEP: Destroying namespace "nsdeletetest-7372" for this suite.
Aug 14 04:47:15.085: INFO: Namespace nsdeletetest-7372 was already deleted
STEP: Destroying namespace "nsdeletetest-2634" for this suite.

• [SLOW TEST:6.681 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":275,"completed":261,"skipped":4473,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:47:15.098: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:47:15.378: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 04:47:15.411: INFO: Number of nodes with available pods: 0
Aug 14 04:47:15.411: INFO: Node master1 is running more than one daemon pod
Aug 14 04:47:16.497: INFO: Number of nodes with available pods: 0
Aug 14 04:47:16.497: INFO: Node master1 is running more than one daemon pod
Aug 14 04:47:17.438: INFO: Number of nodes with available pods: 0
Aug 14 04:47:17.438: INFO: Node master1 is running more than one daemon pod
Aug 14 04:47:18.436: INFO: Number of nodes with available pods: 4
Aug 14 04:47:18.436: INFO: Node master1 is running more than one daemon pod
Aug 14 04:47:19.435: INFO: Number of nodes with available pods: 5
Aug 14 04:47:19.435: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 14 04:47:19.510: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:19.510: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:19.511: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:19.511: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:19.511: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:20.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:20.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:20.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:20.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:20.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:21.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:21.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:21.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:21.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:21.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:22.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:22.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:22.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:22.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:22.538: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:22.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:23.595: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:23.595: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:23.595: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:23.595: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:23.595: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:23.595: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:24.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:24.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:24.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:24.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:24.538: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:24.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:25.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:25.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:25.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:25.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:25.538: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:25.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:26.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:26.539: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:26.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:26.539: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:26.539: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:26.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:27.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:27.539: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:27.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:27.540: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:27.540: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:27.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:28.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:28.540: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:28.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:28.540: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:28.540: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:28.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:29.541: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:29.541: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:29.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:29.541: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:29.541: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:29.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:30.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:30.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:30.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:30.538: INFO: Wrong image for pod: daemon-set-xqwvm. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:30.538: INFO: Pod daemon-set-xqwvm is not available
Aug 14 04:47:30.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:31.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:31.538: INFO: Pod daemon-set-m4zbg is not available
Aug 14 04:47:31.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:31.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:31.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:32.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:32.540: INFO: Pod daemon-set-m4zbg is not available
Aug 14 04:47:32.540: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:32.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:32.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:33.540: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:33.540: INFO: Pod daemon-set-m4zbg is not available
Aug 14 04:47:33.540: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:33.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:33.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:34.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:34.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:34.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:34.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:35.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:35.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:35.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:35.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:36.591: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:36.591: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:36.591: INFO: Pod daemon-set-m9vmv is not available
Aug 14 04:47:36.591: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:36.591: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:37.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:37.539: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:37.539: INFO: Pod daemon-set-m9vmv is not available
Aug 14 04:47:37.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:37.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:38.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:38.539: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:38.539: INFO: Pod daemon-set-m9vmv is not available
Aug 14 04:47:38.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:38.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:39.541: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:39.541: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:39.541: INFO: Pod daemon-set-m9vmv is not available
Aug 14 04:47:39.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:39.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:40.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:40.538: INFO: Wrong image for pod: daemon-set-m9vmv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:40.538: INFO: Pod daemon-set-m9vmv is not available
Aug 14 04:47:40.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:40.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:41.542: INFO: Pod daemon-set-f7jrp is not available
Aug 14 04:47:41.542: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:41.542: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:41.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:42.542: INFO: Pod daemon-set-f7jrp is not available
Aug 14 04:47:42.542: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:42.542: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:42.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:43.592: INFO: Pod daemon-set-f7jrp is not available
Aug 14 04:47:43.592: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:43.592: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:43.592: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:44.543: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:44.543: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:44.543: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:45.540: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:45.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:45.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:46.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:46.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:46.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:46.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:47.541: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:47.541: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:47.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:47.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:48.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:48.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:48.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:48.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:49.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:49.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:49.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:49.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:50.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:50.540: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:50.540: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:50.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:51.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:51.538: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:51.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:51.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:52.590: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:52.590: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:52.590: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:52.590: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:53.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:53.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:53.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:53.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:54.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:54.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:54.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:54.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:55.538: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:55.538: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:55.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:55.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:56.539: INFO: Wrong image for pod: daemon-set-hxshs. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:56.539: INFO: Pod daemon-set-hxshs is not available
Aug 14 04:47:56.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:56.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:57.538: INFO: Pod daemon-set-6jtlt is not available
Aug 14 04:47:57.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:57.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:58.541: INFO: Pod daemon-set-6jtlt is not available
Aug 14 04:47:58.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:58.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:59.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:47:59.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:00.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:00.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:01.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:01.538: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:01.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:02.542: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:02.542: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:02.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:03.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:03.541: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:03.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:04.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:04.538: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:04.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:05.538: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:05.538: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:05.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:06.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:06.539: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:06.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:07.541: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:07.542: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:07.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:08.550: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:08.550: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:08.550: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:09.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:09.539: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:09.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:10.539: INFO: Wrong image for pod: daemon-set-mqhc5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:10.539: INFO: Pod daemon-set-mqhc5 is not available
Aug 14 04:48:10.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:11.591: INFO: Pod daemon-set-lp2r4 is not available
Aug 14 04:48:11.591: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:12.542: INFO: Pod daemon-set-lp2r4 is not available
Aug 14 04:48:12.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:13.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:14.542: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:14.542: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:15.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:15.539: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:16.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:16.538: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:17.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:17.541: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:18.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:18.540: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:19.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:19.540: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:20.540: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:20.540: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:21.539: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:21.539: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:22.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:22.541: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:23.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:23.542: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:24.541: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:24.541: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:25.538: INFO: Wrong image for pod: daemon-set-zmjww. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Aug 14 04:48:25.538: INFO: Pod daemon-set-zmjww is not available
Aug 14 04:48:26.539: INFO: Pod daemon-set-vxwtz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 14 04:48:26.617: INFO: Number of nodes with available pods: 4
Aug 14 04:48:26.617: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:48:27.643: INFO: Number of nodes with available pods: 4
Aug 14 04:48:27.644: INFO: Node worker2 is running more than one daemon pod
Aug 14 04:48:28.641: INFO: Number of nodes with available pods: 5
Aug 14 04:48:28.641: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6193, will wait for the garbage collector to delete the pods
Aug 14 04:48:28.765: INFO: Deleting DaemonSet.extensions daemon-set took: 21.369653ms
Aug 14 04:48:29.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.443123ms
Aug 14 04:48:40.774: INFO: Number of nodes with available pods: 0
Aug 14 04:48:40.774: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 04:48:40.781: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6193/daemonsets","resourceVersion":"4179770"},"items":null}

Aug 14 04:48:40.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6193/pods","resourceVersion":"4179770"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:48:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6193" for this suite.

• [SLOW TEST:85.768 seconds]
[sig-apps] Daemon set [Serial]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":275,"completed":262,"skipped":4483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:48:40.866: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9399 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9399;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9399 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9399;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9399.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9399.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9399.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9399.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9399.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9399.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9399.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9399.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9399.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.150.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.150.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.150.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.150.238_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9399 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9399;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9399 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9399;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9399.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9399.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9399.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9399.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9399.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9399.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9399.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9399.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9399.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9399.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.150.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.150.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.150.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.150.238_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:48:45.157: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.168: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-9399 from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9399 from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.212: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.225: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.239: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.318: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.329: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.339: INFO: Unable to read jessie_udp@dns-test-service.dns-9399 from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9399 from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.359: INFO: Unable to read jessie_udp@dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.370: INFO: Unable to read jessie_tcp@dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.381: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.391: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9399.svc from pod dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f: the server could not find the requested resource (get pods dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f)
Aug 14 04:48:45.453: INFO: Lookups using dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9399 wheezy_tcp@dns-test-service.dns-9399 wheezy_udp@dns-test-service.dns-9399.svc wheezy_tcp@dns-test-service.dns-9399.svc wheezy_udp@_http._tcp.dns-test-service.dns-9399.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9399.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9399 jessie_tcp@dns-test-service.dns-9399 jessie_udp@dns-test-service.dns-9399.svc jessie_tcp@dns-test-service.dns-9399.svc jessie_udp@_http._tcp.dns-test-service.dns-9399.svc jessie_tcp@_http._tcp.dns-test-service.dns-9399.svc]

Aug 14 04:48:50.768: INFO: DNS probes using dns-9399/dns-test-9dd6fa68-0c98-4b53-b361-f738f195c62f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:48:50.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9399" for this suite.

• [SLOW TEST:10.060 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":275,"completed":263,"skipped":4506,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:48:50.934: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5183.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5183.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:48:57.268: INFO: DNS probes using dns-5183/dns-test-ba875c08-820a-4b9a-9b5c-81117aed7f2e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:48:57.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5183" for this suite.

• [SLOW TEST:6.395 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":275,"completed":264,"skipped":4509,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:48:57.331: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:48:59.041: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:49:01.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977339, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977339, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977339, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977339, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:49:04.129: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:49:04.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5764" for this suite.
STEP: Destroying namespace "webhook-5764-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.437 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":275,"completed":265,"skipped":4520,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:49:04.768: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:49:16.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5233" for this suite.

• [SLOW TEST:11.537 seconds]
[sig-api-machinery] ResourceQuota
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":275,"completed":266,"skipped":4524,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:49:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-ee2a91a1-0e98-4305-88cf-69871006e400
[AfterEach] [sig-api-machinery] Secrets
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:49:16.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9154" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":275,"completed":267,"skipped":4525,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:49:16.541: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:49:16.747: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 14 04:49:16.766: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 14 04:49:21.777: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 04:49:21.777: INFO: Creating deployment "test-rolling-update-deployment"
Aug 14 04:49:21.788: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 14 04:49:21.801: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 14 04:49:23.817: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 14 04:49:23.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977361, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977361, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977361, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977361, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 04:49:25.833: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Aug 14 04:49:25.859: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6110 /apis/apps/v1/namespaces/deployment-6110/deployments/test-rolling-update-deployment 4f5800f5-829a-4ff4-9864-ccfd8ef7b840 4180331 1 2020-08-14 04:49:21 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-08-14 04:49:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:49:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x4005122118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-14 04:49:21 +0000 UTC,LastTransitionTime:2020-08-14 04:49:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-08-14 04:49:24 +0000 UTC,LastTransitionTime:2020-08-14 04:49:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 04:49:25.867: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-6110 /apis/apps/v1/namespaces/deployment-6110/replicasets/test-rolling-update-deployment-59d5cb45c7 67070484-f74b-4cbf-a975-e7641b28bb5a 4180319 1 2020-08-14 04:49:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4f5800f5-829a-4ff4-9864-ccfd8ef7b840 0x4005122667 0x4005122668}] []  [{kube-controller-manager Update apps/v1 2020-08-14 04:49:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 102 53 56 48 48 102 53 45 56 50 57 97 45 52 102 102 52 45 57 56 54 52 45 99 99 102 100 56 101 102 55 98 56 52 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0x40051226f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:49:25.867: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 14 04:49:25.867: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6110 /apis/apps/v1/namespaces/deployment-6110/replicasets/test-rolling-update-controller 1ed12160-fdd0-4d6e-b6ba-60c0e6139b0c 4180329 2 2020-08-14 04:49:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4f5800f5-829a-4ff4-9864-ccfd8ef7b840 0x4005122557 0x4005122558}] []  [{e2e.test Update apps/v1 2020-08-14 04:49:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-08-14 04:49:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 102 53 56 48 48 102 53 45 56 50 57 97 45 52 102 102 52 45 57 56 54 52 45 99 99 102 100 56 101 102 55 98 56 52 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x40051225f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 14 04:49:25.878: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-p2gkj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-p2gkj test-rolling-update-deployment-59d5cb45c7- deployment-6110 /api/v1/namespaces/deployment-6110/pods/test-rolling-update-deployment-59d5cb45c7-p2gkj 52d10afa-9583-4caa-b639-458133482d32 4180318 0 2020-08-14 04:49:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 67070484-f74b-4cbf-a975-e7641b28bb5a 0x4005122bc7 0x4005122bc8}] []  [{kube-controller-manager Update v1 2020-08-14 04:49:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 55 48 55 48 52 56 52 45 102 55 52 98 45 52 99 98 102 45 97 57 55 53 45 101 55 54 52 49 98 50 56 98 98 53 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-08-14 04:49:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 48 46 49 48 49 46 49 54 54 46 50 50 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wdsbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wdsbc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wdsbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:49:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:49:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-14 04:49:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.21.99,PodIP:100.101.166.229,StartTime:2020-08-14 04:49:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-14 04:49:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker://sha256:750232f684b5317e3e93c97d2443c81838ff822811a31cbedd7c96b1ec2c47b8,ContainerID:docker://94125cb770f69884689cd82c5d13d2bab3528f60bf4a3e62c1ef735ab2fa9a78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.166.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:49:25.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6110" for this suite.

• [SLOW TEST:9.361 seconds]
[sig-apps] Deployment
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":268,"skipped":4530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:49:25.904: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:49:30.187: INFO: DNS probes using dns-test-9d5ab56d-3344-4e8f-bee5-80f924c3cbfc succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:49:34.304: INFO: File jessie_udp@dns-test-service-3.dns-6077.svc.cluster.local from pod  dns-6077/dns-test-d4036bff-7c90-40c0-ad77-f2f609334df1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 14 04:49:34.304: INFO: Lookups using dns-6077/dns-test-d4036bff-7c90-40c0-ad77-f2f609334df1 failed for: [jessie_udp@dns-test-service-3.dns-6077.svc.cluster.local]

Aug 14 04:49:39.325: INFO: DNS probes using dns-test-d4036bff-7c90-40c0-ad77-f2f609334df1 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6077.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6077.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 04:49:45.602: INFO: DNS probes using dns-test-199489e8-7c2d-43ca-9f13-ebb472f1fc83 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:49:45.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6077" for this suite.

• [SLOW TEST:19.798 seconds]
[sig-network] DNS
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":275,"completed":269,"skipped":4567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:49:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 04:49:52.012: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:49:52.021: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 04:49:54.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:49:54.033: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 04:49:56.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:49:56.032: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 04:49:58.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:49:58.031: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 04:50:00.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:50:00.031: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 04:50:02.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 04:50:02.031: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:50:02.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-456" for this suite.

• [SLOW TEST:16.399 seconds]
[k8s.io] Container Lifecycle Hook
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":275,"completed":270,"skipped":4594,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:50:02.101: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2056
[It] should have a working scale subresource [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-2056
Aug 14 04:50:02.349: INFO: Found 0 stateful pods, waiting for 1
Aug 14 04:50:12.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Aug 14 04:50:12.426: INFO: Deleting all statefulset in ns statefulset-2056
Aug 14 04:50:12.434: INFO: Scaling statefulset ss to 0
Aug 14 04:50:32.538: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 04:50:32.547: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:50:32.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2056" for this suite.

• [SLOW TEST:30.562 seconds]
[sig-apps] StatefulSet
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":275,"completed":271,"skipped":4599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:50:32.664: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-1064
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1064 to expose endpoints map[]
Aug 14 04:50:32.898: INFO: Get endpoints failed (8.643721ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 14 04:50:33.906: INFO: successfully validated that service endpoint-test2 in namespace services-1064 exposes endpoints map[] (1.016557113s elapsed)
STEP: Creating pod pod1 in namespace services-1064
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1064 to expose endpoints map[pod1:[80]]
Aug 14 04:50:36.998: INFO: successfully validated that service endpoint-test2 in namespace services-1064 exposes endpoints map[pod1:[80]] (3.070998872s elapsed)
STEP: Creating pod pod2 in namespace services-1064
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1064 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 14 04:50:40.204: INFO: successfully validated that service endpoint-test2 in namespace services-1064 exposes endpoints map[pod1:[80] pod2:[80]] (3.19076507s elapsed)
STEP: Deleting pod pod1 in namespace services-1064
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1064 to expose endpoints map[pod2:[80]]
Aug 14 04:50:41.262: INFO: successfully validated that service endpoint-test2 in namespace services-1064 exposes endpoints map[pod2:[80]] (1.036920808s elapsed)
STEP: Deleting pod pod2 in namespace services-1064
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1064 to expose endpoints map[]
Aug 14 04:50:42.297: INFO: successfully validated that service endpoint-test2 in namespace services-1064 exposes endpoints map[] (1.01772467s elapsed)
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:50:42.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1064" for this suite.
[AfterEach] [sig-network] Services
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.712 seconds]
[sig-network] Services
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":275,"completed":272,"skipped":4625,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:50:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Aug 14 04:50:42.606: INFO: Waiting up to 5m0s for pod "var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2" in namespace "var-expansion-8544" to be "Succeeded or Failed"
Aug 14 04:50:42.617: INFO: Pod "var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541717ms
Aug 14 04:50:44.627: INFO: Pod "var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020454453s
Aug 14 04:50:46.636: INFO: Pod "var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03025459s
STEP: Saw pod success
Aug 14 04:50:46.636: INFO: Pod "var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2" satisfied condition "Succeeded or Failed"
Aug 14 04:50:46.645: INFO: Trying to get logs from node worker2 pod var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2 container dapi-container: <nil>
STEP: delete the pod
Aug 14 04:50:46.696: INFO: Waiting for pod var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2 to disappear
Aug 14 04:50:46.705: INFO: Pod var-expansion-6cfa3084-fa76-414a-898a-6f15710904a2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:50:46.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8544" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":275,"completed":273,"skipped":4627,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:50:46.731: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9621
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Aug 14 04:50:46.938: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:51:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9621" for this suite.

• [SLOW TEST:34.616 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":275,"completed":274,"skipped":4647,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Aug 14 04:51:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 14 04:51:22.937: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 14 04:51:24.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977482, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977482, loc:(*time.Location)(0x68e32c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977483, loc:(*time.Location)(0x68e32c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63732977482, loc:(*time.Location)(0x68e32c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 14 04:51:27.994: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Aug 14 04:51:28.003: INFO: >>> kubeConfig: /tmp/kubeconfig-133789455
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Aug 14 04:51:34.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1447" for this suite.
STEP: Destroying namespace "webhook-1447-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.589 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /root/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":275,"completed":275,"skipped":4659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 14 04:51:34.938: INFO: Running AfterSuite actions on all nodes
Aug 14 04:51:34.938: INFO: Running AfterSuite actions on node 1
Aug 14 04:51:34.938: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":275,"completed":275,"skipped":4717,"failed":0}

Ran 275 of 4992 Specs in 4860.113 seconds
SUCCESS! -- 275 Passed | 0 Failed | 0 Pending | 4717 Skipped
PASS

Ginkgo ran 1 suite in 1h21m2.978259898s
Test Suite Passed
